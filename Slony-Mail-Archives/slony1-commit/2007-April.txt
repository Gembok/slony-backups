From cbbrowne at lists.slony.info  Mon Apr  2 11:51:58 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:00 2007
Subject: [Slony1-commit] slony1-engine/tests/test2 generate_dml.sh
Message-ID: <20070402185158.5DD3B29043B@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/test2
In directory main.slony.info:/tmp/cvs-serv28705/test2

Modified Files:
      Tag: REL_1_2_STABLE
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/test2/generate_dml.sh,v
retrieving revision 1.4.4.1
retrieving revision 1.4.4.2
diff -C2 -d -r1.4.4.1 -r1.4.4.2
*** generate_dml.sh	27 Oct 2006 14:40:20 -0000	1.4.4.1
--- generate_dml.sh	2 Apr 2007 18:51:56 -0000	1.4.4.2
***************
*** 31,34 ****
--- 31,36 ----
    percent=`expr $j \* 5`
    status "$percent %"
+   GENDATA="$mktmp/generate.data"
+   echo "" > ${GENDATA}
    while : ; do
      txtalen=$(random_number 1 100)
***************
*** 40,45 ****
  
      echo "INSERT INTO table1(data) VALUES ('${txta}');" >> $mktmp/generate.data
!     echo "INSERT INTO table2(table1_id,data) SELECT id, '${txtb}' FROM table1 WHERE data='${txta}';" >> $mktmp/generate.data
!     echo "INSERT INTO table3(table2_id) SELECT id FROM table2 WHERE data ='${txtb}';" >> $mktmp/generate.data
      if [ ${i} -ge ${numrows} ]; then
        break;
--- 42,47 ----
  
      echo "INSERT INTO table1(data) VALUES ('${txta}');" >> $mktmp/generate.data
!     echo "INSERT INTO table2(table1_id,data) SELECT id, '${txtb}' FROM table1 WHERE data='${txta}';" >> ${GENDATA}
!     echo "INSERT INTO table3(table2_id) SELECT id FROM table2 WHERE data ='${txtb}';" >> ${GENDATA}
      if [ ${i} -ge ${numrows} ]; then
        break;

From cbbrowne at lists.slony.info  Mon Apr  2 11:51:58 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:00 2007
Subject: [Slony1-commit] slony1-engine/tests/test1 generate_dml.sh
Message-ID: <20070402185158.597212903FC@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/test1
In directory main.slony.info:/tmp/cvs-serv28705/test1

Modified Files:
      Tag: REL_1_2_STABLE
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/test1/generate_dml.sh,v
retrieving revision 1.8.2.4
retrieving revision 1.8.2.5
diff -C2 -d -r1.8.2.4 -r1.8.2.5
*** generate_dml.sh	8 Feb 2007 22:55:58 -0000	1.8.2.4
--- generate_dml.sh	2 Apr 2007 18:51:56 -0000	1.8.2.5
***************
*** 32,35 ****
--- 32,36 ----
    status "$percent %"
    GENDATA="$mktmp/generate.data"
+   echo "" > ${GENDATA}
    while : ; do
      txtalen=$(random_number 1 100)

From cbbrowne at lists.slony.info  Mon Apr  2 11:51:58 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:01 2007
Subject: [Slony1-commit] slony1-engine/tests/testddl individual_ddl.sh
Message-ID: <20070402185158.62BF229043E@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testddl
In directory main.slony.info:/tmp/cvs-serv28705/testddl

Modified Files:
      Tag: REL_1_2_STABLE
	individual_ddl.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: individual_ddl.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testddl/individual_ddl.sh,v
retrieving revision 1.1.2.1
retrieving revision 1.1.2.2
diff -C2 -d -r1.1.2.1 -r1.1.2.2
*** individual_ddl.sh	30 Mar 2007 22:43:04 -0000	1.1.2.1
--- individual_ddl.sh	2 Apr 2007 18:51:56 -0000	1.1.2.2
***************
*** 5,9 ****
         SET ID = 1,
         FILENAME = '${testname}/ddl_update_part2.sql',
!        EVENT NODE = 1
         EXECUTE ONLY ON = ${node}
      );
--- 5,9 ----
         SET ID = 1,
         FILENAME = '${testname}/ddl_update_part2.sql',
!        EVENT NODE = 1,
         EXECUTE ONLY ON = ${node}
      );

From cbbrowne at lists.slony.info  Mon Apr  2 11:51:58 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:01 2007
Subject: [Slony1-commit] slony1-engine/tests/testinherit generate_dml.sh
Message-ID: <20070402185158.77449290445@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testinherit
In directory main.slony.info:/tmp/cvs-serv28705/testinherit

Modified Files:
      Tag: REL_1_2_STABLE
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testinherit/generate_dml.sh,v
retrieving revision 1.2.2.2
retrieving revision 1.2.2.3
diff -C2 -d -r1.2.2.2 -r1.2.2.3
*** generate_dml.sh	22 Mar 2007 20:46:12 -0000	1.2.2.2
--- generate_dml.sh	2 Apr 2007 18:51:56 -0000	1.2.2.3
***************
*** 32,35 ****
--- 32,36 ----
    status "$percent %"
    GENDATA="$mktmp/generate.data"
+   echo "" > ${GENDATA}
    while : ; do
      txtalen=$(random_number 1 100)

From cbbrowne at lists.slony.info  Mon Apr  2 11:51:58 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:01 2007
Subject: [Slony1-commit] slony1-engine/tests/testlargetuples generate_dml.sh
Message-ID: <20070402185158.7DBEC290448@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testlargetuples
In directory main.slony.info:/tmp/cvs-serv28705/testlargetuples

Modified Files:
      Tag: REL_1_2_STABLE
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testlargetuples/generate_dml.sh,v
retrieving revision 1.1.2.3
retrieving revision 1.1.2.4
diff -C2 -d -r1.1.2.3 -r1.1.2.4
*** generate_dml.sh	22 Mar 2007 20:46:12 -0000	1.1.2.3
--- generate_dml.sh	2 Apr 2007 18:51:56 -0000	1.1.2.4
***************
*** 32,35 ****
--- 32,36 ----
    status "$percent %"
    GENDATA="$mktmp/generate.data"
+   echo "" > ${GENDATA}
    while : ; do
      txtalen=$(random_number 1 100)

From cbbrowne at lists.slony.info  Mon Apr  2 11:51:58 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:01 2007
Subject: [Slony1-commit] slony1-engine/tests/testmultiplemoves
	generate_dml.sh
Message-ID: <20070402185158.8A35F29044D@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testmultiplemoves
In directory main.slony.info:/tmp/cvs-serv28705/testmultiplemoves

Modified Files:
      Tag: REL_1_2_STABLE
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testmultiplemoves/generate_dml.sh,v
retrieving revision 1.3.2.1
retrieving revision 1.3.2.2
diff -C2 -d -r1.3.2.1 -r1.3.2.2
*** generate_dml.sh	27 Oct 2006 14:40:21 -0000	1.3.2.1
--- generate_dml.sh	2 Apr 2007 18:51:56 -0000	1.3.2.2
***************
*** 32,35 ****
--- 32,36 ----
    status "$percent %"
    GENDATA="$mktmp/generate.data"
+   echo "" > ${GENDATA}
    while : ; do
      for set in 1 2 3; do

From cbbrowne at lists.slony.info  Mon Apr  2 11:51:58 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:01 2007
Subject: [Slony1-commit] slony1-engine/tests/testschemanames generate_dml.sh
Message-ID: <20070402185158.9DE67290452@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testschemanames
In directory main.slony.info:/tmp/cvs-serv28705/testschemanames

Modified Files:
      Tag: REL_1_2_STABLE
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testschemanames/generate_dml.sh,v
retrieving revision 1.3.2.1
retrieving revision 1.3.2.2
diff -C2 -d -r1.3.2.1 -r1.3.2.2
*** generate_dml.sh	27 Oct 2006 14:40:21 -0000	1.3.2.1
--- generate_dml.sh	2 Apr 2007 18:51:56 -0000	1.3.2.2
***************
*** 32,35 ****
--- 32,36 ----
    percent=`expr $j \* 5`
    status "$percent %"
+   echo "" > ${GENDATA}
    while : ; do
      txtalen=$(random_number 1 100)

From cbbrowne at lists.slony.info  Mon Apr  2 11:51:58 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:01 2007
Subject: [Slony1-commit] slony1-engine/tests/testseqnames generate_dml.sh
Message-ID: <20070402185158.A2488290458@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testseqnames
In directory main.slony.info:/tmp/cvs-serv28705/testseqnames

Modified Files:
      Tag: REL_1_2_STABLE
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testseqnames/generate_dml.sh,v
retrieving revision 1.3.2.1
retrieving revision 1.3.2.2
diff -C2 -d -r1.3.2.1 -r1.3.2.2
*** generate_dml.sh	27 Oct 2006 14:40:21 -0000	1.3.2.1
--- generate_dml.sh	2 Apr 2007 18:51:56 -0000	1.3.2.2
***************
*** 24,27 ****
--- 24,28 ----
  {
    GENDATA="$mktmp/generate.data"
+   echo "" > ${GENDATA}
    numrows=$(random_number 50 1000)
    i=0;

From cbbrowne at lists.slony.info  Mon Apr  2 11:51:58 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:02 2007
Subject: [Slony1-commit] slony1-engine/tests/testutf8 generate_dml.sh
Message-ID: <20070402185158.C348F29045A@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testutf8
In directory main.slony.info:/tmp/cvs-serv28705/testutf8

Modified Files:
      Tag: REL_1_2_STABLE
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testutf8/generate_dml.sh,v
retrieving revision 1.3.2.1
retrieving revision 1.3.2.2
diff -C2 -d -r1.3.2.1 -r1.3.2.2
*** generate_dml.sh	27 Oct 2006 14:40:21 -0000	1.3.2.1
--- generate_dml.sh	2 Apr 2007 18:51:56 -0000	1.3.2.2
***************
*** 24,27 ****
--- 24,28 ----
  {
    GENDATA="$mktmp/generate.data"
+   echo "" > ${GENDATA}
    numrows=$(random_number 50 1000)
    i=0;

From cbbrowne at lists.slony.info  Mon Apr  2 11:51:58 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:02 2007
Subject: [Slony1-commit] slony1-engine/tests/testtabnames generate_dml.sh
Message-ID: <20070402185158.B10C0290459@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testtabnames
In directory main.slony.info:/tmp/cvs-serv28705/testtabnames

Modified Files:
      Tag: REL_1_2_STABLE
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testtabnames/generate_dml.sh,v
retrieving revision 1.3.2.1
retrieving revision 1.3.2.2
diff -C2 -d -r1.3.2.1 -r1.3.2.2
*** generate_dml.sh	27 Oct 2006 14:40:21 -0000	1.3.2.1
--- generate_dml.sh	2 Apr 2007 18:51:56 -0000	1.3.2.2
***************
*** 24,27 ****
--- 24,28 ----
  {
    GENDATA="$mktmp/generate.data"
+   echo "" > ${GENDATA}
    numrows=$(random_number 50 1000)
    i=0;

From cbbrowne at lists.slony.info  Mon Apr  2 11:52:20 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:23 2007
Subject: [Slony1-commit] slony1-engine/tests/test2 generate_dml.sh
Message-ID: <20070402185220.D28FD29043B@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/test2
In directory main.slony.info:/tmp/cvs-serv28818/test2

Modified Files:
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/test2/generate_dml.sh,v
retrieving revision 1.5
retrieving revision 1.6
diff -C2 -d -r1.5 -r1.6
*** generate_dml.sh	27 Oct 2006 14:45:51 -0000	1.5
--- generate_dml.sh	2 Apr 2007 18:52:18 -0000	1.6
***************
*** 31,34 ****
--- 31,36 ----
    percent=`expr $j \* 5`
    status "$percent %"
+   GENDATA="$mktmp/generate.data"
+   echo "" > ${GENDATA}
    while : ; do
      txtalen=$(random_number 1 100)
***************
*** 40,45 ****
  
      echo "INSERT INTO table1(data) VALUES ('${txta}');" >> $mktmp/generate.data
!     echo "INSERT INTO table2(table1_id,data) SELECT id, '${txtb}' FROM table1 WHERE data='${txta}';" >> $mktmp/generate.data
!     echo "INSERT INTO table3(table2_id) SELECT id FROM table2 WHERE data ='${txtb}';" >> $mktmp/generate.data
      if [ ${i} -ge ${numrows} ]; then
        break;
--- 42,47 ----
  
      echo "INSERT INTO table1(data) VALUES ('${txta}');" >> $mktmp/generate.data
!     echo "INSERT INTO table2(table1_id,data) SELECT id, '${txtb}' FROM table1 WHERE data='${txta}';" >> ${GENDATA}
!     echo "INSERT INTO table3(table2_id) SELECT id FROM table2 WHERE data ='${txtb}';" >> ${GENDATA}
      if [ ${i} -ge ${numrows} ]; then
        break;

From cbbrowne at lists.slony.info  Mon Apr  2 11:52:20 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:23 2007
Subject: [Slony1-commit] slony1-engine/tests/test1 generate_dml.sh
Message-ID: <20070402185220.B896B2903FC@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/test1
In directory main.slony.info:/tmp/cvs-serv28818/test1

Modified Files:
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/test1/generate_dml.sh,v
retrieving revision 1.13
retrieving revision 1.14
diff -C2 -d -r1.13 -r1.14
*** generate_dml.sh	14 Mar 2007 15:53:17 -0000	1.13
--- generate_dml.sh	2 Apr 2007 18:52:18 -0000	1.14
***************
*** 32,35 ****
--- 32,36 ----
    status "$percent %"
    GENDATA="$mktmp/generate.data"
+   echo "" > ${GENDATA}
    while : ; do
      txtalen=$(random_number 1 100)

From cbbrowne at lists.slony.info  Mon Apr  2 11:52:20 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:23 2007
Subject: [Slony1-commit] slony1-engine/tests/testddl individual_ddl.sh
Message-ID: <20070402185220.D81B529043E@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testddl
In directory main.slony.info:/tmp/cvs-serv28818/testddl

Modified Files:
	individual_ddl.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: individual_ddl.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testddl/individual_ddl.sh,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** individual_ddl.sh	30 Mar 2007 22:44:05 -0000	1.2
--- individual_ddl.sh	2 Apr 2007 18:52:18 -0000	1.3
***************
*** 5,9 ****
         SET ID = 1,
         FILENAME = '${testname}/ddl_update_part2.sql',
!        EVENT NODE = 1
         EXECUTE ONLY ON = ${node}
      );
--- 5,9 ----
         SET ID = 1,
         FILENAME = '${testname}/ddl_update_part2.sql',
!        EVENT NODE = 1,
         EXECUTE ONLY ON = ${node}
      );

From cbbrowne at lists.slony.info  Mon Apr  2 11:52:21 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:23 2007
Subject: [Slony1-commit] slony1-engine/tests/testlargetuples generate_dml.sh
Message-ID: <20070402185221.162A3290452@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testlargetuples
In directory main.slony.info:/tmp/cvs-serv28818/testlargetuples

Modified Files:
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testlargetuples/generate_dml.sh,v
retrieving revision 1.3
retrieving revision 1.4
diff -C2 -d -r1.3 -r1.4
*** generate_dml.sh	22 Mar 2007 20:44:56 -0000	1.3
--- generate_dml.sh	2 Apr 2007 18:52:18 -0000	1.4
***************
*** 32,35 ****
--- 32,36 ----
    status "$percent %"
    GENDATA="$mktmp/generate.data"
+   echo "" > ${GENDATA}
    while : ; do
      txtalen=$(random_number 1 100)

From cbbrowne at lists.slony.info  Mon Apr  2 11:52:21 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:23 2007
Subject: [Slony1-commit] slony1-engine/tests/testmultiplemoves
	generate_dml.sh
Message-ID: <20070402185221.21CEE290453@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testmultiplemoves
In directory main.slony.info:/tmp/cvs-serv28818/testmultiplemoves

Modified Files:
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testmultiplemoves/generate_dml.sh,v
retrieving revision 1.4
retrieving revision 1.5
diff -C2 -d -r1.4 -r1.5
*** generate_dml.sh	27 Oct 2006 14:45:51 -0000	1.4
--- generate_dml.sh	2 Apr 2007 18:52:19 -0000	1.5
***************
*** 32,35 ****
--- 32,36 ----
    status "$percent %"
    GENDATA="$mktmp/generate.data"
+   echo "" > ${GENDATA}
    while : ; do
      for set in 1 2 3; do

From cbbrowne at lists.slony.info  Mon Apr  2 11:52:20 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:23 2007
Subject: [Slony1-commit] slony1-engine/tests/testinherit generate_dml.sh
Message-ID: <20070402185221.254B529045D@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testinherit
In directory main.slony.info:/tmp/cvs-serv28818/testinherit

Modified Files:
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testinherit/generate_dml.sh,v
retrieving revision 1.4
retrieving revision 1.5
diff -C2 -d -r1.4 -r1.5
*** generate_dml.sh	22 Mar 2007 20:44:56 -0000	1.4
--- generate_dml.sh	2 Apr 2007 18:52:18 -0000	1.5
***************
*** 32,35 ****
--- 32,36 ----
    status "$percent %"
    GENDATA="$mktmp/generate.data"
+   echo "" > ${GENDATA}
    while : ; do
      txtalen=$(random_number 1 100)

From cbbrowne at lists.slony.info  Mon Apr  2 11:52:21 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:24 2007
Subject: [Slony1-commit] slony1-engine/tests/testschemanames generate_dml.sh
Message-ID: <20070402185221.272DC29045F@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testschemanames
In directory main.slony.info:/tmp/cvs-serv28818/testschemanames

Modified Files:
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testschemanames/generate_dml.sh,v
retrieving revision 1.4
retrieving revision 1.5
diff -C2 -d -r1.4 -r1.5
*** generate_dml.sh	27 Oct 2006 14:45:51 -0000	1.4
--- generate_dml.sh	2 Apr 2007 18:52:19 -0000	1.5
***************
*** 32,35 ****
--- 32,36 ----
    percent=`expr $j \* 5`
    status "$percent %"
+   echo "" > ${GENDATA}
    while : ; do
      txtalen=$(random_number 1 100)

From cbbrowne at lists.slony.info  Mon Apr  2 11:52:21 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:24 2007
Subject: [Slony1-commit] slony1-engine/tests/testseqnames generate_dml.sh
Message-ID: <20070402185221.31DF029046E@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testseqnames
In directory main.slony.info:/tmp/cvs-serv28818/testseqnames

Modified Files:
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testseqnames/generate_dml.sh,v
retrieving revision 1.4
retrieving revision 1.5
diff -C2 -d -r1.4 -r1.5
*** generate_dml.sh	27 Oct 2006 14:45:52 -0000	1.4
--- generate_dml.sh	2 Apr 2007 18:52:19 -0000	1.5
***************
*** 24,27 ****
--- 24,28 ----
  {
    GENDATA="$mktmp/generate.data"
+   echo "" > ${GENDATA}
    numrows=$(random_number 50 1000)
    i=0;

From cbbrowne at lists.slony.info  Mon Apr  2 11:52:21 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:24 2007
Subject: [Slony1-commit] slony1-engine/tests/testtabnames generate_dml.sh
Message-ID: <20070402185221.40916290472@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testtabnames
In directory main.slony.info:/tmp/cvs-serv28818/testtabnames

Modified Files:
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testtabnames/generate_dml.sh,v
retrieving revision 1.4
retrieving revision 1.5
diff -C2 -d -r1.4 -r1.5
*** generate_dml.sh	27 Oct 2006 14:45:52 -0000	1.4
--- generate_dml.sh	2 Apr 2007 18:52:19 -0000	1.5
***************
*** 24,27 ****
--- 24,28 ----
  {
    GENDATA="$mktmp/generate.data"
+   echo "" > ${GENDATA}
    numrows=$(random_number 50 1000)
    i=0;

From cbbrowne at lists.slony.info  Mon Apr  2 11:52:21 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Apr  2 11:52:24 2007
Subject: [Slony1-commit] slony1-engine/tests/testutf8 generate_dml.sh
Message-ID: <20070402185221.59031290459@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testutf8
In directory main.slony.info:/tmp/cvs-serv28818/testutf8

Modified Files:
	generate_dml.sh 
Log Message:
Augment DDL test to generate some statements that are only run on
individual nodes.

Also, found a problem where the generated SQL (that gets injected
to test that replication is working) would not get emptied out when
new data was being generated for repeated iterations.  Rectified that.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testutf8/generate_dml.sh,v
retrieving revision 1.4
retrieving revision 1.5
diff -C2 -d -r1.4 -r1.5
*** generate_dml.sh	27 Oct 2006 14:45:52 -0000	1.4
--- generate_dml.sh	2 Apr 2007 18:52:19 -0000	1.5
***************
*** 24,27 ****
--- 24,28 ----
  {
    GENDATA="$mktmp/generate.data"
+   echo "" > ${GENDATA}
    numrows=$(random_number 50 1000)
    i=0;

From xfade at lists.slony.info  Tue Apr  3 08:10:47 2007
From: xfade at lists.slony.info (Niels Breet)
Date: Tue Apr  3 08:10:49 2007
Subject: [Slony1-commit] slony1-www index.php
Message-ID: <20070403151047.86C8229004F@main.slony.info>

Update of /home/cvsd/slony1/slony1-www
In directory main.slony.info:/tmp/cvs-serv1964

Modified Files:
	index.php 
Log Message:
Make sure we only show existing pages. Return a 404 when no page is found. Problem observed by Devrim.

Index: index.php
===================================================================
RCS file: /home/cvsd/slony1/slony1-www/index.php,v
retrieving revision 1.10
retrieving revision 1.11
diff -C2 -d -r1.10 -r1.11
*** index.php	27 Mar 2007 12:22:45 -0000	1.10
--- index.php	3 Apr 2007 15:10:45 -0000	1.11
***************
*** 11,20 ****
  	break;
    case 'docs':
          $file = str_replace(array("/",".."),"",$_GET["file"]);
          if ($file == "") $file="index.html";
!         echo layout_header();
!         $text=file_get_contents('./adminguide/current/doc/adminguide/'.$file);
! 	$title=between("TITLE\n>","</TITLE",$text);
!         echo layout_defaultblock($title,$link,$text);
          break;
    case 'download':
--- 11,27 ----
  	break;
    case 'docs':
+ 	$text = '';
          $file = str_replace(array("/",".."),"",$_GET["file"]);
          if ($file == "") $file="index.html";
!         $text=@file_get_contents('./adminguide/current/doc/adminguide/'.$file);
! 	if ($text == ''){
! 		header("HTTP/1.0 404 Not Found");
! 		echo layout_header();
! 		echo layout_defaultblock('404, NOT FOUND','',file_get_contents('./content/404.txt'));
! 	} else {
!         	echo layout_header();
! 		$title=between("TITLE\n>","</TITLE",$text);
! 	        echo layout_defaultblock($title,$link,$text);
! 	}
          break;
    case 'download':

From cbbrowne at lists.slony.info  Tue Apr  3 14:55:05 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Apr  3 14:55:07 2007
Subject: [Slony1-commit] slony1-engine/doc/adminguide slonik_ref.sgml
Message-ID: <20070403215505.DCE7929004F@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv3668/doc/adminguide

Modified Files:
      Tag: REL_1_2_STABLE
	slonik_ref.sgml 
Log Message:
Update to DDL handling - when a script is specified with "EXECUTE ONLY ON"
a specific node, it should be invoked, by slonik, only on that node.


Index: slonik_ref.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/slonik_ref.sgml,v
retrieving revision 1.61.2.5
retrieving revision 1.61.2.6
diff -C2 -d -r1.61.2.5 -r1.61.2.6
*** slonik_ref.sgml	16 Mar 2007 19:01:26 -0000	1.61.2.5
--- slonik_ref.sgml	3 Apr 2007 21:55:03 -0000	1.61.2.6
***************
*** 2549,2557 ****
       </varlistentry>
       <varlistentry><term><literal> EXECUTE ONLY ON = ival
!        </literal></term> <listitem><para> (Optional) The ID of the only
! 	node to actually execute the script.  This option causes the
! 	script to be propagated by all nodes but executed only by one.
! 	The default is to execute the script on all nodes that are
! 	subscribed to the set.</para></listitem>
        
       </varlistentry>
--- 2549,2559 ----
       </varlistentry>
       <varlistentry><term><literal> EXECUTE ONLY ON = ival
! 
!        </literal></term> <listitem><para> (Optional) The ID of the
!        only node to actually execute the script.  This option causes
!        the script to be executed, by <xref linkend="slonik">,
!        <emphasis>only</emphasis> on the one node specified.  The
!        default is to execute the script on all nodes that are
!        subscribed to the set.</para></listitem>
        
       </varlistentry>

From cbbrowne at lists.slony.info  Tue Apr  3 14:55:05 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Apr  3 14:55:08 2007
Subject: [Slony1-commit] slony1-engine/src/backend slony1_funcs.sql
Message-ID: <20070403215506.067AE2903AE@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/backend
In directory main.slony.info:/tmp/cvs-serv3668/src/backend

Modified Files:
      Tag: REL_1_2_STABLE
	slony1_funcs.sql 
Log Message:
Update to DDL handling - when a script is specified with "EXECUTE ONLY ON"
a specific node, it should be invoked, by slonik, only on that node.


Index: slony1_funcs.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.sql,v
retrieving revision 1.98.2.13
retrieving revision 1.98.2.14
diff -C2 -d -r1.98.2.13 -r1.98.2.14
*** slony1_funcs.sql	22 Mar 2007 20:41:27 -0000	1.98.2.13
--- slony1_funcs.sql	3 Apr 2007 21:55:03 -0000	1.98.2.14
***************
*** 3684,3689 ****
--- 3684,3692 ----
  	lock table @NAMESPACE@.sl_config_lock;
  
+ 	
  	-- ----
  	-- Check that the set exists and originates here
+ 	-- unless only_on_node was specified (then it can be applied to
+ 	-- that node because that is what the user wanted)
  	-- ----
  	select set_origin into v_set_origin
***************
*** 3694,3707 ****
  		raise exception ''Slony-I: set % not found'', p_set_id;
  	end if;
! 	if v_set_origin <> @NAMESPACE@.getLocalNodeId(''_@CLUSTERNAME@'') then
! 		raise exception ''Slony-I: set % does not originate on local node'',
  				p_set_id;
  	end if;
- 
- 	-- ----
- 	-- Create a SYNC event, run the script and generate the DDL_SCRIPT event
- 	-- ----
- 	perform @NAMESPACE@.createEvent(''_@CLUSTERNAME@'', ''SYNC'', NULL);
- 	perform @NAMESPACE@.alterTableRestore(tab_id) from @NAMESPACE@.sl_table where tab_set in (select set_id from @NAMESPACE@.sl_set where set_origin = @NAMESPACE@.getLocalNodeId(''_@CLUSTERNAME@''));
  	return 1;
  end;
--- 3697,3717 ----
  		raise exception ''Slony-I: set % not found'', p_set_id;
  	end if;
! 
! 	if p_only_on_node = -1 then
! 		if v_set_origin <> @NAMESPACE@.getLocalNodeId(''_@CLUSTERNAME@'') then
! 			raise exception ''Slony-I: set % does not originate on local node'',
  				p_set_id;
+ 		end if;
+ 		-- ----
+ 		-- Create a SYNC event, run the script and generate the DDL_SCRIPT event
+ 		-- ----
+ 		perform @NAMESPACE@.createEvent(''_@CLUSTERNAME@'', ''SYNC'', NULL);
+ 		perform @NAMESPACE@.alterTableRestore(tab_id) from @NAMESPACE@.sl_table where tab_set in (select set_id from @NAMESPACE@.sl_set where set_origin = @NAMESPACE@.getLocalNodeId(''_@CLUSTERNAME@''));
+ 	else
+ 		-- ----
+ 		-- If doing "only on one node" - restore ALL tables irrespective of set
+ 		-- ----
+ 		perform @NAMESPACE@.alterTableRestore(tab_id) from @NAMESPACE@.sl_table;
  	end if;
  	return 1;
  end;
***************
*** 3729,3734 ****
--- 3739,3747 ----
  	perform @NAMESPACE@.updateRelname(p_set_id, p_only_on_node);
  	perform @NAMESPACE@.alterTableForReplication(tab_id) from @NAMESPACE@.sl_table where tab_set in (select set_id from @NAMESPACE@.sl_set where set_origin = @NAMESPACE@.getLocalNodeId(''_@CLUSTERNAME@''));
+ 	if p_only_on_node = -1 then
  	return  @NAMESPACE@.createEvent(''_@CLUSTERNAME@'', ''DDL_SCRIPT'', 
  			p_set_id, p_script, p_only_on_node);
+ 	end if;
+ 	return NULL;
  end;
  ' language plpgsql;
***************
*** 5905,5907 ****
  
  In PG versions > 7.3, this looks like (field1,field2,...fieldn)';
- 
--- 5918,5919 ----

From cbbrowne at lists.slony.info  Tue Apr  3 14:55:06 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Apr  3 14:55:08 2007
Subject: [Slony1-commit] slony1-engine/src/slonik slonik.c
Message-ID: <20070403215506.34F9F2903F6@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slonik
In directory main.slony.info:/tmp/cvs-serv3668/src/slonik

Modified Files:
      Tag: REL_1_2_STABLE
	slonik.c 
Log Message:
Update to DDL handling - when a script is specified with "EXECUTE ONLY ON"
a specific node, it should be invoked, by slonik, only on that node.


Index: slonik.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/slonik.c,v
retrieving revision 1.67.2.6
retrieving revision 1.67.2.7
diff -C2 -d -r1.67.2.6 -r1.67.2.7
*** slonik.c	15 Mar 2007 18:52:02 -0000	1.67.2.6
--- slonik.c	3 Apr 2007 21:55:04 -0000	1.67.2.7
***************
*** 3850,3853 ****
--- 3850,3854 ----
  	ExecStatusType rstat;
  
+ 
  #define PARMCOUNT 1  
  
***************
*** 3856,3860 ****
          int paramfmts[PARMCOUNT];
  
! 	adminfo1 = get_active_adminfo((SlonikStmt *) stmt, stmt->ev_origin);
  	if (adminfo1 == NULL)
  		return -1;
--- 3857,3865 ----
          int paramfmts[PARMCOUNT];
  
! 	if(stmt->only_on_node > -1) {
! 		adminfo1 = get_active_adminfo((SlonikStmt*) stmt,stmt->only_on_node);
! 	} else {
! 		adminfo1 = get_active_adminfo((SlonikStmt *) stmt, stmt->ev_origin);
! 	}
  	if (adminfo1 == NULL)
  		return -1;
***************
*** 3883,3886 ****
--- 3888,3892 ----
  		     "select \"_%s\".ddlScript_prepare(%d, %d); ",
  		     stmt->hdr.script->clustername,
+ 
  		     stmt->ddl_setid, /* dstring_data(&script),  */ 
  		     stmt->only_on_node);
***************
*** 3936,3966 ****
  	}
  	
! 	printf("Submit DDL Event to subscribers...\n");
! 
! 	slon_mkquery(&query, "select \"_%s\".ddlScript_complete(%d, $1::text, %d); ", 
! 		     stmt->hdr.script->clustername,
! 		     stmt->ddl_setid,  
! 		     stmt->only_on_node);
! 
! 	paramlens[PARMCOUNT-1] = 0;
! 	paramfmts[PARMCOUNT-1] = 0;
! 	params[PARMCOUNT-1] = dstring_data(&script);
! 
! 	res = PQexecParams(adminfo1->dbconn, dstring_data(&query), PARMCOUNT,
! 			   NULL, params, paramlens, paramfmts, 0);
! 	
! 	if (PQresultStatus(res) != PGRES_COMMAND_OK && 
! 	    PQresultStatus(res) != PGRES_TUPLES_OK &&
! 	    PQresultStatus(res) != PGRES_EMPTY_QUERY)
! 	{
! 		rstat = PQresultStatus(res);
! 		printf("Event submission for DDL failed - %s\n", PQresStatus(rstat));
! 		dstring_free(&query);
! 		return -1;
! 	} else {
! 		rstat = PQresultStatus(res);
! 		printf ("DDL on origin - %s\n", PQresStatus(rstat));
! 	}
! 	
  	dstring_free(&script);
  	dstring_free(&query);
--- 3942,3971 ----
  	}
  	
! 		printf("Complete DDL Event...\n");
! 		
! 		slon_mkquery(&query, "select \"_%s\".ddlScript_complete(%d, $1::text, %d); ", 
! 					 stmt->hdr.script->clustername,
! 					 stmt->ddl_setid,  
! 					 stmt->only_on_node);
! 		
! 		paramlens[PARMCOUNT-1] = 0;
! 		paramfmts[PARMCOUNT-1] = 0;
! 		params[PARMCOUNT-1] = dstring_data(&script);
! 		
! 		res = PQexecParams(adminfo1->dbconn, dstring_data(&query), PARMCOUNT,
! 						   NULL, params, paramlens, paramfmts, 0);
! 		
! 		if (PQresultStatus(res) != PGRES_COMMAND_OK && 
! 			PQresultStatus(res) != PGRES_TUPLES_OK &&
! 			PQresultStatus(res) != PGRES_EMPTY_QUERY)
! 			{
! 				rstat = PQresultStatus(res);
! 				printf("Event submission for DDL failed - %s\n", PQresStatus(rstat));
! 				dstring_free(&query);
! 				return -1;
! 			} else {
! 				rstat = PQresultStatus(res);
! 				printf ("DDL submission to initial node - %s\n", PQresStatus(rstat));
! 			}
  	dstring_free(&script);
  	dstring_free(&query);

From cbbrowne at lists.slony.info  Tue Apr  3 14:55:06 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Apr  3 14:55:10 2007
Subject: [Slony1-commit] slony1-engine/src/slon remote_worker.c
Message-ID: <20070403215506.33D3D2903B2@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slon
In directory main.slony.info:/tmp/cvs-serv3668/src/slon

Modified Files:
      Tag: REL_1_2_STABLE
	remote_worker.c 
Log Message:
Update to DDL handling - when a script is specified with "EXECUTE ONLY ON"
a specific node, it should be invoked, by slonik, only on that node.


Index: remote_worker.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/remote_worker.c,v
retrieving revision 1.124.2.12
retrieving revision 1.124.2.13
diff -C2 -d -r1.124.2.12 -r1.124.2.13
*** remote_worker.c	6 Mar 2007 18:47:45 -0000	1.124.2.12
--- remote_worker.c	3 Apr 2007 21:55:03 -0000	1.124.2.13
***************
*** 269,272 ****
--- 269,275 ----
  static void compress_actionseq(const char *ssy_actionseq, SlonDString * action_subquery);
  
+ static int process_ddl_script(SlonWorkMsg_event * event,SlonNode * node,
+ 							  PGconn * local_dbconn, char * seqbuf );
+ static int check_set_subscriber(int set_id, int node_id,PGconn * local_dbconn);
  
  /* ----------
***************
*** 1340,1438 ****
  			else if (strcmp(event->ev_type, "DDL_SCRIPT") == 0)
  			{
! 				int			ddl_setid = (int)strtol(event->ev_data1, NULL, 10);
! 				char	   *ddl_script = event->ev_data2;
! 				int			ddl_only_on_node = (int)strtol(event->ev_data3, NULL, 10);
! 				int num_statements = -1, stmtno;
! 
! 				PGresult *res;
! 				ExecStatusType rstat;
! 
! 
! 				slon_appendquery(&query1,
! 						 "select %s.ddlScript_prepare_int(%d, %d); ",
! 						 rtcfg_namespace,
! 						 ddl_setid, ddl_only_on_node);
! 
! 				if (query_execute(node, local_dbconn, &query1) < 0) {
! 						slon_log(SLON_ERROR, "remoteWorkerThread_%d: DDL preparation failed - set %d - only on node %\n",
! 							 node->no_id, ddl_setid, ddl_only_on_node);
! 						slon_retry();
! 				}
! 
! 				num_statements = scan_for_statements (ddl_script);
! 				slon_log(SLON_CONFIG, "remoteWorkerThread_%d: DDL request with %d statements\n",
! 					 node->no_id, num_statements);
! 				if ((num_statements < 0) || (num_statements >= MAXSTATEMENTS)) {
! 					slon_log(SLON_ERROR, "remoteWorkerThread_%d: DDL had invalid number of statements - %d\n", 
! 						 node->no_id, num_statements);
! 					slon_retry();
! 				}
! 				
! 				for (stmtno=0; stmtno < num_statements;  stmtno++) {
! 					int startpos, endpos;
! 					char *dest;
! 					if (stmtno == 0)
! 						startpos = 0;
! 					else
! 						startpos = STMTS[stmtno-1];
! 
! 					endpos = STMTS[stmtno];
! 					dest = (char *) malloc (endpos - startpos + 1);
! 					if (dest == 0) {
! 						slon_log(SLON_ERROR, "remoteWorkerThread_%d: malloc() failure in DDL_SCRIPT - could not allocate %d bytes of memory\n", 
! 							 node->no_id, endpos - startpos + 1);
! 						slon_retry();
! 					}
! 					strncpy(dest, ddl_script + startpos, endpos-startpos);
! 					dest[STMTS[stmtno]-startpos] = 0;
! 					slon_mkquery(&query1, dest);
! 					slon_log(SLON_CONFIG, "remoteWorkerThread_%d: DDL Statement %d: [%s]\n", 
! 						 node->no_id, stmtno, dest);						 
! 					free(dest);
! 
! 					res = PQexec(local_dbconn, dstring_data(&query1));
! 
! 					if (PQresultStatus(res) != PGRES_COMMAND_OK && 
! 					    PQresultStatus(res) != PGRES_TUPLES_OK &&
! 					    PQresultStatus(res) != PGRES_EMPTY_QUERY)
! 					{
! 						rstat = PQresultStatus(res);
! 						slon_log(SLON_ERROR, "DDL Statement failed - %s\n", PQresStatus(rstat));
! 						dstring_free(&query1);
! 						slon_retry();
! 					}
! 					rstat = PQresultStatus(res);
! 					slon_log (SLON_CONFIG, "DDL success - %s\n", PQresStatus(rstat));
! 				}
! 	
! 				slon_mkquery(&query1, "select %s.ddlScript_complete_int(%d, %d); ", 
! 					     rtcfg_namespace,
! 					     ddl_setid,
! 					     ddl_only_on_node);
! 
! 				/* DDL_SCRIPT needs to be turned into a log shipping script */
! 				/* Note that the issue about parsing that mandates breaking 
! 				   up compound statements into
! 				   individually-processed statements does not apply to log
! 				   shipping as psql parses and processes each statement
! 				   individually */
! 
! 				if (archive_dir)
! 				{
! 					if ((ddl_only_on_node < 1) || (ddl_only_on_node == rtcfg_nodeid))
! 					{
! 
! 						if (archive_open(node, seqbuf) < 0)
! 							slon_retry();
! 						if (archive_tracking(node, rtcfg_namespace, 
! 								ddl_setid, seqbuf, seqbuf, 
! 								event->ev_timestamp_c) < 0)
! 							slon_retry();
! 						if (archive_append_str(node, ddl_script) < 0)
! 							slon_retry();
! 						if (archive_close(node) < 0)
! 							slon_retry();
! 					}
! 				}
  			}
  			else if (strcmp(event->ev_type, "RESET_CONFIG") == 0)
--- 1343,1347 ----
  			else if (strcmp(event->ev_type, "DDL_SCRIPT") == 0)
  			{
! 				process_ddl_script(event,node,local_dbconn,seqbuf);
  			}
  			else if (strcmp(event->ev_type, "RESET_CONFIG") == 0)
***************
*** 6098,6099 ****
--- 6007,6181 ----
  	slon_log(SLON_DEBUG4, " compressed actionseq subquery... %s\n", dstring_data(action_subquery));
  }
+ 
+ 
+ /**
+  *
+  * Process a ddl_script command.
+  */
+ static int process_ddl_script(SlonWorkMsg_event * event,SlonNode * node,
+ 							  PGconn * local_dbconn,
+ 							  char * seqbuf) 
+ {
+ 	int			ddl_setid = (int)strtol(event->ev_data1, NULL, 10);
+ 	char	   *ddl_script = event->ev_data2;
+ 	int			ddl_only_on_node = (int)strtol(event->ev_data3, NULL, 10);
+ 	int num_statements = -1, stmtno;
+ 	int node_in_set;
+ 	int localNodeId;
+ 	PGresult *res;
+ 	ExecStatusType rstat;
+ 	SlonDString query1;
+ 
+ 	
+ 
+ 	dstring_init(&query1);
+ 	/**
+ 	 * Check to make sure this node is part of the set
+ 	 */
+ 	slon_log(SLON_INFO, "Checking local node id\n");
+ 	localNodeId = db_getLocalNodeId(local_dbconn);
+ 	slon_log(SLON_INFO,"Found local node id\n");
+ 	node_in_set = check_set_subscriber(ddl_setid,localNodeId,local_dbconn);
+ 	
+ 	if(!node_in_set) {
+ 		/**
+ 		 *
+ 		 * Node is not part of the set.  
+ 		 * Do not forward teh DDL to the node,
+ 		 * nor should it be included in the log for log-shipping.
+ 		 */
+ 		slon_log(SLON_INFO,"Not forwarding DDL to node %d for set %d\n",
+ 				 node->no_id,ddl_setid);
+ 		
+ 	}
+ 	else 
+ 	{
+ 		slon_appendquery(&query1,
+ 						 "select %s.ddlScript_prepare_int(%d, %d); ",
+ 						 rtcfg_namespace,
+ 						 ddl_setid, ddl_only_on_node);
+ 		
+ 		if (query_execute(node, local_dbconn, &query1) < 0) {
+ 			slon_log(SLON_ERROR, "remoteWorkerThread_%d: DDL preparation failed - set %d - only on node %\n",
+ 					 node->no_id, ddl_setid, ddl_only_on_node);			
+ 			slon_retry();
+ 		}
+ 		
+ 		num_statements = scan_for_statements (ddl_script);
+ 		slon_log(SLON_CONFIG, "remoteWorkerThread_%d: DDL request with %d statements\n",
+ 				 node->no_id, num_statements);
+ 		if ((num_statements < 0) || (num_statements >= MAXSTATEMENTS)) {
+ 			slon_log(SLON_ERROR, "remoteWorkerThread_%d: DDL had invalid number of statements - %d\n", 
+ 					 node->no_id, num_statements);
+ 			slon_retry();
+ 		}
+ 		
+ 		for (stmtno=0; stmtno < num_statements;  stmtno++) {
+ 			int startpos, endpos;
+ 			char *dest;
+ 			if (stmtno == 0)
+ 				startpos = 0;
+ 			else
+ 				startpos = STMTS[stmtno-1];
+ 			
+ 			endpos = STMTS[stmtno];
+ 			dest = (char *) malloc (endpos - startpos + 1);
+ 			if (dest == 0) {
+ 				slon_log(SLON_ERROR, "remoteWorkerThread_%d: malloc() failure in DDL_SCRIPT - could not allocate %d bytes of memory\n", 
+ 						 node->no_id, endpos - startpos + 1);
+ 				slon_retry();
+ 			}
+ 			strncpy(dest, ddl_script + startpos, endpos-startpos);
+ 			dest[STMTS[stmtno]-startpos] = 0;
+ 			slon_mkquery(&query1, dest);
+ 			slon_log(SLON_CONFIG, "remoteWorkerThread_%d: DDL Statement %d: [%s]\n", 
+ 					 node->no_id, stmtno, dest);						 
+ 			free(dest);
+ 			
+ 			res = PQexec(local_dbconn, dstring_data(&query1));
+ 			
+ 			if (PQresultStatus(res) != PGRES_COMMAND_OK && 
+ 				PQresultStatus(res) != PGRES_TUPLES_OK &&
+ 				PQresultStatus(res) != PGRES_EMPTY_QUERY)
+ 				{
+ 					rstat = PQresultStatus(res);
+ 					slon_log(SLON_ERROR, "DDL Statement failed - %s\n", PQresStatus(rstat));
+ 					dstring_free(&query1);
+ 					slon_retry();
+ 				}
+ 			rstat = PQresultStatus(res);
+ 			slon_log (SLON_CONFIG, "DDL success - %s\n", PQresStatus(rstat));
+ 		}
+ 		
+ 		slon_mkquery(&query1, "select %s.ddlScript_complete_int(%d, %d); ", 
+ 					 rtcfg_namespace,
+ 					 ddl_setid,
+ 					 ddl_only_on_node);
+ 
+ 		if (query_execute(node, local_dbconn, &query1) < 0) {
+ 			slon_log(SLON_ERROR, "remoteWorkerThread_%d: DDL completion failed - set %d - only on node %\n",
+ 					 node->no_id, ddl_setid, ddl_only_on_node);			
+ 			slon_retry();
+ 		}
+ 		
+ 		/* DDL_SCRIPT needs to be turned into a log shipping script */
+ 		/* Note that the issue about parsing that mandates breaking 
+ 		   up compound statements into
+ 		   individually-processed statements does not apply to log
+ 		   shipping as psql parses and processes each statement
+ 		   individually */
+ 		
+ 		if (archive_dir)
+ 			{
+ 				if ((ddl_only_on_node < 1) || (ddl_only_on_node == rtcfg_nodeid))
+ 					{
+ 						
+ 						if (archive_open(node, seqbuf) < 0)
+ 							slon_retry();
+ 						if (archive_tracking(node, rtcfg_namespace, 
+ 											 ddl_setid, seqbuf, seqbuf, 
+ 											 event->ev_timestamp_c) < 0)
+ 							slon_retry();
+ 						if (archive_append_str(node, ddl_script) < 0)
+ 							slon_retry();
+ 						if (archive_close(node) < 0)
+ 							slon_retry();
+ 					}
+ 			}
+ 	}/*else node a subscriber */
+ 	
+ 	dstring_free(&query1);
+ 
+ }
+ 
+ /**
+  * Checks to see if the node specified is a member of the set.
+  *
+  */
+ static int check_set_subscriber(int set_id, int node_id,PGconn * local_dbconn) 
+ {
+   
+   
+   SlonDString query1;
+   PGresult* res;
+   dstring_init(&query1);
+ 
+   slon_appendquery(&query1,"select 1 from %s.sl_subscribe WHERE sub_set=%d AND sub_receiver=%d for update"
+ 	       ,rtcfg_namespace,set_id,node_id);
+   res = PQexec(local_dbconn,dstring_data(&query1));
+   if(PQresultStatus(res)!=PGRES_TUPLES_OK) {
+     slon_log(SLON_ERROR,"remoteWorkerThread_%d: DDL preperation can not check set membership"
+ 	     ,node_id);
+ 	dstring_free(&query1);
+     slon_retry();
+   }
+   dstring_free(&query1);
+   if(PQntuples(res)==0) {
+     PQclear(res);
+     return 0;
+   }
+   PQclear(res);
+   return 1;
+ 
+ 
+ }

From cbbrowne at lists.slony.info  Tue Apr  3 14:55:05 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Apr  3 14:55:10 2007
Subject: [Slony1-commit] slony1-engine RELEASE-1.2.10 configure
Message-ID: <20070403215507.1A25229044D@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine
In directory main.slony.info:/tmp/cvs-serv3668

Modified Files:
      Tag: REL_1_2_STABLE
	configure 
Added Files:
      Tag: REL_1_2_STABLE
	RELEASE-1.2.10 
Log Message:
Update to DDL handling - when a script is specified with "EXECUTE ONLY ON"
a specific node, it should be invoked, by slonik, only on that node.


--- NEW FILE: RELEASE-1.2.10 ---
$Id: RELEASE-1.2.10,v 1.1.2.1 2007-04-03 21:55:03 cbbrowne Exp $

- Fixed problem with EXECUTE SCRIPT (EXECUTE ONLY ON = <node>)

  - The script was being executed on too many nodes...

Index: configure
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/configure,v
retrieving revision 1.70.2.3
retrieving revision 1.70.2.4
diff -C2 -d -r1.70.2.3 -r1.70.2.4
*** configure	12 Mar 2007 22:17:19 -0000	1.70.2.3
--- configure	3 Apr 2007 21:55:03 -0000	1.70.2.4
***************
*** 687,690 ****
--- 687,691 ----
  EGREP
  HAVE_POSIX_SIGNALS
+ enable_engine
  NEED_PG_DLLINIT
  NLSLIB
***************
*** 1307,1310 ****
--- 1308,1312 ----
    --enable-debug          build with debugging symbols (-g)
    --disable-rpath         do not embed shared library search path in executables
+   --disable-engine          Don't build slony1-engine source. (Used when building documentation only)
  
  Optional Packages:
***************
*** 6596,6599 ****
--- 6598,6630 ----
  
  
+ 
+ 
+ 
+ # Check whether --enable-engine was given.
+ if test "${enable_engine+set}" = set; then
+   enableval=$enable_engine;
+   case $enableval in
+     yes)
+       :
+       ;;
+     no)
+       :
+       ;;
+     *)
+       { { echo "$as_me:$LINENO: error: no argument expected for --enable-engine option" >&5
+ echo "$as_me: error: no argument expected for --enable-engine option" >&2;}
+    { (exit 1); exit 1; }; }
+       ;;
+   esac
+ 
+ else
+   enable_engine=yes
+ 
+ fi
+ 
+ 
+ 
+ 
+ 
  #Our current path
  SLONYPATH=`pwd`
***************
*** 6602,6605 ****
--- 6633,6643 ----
  # PostgreSQL checks
  # ----
+ { echo "$as_me:$LINENO: checking if you have requested slony1-engine building" >&5
+ echo $ECHO_N "checking if you have requested slony1-engine building... $ECHO_C" >&6; }
+ { echo "$as_me:$LINENO: result: $enable_engine" >&5
+ echo "${ECHO_T}$enable_engine" >&6; }
+ 
+ if test "$enable_engine" = "yes"; then
+ 
  
  
***************
*** 10055,10058 ****
--- 10093,10098 ----
  
  
+ fi
+ 
  # ----
  # Documentation checks
***************
*** 11384,11387 ****
--- 11424,11428 ----
  EGREP!$EGREP$ac_delim
  HAVE_POSIX_SIGNALS!$HAVE_POSIX_SIGNALS$ac_delim
+ enable_engine!$enable_engine$ac_delim
  NEED_PG_DLLINIT!$NEED_PG_DLLINIT$ac_delim
  NLSLIB!$NLSLIB$ac_delim
***************
*** 11407,11411 ****
  CONVERT!$CONVERT$ac_delim
  PGAUTODOC!$PGAUTODOC$ac_delim
- NSGMLS!$NSGMLS$ac_delim
  _ACEOF
  
--- 11448,11451 ----
***************
*** 11449,11452 ****
--- 11489,11493 ----
  for ac_last_try in false false false false false :; do
    cat >conf$$subs.sed <<_ACEOF
+ NSGMLS!$NSGMLS$ac_delim
  SGMLSPL!$SGMLSPL$ac_delim
  d2mdir!$d2mdir$ac_delim
***************
*** 11460,11464 ****
  _ACEOF
  
!   if test `sed -n "s/.*$ac_delim\$/X/p" conf$$subs.sed | grep -c X` = 9; then
      break
    elif $ac_last_try; then
--- 11501,11505 ----
  _ACEOF
  
!   if test `sed -n "s/.*$ac_delim\$/X/p" conf$$subs.sed | grep -c X` = 10; then
      break
    elif $ac_last_try; then

From cbbrowne at lists.slony.info  Tue Apr  3 14:55:06 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Apr  3 14:55:10 2007
Subject: [Slony1-commit] slony1-engine/tests/testddl ddl_update_part2.sql
	generate_dml.sh individual_ddl.sh init_subscribe_set.ik
Message-ID: <20070403215507.82FE22903B2@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testddl
In directory main.slony.info:/tmp/cvs-serv3668/tests/testddl

Modified Files:
      Tag: REL_1_2_STABLE
	ddl_update_part2.sql generate_dml.sh individual_ddl.sh 
	init_subscribe_set.ik 
Log Message:
Update to DDL handling - when a script is specified with "EXECUTE ONLY ON"
a specific node, it should be invoked, by slonik, only on that node.


Index: init_subscribe_set.ik
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testddl/init_subscribe_set.ik,v
retrieving revision 1.1.4.1
retrieving revision 1.1.4.2
diff -C2 -d -r1.1.4.1 -r1.1.4.2
*** init_subscribe_set.ik	30 Mar 2007 22:43:04 -0000	1.1.4.1
--- init_subscribe_set.ik	3 Apr 2007 21:55:04 -0000	1.1.4.2
***************
*** 1,2 ****
--- 1,4 ----
+ echo 'sleep a couple seconds';
+ sleep (seconds = 2);
  subscribe set ( id = 1, provider = 1, receiver = 2, forward = no);
  sync(id=1);

Index: individual_ddl.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testddl/individual_ddl.sh,v
retrieving revision 1.1.2.2
retrieving revision 1.1.2.3
diff -C2 -d -r1.1.2.2 -r1.1.2.3
*** individual_ddl.sh	2 Apr 2007 18:51:56 -0000	1.1.2.2
--- individual_ddl.sh	3 Apr 2007 21:55:04 -0000	1.1.2.3
***************
*** 5,9 ****
         SET ID = 1,
         FILENAME = '${testname}/ddl_update_part2.sql',
!        EVENT NODE = 1,
         EXECUTE ONLY ON = ${node}
      );
--- 5,9 ----
         SET ID = 1,
         FILENAME = '${testname}/ddl_update_part2.sql',
!        EVENT NODE = ${node},
         EXECUTE ONLY ON = ${node}
      );

Index: ddl_update_part2.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testddl/ddl_update_part2.sql,v
retrieving revision 1.1.2.1
retrieving revision 1.1.2.2
diff -C2 -d -r1.1.2.1 -r1.1.2.2
*** ddl_update_part2.sql	30 Mar 2007 22:43:04 -0000	1.1.2.1
--- ddl_update_part2.sql	3 Apr 2007 21:55:04 -0000	1.1.2.2
***************
*** 4,5 ****
--- 4,6 ----
  update table1 set seqed = nextval('t1seq');
  alter table table1 add constraint seqed_unique UNIQUE(seqed);
+ 

Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testddl/generate_dml.sh,v
retrieving revision 1.3.2.4
retrieving revision 1.3.2.5
diff -C2 -d -r1.3.2.4 -r1.3.2.5
*** generate_dml.sh	30 Mar 2007 22:43:04 -0000	1.3.2.4
--- generate_dml.sh	3 Apr 2007 21:55:04 -0000	1.3.2.5
***************
*** 23,27 ****
  generate_initdata()
  {
!   numrows=$(random_number 50 1000)
    i=0;
    trippoint=`expr $numrows / 20`
--- 23,27 ----
  generate_initdata()
  {
!   numrows=$(random_number 150 350)
    i=0;
    trippoint=`expr $numrows / 20`
***************
*** 93,97 ****
        init_preamble
        sh ${testname}/individual_ddl.sh ${testname} ${node} >> ${SCRIPT}
!       status "execute DDL script only on node ${node}"
        do_ik
    done
--- 93,97 ----
        init_preamble
        sh ${testname}/individual_ddl.sh ${testname} ${node} >> ${SCRIPT}
!       status "execute DDL script on node ${node}"
        do_ik
    done

From cbbrowne at lists.slony.info  Tue Apr 17 14:58:49 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Apr 17 14:58:51 2007
Subject: [Slony1-commit] slony1-engine/doc/adminguide maintenance.sgml
Message-ID: <20070417215849.E70292903EF@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv28280/doc/adminguide

Modified Files:
	maintenance.sgml 
Log Message:
slon-mkservice and logrep-mkservice documentation

Contributed by Andrew Hammond


Index: maintenance.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/maintenance.sgml,v
retrieving revision 1.25
retrieving revision 1.26
diff -C2 -d -r1.25 -r1.26
*** maintenance.sgml	2 Aug 2006 18:34:59 -0000	1.25
--- maintenance.sgml	17 Apr 2007 21:58:47 -0000	1.26
***************
*** 287,290 ****
--- 287,384 ----
  </para>
  </sect2>
+ <sect2><title>mkservice </title>
+ 
+ <sect3><title>slon-mkservice.sh</title>
+ 
+ <para> Create a slon service directory for use with svscan from
+ daemontools.  This uses multilog in a pretty basic way, which seems to
+ be standard for daemontools / multilog setups. If you want clever
+ logging, see logrep below. Currently this script has very limited
+ error handling capabilities.</para>
+ 
+ <para> For non-interactive use, set the following environment
+ variables.  <envar>BASEDIR</envar> <envar>SYSUSR</envar>
+ <envar>PASSFILE</envar> <envar>DBUSER</envar> <envar>HOST</envar>
+ <envar>PORT</envar> <envar>DATABASE</envar> <envar>CLUSTER</envar>
+ <envar>SLON_BINARY</envar> If any of the above are not set, the script
+ asks for configuration information interactively.</para>
+ 
+ <itemizedlist>
+ <listitem><para>
+ <envar>BASEDIR</envar> where you want the service directory structure for the slon
+ to be created. This should <emphasis>not</emphasis> be the <filename>/var/service</filename> directory.</para></listitem>
+ <listitem><para>
+ <envar>SYSUSR</envar> the unix user under which the slon (and multilog) process should run.</para></listitem>
+ <listitem><para>
+ <envar>PASSFILE</envar> location of the <filename>.pgpass</filename> file to be used. (default <filename>~sysusr/.pgpass</filename>)</para></listitem>
+ <listitem><para>
+ <envar>DBUSER</envar> the postgres user the slon should connect as (default slony)</para></listitem>
+ <listitem><para>
+ <envar>HOST</envar> what database server to connect to (default localhost)</para></listitem>
+ <listitem><para>
+ <envar>PORT</envar> what port to connect to (default 5432)</para></listitem>
+ <listitem><para>
+ <envar>DATABASE</envar> which database to connect to (default dbuser)</para></listitem>
+ <listitem><para>
+ <envar>CLUSTER</envar> the name of your Slony1 cluster? (default database)</para></listitem>
+ <listitem><para>
+ <envar>SLON_BINARY</envar> the full path name of the slon binary (default <command>which slon</command>)</para></listitem>
+ </itemizedlist>
+ 
+ <sect3><title>logrep-mkservice.sh</title>
+ 
+ <para>This uses <command>tail -F</command> to pull data from log files allowing
+ you to use multilog filters (by setting the CRITERIA) to create
+ special purpose log files. The goal is to provide a way to monitor log
+ files in near realtime for <quote>interesting</quote> data without either
+ hacking up the initial log file or wasting CPU/IO by re-scanning the
+ same log repeatedly.
+ </para>
+ 
+ <para>For non-interactive use, set the following environment
+ variables.  <envar>BASEDIR</envar> <envar>SYSUSR</envar> <envar>SOURCE</envar>
+ <envar>EXTENSION</envar> <envar>CRITERIA</envar> If any of the above are not set,
+ the script asks for configuration information interactively.
+ </para>
+ 
+ <itemizedlist>
+ <listitem><para>
+ <envar>BASEDIR</envar> where you want the service directory structure for the logrep
+ to be created. This should <emphasis>not</emphasis> be the <filename>/var/service</filename> directory.</para></listitem>
+ <listitem><para><envar>SYSUSR</envar> unix user under which the service should run.</para></listitem>
+ <listitem><para><envar>SOURCE</envar> name of the service with the log you want to follow.</para></listitem>
+ <listitem><para><envar>EXTENSION</envar> a tag to differentiate this logrep from others using the same source.</para></listitem>
+ <listitem><para><envar>CRITERIA</envar> the multilog filter you want to use.</para></listitem>
+ </itemizedlist>
+ 
+ <para> A trivial example of this would be to provide a log file of all slon
+ ERROR messages which could be used to trigger a nagios alarm.
+ <command>EXTENSION='ERRORS'</command>
+ <command>CRITERIA="'-*' '+* * ERROR*'"</command>
+ (Reset the monitor by rotating the log using <command>svc -a $svc_dir</command>)
+ </para>
+ 
+ <para> A more interesting application is a subscription progress log.
+ <command>EXTENSION='COPY'</command>
+ <command>CRITERIA="'-*' '+* * ERROR*' '+* * WARN*' '+* * CONFIG enableSubscription*' '+* * DEBUG2 remoteWorkerThread_* prepare to copy table*' '+* * DEBUG2 remoteWorkerThread_* all tables for set * found on subscriber*' '+* * DEBUG2 remoteWorkerThread_* copy*' '+* * DEBUG2 remoteWorkerThread_* Begin COPY of table*' '+* * DEBUG2 remoteWorkerThread_* * bytes copied for table*' '+* * DEBUG2 remoteWorkerThread_* * seconds to*' '+* * DEBUG2 remoteWorkerThread_* set last_value of sequence*' '+* * DEBUG2 remoteWorkerThread_* copy_set*'"</command>
+ </para>
+ 
+ <para>If you have a subscription log then it's easy to determine if a given
+ slon is in the process of handling copies or other subscription activity.
+ If the log isn't empty, and doesn't end with a 
+ <command>"CONFIG enableSubscription: sub_set:1"</command>
+ (or whatever set number you've subscribed) then the slon is currently in
+ the middle of initial copies.</para>
+ 
+ <para> If you happen to be monitoring the mtime of your primary slony logs to 
+ determine if your slon has gone brain-dead, checking this is a good way
+ to avoid mistakenly clobbering it in the middle of a subscribe. As a bonus,
+ recall that since the the slons are running under svscan, you only need to
+ kill it (via the svc interface) and let svscan start it up again laster.
+ I've also found the COPY logs handy for following subscribe activity 
+ interactively.</para>
+ </sect3>
+ 
+ </sect2>
  </sect1>
  <!-- Keep this comment at the end of the file

From cbbrowne at lists.slony.info  Tue Apr 17 14:58:49 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Apr 17 14:58:51 2007
Subject: [Slony1-commit] slony1-engine/tools/mkservice README
Message-ID: <20070417215849.E9CE529043A@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tools/mkservice
In directory main.slony.info:/tmp/cvs-serv28280/tools/mkservice

Modified Files:
	README 
Log Message:
slon-mkservice and logrep-mkservice documentation

Contributed by Andrew Hammond


Index: README
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tools/mkservice/README,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** README	30 Mar 2007 14:49:46 -0000	1.1
--- README	17 Apr 2007 21:58:47 -0000	1.2
***************
*** 6,23 ****
  -------------------
  Create a slon service directory for use with svscan from daemontools.
! This uses multilog in a pretty basic way. See logrep for cleverness.
! Currently very limited error handling. This probably needs to be
! fixed...
! 
! It asks for configuration information interactively, if run
! interactively.
  
  For non-interactive use, set the following environment variables.
  BASEDIR SYSUSR PASSFILE DBUSER HOST PORT DATABASE CLUSTER SLON_BINARY
  
  logrep-mkservice.sh
  --------------------
  
! This uses "tail -F" to pull data from Slony-I log files to allow
! creating special purpose log files to report specially desired
! information.
--- 6,72 ----
  -------------------
  Create a slon service directory for use with svscan from daemontools.
! This uses multilog in a pretty basic way, which seems to be standard 
! for daemontools / multilog setups. If you want clever logging, see
! logrep below. Currently this script has very limited error handling
! capabilities.
  
  For non-interactive use, set the following environment variables.
  BASEDIR SYSUSR PASSFILE DBUSER HOST PORT DATABASE CLUSTER SLON_BINARY
+ If any of the above are not set, the script asks for configuration
+ information interactively.
+ 
+ BASEDIR where you want the service directory structure for the slon
+ to be created. This should _not_ be the /var/service directory.
+ SYSUSR the unix user under which the slon (and multilog) process should run.
+ PASSFILE location of the .pgpass file to be used. (default ~sysusr/.pgpass)
+ DBUSER the postgres user the slon should connect as (default slony)
+ HOST what database server to connect to (default localhost)
+ PORT what port to connect to (default 5432)
+ DATABASE which database to connect to (default dbuser)
+ CLUSTER the name of your Slony1 cluster? (default database)
+ SLON_BINARY the full path name of the slon binary (default `which slon`)
  
  logrep-mkservice.sh
  --------------------
  
! This uses "tail -F" to pull data from log files allowing you to use
! multilog filters (by setting the CRITERIA) to create special purpose
! log files. The goal is to provide a way to monitor log files in near 
! realtime for "interesting" data without either hacking up the initial
! log file or wasting CPU/IO by re-scanning the same log repeatedly.
! 
! For non-interactive use, set the following environment variables.
! BASEDIR SYSUSR SOURCE EXTENSION CRITERIA 
! If any of the above are not set, the script asks for configuration
! information interactively.
! 
! BASEDIR where you want the service directory structure for the logrep
! to be created. This should _not_ be the /var/service directory.
! SYSUSR unix user under which the service should run.
! SOURCE name of the service with the log you want to follow.
! EXTENSION a tag to differentiate this logrep from others using the same source.
! CRITERIA the multilog filter you want to use.
! 
! A trivial example of this would be to provide a log file of all slon
! ERROR messages which could be used to trigger a nagios alarm.
! EXTENSION='ERRORS'
! CRITERIA="'-*' '+* * ERROR*'"
! (Reset the monitor by rotating the log using svc -a $svc_dir)
! 
! A more interesting application is a subscription progress log.
! EXTENSION='COPY'
! CRITERIA="'-*' '+* * ERROR*' '+* * WARN*' '+* * CONFIG enableSubscription*' '+* * DEBUG2 remoteWorkerThread_* prepare to copy table*' '+* * DEBUG2 remoteWorkerThread_* all tables for set * found on subscriber*' '+* * DEBUG2 remoteWorkerThread_* copy*' '+* * DEBUG2 remoteWorkerThread_* Begin COPY of table*' '+* * DEBUG2 remoteWorkerThread_* * bytes copied for table*' '+* * DEBUG2 remoteWorkerThread_* * seconds to*' '+* * DEBUG2 remoteWorkerThread_* set last_value of sequence*' '+* * DEBUG2 remoteWorkerThread_* copy_set*'"
! 
! If you have a subscription log then it's easy to determine if a given
! slon is in the process of handling copies or other subscription activity.
! If the log isn't empty, and doesn't end with a 
! "CONFIG enableSubscription: sub_set:1"
! (or whatever set number you've subscribed) then the slon is currently in
! the middle of initial copies.
! If you happen to be monitoring the mtime of your primary slony logs to 
! determine if your slon has gone brain-dead, checking this is a good way
! to avoid mistakenly clobbering it in the middle of a subscribe. As a bonus,
! recall that since the the slons are running under svscan, you only need to
! kill it (via the svc interface) and let svscan start it up again laster.
! I've also found the COPY logs handy for following subscribe activity 
! interactively.

From cbbrowne at lists.slony.info  Wed Apr 18 07:57:33 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 07:57:36 2007
Subject: [Slony1-commit] slony1-engine/tools/mkservice README
	logrep-mkservice.sh slon-mkservice.sh
Message-ID: <20070418145733.EEBAD290046@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tools/mkservice
In directory main.slony.info:/tmp/cvs-serv3198

Modified Files:
	README logrep-mkservice.sh slon-mkservice.sh 
Log Message:
Further changes to mkservice tools, per Andrew Hammond.


Index: README
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tools/mkservice/README,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** README	17 Apr 2007 21:58:47 -0000	1.2
--- README	18 Apr 2007 14:57:31 -0000	1.3
***************
*** 12,21 ****
  
  For non-interactive use, set the following environment variables.
! BASEDIR SYSUSR PASSFILE DBUSER HOST PORT DATABASE CLUSTER SLON_BINARY
  If any of the above are not set, the script asks for configuration
! information interactively.
  
  BASEDIR where you want the service directory structure for the slon
  to be created. This should _not_ be the /var/service directory.
  SYSUSR the unix user under which the slon (and multilog) process should run.
  PASSFILE location of the .pgpass file to be used. (default ~sysusr/.pgpass)
--- 12,26 ----
  
  For non-interactive use, set the following environment variables.
! BASEDIR LOGBASE SYSUSR PASSFILE DBUSER HOST PORT DATABASE CLUSTER SLON_BINARY
  If any of the above are not set, the script asks for configuration
! information interactively. The following environment variables are optional.
! LOGMAX LOGNUM
! If they are not set, they will silently default to reasonable values.
  
  BASEDIR where you want the service directory structure for the slon
  to be created. This should _not_ be the /var/service directory.
+ (default /usr/local/etc)
+ LOGBASE where you want your logs to end up. (default /var/log)
+ if set to - then revert to old behaviour and put logs under log/main.
  SYSUSR the unix user under which the slon (and multilog) process should run.
  PASSFILE location of the .pgpass file to be used. (default ~sysusr/.pgpass)
***************
*** 26,29 ****
--- 31,36 ----
  CLUSTER the name of your Slony1 cluster? (default database)
  SLON_BINARY the full path name of the slon binary (default `which slon`)
+ LOGMAX maximum size (in bytes) of logfiles (default 10485760 which is 10MB)
+ LOGNUM number of files to maintain (default 99, assume other tool prunes)
  
  logrep-mkservice.sh
***************
*** 37,50 ****
  
  For non-interactive use, set the following environment variables.
! BASEDIR SYSUSR SOURCE EXTENSION CRITERIA 
  If any of the above are not set, the script asks for configuration
! information interactively.
  
  BASEDIR where you want the service directory structure for the logrep
  to be created. This should _not_ be the /var/service directory.
  SYSUSR unix user under which the service should run.
  SOURCE name of the service with the log you want to follow.
  EXTENSION a tag to differentiate this logrep from others using the same source.
  CRITERIA the multilog filter you want to use.
  
  A trivial example of this would be to provide a log file of all slon
--- 44,63 ----
  
  For non-interactive use, set the following environment variables.
! BASEDIR LOGBASE SYSUSR SOURCE EXTENSION CRITERIA 
  If any of the above are not set, the script asks for configuration
! information interactively. The following environment variables are optional.
! LOGMAX LOGNUM
! If they are not set, they will silently default to reasonable values.
  
  BASEDIR where you want the service directory structure for the logrep
  to be created. This should _not_ be the /var/service directory.
+ LOGBASE where you want your logs to end up. (default /var/log)
+ if set to - then revert to old behaviour and put logs under log/main.
  SYSUSR unix user under which the service should run.
  SOURCE name of the service with the log you want to follow.
  EXTENSION a tag to differentiate this logrep from others using the same source.
  CRITERIA the multilog filter you want to use.
+ LOGMAX maximum size (in bytes) of logfiles (default 10485760 which is 10MB)
+ LOGNUM number of files to maintain (default 99, assume other tool prunes)
  
  A trivial example of this would be to provide a log file of all slon

Index: slon-mkservice.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tools/mkservice/slon-mkservice.sh,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** slon-mkservice.sh	30 Mar 2007 14:49:46 -0000	1.1
--- slon-mkservice.sh	18 Apr 2007 14:57:31 -0000	1.2
***************
*** 6,17 ****
  #
  # Create a slon service directory for use with svscan from deamontools.
! # This uses multilog in a pretty basic way. See logrep for cleverness.
! # Currently very limited error handling. This probably needs to be fixed...
! #
  # For non-interactive use, set the following environment variables.
! # BASEDIR SYSUSR PASSFILE DBUSER HOST PORT DATABASE CLUSTER SLON_BINARY
  
  DEFAULT_SLON_BINARY=`which slon`                # silly, wild-ass guess
  DEFAULT_BASEDIR='/usr/local/etc'
  DEFAULT_SYSUSR='pgsql'                          # FreeBSD-centric. Oh well.
  DEFAULT_DBUSR='slony'                           # Best Practice...
--- 6,40 ----
  #
  # Create a slon service directory for use with svscan from deamontools.
! # This uses multilog in a pretty basic way, which seems to be standard 
! # for daemontools / multilog setups. If you want clever logging, see
! # logrep below. Currently this script has very limited error handling
! # capabilities.
! # 
  # For non-interactive use, set the following environment variables.
! # BASEDIR LOGBASE SYSUSR PASSFILE DBUSER HOST PORT DATABASE CLUSTER SLON_BINARY
! # If any of the above are not set, the script asks for configuration
! # information interactively. The following environment variables are optional.
! # LOGMAX LOGNUM
! # If they are not set, they will silently default to reasonable values.
! # 
! # BASEDIR where you want the service directory structure for the slon
! # to be created. This should _not_ be the /var/service directory.
! # (default /usr/local/etc)
! # LOGBASE where you want your logs to end up. (default /var/log)
! # if set to - then revert to old behaviour and put logs under log/main.
! # SYSUSR the unix user under which the slon (and multilog) process should run.
! # PASSFILE location of the .pgpass file to be used. (default ~sysusr/.pgpass)
! # DBUSER the postgres user the slon should connect as (default slony)
! # HOST what database server to connect to (default localhost)
! # PORT what port to connect to (default 5432)
! # DATABASE which database to connect to (default dbuser)
! # CLUSTER name of your Slony1 cluster? (default database)
! # SLON_BINARY full path name of the slon binary (default `which slon`)
! # LOGMAX maximum size (in bytes) of logfiles (default 10485760 which is 10MB)
! # LOGNUM number of files to maintain (default 99, assume other tool prunes)
  
  DEFAULT_SLON_BINARY=`which slon`                # silly, wild-ass guess
  DEFAULT_BASEDIR='/usr/local/etc'
+ DEFAULT_LOGBASE='/var/log'
  DEFAULT_SYSUSR='pgsql'                          # FreeBSD-centric. Oh well.
  DEFAULT_DBUSR='slony'                           # Best Practice...
***************
*** 21,25 ****
  
  if [ -z "$BASEDIR" ]; then
!     echo -n "Where do you want the service dir created? Don't want to created this in /service or /var/service. Once it's created, either link or move it to the service directory (since linking is an atomic filesystem action). If your service directory is on a small, relatively static partition, you will almost certainly want to put this on a partition that can handle some log files and then link.
  [$DEFAULT_BASEDIR]: "
      read BASEDIR
--- 44,52 ----
  
  if [ -z "$BASEDIR" ]; then
!     echo -n "Where do you want the service dir created? Don't create this in 
! /service or /var/service. Once it's created, either symlink or move
! it to the service directory (since linking is an atomic filesystem action). 
! Note that log files will not be stored here (that's the next question), so 
! this doesn't have to be on a high storage / IO capacity filesystem.
  [$DEFAULT_BASEDIR]: "
      read BASEDIR
***************
*** 30,33 ****
--- 57,73 ----
  echo "BASEDIR=$BASEDIR"
  
+ if [ -z "$LOGBASE" ]; then
+     echo -n "Where should the logfiles live? You probably want to put this
+ somewhere with plenty of storage and some IO capacity. Note that this
+ creates a subdirectory where the actual log files are stored.
+ Use - to disable this (putting the log files under log/main according to
+ daemontools convention).
+ [$DEFAULT_LOGDIR]: "
+     read LOGDIR
+     if [ -z "$LOGDIR" ]; then
+         LOGDIR="$DEFAULT_LOGDIR"
+     fi
+ fi
+ 
  if [ -z "$SYSUSR" ]; then
      echo -n "System user name for slon to run under [$DEFAULT_SYSUSR]: "
***************
*** 115,125 ****
  echo "SLON_BINARY=$SLON_BINARY"
  
! DIR="$BASEDIR/slon_${CLUSTER}_${HOST}_${PORT}_$DATABASE"
  CONFIGFILE="$DIR/slon.conf"
  echo "CONFIGFILE=$CONFIGFILE"
  
  echo "Service dir will be created under $DIR"
  
! mkdir -p "$DIR/env" "$DIR/supervise" "$DIR/log/supervise" "$DIR/log/main" || exit -1
  # Make sure the log file initially exists. This allows others to tail -F it
  # before it starts getting populated. go go logrep!
--- 155,174 ----
  echo "SLON_BINARY=$SLON_BINARY"
  
! SVCNAME="slon_${CLUSTER}_${HOST}_${PORT}_$DATABASE"
! DIR="$BASEDIR/$SVCNAME"
! LOGDIR="$DIR/log/main"
! if [ '-' != "$LOGBASE" ]; then      # - means don't use a different logdir
!     LOGDIR="$LOGBASE/$SVCNAME"      # otherwise we're logging somewhere else
! fi
  CONFIGFILE="$DIR/slon.conf"
  echo "CONFIGFILE=$CONFIGFILE"
  
  echo "Service dir will be created under $DIR"
+ echo "Logs will live under $LOGDIR"
  
! mkdir -p "$DIR/env" "$DIR/supervise" "$DIR/log/env" "$DIR/log/supervise" "$LOGDIR" || exit -1
! if [ '-' != "$LOGBASE" ]; then          # - means it's not a linked logdir
!     ln -s "$LOGDIR" "$DIR/log/main"
! fi
  # Make sure the log file initially exists. This allows others to tail -F it
  # before it starts getting populated. go go logrep!
***************
*** 277,281 ****
  EOF
  
! # Set up the envdir contents. Generously.
  echo "$SLON_BINARY"                 > $DIR/env/SLON_BINARY
  echo "$CONFIGFILE"                  > $DIR/env/CONFIGFILE
--- 326,330 ----
  EOF
  
! # Set up the envdir contents for the admins. Generously.
  echo "$SLON_BINARY"                 > $DIR/env/SLON_BINARY
  echo "$CONFIGFILE"                  > $DIR/env/CONFIGFILE
***************
*** 284,288 ****
  echo "$PORT"                        > $DIR/env/PGPORT
  echo "$DATABASE"                    > $DIR/env/PGDATABASE
! # The absence of PGPASSWORD is not an oversight. Use .pgpass
  # Configure the location of .pgpass file here...
  # I'd like a better solution than this for expanding the homedir.
--- 333,338 ----
  echo "$PORT"                        > $DIR/env/PGPORT
  echo "$DATABASE"                    > $DIR/env/PGDATABASE
! # The absence of PGPASSWORD is not an oversight. Use .pgpass, see
! # http://www.postgresql.org/docs/current/interactive/libpq-pgpass.html
  # Configure the location of .pgpass file here...
  # I'd like a better solution than this for expanding the homedir.
***************
*** 293,301 ****
  echo 'UTC'                          > $DIR/env/PGTZ
  
  # create the run script for the slon
  cat > "$DIR/run" <<EOF
  #!/bin/sh
! # Note that the slon binary is a variable, so you can edit your envdir
! # settings to upgrade slons then restart them.
  exec 2>&1
  exec envdir ./env sh -c 'exec setuidgid ${SYSUSR} "\${SLON_BINARY}" -f "\${CONFIGFILE}"'
--- 343,394 ----
  echo 'UTC'                          > $DIR/env/PGTZ
  
+ # Avoid some subtle errors by documenting stuff... such as
+ cat > "$DIR/README.txt" <<EOF
+ This service will start on boot. If you do not want it to, then
+ touch $DIR/down
+ 
+ To upgrade your slon, first update env/SLON_BINARY to the full
+ path and name of the new slon binary. Then stop the slon.
+ svc -d $DIR
+ Apply your slonik UPDATE FUNCTIONS script(s) then restart your slon.
+ svc -u $DIR
+ Finally, check your logs to ensure that the new slon has started and
+ is running happily.
+ 
+ If you need to have a special purpose config file, or test version,
+ then you can simply copy the existing slon.conf to some other name,
+ make your changes there, update env/CONFIGFILE to point at the new
+ config and restart the slon.
+ svc -k $DIR
+ 
+ Note that changing variables such as CLUSTER, PGHOST, PGPORT,
+ PGDATABASE and PGUSER in the env directory will not change where
+ the slon connects. They are only there for admin/DBA convenience.
+ exec envdir $DIR/env bash
+ Is a quick way to get your variables all set up.
+ 
+ If you want to change where the slon connects, you need to edit
+ $CONFIGFILE
+ But you probably should not be doing that anyway, because then you
+ have to rename a whole bunch of stuff and edit all over the place
+ to keep the naming scheme consistent. Yuck. You should probably
+ just create a new slon service directory with the correct information,
+ and shut this one down.
+ touch $DIR/down; svc -dx $DIR $DIR/log
+ 
+ EOF
+ 
+ cat > "$DIR/env/README.txt" <<EOF
+ Many of these environment variables are only set as a convenience
+ for administrators and DBAs. To load them, try
+ exec envdir $DIR/env bash
+ Before you change stuff here, please read ../README.txt
+ EOF
+ 
  # create the run script for the slon
  cat > "$DIR/run" <<EOF
  #!/bin/sh
! # Note that the slon binary is a variable, so you can edit the value in
! # env/SLON_BINARY and restart to upgrade slons. See README.txt in this dir.
  exec 2>&1
  exec envdir ./env sh -c 'exec setuidgid ${SYSUSR} "\${SLON_BINARY}" -f "\${CONFIGFILE}"'
***************
*** 304,307 ****
--- 397,427 ----
  echo "$DIR/run created"
  
+ # setup an envdir for multilog
+ echo ${LOGMAX-"10485760"}           > $DIR/log/env/LOGMAX
+ echo ${LOGNUM-"99"}                 > $DIR/log/env/LOGNUM
+ 
+ cat > "$DIR/log/README.txt" <<EOF
+ To force a log rotation, use
+ svc -a $DIR/log
+ 
+ The size (in bytes) of the log files (before they get rotated) is controlled
+ by the s parameter for multilog. This is set up as an envdir variable at
+ $DIR/log/env/LOGMAX
+ You might want to increase or decrease this. It goes up to a maximum of
+ 16777215 (15MB) and defaults to 99999 (97kB) if unset. Leaving it unset 
+ will break this script. It defaults to 10485760 (which is 10MB).
+ You need to restart multilog for changes to this to take effect.
+ svc -k $DIR/log
+ 
+ The n paramter decides how many old log files to keep around. This is set
+ up as an envdir variable at
+ $DIR/log/env/LOGNUM
+ You will probably want to decrease this if you are not using some other
+ tool to manage old logfiles. Multilog defaults to 10 if this is unset, but
+ like the size above, it will break this script if left unset. The script
+ defaults to 99 under the assumption that you are using some other, system
+ wide tool (like cfengine) to prune your logs.
+ EOF
+ 
  # create the run file for the multilog
  cat > "$DIR/log/run" <<EOF
***************
*** 309,330 ****
  # This puts everything in the main log. Unfortunately multilog only allows
  # you to select which log you want to write to as opposed to writing each
! # line to every log which matches the criteria.
! #
! # Note that size (in bytes) of the log files (before they get rotated) is 
! # controlled by the s parameter. You might want to increase this. It goes
! # up to a maximum of 16777215 (15MB) and defaults to 99999 (97kB).
! # I'm using 10485760 (10MB)
! # The n paramter decides how many old log files to keep around. Defaults
! # to 10. 
! exec setuidgid $SYSUSR multilog t s10485760 n99 ./main
  EOF
  chmod a+x "$DIR/log/run"
  echo "$DIR/log/run created"
  
! # create and fix permissions for .pgpass appropriately
  touch "$PASSFILE"
! chmod 600 "$PASSSFILE"
  if [ ! -s "$PASSFILE" ]; then
!     echo "Populating $PASSFILE"
      cat > "$PASSFILE" <<EOF
  #hostname:port:database:username:password
--- 429,446 ----
  # This puts everything in the main log. Unfortunately multilog only allows
  # you to select which log you want to write to as opposed to writing each
! # line to every log which matches the criteria. Split up logs would make
! # debugging harder. See also README.txt in this directory.
! 
! exec envdir ./env sh -c 'exec setuidgid $SYSUSR multilog t s"\$LOGMAX" n"\$LOGNUM" ./main'
  EOF
  chmod a+x "$DIR/log/run"
  echo "$DIR/log/run created"
  
! # create and fix ownerships and permissions for .pgpass appropriately
  touch "$PASSFILE"
! chown "$SYSUSR" "$PASSFILE"
! chmod 600 "$PASSFILE"
  if [ ! -s "$PASSFILE" ]; then
!     echo "Populating $PASSFILE with header and example."
      cat > "$PASSFILE" <<EOF
  #hostname:port:database:username:password
***************
*** 357,360 ****
--- 473,477 ----
  has been created and populated with some sample data.
  
+ Logfiles can be found at $LOGDIR
  You may also want to set up a logrep to filter out the more intresting
  log lines. See logrep-mkservice.sh.

Index: logrep-mkservice.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tools/mkservice/logrep-mkservice.sh,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** logrep-mkservice.sh	30 Mar 2007 14:49:46 -0000	1.1
--- logrep-mkservice.sh	18 Apr 2007 14:57:31 -0000	1.2
***************
*** 5,21 ****
  # contributed by Andrew Hammond <andrew.george.hammond@gmail.com>
  #
! # logrep: use tail -F to pull from another log file for filtering to create
! # special purpose log files. Some example filters follow.
! #
  # For non-interactive use, set the following environment variables.
! # BASEDIR SYSUSR SOURCE EXTENSION CRITERIA 
! #
! # Slony subscribe specific information
! # '-*' '+* * ERROR*' '+* * WARN*' '+* * CONFIG enableSubscription*' '+* * DEBUG2 remoteWorkerThread_* prepare to copy table*' '+* * DEBUG2 remoteWorkerThread_* all tables for set * found on subscriber*' '+* * DEBUG2 remoteWorkerThread_* copy*' '+* * DEBUG2 remoteWorkerThread_* Begin COPY of table*' '+* * DEBUG2 remoteWorkerThread_* * bytes copied for table*' '+* * DEBUG2 remoteWorkerThread_* * seconds to*' '+* * DEBUG2 remoteWorkerThread_* set last_value of sequence*' '+* * DEBUG2 remoteWorkerThread_* copy_set*'
! #
! # Errors to trigger nagios
! # '-*' '+* * ERROR*'
  
  DEFAULT_BASEDIR='/usr/local/etc'
  DEFAULT_SYSUSR='pgsql'                          # FreeBSD-centric. Oh well.
  DEFAULT_SOURCE='slon_123'
--- 5,58 ----
  # contributed by Andrew Hammond <andrew.george.hammond@gmail.com>
  #
! # This uses "tail -F" to pull data from log files allowing you to use
! # multilog filters (by setting the CRITERIA) to create special purpose
! # log files. The goal is to provide a way to monitor log files in near 
! # realtime for "interesting" data without either hacking up the initial
! # log file or wasting CPU/IO by re-scanning the same log repeatedly.
! # 
  # For non-interactive use, set the following environment variables.
! # BASEDIR LOGBASE SYSUSR SOURCE EXTENSION CRITERIA 
! # If any of the above are not set, the script asks for configuration
! # information interactively. The following environment variables are optional.
! # LOGMAX LOGNUM
! # If they are not set, they will silently default to reasonable values.
! # 
! # BASEDIR where you want the service directory structure for the logrep
! # to be created. This should _not_ be the /var/service directory.
! # LOGBASE where you want your logs to end up. (default /var/log)
! # if set to - then revert to old behaviour and put logs under log/main.
! # SYSUSR unix user under which the service should run.
! # SOURCE name of the service with the log you want to follow.
! # EXTENSION a tag to differentiate this logrep from others using the same source.
! # CRITERIA the multilog filter you want to use.
! # LOGMAX maximum size (in bytes) of logfiles (default 10485760 which is 10MB)
! # LOGNUM number of files to maintain (default 99, assume other tool prunes)
! # 
! # A trivial example of this would be to provide a log file of all slon
! # ERROR messages which could be used to trigger a nagios alarm.
! # EXTENSION='ERRORS'
! # CRITERIA="'-*' '+* * ERROR*'"
! # (Reset the monitor by rotating the log using svc -a $svc_dir)
! # 
! # A more interesting application is a subscription progress log.
! # EXTENSION='COPY'
! # CRITERIA="'-*' '+* * ERROR*' '+* * WARN*' '+* * CONFIG enableSubscription*' '+* * DEBUG2 remoteWorkerThread_* prepare to copy table*' '+* * DEBUG2 remoteWorkerThread_* all tables for set * found on subscriber*' '+* * DEBUG2 remoteWorkerThread_* copy*' '+* * DEBUG2 remoteWorkerThread_* Begin COPY of table*' '+* * DEBUG2 remoteWorkerThread_* * bytes copied for table*' '+* * DEBUG2 remoteWorkerThread_* * seconds to*' '+* * DEBUG2 remoteWorkerThread_* set last_value of sequence*' '+* * DEBUG2 remoteWorkerThread_* copy_set*'"
! # 
! # If you have a subscription log then it's easy to determine if a given
! # slon is in the process of handling copies or other subscription activity.
! # If the log isn't empty, and doesn't end with a 
! # "CONFIG enableSubscription: sub_set:1"
! # (or whatever set number you've subscribed) then the slon is currently in
! # the middle of initial copies.
! # If you happen to be monitoring the mtime of your primary slony logs to 
! # determine if your slon has gone brain-dead, checking this is a good way
! # to avoid mistakenly clobbering it in the middle of a subscribe. As a bonus,
! # recall that since the the slons are running under svscan, you only need to
! # kill it (via the svc interface) and let svscan start it up again laster.
! # I've also found the COPY logs handy for following subscribe activity 
! # interactively.
  
  DEFAULT_BASEDIR='/usr/local/etc'
+ DEFAULT_LOGDIR='/var/log'
  DEFAULT_SYSUSR='pgsql'                          # FreeBSD-centric. Oh well.
  DEFAULT_SOURCE='slon_123'
***************
*** 24,28 ****
  
  if [ -z "$BASEDIR" ]; then
!     echo -n "Where do you want the service dir created? Don't want to created this in /service or /var/service. Once it's created, either link or move it to the service directory (since linking is an atomic filesystem action). If your service directory is on a small, relatively static partition, you will almost certainly want to put this on a partition that can handle some log files and then link.
  [$DEFAULT_BASEDIR]: "
      read BASEDIR
--- 61,69 ----
  
  if [ -z "$BASEDIR" ]; then
!     echo -n "Where do you want the service dir created? Don't create this in 
! /service or /var/service. Once it's created, either symlink or move
! it to the service directory (since linking is an atomic filesystem action). 
! Note that log files will not be stored here (that's the next question), so 
! this doesn't have to be on a high storage / IO capacity filesystem.
  [$DEFAULT_BASEDIR]: "
      read BASEDIR
***************
*** 33,36 ****
--- 74,90 ----
  echo "BASEDIR=$BASEDIR"
  
+ if [ -z "$LOGBASE" ]; then
+     echo -n "Where should the logfiles live? You probably want to put this 
+ somewhere with plenty of storage and some IO capacity. Note that this
+ creates a subdirectory where the actual log files are stored.
+ Use - to disable this (putting the log files under log/main according to
+ daemontools convention).
+ [$DEFAULT_LOGDIR]: "
+     read LOGDIR
+     if [ -z "$LOGDIR" ]; then
+         LOGDIR="$DEFAULT_LOGDIR"
+     fi
+ fi
+ 
  if [ -z "$SYSUSR" ]; then
      echo -n "System user name for followgrep to run under [$DEFAULT_SYSUSR]: "
***************
*** 69,78 ****
  echo "CRITERIA=$CRITERIA"
  
! DIR="$BASEDIR/logrep_$SOURCE$EXTENSION"
  echo "Service dir will be created under $DIR"
  
! mkdir -p "$DIR/supervise" "$DIR/log/supervise" "$DIR/log/main" || exit -1
  # Make sure the log file initially exists. This allows others to tail -F it
! # before it starts getting populated.
  touch "$DIR/log/main/current" || exit -1
  
--- 123,144 ----
  echo "CRITERIA=$CRITERIA"
  
! 
! SVCNAME="logrep_$SOURCE$EXTENSION"
! DIR="$BASEDIR/$SVCNAME"
! LOGDIR="$DIR/log/main"
! if [ '-' != "$LOGBASE" ]; then      # - means don't use a different logdir
!     LOGDIR="$LOGBASE/$SVCNAME"      # otherwise we're logging somewhere else
! fi
! 
  echo "Service dir will be created under $DIR"
+ echo "Logs will live under $LOGDIR"
  
! 
! mkdir -p "$DIR/env" "$DIR/supervise" "$DIR/log/env" "$DIR/log/supervise" "$LOGDIR" || exit -1
! if [ '-' != "$LOGBASE" ]; then          # - means it's not a linked logdir
!     ln -s "$LOGDIR" "$DIR/log/main"
! fi
  # Make sure the log file initially exists. This allows others to tail -F it
! # before it starts getting populated. go go recursive logrep!
  touch "$DIR/log/main/current" || exit -1
  
***************
*** 81,101 ****
  #!/bin/sh
  exec 2>&1
! exec setuidgid $SYSUSR tail -F "$BASEDIR/$SOURCE/log/main/current"
  EOF
  chmod a+x "$DIR/run"
  echo "$DIR/run created"
  
  # create the run file for the multilog
  cat > "$DIR/log/run" <<EOF
  #!/bin/sh
! # DO NOT add another timestamp using the t parameter to multilog. Unless
! # of course you like being confused.
! # Note that size (in bytes) of the log files (before they get rotated) is 
! # controlled by the s parameter. You might want to increase this. It goes
! # up to a maximum of 16777215 (15MB) and defaults to 99999 (97kB).
! # I'm using 10485760 (10MB)
! # The n paramter decides how many old log files to keep around. Defaults
! # to 10. 
! exec setuidgid $SYSUSR multilog s10485760 n99 $CRITERIA ./main
  EOF
  chmod a+x "$DIR/log/run"
--- 147,193 ----
  #!/bin/sh
  exec 2>&1
! exec env ./env setuidgid $SYSUSR tail -F "$BASEDIR/$SOURCE/log/main/current"
  EOF
  chmod a+x "$DIR/run"
  echo "$DIR/run created"
  
+ # setup an envdir for multilog
+ echo ${LOGMAX-"10485760"}           > $DIR/log/env/LOGMAX
+ echo ${LOGNUM-"99"}                 > $DIR/log/env/LOGNUM
+ 
+ cat > "$DIR/log/README.txt" <<EOF
+ To force a log rotation, use
+ svc -a $DIR/log
+ 
+ The size (in bytes) of the log files (before they get rotated) is controlled
+ by the s parameter for multilog. This is set up as an envdir variable at
+ $DIR/log/env/LOGMAX
+ You might want to increase or decrease this. It goes up to a maximum of
+ 16777215 (15MB) and defaults to 99999 (97kB) if unset. Leaving it unset 
+ will break this script. It defaults to 10485760 (which is 10MB).
+ You need to restart multilog for changes to this to take effect.
+ svc -k $DIR/log
+ 
+ The n paramter decides how many old log files to keep around. This is set
+ up as an envdir variable at
+ $DIR/log/env/LOGNUM
+ You will probably want to decrease this if you are not using some other
+ tool to manage old logfiles. Multilog defaults to 10 if this is unset, but
+ like the size above, it will break this script if left unset. The script
+ defaults to 99 under the assumption that you are using some other, system
+ wide tool (like cfengine) to prune your logs.
+ EOF
+ 
  # create the run file for the multilog
  cat > "$DIR/log/run" <<EOF
  #!/bin/sh
! # This puts everything in the main log. Unfortunately multilog only allows
! # you to select which log you want to write to as opposed to writing each
! # line to every log which matches the criteria. Split up logs would make
! # debugging harder. See also README.txt in this directory.
! # Since we're presumably drawing data from another logfile which already 
! # has timestamps, adding another would simply sow confusion.
! 
! exec envdir ./env sh -c 'exec setuidgid $SYSUSR multilog s"\$LOGMAX" n"\$LOGNUM" ./main'
  EOF
  chmod a+x "$DIR/log/run"

From cbbrowne at lists.slony.info  Wed Apr 18 08:03:53 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 08:03:58 2007
Subject: [Slony1-commit] slony1-engine SAMPLE
Message-ID: <20070418150353.1B6672902FE@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine
In directory main.slony.info:/tmp/cvs-serv3274

Modified Files:
	SAMPLE 
Log Message:
Patch that removes TABLE ADD KEY from CVS HEAD.  This supercedes the
previous patch:

http://lists.slony.info/pipermail/slony1-patches/2007-April/000008.html

Per Bill Moran's comments, this also drops out usage of sl_rowid_seq.
http://lists.slony.info/pipermail/slony1-general/2007-April/005883.html

It passes test1, as revised in the patch (e.g. - to remove usage of
TABLE ADD KEY)...

----------------------------------------------------
creating origin DB: cbbrowne -h localhost -U cbbrowne -p 5882 slonyregress1
add plpgsql to Origin
loading origin DB with test1/init_schema.sql
setting up user cbbrowne to have weak access to data
done
creating subscriber 2 DB: cbbrowne -h localhost -U cbbrowne -p 5882
slonyregress2
add plpgsql to subscriber
loading subscriber 2 DB from slonyregress1
done
creating cluster
done
storing nodes
done
Granting weak access on Slony-I schema
done
storing paths
done
launching originnode : /opt/OXRS/dbs/pgsql82/bin/slon -s500 -g10 -d2
slony_regress1 "dbname=slonyregress1 host=localhost user=cbbrowne port=5882"
launching: /opt/OXRS/dbs/pgsql82/bin/slon -s500 -g10 -d2 slony_regress1
"dbname=slonyregress2 host=localhost user=cbbrowne port=5882"
subscribing
done
generating 468 transactions of random data
0 %
5 %
10 %
15 %
20 %
25 %
30 %
35 %
40 %
45 %
50 %
55 %
60 %
65 %
70 %
75 %
80 %
85 %
90 %
95 %
100 %
done
launching polling script
loading data
data load complete
completed generate_sync_event() test
completed make_function_strict() test
done
slony is caught up
getting data from origin DB for diffing
done
getting data from node 2 for diffing against origin
comparing
subscriber node 2 is the same as origin node 1
done
**** killing slon node 1
**** killing slon node 2
waiting for slons to die
done
dropping database
slonyregress1
slonyregress2
done
***************************
test test1 completed successfully
***************************


Index: SAMPLE
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/SAMPLE,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** SAMPLE	14 Dec 2005 07:54:16 -0000	1.2
--- SAMPLE	18 Apr 2007 15:03:51 -0000	1.3
***************
*** 1,4 ****
  Creating a sample database with application
! --------------------------------------------
  
  $Id$
--- 1,4 ----
  Creating a sample database with application
! -----------------------------------------------------------------
  
  $Id$

From cbbrowne at lists.slony.info  Wed Apr 18 08:03:53 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 08:03:58 2007
Subject: [Slony1-commit] slony1-engine/tests run_test.sh
Message-ID: <20070418150353.8D15E29047A@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests
In directory main.slony.info:/tmp/cvs-serv3274/tests

Modified Files:
	run_test.sh 
Log Message:
Patch that removes TABLE ADD KEY from CVS HEAD.  This supercedes the
previous patch:

http://lists.slony.info/pipermail/slony1-patches/2007-April/000008.html

Per Bill Moran's comments, this also drops out usage of sl_rowid_seq.
http://lists.slony.info/pipermail/slony1-general/2007-April/005883.html

It passes test1, as revised in the patch (e.g. - to remove usage of
TABLE ADD KEY)...

----------------------------------------------------
creating origin DB: cbbrowne -h localhost -U cbbrowne -p 5882 slonyregress1
add plpgsql to Origin
loading origin DB with test1/init_schema.sql
setting up user cbbrowne to have weak access to data
done
creating subscriber 2 DB: cbbrowne -h localhost -U cbbrowne -p 5882
slonyregress2
add plpgsql to subscriber
loading subscriber 2 DB from slonyregress1
done
creating cluster
done
storing nodes
done
Granting weak access on Slony-I schema
done
storing paths
done
launching originnode : /opt/OXRS/dbs/pgsql82/bin/slon -s500 -g10 -d2
slony_regress1 "dbname=slonyregress1 host=localhost user=cbbrowne port=5882"
launching: /opt/OXRS/dbs/pgsql82/bin/slon -s500 -g10 -d2 slony_regress1
"dbname=slonyregress2 host=localhost user=cbbrowne port=5882"
subscribing
done
generating 468 transactions of random data
0 %
5 %
10 %
15 %
20 %
25 %
30 %
35 %
40 %
45 %
50 %
55 %
60 %
65 %
70 %
75 %
80 %
85 %
90 %
95 %
100 %
done
launching polling script
loading data
data load complete
completed generate_sync_event() test
completed make_function_strict() test
done
slony is caught up
getting data from origin DB for diffing
done
getting data from node 2 for diffing against origin
comparing
subscriber node 2 is the same as origin node 1
done
**** killing slon node 1
**** killing slon node 2
waiting for slons to die
done
dropping database
slonyregress1
slonyregress2
done
***************************
test test1 completed successfully
***************************


Index: run_test.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/run_test.sh,v
retrieving revision 1.12
retrieving revision 1.13
diff -C2 -d -r1.12 -r1.13
*** run_test.sh	1 Mar 2007 21:02:31 -0000	1.12
--- run_test.sh	18 Apr 2007 15:03:51 -0000	1.13
***************
*** 316,320 ****
    sl_event_seq sl_listen sl_local_node_id sl_log_1 sl_log_2
    sl_log_status sl_node  sl_path sl_registry
!   sl_rowid_seq sl_seqlastvalue sl_seqlog sl_sequence sl_set sl_setsync
    sl_status sl_subscribe sl_table sl_trigger"
  
--- 316,320 ----
    sl_event_seq sl_listen sl_local_node_id sl_log_1 sl_log_2
    sl_log_status sl_node  sl_path sl_registry
!   sl_seqlastvalue sl_seqlog sl_sequence sl_set sl_setsync
    sl_status sl_subscribe sl_table sl_trigger"
  

From cbbrowne at lists.slony.info  Wed Apr 18 08:03:53 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 08:03:58 2007
Subject: [Slony1-commit] slony1-engine/src/slon misc.h remote_worker.c
Message-ID: <20070418150353.65EAE29046B@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slon
In directory main.slony.info:/tmp/cvs-serv3274/src/slon

Modified Files:
	misc.h remote_worker.c 
Log Message:
Patch that removes TABLE ADD KEY from CVS HEAD.  This supercedes the
previous patch:

http://lists.slony.info/pipermail/slony1-patches/2007-April/000008.html

Per Bill Moran's comments, this also drops out usage of sl_rowid_seq.
http://lists.slony.info/pipermail/slony1-general/2007-April/005883.html

It passes test1, as revised in the patch (e.g. - to remove usage of
TABLE ADD KEY)...

----------------------------------------------------
creating origin DB: cbbrowne -h localhost -U cbbrowne -p 5882 slonyregress1
add plpgsql to Origin
loading origin DB with test1/init_schema.sql
setting up user cbbrowne to have weak access to data
done
creating subscriber 2 DB: cbbrowne -h localhost -U cbbrowne -p 5882
slonyregress2
add plpgsql to subscriber
loading subscriber 2 DB from slonyregress1
done
creating cluster
done
storing nodes
done
Granting weak access on Slony-I schema
done
storing paths
done
launching originnode : /opt/OXRS/dbs/pgsql82/bin/slon -s500 -g10 -d2
slony_regress1 "dbname=slonyregress1 host=localhost user=cbbrowne port=5882"
launching: /opt/OXRS/dbs/pgsql82/bin/slon -s500 -g10 -d2 slony_regress1
"dbname=slonyregress2 host=localhost user=cbbrowne port=5882"
subscribing
done
generating 468 transactions of random data
0 %
5 %
10 %
15 %
20 %
25 %
30 %
35 %
40 %
45 %
50 %
55 %
60 %
65 %
70 %
75 %
80 %
85 %
90 %
95 %
100 %
done
launching polling script
loading data
data load complete
completed generate_sync_event() test
completed make_function_strict() test
done
slony is caught up
getting data from origin DB for diffing
done
getting data from node 2 for diffing against origin
comparing
subscriber node 2 is the same as origin node 1
done
**** killing slon node 1
**** killing slon node 2
waiting for slons to die
done
dropping database
slonyregress1
slonyregress2
done
***************************
test test1 completed successfully
***************************


Index: remote_worker.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/remote_worker.c,v
retrieving revision 1.135
retrieving revision 1.136
diff -C2 -d -r1.135 -r1.136
*** remote_worker.c	14 Mar 2007 15:55:06 -0000	1.135
--- remote_worker.c	18 Apr 2007 15:03:51 -0000	1.136
***************
*** 2660,2747 ****
  				 node->no_id, tab_fqname);
  
! 		/*
! 		 * Find out if the table we're copying has the special slony serial
! 		 * number key on the provider DB
! 		 */
! 		slon_mkquery(&query1,
! 					 "select %s.tableHasSerialKey('%q');",
! 					 rtcfg_namespace, tab_fqname);
! 		res2 = PQexec(pro_dbconn, dstring_data(&query1));
  		if (PQresultStatus(res2) != PGRES_TUPLES_OK)
  		{
! 			slon_log(SLON_ERROR, "remoteWorkerThread_%d: \"%s\" %s",
! 					 node->no_id, dstring_data(&query1),
! 					 PQresultErrorMessage(res2));
  			PQclear(res2);
  			PQclear(res1);
  			slon_disconnectdb(pro_conn);
  			dstring_free(&query1);
- 			dstring_free(&query2);
  			dstring_free(&query3);
- 			dstring_free(&lsquery);
- 			dstring_free(&indexregenquery);
  			archive_terminate(node);
  			return -1;
  		}
- 		rc = *PQgetvalue(res2, 0, 0) == 't';
  		PQclear(res2);
- 
- 		if (rc)
- 		{
- 			/*
- 			 * It has, check if the table has this on the local DB too.
- 			 */
- 			slon_log(SLON_DEBUG3, "remoteWorkerThread_%d: "
- 					 "table %s will require Slony-I serial key\n",
- 					 node->no_id, tab_fqname);
- 			res2 = PQexec(loc_dbconn, dstring_data(&query1));
- 			if (PQresultStatus(res2) != PGRES_TUPLES_OK)
- 			{
- 				slon_log(SLON_ERROR, "remoteWorkerThread_%d: \"%s\" %s",
- 						 node->no_id, dstring_data(&query1),
- 						 PQresultErrorMessage(res2));
- 				PQclear(res2);
- 				PQclear(res1);
- 				slon_disconnectdb(pro_conn);
- 				dstring_free(&query1);
- 				dstring_free(&query2);
- 				dstring_free(&query3);
- 				dstring_free(&lsquery);
- 				dstring_free(&indexregenquery);
- 				archive_terminate(node);
- 				return -1;
- 			}
- 			rc = *PQgetvalue(res2, 0, 0) == 't';
- 			PQclear(res2);
- 
- 			if (!rc)
- 			{
- 				slon_log(SLON_DEBUG3, "remoteWorkerThread_%d: "
- 						 "table %s Slony-I serial key to be added local\n",
- 						 node->no_id, tab_fqname);
- 			}
- 		}
- 		else
- 		{
- 			slon_log(SLON_DEBUG3, "remoteWorkerThread_%d: "
- 					 "table %s does not require Slony-I serial key\n",
- 					 node->no_id, tab_fqname);
- 			slon_mkquery(&query3, "select * from %s limit 0;",
- 						 tab_fqname);
- 			res2 = PQexec(loc_dbconn, dstring_data(&query3));
- 			if (PQresultStatus(res2) != PGRES_TUPLES_OK)
- 			{
- 				slon_log(SLON_ERROR, "remoteWorkerThread_%d: Could not find table %s "
- 						 "on subscriber\n", node->no_id, tab_fqname);
- 				PQclear(res2);
- 				PQclear(res1);
- 				slon_disconnectdb(pro_conn);
- 				dstring_free(&query1);
- 				dstring_free(&query3);
- 				archive_terminate(node);
- 				return -1;
- 			}
- 			PQclear(res2);
- 		}
  		/* Request an exclusive lock on each table
  
--- 2660,2679 ----
  				 node->no_id, tab_fqname);
  
! 		slon_mkquery(&query3, "select * from %s limit 0;",
! 			     tab_fqname);
! 		res2 = PQexec(loc_dbconn, dstring_data(&query3));
  		if (PQresultStatus(res2) != PGRES_TUPLES_OK)
  		{
! 			slon_log(SLON_ERROR, "remoteWorkerThread_%d: Could not find table %s "
! 				 "on subscriber\n", node->no_id, tab_fqname);
  			PQclear(res2);
  			PQclear(res1);
  			slon_disconnectdb(pro_conn);
  			dstring_free(&query1);
  			dstring_free(&query3);
  			archive_terminate(node);
  			return -1;
  		}
  		PQclear(res2);
  		/* Request an exclusive lock on each table
  
***************
*** 2891,2990 ****
  
  		/*
- 		 * Find out if the table we're copying has the special slony serial
- 		 * number key on the provider DB
- 		 */
- 		slon_mkquery(&query1,
- 					 "select %s.tableHasSerialKey('%q');",
- 					 rtcfg_namespace, tab_fqname);
- 		res2 = PQexec(pro_dbconn, dstring_data(&query1));
- 		if (PQresultStatus(res2) != PGRES_TUPLES_OK)
- 		{
- 			slon_log(SLON_ERROR, "remoteWorkerThread_%d: \"%s\" %s",
- 					 node->no_id, dstring_data(&query1),
- 					 PQresultErrorMessage(res2));
- 			PQclear(res2);
- 			PQclear(res1);
- 			slon_disconnectdb(pro_conn);
- 			dstring_free(&query1);
- 			dstring_free(&query2);
- 			dstring_free(&query3);
- 			dstring_free(&lsquery);
- 			dstring_free(&indexregenquery);
- 			archive_terminate(node);
- 			return -1;
- 		}
- 		rc = *PQgetvalue(res2, 0, 0) == 't';
- 		PQclear(res2);
- 
- 		if (rc)
- 		{
- 			/*
- 			 * It has, check if the table has this on the local DB too.
- 			 */
- 			slon_log(SLON_DEBUG3, "remoteWorkerThread_%d: "
- 					 "table %s requires Slony-I serial key\n",
- 					 node->no_id, tab_fqname);
- 			res2 = PQexec(loc_dbconn, dstring_data(&query1));
- 			if (PQresultStatus(res2) != PGRES_TUPLES_OK)
- 			{
- 				slon_log(SLON_ERROR, "remoteWorkerThread_%d: \"%s\" %s",
- 						 node->no_id, dstring_data(&query1),
- 						 PQresultErrorMessage(res2));
- 				PQclear(res2);
- 				PQclear(res1);
- 				slon_disconnectdb(pro_conn);
- 				dstring_free(&query1);
- 				dstring_free(&query2);
- 				dstring_free(&query3);
- 				dstring_free(&lsquery);
- 				dstring_free(&indexregenquery);
- 				archive_terminate(node);
- 				return -1;
- 			}
- 			rc = *PQgetvalue(res2, 0, 0) == 't';
- 			PQclear(res2);
- 
- 			if (!rc)
- 			{
- 				/*
- 				 * Nope, so we gotta add the key here.
- 				 */
- 				slon_mkquery(&query1,
- 							 "select %s.tableAddKey('%q'); "
- 							 "select %s.determineAttkindSerial('%q'); ",
- 							 rtcfg_namespace, tab_fqname,
- 							 rtcfg_namespace, tab_fqname);
- 				if (query_execute(node, loc_dbconn, &query1) < 0)
- 				{
- 					PQclear(res1);
- 					slon_disconnectdb(pro_conn);
- 					dstring_free(&query1);
- 					dstring_free(&query2);
- 					dstring_free(&query3);
- 					dstring_free(&lsquery);
- 					dstring_free(&indexregenquery);
- 					archive_terminate(node);
- 					return -1;
- 				}
- 				slon_log(SLON_DEBUG3, "remoteWorkerThread_%d: "
- 						 "table %s Slony-I serial key added local\n",
- 						 node->no_id, tab_fqname);
- 			}
- 			else
- 			{
- 				slon_log(SLON_DEBUG3, "remoteWorkerThread_%d: "
- 						 "local table %s already has Slony-I serial key\n",
- 						 node->no_id, tab_fqname);
- 			}
- 		}
- 		else
- 		{
- 			slon_log(SLON_DEBUG3, "remoteWorkerThread_%d: "
- 					 "table %s does not require Slony-I serial key\n",
- 					 node->no_id, tab_fqname);
- 		}
- 
- 
- 		/*
  		 * Call the setAddTable_int() stored procedure. Up to now, while we
  		 * have not been subscribed to the set, this should have been
--- 2823,2826 ----
***************
*** 4803,4851 ****
  
  	/*
- 	 * Get the nodes rowid sequence at that sync time just in case we are
- 	 * later on asked to restore the node after a failover.
- 	 */
- 	slon_mkquery(&query,
- 				 "select seql_last_value from %s.sl_seqlog "
- 				 "	where seql_seqid = 0 "
- 				 "	and seql_origin = %d "
- 				 "	and seql_ev_seqno = '%s'; ",
- 				 rtcfg_namespace, node->no_id,
- 				 seqbuf);
- 	res1 = PQexec(wd->provider_head->conn->dbconn, dstring_data(&query));
- 	if (PQresultStatus(res1) != PGRES_TUPLES_OK)
- 	{
- 		slon_log(SLON_ERROR, "remoteWorkerThread_%d: \"%s\" %s",
- 				 node->no_id, dstring_data(&query),
- 				 PQresultErrorMessage(res1));
- 		PQclear(res1);
- 		dstring_free(&query);
- 		dstring_free(&lsquery);
- 		archive_terminate(node);
- 		return 60;
- 	}
- 	if (PQntuples(res1) > 0)
- 	{
- 		slon_mkquery(&query,
- 			     "insert into %s.sl_seqlog "
- 			     "	(seql_seqid, seql_origin, seql_ev_seqno, seql_last_value) "
- 			     "	values (0, %d, '%s', '%s'); ",
- 					 rtcfg_namespace, node->no_id,
- 					 seqbuf, PQgetvalue(res1, 0, 0));
- 		if (query_execute(node, local_dbconn, &query) < 0)
- 		{
- 			PQclear(res1);
- 			dstring_free(&query);
- 			dstring_free(&lsquery);
- 			archive_terminate(node);
- 			return 60;
- 		}
- 		slon_log(SLON_DEBUG2, "remoteWorkerThread_%d: "
- 				 "new sl_rowid_seq value: %s\n",
- 				 node->no_id, PQgetvalue(res1, 0, 0));
- 	}
- 	PQclear(res1);
- 
- 	/*
  	 * Add the final commit to the archive log, close it and rename the
  	 * temporary file to the real log chunk filename.
--- 4639,4642 ----

Index: misc.h
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/misc.h,v
retrieving revision 1.8
retrieving revision 1.9
diff -C2 -d -r1.8 -r1.9
*** misc.h	27 Oct 2006 20:10:57 -0000	1.8
--- misc.h	18 Apr 2007 15:03:51 -0000	1.9
***************
*** 35,38 ****
--- 35,45 ----
  #endif
  
+ /* Adjustment windows */
+ #ifdef WIN32
+ #define sleep(x) Sleep(x*1000)
+ #define strtoll(x,y,z) (__int64) strtol(x,y,z)
+ #define strncasecmp(x,y,z)	strnicmp(x,y,z)
+ #endif
+ 
  /*
   * Local Variables:

From cbbrowne at lists.slony.info  Wed Apr 18 08:03:53 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 08:03:58 2007
Subject: [Slony1-commit] slony1-engine/src/slonik dbutil.c parser.y scan.l
	slonik.c slonik.h
Message-ID: <20070418150353.E6CA7290BCC@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slonik
In directory main.slony.info:/tmp/cvs-serv3274/src/slonik

Modified Files:
	dbutil.c parser.y scan.l slonik.c slonik.h 
Log Message:
Patch that removes TABLE ADD KEY from CVS HEAD.  This supercedes the
previous patch:

http://lists.slony.info/pipermail/slony1-patches/2007-April/000008.html

Per Bill Moran's comments, this also drops out usage of sl_rowid_seq.
http://lists.slony.info/pipermail/slony1-general/2007-April/005883.html

It passes test1, as revised in the patch (e.g. - to remove usage of
TABLE ADD KEY)...

----------------------------------------------------
creating origin DB: cbbrowne -h localhost -U cbbrowne -p 5882 slonyregress1
add plpgsql to Origin
loading origin DB with test1/init_schema.sql
setting up user cbbrowne to have weak access to data
done
creating subscriber 2 DB: cbbrowne -h localhost -U cbbrowne -p 5882
slonyregress2
add plpgsql to subscriber
loading subscriber 2 DB from slonyregress1
done
creating cluster
done
storing nodes
done
Granting weak access on Slony-I schema
done
storing paths
done
launching originnode : /opt/OXRS/dbs/pgsql82/bin/slon -s500 -g10 -d2
slony_regress1 "dbname=slonyregress1 host=localhost user=cbbrowne port=5882"
launching: /opt/OXRS/dbs/pgsql82/bin/slon -s500 -g10 -d2 slony_regress1
"dbname=slonyregress2 host=localhost user=cbbrowne port=5882"
subscribing
done
generating 468 transactions of random data
0 %
5 %
10 %
15 %
20 %
25 %
30 %
35 %
40 %
45 %
50 %
55 %
60 %
65 %
70 %
75 %
80 %
85 %
90 %
95 %
100 %
done
launching polling script
loading data
data load complete
completed generate_sync_event() test
completed make_function_strict() test
done
slony is caught up
getting data from origin DB for diffing
done
getting data from node 2 for diffing against origin
comparing
subscriber node 2 is the same as origin node 1
done
**** killing slon node 1
**** killing slon node 2
waiting for slons to die
done
dropping database
slonyregress1
slonyregress2
done
***************************
test test1 completed successfully
***************************


Index: slonik.h
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/slonik.h,v
retrieving revision 1.29
retrieving revision 1.30
diff -C2 -d -r1.29 -r1.30
*** slonik.h	31 Oct 2006 22:09:40 -0000	1.29
--- slonik.h	18 Apr 2007 15:03:51 -0000	1.30
***************
*** 38,42 ****
  typedef struct SlonikStmt_set_move_table_s SlonikStmt_set_move_table;
  typedef struct SlonikStmt_set_move_sequence_s SlonikStmt_set_move_sequence;
- typedef struct SlonikStmt_table_add_key_s SlonikStmt_table_add_key;
  typedef struct SlonikStmt_store_trigger_s SlonikStmt_store_trigger;
  typedef struct SlonikStmt_drop_trigger_s SlonikStmt_drop_trigger;
--- 38,41 ----
***************
*** 83,87 ****
  	STMT_STORE_TRIGGER,
  	STMT_SUBSCRIBE_SET,
- 	STMT_TABLE_ADD_KEY,
  	STMT_UNINSTALL_NODE,
  	STMT_UNLOCK_SET,
--- 82,85 ----
***************
*** 280,284 ****
  	int			set_origin;
  	int			tab_id;
- 	int			use_serial;
  	char	   *use_key;
  	char	   *tab_fqname;
--- 278,281 ----
***************
*** 332,343 ****
  
  
- struct SlonikStmt_table_add_key_s
- {
- 	SlonikStmt	hdr;
- 	int			no_id;
- 	char	   *tab_fqname;
- };
- 
- 
  struct SlonikStmt_store_trigger_s
  {
--- 329,332 ----
***************
*** 559,563 ****
  extern int	slonik_set_move_table(SlonikStmt_set_move_table * stmt);
  extern int	slonik_set_move_sequence(SlonikStmt_set_move_sequence * stmt);
- extern int	slonik_table_add_key(SlonikStmt_table_add_key * stmt);
  extern int	slonik_store_trigger(SlonikStmt_store_trigger * stmt);
  extern int	slonik_drop_trigger(SlonikStmt_drop_trigger * stmt);
--- 548,551 ----

Index: parser.y
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/parser.y,v
retrieving revision 1.27
retrieving revision 1.28
diff -C2 -d -r1.27 -r1.28
*** parser.y	31 Oct 2006 22:09:40 -0000	1.27
--- parser.y	18 Apr 2007 15:03:51 -0000	1.28
***************
*** 42,46 ****
  	O_SECONDS,
  	O_SERVER,
- 	O_SER_KEY,
  	O_SET_ID,
  	O_SPOOLNODE,
--- 42,45 ----
***************
*** 149,153 ****
  %type <statement>	stmt_set_move_table
  %type <statement>	stmt_set_move_sequence
- %type <statement>	stmt_table_add_key
  %type <statement>	stmt_store_trigger
  %type <statement>	stmt_drop_trigger
--- 148,151 ----
***************
*** 227,231 ****
  %token  K_SECONDS
  %token	K_SEQUENCE
- %token	K_SERIAL
  %token	K_SERVER
  %token	K_SET
--- 225,228 ----
***************
*** 449,454 ****
  					| stmt_merge_set
  						{ $$ = $1; }
- 					| stmt_table_add_key
- 						{ $$ = $1; }
  					| stmt_set_add_table
  						{ $$ = $1; }
--- 446,449 ----
***************
*** 920,950 ****
  					;
  
- stmt_table_add_key	: lno K_TABLE K_ADD K_KEY option_list
- 					{
- 						SlonikStmt_table_add_key *new;
- 						statement_option opt[] = {
- 							STMT_OPTION_INT( O_NODE_ID, -1 ),
- 							STMT_OPTION_STR( O_FQNAME, NULL ),
- 							STMT_OPTION_END
- 						};
- 
- 						new = (SlonikStmt_table_add_key *)
- 								malloc(sizeof(SlonikStmt_table_add_key));
- 						memset(new, 0, sizeof(SlonikStmt_table_add_key));
- 						new->hdr.stmt_type		= STMT_TABLE_ADD_KEY;
- 						new->hdr.stmt_filename	= current_file;
- 						new->hdr.stmt_lno		= $1;
- 
- 						if (assign_options(opt, $5) == 0)
- 						{
- 							new->no_id			= opt[0].ival;
- 							new->tab_fqname		= opt[1].str;
- 						}
- 						else
- 							parser_errors++;
- 
- 						$$ = (SlonikStmt *)new;
- 					}
- 					;
  
  stmt_set_add_table	: lno K_SET K_ADD K_TABLE option_list
--- 915,918 ----
***************
*** 957,961 ****
  							STMT_OPTION_STR( O_FQNAME, NULL ),
  							STMT_OPTION_STR( O_USE_KEY, NULL ),
- 							STMT_OPTION_INT( O_SER_KEY, 0 ),
  							STMT_OPTION_STR( O_COMMENT, NULL ),
  							STMT_OPTION_END
--- 925,928 ----
***************
*** 976,981 ****
  							new->tab_fqname		= opt[3].str;
  							new->use_key		= opt[4].str;
! 							new->use_serial		= opt[5].ival;
! 							new->tab_comment	= opt[6].str;
  						}
  						else
--- 943,947 ----
  							new->tab_fqname		= opt[3].str;
  							new->use_key		= opt[4].str;
! 							new->tab_comment	= opt[5].str;
  						}
  						else
***************
*** 1654,1670 ****
  						$$ = $3;
  					}
- 					| K_KEY '=' K_SERIAL
- 					{
- 						option_list *new;
- 						new = (option_list *)malloc(sizeof(option_list));
- 
- 						new->opt_code	= O_SER_KEY;
- 						new->ival	= 1;
- 						new->str	= NULL;
- 						new->lineno	= yylineno;
- 						new->next	= NULL;
- 
- 						$$ = new;
- 					}
  					| K_FORWARD '=' option_item_yn
  					{
--- 1620,1623 ----
***************
*** 1868,1872 ****
      	case O_SECONDS:         return "seconds";
  		case O_SERVER:			return "server";
- 		case O_SER_KEY:			return "key";
  		case O_SET_ID:			return "set id";
  		case O_SPOOLNODE:		return "spoolnode";
--- 1821,1824 ----

Index: slonik.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/slonik.c,v
retrieving revision 1.74
retrieving revision 1.75
diff -C2 -d -r1.74 -r1.75
*** slonik.c	16 Mar 2007 22:38:01 -0000	1.74
--- slonik.c	18 Apr 2007 15:03:51 -0000	1.75
***************
*** 12,19 ****
  
  
  #include <stdio.h>
  #include <stdlib.h>
  #include <stdarg.h>
- #ifndef WIN32
  #include <unistd.h>
  #include <fcntl.h>
--- 12,19 ----
  
  
+ #ifndef WIN32
  #include <stdio.h>
  #include <stdlib.h>
  #include <stdarg.h>
  #include <unistd.h>
  #include <fcntl.h>
***************
*** 21,31 ****
  #include <sys/types.h>
  #include <sys/wait.h>
  #else
  #define sleep(x) Sleep(x*1000)
  #define vsnprintf _vsnprintf
- #define INT64_FORMAT "%I64d"
  #endif
- #include <errno.h>
- #include <time.h>
  
  #include "postgres.h"
--- 21,30 ----
  #include <sys/types.h>
  #include <sys/wait.h>
+ #include <errno.h>
+ #include <time.h>
  #else
  #define sleep(x) Sleep(x*1000)
  #define vsnprintf _vsnprintf
  #endif
  
  #include "postgres.h"
***************
*** 555,567 ****
  						errors++;
  					}
- 					if (stmt->use_serial && stmt->use_key != NULL)
- 					{
- 						printf("%s:%d: Error: "
- 							   "unique key name or SERIAL are "
- 							   "mutually exclusive\n",
- 							   hdr->stmt_filename, hdr->stmt_lno);
- 						errors++;
- 					}
- 
  					if (stmt->tab_comment == NULL)
  						stmt->tab_comment = strdup(stmt->tab_fqname);
--- 554,557 ----
***************
*** 778,815 ****
  				break;
  
- 			case STMT_TABLE_ADD_KEY:
- 				{
- 					SlonikStmt_table_add_key *stmt =
- 					(SlonikStmt_table_add_key *) hdr;
- 
- 					/*
- 					 * Check that we have the node id and that we can reach
- 					 * it.
- 					 */
- 					if (stmt->no_id < 0)
- 					{
- 						printf("%s:%d: Error: "
- 							   "node id must be specified\n",
- 							   hdr->stmt_filename, hdr->stmt_lno);
- 						errors++;
- 					}
- 					else
- 					{
- 						if (script_check_adminfo(hdr, stmt->no_id) < 0)
- 							errors++;
- 					}
- 
- 					/*
- 					 * Check that we have the table name
- 					 */
- 					if (stmt->tab_fqname == NULL)
- 					{
- 						printf("%s:%d: Error: "
- 							   "table FQ-name must be specified\n",
- 							   hdr->stmt_filename, hdr->stmt_lno);
- 						errors++;
- 					}
- 				}
- 				break;
  
  			case STMT_STORE_TRIGGER:
--- 768,771 ----
***************
*** 1447,1460 ****
  				break;
  
- 			case STMT_TABLE_ADD_KEY:
- 				{
- 					SlonikStmt_table_add_key *stmt =
- 					(SlonikStmt_table_add_key *) hdr;
- 
- 					if (slonik_table_add_key(stmt) < 0)
- 						errors++;
- 				}
- 				break;
- 
  			case STMT_STORE_TRIGGER:
  				{
--- 1403,1406 ----
***************
*** 2402,2433 ****
  	PQclear(res);
  	
- 	/*
- 	 * If available, bump the rowid sequence to the last known value.
- 	 */
- 	slon_mkquery(&query,
- 		     "select max(seql_last_value) from \"_%s\".sl_seqlog "
- 		     "	where seql_seqid = 0 "
- 		     "	and seql_origin = %d; ",
- 		     stmt->hdr.script->clustername, stmt->no_id);
- 	res = db_exec_select((SlonikStmt *) stmt, adminfo2, &query);
- 	if (res == NULL)
- 	{
- 		dstring_free(&query);
- 		return -1;
- 	}
- 	if (PQntuples(res) == 1 && !PQgetisnull(res, 0, 0))
- 	{
- 		slon_mkquery(&query,
- 			     "select \"pg_catalog\".setval('\"_%s\".sl_rowid_seq', '%s'); ",
- 			     stmt->hdr.script->clustername, PQgetvalue(res, 0, 0));
- 		if (db_exec_command((SlonikStmt *) stmt, adminfo1, &query) < 0)
- 		{
- 			dstring_free(&query);
- 			PQclear(res);
- 			return -1;
- 		}
- 	}
- 	PQclear(res);
- 
  	/* On the existing node, call storeNode() and enableNode() */
  	slon_mkquery(&query,
--- 2348,2351 ----
***************
*** 3316,3348 ****
  	dstring_init(&query);
  
! 	/*
! 	 * Determine the attkind of the table. The stored procedure for KEY =
! 	 * SERIAL might actually add a bigserial column to the table.
! 	 */
! 	if (stmt->use_serial)
  	{
  		slon_mkquery(&query,
! 					 "select \"_%s\".determineIdxnameSerial('%q'), "
! 					 "       \"_%s\".determineAttKindSerial('%q'); ",
! 					 stmt->hdr.script->clustername, stmt->tab_fqname,
! 					 stmt->hdr.script->clustername, stmt->tab_fqname);
  
  	}
  	else
  	{
! 		if (stmt->use_key == NULL)
! 		{
! 			slon_mkquery(&query,
! 					   "select \"_%s\".determineIdxnameUnique('%q', NULL); ",
! 						 stmt->hdr.script->clustername, stmt->tab_fqname);
! 
! 		}
! 		else
! 		{
! 			slon_mkquery(&query,
! 					   "select \"_%s\".determineIdxnameUnique('%q', '%q'); ",
! 						 stmt->hdr.script->clustername,
! 						 stmt->tab_fqname, stmt->use_key);
! 		}
  	}
  
--- 3234,3250 ----
  	dstring_init(&query);
  
! 	if (stmt->use_key == NULL)
  	{
  		slon_mkquery(&query,
! 			     "select \"_%s\".determineIdxnameUnique('%q', NULL); ",
! 			     stmt->hdr.script->clustername, stmt->tab_fqname);
  
  	}
  	else
  	{
! 		slon_mkquery(&query,
! 			     "select \"_%s\".determineIdxnameUnique('%q', '%q'); ",
! 			     stmt->hdr.script->clustername,
! 			     stmt->tab_fqname, stmt->use_key);
  	}
  
***************
*** 3532,3572 ****
  }
  
- 
- int
- slonik_table_add_key(SlonikStmt_table_add_key * stmt)
- {
- 	SlonikAdmInfo *adminfo1;
- 	SlonDString query;
- 
- 	adminfo1 = get_active_adminfo((SlonikStmt *) stmt, stmt->no_id);
- 	if (adminfo1 == NULL)
- 		return -1;
- 
- 	if (db_begin_xact((SlonikStmt *) stmt, adminfo1) < 0)
- 		return -1;
- 
- 	dstring_init(&query);
- 
- 	/*
- 	 * call tableAddKey()
- 	 */
- 	db_notice_silent = true;
- 	slon_mkquery(&query,
- 				 "select \"_%s\".tableAddKey('%q'); ",
- 				 stmt->hdr.script->clustername,
- 				 stmt->tab_fqname);
- 	if (db_exec_command((SlonikStmt *) stmt, adminfo1, &query) < 0)
- 	{
- 		db_notice_silent = false;
- 		dstring_free(&query);
- 		return -1;
- 	}
- 	db_notice_silent = false;
- 
- 	dstring_free(&query);
- 	return 0;
- }
- 
- 
  int
  slonik_store_trigger(SlonikStmt_store_trigger * stmt)
--- 3434,3437 ----

Index: dbutil.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/dbutil.c,v
retrieving revision 1.12
retrieving revision 1.13
diff -C2 -d -r1.12 -r1.13
*** dbutil.c	28 Jun 2006 04:47:14 -0000	1.12
--- dbutil.c	18 Apr 2007 15:03:51 -0000	1.13
***************
*** 12,19 ****
  
  
  #include <stdio.h>
  #include <stdlib.h>
  #include <stdarg.h>
- #ifndef WIN32
  #include <unistd.h>
  #include <sys/types.h>
--- 12,19 ----
  
  
+ #ifndef WIN32
  #include <stdio.h>
  #include <stdlib.h>
  #include <stdarg.h>
  #include <unistd.h>
  #include <sys/types.h>

Index: scan.l
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/scan.l,v
retrieving revision 1.26
retrieving revision 1.27
diff -C2 -d -r1.26 -r1.27
*** scan.l	31 Oct 2006 22:09:40 -0000	1.26
--- scan.l	18 Apr 2007 15:03:51 -0000	1.27
***************
*** 119,123 ****
  seconds         { return K_SECONDS;         }
  sequence		{ return K_SEQUENCE;		}
- serial			{ return K_SERIAL;			}
  server			{ return K_SERVER;			}
  set				{ return K_SET;				}
--- 119,122 ----

From cbbrowne at lists.slony.info  Wed Apr 18 08:03:53 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 08:03:58 2007
Subject: [Slony1-commit] slony1-engine/tests/test1 README init_add_tables.ik
	init_data.sql init_schema.sql schema.diff
Message-ID: <20070418150353.EFE60290BD0@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/test1
In directory main.slony.info:/tmp/cvs-serv3274/tests/test1

Modified Files:
	README init_add_tables.ik init_data.sql init_schema.sql 
	schema.diff 
Log Message:
Patch that removes TABLE ADD KEY from CVS HEAD.  This supercedes the
previous patch:

http://lists.slony.info/pipermail/slony1-patches/2007-April/000008.html

Per Bill Moran's comments, this also drops out usage of sl_rowid_seq.
http://lists.slony.info/pipermail/slony1-general/2007-April/005883.html

It passes test1, as revised in the patch (e.g. - to remove usage of
TABLE ADD KEY)...

----------------------------------------------------
creating origin DB: cbbrowne -h localhost -U cbbrowne -p 5882 slonyregress1
add plpgsql to Origin
loading origin DB with test1/init_schema.sql
setting up user cbbrowne to have weak access to data
done
creating subscriber 2 DB: cbbrowne -h localhost -U cbbrowne -p 5882
slonyregress2
add plpgsql to subscriber
loading subscriber 2 DB from slonyregress1
done
creating cluster
done
storing nodes
done
Granting weak access on Slony-I schema
done
storing paths
done
launching originnode : /opt/OXRS/dbs/pgsql82/bin/slon -s500 -g10 -d2
slony_regress1 "dbname=slonyregress1 host=localhost user=cbbrowne port=5882"
launching: /opt/OXRS/dbs/pgsql82/bin/slon -s500 -g10 -d2 slony_regress1
"dbname=slonyregress2 host=localhost user=cbbrowne port=5882"
subscribing
done
generating 468 transactions of random data
0 %
5 %
10 %
15 %
20 %
25 %
30 %
35 %
40 %
45 %
50 %
55 %
60 %
65 %
70 %
75 %
80 %
85 %
90 %
95 %
100 %
done
launching polling script
loading data
data load complete
completed generate_sync_event() test
completed make_function_strict() test
done
slony is caught up
getting data from origin DB for diffing
done
getting data from node 2 for diffing against origin
comparing
subscriber node 2 is the same as origin node 1
done
**** killing slon node 1
**** killing slon node 2
waiting for slons to die
done
dropping database
slonyregress1
slonyregress2
done
***************************
test test1 completed successfully
***************************


Index: README
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/test1/README,v
retrieving revision 1.8
retrieving revision 1.9
diff -C2 -d -r1.8 -r1.9
*** README	22 Mar 2007 20:44:13 -0000	1.8
--- README	18 Apr 2007 15:03:51 -0000	1.9
***************
*** 12,25 ****
  2.  table2 lacks a formal primary key, but has a candidate primary key
  
! 3.  table3 has no candidate primary key; Slony-I is expected to
!     generate one on its own.
! 
! It actually tries replicating a fourth table, which has an invalid
! candidate primary key (columns not defined NOT NULL), which should
! cause it to be rejected.  That is done in a slonik TRY {} block.
  
  It also creates...
  
! 5.  table5 which has columns of all sorts of vaguely esoteric types to
  exercise that points, paths, bitmaps, mac addresses, and inet types
  replicate properly.
--- 12,22 ----
  2.  table2 lacks a formal primary key, but has a candidate primary key
  
! It tries replicating a third table, which has an invalid candidate
! primary key (columns not defined NOT NULL), which should cause it to
! be rejected.  That is done in a slonik TRY {} block.
  
  It also creates...
  
! 5.  table4 which has columns of all sorts of vaguely esoteric types to
  exercise that points, paths, bitmaps, mac addresses, and inet types
  replicate properly.

Index: init_data.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/test1/init_data.sql,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** init_data.sql	4 Aug 2006 20:06:24 -0000	1.2
--- init_data.sql	18 Apr 2007 15:03:51 -0000	1.3
***************
*** 3,10 ****
  INSERT INTO table2(table1_id,data) VALUES (1,'placeholder 1');
  INSERT INTO table2(table1_id,data) VALUES (2,'placeholder 2');
- INSERT INTO table3(table2_id) VALUES (1);
- INSERT INTO table3(table2_id) VALUES (2);
  
! INSERT INTO table5(numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol,bitcol) values ('74.0','7.40','(7,4)','((7,7),(4,4),(0,0),(7,0))','((7,4),(0,7),(4,0),(0,4))','<(7,4),0>','192.168.7.40','08:00:2d:07:04:00',X'740');
  
! INSERT INTO table5(numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol,bitcol) values ('93.1','9.31','(9,3)','((9,9),(3,3),(1,1),(9,1))','((9,3),(1,9),(3,1),(1,3))','<(9,3),1>','192.168.9.31','08:00:2d:09:03:01',X'931');
\ No newline at end of file
--- 3,8 ----
  INSERT INTO table2(table1_id,data) VALUES (1,'placeholder 1');
  INSERT INTO table2(table1_id,data) VALUES (2,'placeholder 2');
  
! INSERT INTO table4(numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol,bitcol) values ('74.0','7.40','(7,4)','((7,7),(4,4),(0,0),(7,0))','((7,4),(0,7),(4,0),(0,4))','<(7,4),0>','192.168.7.40','08:00:2d:07:04:00',X'740');
  
! INSERT INTO table4(numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol,bitcol) values ('93.1','9.31','(9,3)','((9,9),(3,3),(1,1),(9,1))','((9,3),(1,9),(3,1),(1,3))','<(9,3),1>','192.168.9.31','08:00:2d:09:03:01',X'931');
\ No newline at end of file

Index: init_add_tables.ik
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/test1/init_add_tables.ik,v
retrieving revision 1.8
retrieving revision 1.9
diff -C2 -d -r1.8 -r1.9
*** init_add_tables.ik	4 Aug 2006 20:06:24 -0000	1.8
--- init_add_tables.ik	18 Apr 2007 15:03:51 -0000	1.9
***************
*** 2,16 ****
  set add table (id=2, set id=1, origin=1, fully qualified name = 'public.table2', key='table2_id_key');
  
- table add key (node id = 1, fully qualified name = 'public.table3');
- set add table (id=3, set id=1, origin=1, fully qualified name = 'public.table3', key = SERIAL);
- 
  try {
!    set add table (id=4, set id=1, origin=1, fully qualified name = 'public.table4', key = 'no_good_candidate_pk', comment='bad table - table 4');
  } on error {
!    echo 'Tried to replicate table4 with no good candidate PK - rejected';
  } on success {
!    echo 'Tried to replicate table4 with no good candidate PK - accepted';
     exit 1;
  }
  
! set add table (id=4, set id=1, origin=1, fully qualified name = 'public.table5', comment='a table of many types');
\ No newline at end of file
--- 2,13 ----
  set add table (id=2, set id=1, origin=1, fully qualified name = 'public.table2', key='table2_id_key');
  
  try {
!    set add table (id=3, set id=1, origin=1, fully qualified name = 'public.table3', key = 'no_good_candidate_pk', comment='bad table - table 3');
  } on error {
!    echo 'Tried to replicate table3 with no good candidate PK - rejected';
  } on success {
!    echo 'Tried to replicate table3 with no good candidate PK - accepted';
     exit 1;
  }
  
! set add table (id=4, set id=1, origin=1, fully qualified name = 'public.table4', comment='a table of many types');
\ No newline at end of file

Index: init_schema.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/test1/init_schema.sql,v
retrieving revision 1.5
retrieving revision 1.6
diff -C2 -d -r1.5 -r1.6
*** init_schema.sql	4 Aug 2006 20:06:24 -0000	1.5
--- init_schema.sql	18 Apr 2007 15:03:51 -0000	1.6
***************
*** 11,31 ****
  );
  
! CREATE TABLE table3(
!   id		SERIAL,
!   table2_id	INT4		REFERENCES table2(id)
! 					ON UPDATE SET NULL ON DELETE SET NULL,
!   mod_date	TIMESTAMPTZ	NOT NULL DEFAULT now(),
!   data		FLOAT		NOT NULL DEFAULT random()
!   CONSTRAINT table3_date_check	CHECK (mod_date <= now())
! ); 
! 
! create table table4 (
    id serial NOT NULL,
    id2 integer
  );
  
! create unique index no_good_candidate_pk on table4 (id, id2);
  
! create table table5 (
    id serial primary key,
    numcol numeric(12,4), -- 1.23
--- 11,22 ----
  );
  
! create table table3 (
    id serial NOT NULL,
    id2 integer
  );
  
! create unique index no_good_candidate_pk on table3 (id, id2);
  
! create table table4 (
    id serial primary key,
    numcol numeric(12,4), -- 1.23

Index: schema.diff
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/test1/schema.diff,v
retrieving revision 1.3
retrieving revision 1.4
diff -C2 -d -r1.3 -r1.4
*** schema.diff	15 Nov 2005 21:25:33 -0000	1.3
--- schema.diff	18 Apr 2007 15:03:51 -0000	1.4
***************
*** 1,3 ****
  SELECT id,data FROM table1 ORDER BY id
  SELECT id,table1_id,data FROM table2 ORDER BY id
! SELECT id,table2_id,mod_date, data FROM table3 ORDER BY id
--- 1,3 ----
  SELECT id,data FROM table1 ORDER BY id
  SELECT id,table1_id,data FROM table2 ORDER BY id
! SELECT id,numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol, bitcol from table4 order by id;

From cbbrowne at lists.slony.info  Wed Apr 18 08:03:53 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 08:03:59 2007
Subject: [Slony1-commit] slony1-engine/doc/adminguide addthings.sgml
	bestpractices.sgml defineset.sgml faq.sgml slonik_ref.sgml
	slonyupgrade.sgml
Message-ID: <20070418150354.5BC3D290BD3@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv3274/doc/adminguide

Modified Files:
	addthings.sgml bestpractices.sgml defineset.sgml faq.sgml 
	slonik_ref.sgml slonyupgrade.sgml 
Log Message:
Patch that removes TABLE ADD KEY from CVS HEAD.  This supercedes the
previous patch:

http://lists.slony.info/pipermail/slony1-patches/2007-April/000008.html

Per Bill Moran's comments, this also drops out usage of sl_rowid_seq.
http://lists.slony.info/pipermail/slony1-general/2007-April/005883.html

It passes test1, as revised in the patch (e.g. - to remove usage of
TABLE ADD KEY)...

----------------------------------------------------
creating origin DB: cbbrowne -h localhost -U cbbrowne -p 5882 slonyregress1
add plpgsql to Origin
loading origin DB with test1/init_schema.sql
setting up user cbbrowne to have weak access to data
done
creating subscriber 2 DB: cbbrowne -h localhost -U cbbrowne -p 5882
slonyregress2
add plpgsql to subscriber
loading subscriber 2 DB from slonyregress1
done
creating cluster
done
storing nodes
done
Granting weak access on Slony-I schema
done
storing paths
done
launching originnode : /opt/OXRS/dbs/pgsql82/bin/slon -s500 -g10 -d2
slony_regress1 "dbname=slonyregress1 host=localhost user=cbbrowne port=5882"
launching: /opt/OXRS/dbs/pgsql82/bin/slon -s500 -g10 -d2 slony_regress1
"dbname=slonyregress2 host=localhost user=cbbrowne port=5882"
subscribing
done
generating 468 transactions of random data
0 %
5 %
10 %
15 %
20 %
25 %
30 %
35 %
40 %
45 %
50 %
55 %
60 %
65 %
70 %
75 %
80 %
85 %
90 %
95 %
100 %
done
launching polling script
loading data
data load complete
completed generate_sync_event() test
completed make_function_strict() test
done
slony is caught up
getting data from origin DB for diffing
done
getting data from node 2 for diffing against origin
comparing
subscriber node 2 is the same as origin node 1
done
**** killing slon node 1
**** killing slon node 2
waiting for slons to die
done
dropping database
slonyregress1
slonyregress2
done
***************************
test test1 completed successfully
***************************


Index: defineset.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/defineset.sgml,v
retrieving revision 1.26
retrieving revision 1.27
diff -C2 -d -r1.26 -r1.27
*** defineset.sgml	30 Jan 2007 19:48:35 -0000	1.26
--- defineset.sgml	18 Apr 2007 15:03:51 -0000	1.27
***************
*** 71,80 ****
  
  <listitem><para> If the table hasn't even got a candidate primary key,
! you can ask &slony1; to provide one.  This is done by first using
! <xref linkend="stmttableaddkey"> to add a column populated using a
! &slony1; sequence, and then having the <xref
! linkend="stmtsetaddtable"> include the directive
! <option>key=serial</option>, to indicate that &slony1;'s own column
! should be used.</para></listitem>
  
  </itemizedlist>
--- 71,82 ----
  
  <listitem><para> If the table hasn't even got a candidate primary key,
! you might ask &slony1; to provide one using 
! <xref linkend="stmttableaddkey">.</para>
! 
! <warning><para> <xref linkend="stmttableaddkey"> was always considered
! a <quote>kludge</quote>, at best, and as of version 2.0, it is
! considered such a misfeature that it is being removed.  </para>
! </warning>
! </listitem>
  
  </itemizedlist>
***************
*** 83,92 ****
  <quote>true</quote> primary key or a mere <quote>candidate primary
  key;</quote> it is, however, strongly recommended that you have one of
! those instead of having &slony1; populate the PK column for you. If you
! don't have a suitable primary key, that means that the table hasn't got
! any mechanism, from your application's standpoint, for keeping values
! unique. &slony1; may, therefore, introduce a new failure mode for your
! application, and this also implies that you had a way to enter confusing
! data into the database.</para>
  </sect2>
  
--- 85,94 ----
  <quote>true</quote> primary key or a mere <quote>candidate primary
  key;</quote> it is, however, strongly recommended that you have one of
! those instead of having &slony1; populate the PK column for you. If
! you don't have a suitable primary key, that means that the table
! hasn't got any mechanism, from your application's standpoint, for
! keeping values unique.  &slony1; may, therefore, introduce a new
! failure mode for your application, and this also implies that you had
! a way to enter confusing data into the database.</para>
  </sect2>
  

Index: slonyupgrade.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/slonyupgrade.sgml,v
retrieving revision 1.5
retrieving revision 1.6
diff -C2 -d -r1.5 -r1.6
*** slonyupgrade.sgml	23 Nov 2006 17:16:40 -0000	1.5
--- slonyupgrade.sgml	18 Apr 2007 15:03:51 -0000	1.6
***************
*** 78,81 ****
--- 78,194 ----
  
  </variablelist>
+ 
+ <sect2> <title> TABLE ADD KEY issue in &slony1; 2.0 </title> 
+ 
+ <para> Usually, upgrades between &slony1; versions have required no
+ special attention to the condition of the existing replica.  That is,
+ you fairly much merely need to stop &lslon;s, put new binaries in
+ place, run <xref linkend="stmtupdatefunctions"> against each node, and
+ restart &lslon;s.  Schema changes have been internal to the cluster
+ schema, and <xref linkend="stmtupdatefunctions"> has been capable to
+ make all of the needed alterations.  With version 2, this changes, if
+ there are tables that used <xref linkend="stmttableaddkey">.  Version
+ 2 does not support the <quote>extra</quote> column, and
+ <quote>fixing</quote> the schema to have a proper primary key is not
+ within the scope of what <xref linkend="stmtupdatefunctions"> can
+ perform.  </para>
+ 
+ <para> When upgrading from versions 1.0.x, 1.1.x, or 1.2.x to version
+ 2, it will be necessary to have already eliminated any such
+ &slony1;-managed primary keys. </para>
+ 
+ <para> One may identify the tables affected via the following SQL
+ query: <command> select n.nspname, c.relname from pg_class c,
+ pg_namespace n where c.oid in (select attrelid from pg_attribute where
+ attname like '_Slony-I_%rowID' and not attisdropped) and reltype <> 0
+ and n.oid = c.relnamespace order by n.nspname, c.relname; </command>
+ </para>
+ 
+ <para> The simplest approach that may be taken to rectify the
+ <quote>broken</quote> state of such tables is as follows: </para>
+ 
+ <itemizedlist>
+ 
+ <listitem><para> Drop the table from replication using the &lslonik;
+ command <xref linkend="stmtsetdroptable">. </para>
+ 
+ <para> This does <emphasis>not</emphasis> drop out the &slony1;-generated column. </para>
+ 
+ </listitem>
+ <listitem><para> On each node, run an SQL script to alter the table, dropping the extra column.</para> 
+ <para> <command> alter table whatever drop column "_Slony-I_cluster-rowID";</command> </para>
+ 
+ <para> This needs to be run individually against each node.  Depending
+ on your preferences, you might wish to use <xref
+ linkend="stmtddlscript"> to do this. </para>
+ 
+ <para> If the table is a heavily updated one, it is worth observing
+ that this alteration will require acquiring an exclusive lock on the
+ table.  It will not hold this lock for terribly long; dropping the
+ column should be quite a rapid operation as all it does internally is
+ to mark the column as being dropped; it <emphasis>does not</emphasis>
+ require rewriting the entire contents of the table.  Tuples that have
+ values in that column will continue to have that value; new tuples
+ will leave it NULL, and queries will ignore the column.  Space for
+ those columns will get reclaimed as tuples get updated.  </para>
+ 
+ <para> Note that at this point in the process, this table is not being
+ replicated.  If a failure takes place, replication is not, at this
+ point, providing protection on this table.  This is unfortunate but
+ unavoidable. </para>
+ </listitem>
+ 
+ <listitem><para> Make sure the table has a legitimate candidate for
+ primary key, some set of NOT NULL, UNIQUE columns.  </para>
+ 
+ <para> The possible variations to this are the reason that the
+ developers have made no effort to try to assist automation of
+ this.</para>
+ 
+ <itemizedlist>
+ 
+ <listitem><para> If the table is a small one, it may be perfectly
+ reasonable to do alterations (note that they must be applied to
+ <emphasis>every node</emphasis>!) to add a new column, assign it via a
+ new sequence, and then declare it to be a primary key.  </para>
+ 
+ <para> If there are only a few tuples, this should take a fraction of
+ a second, and, with luck, be unnoticeable to a running
+ application. </para>
+ 
+ <para> Even if the table is fairly large, if it is not frequently
+ accessed by the application, the locking of the table that takes place
+ when you run <command>ALTER TABLE</command> may not cause much
+ inconvenience. </para></listitem>
+ 
+ <listitem> <para> If the table is a large one, and is vital to and
+ heavily accessed by the application, then it may be necessary to take
+ an application outage in order to accomplish the alterations, leaving
+ you necessarily somewhat vulnerable until the process is
+ complete. </para>
+ 
+ <para> If it is troublesome to take outages, then the upgrade to
+ &slony1; version 2 may take some planning... </para>
+ </listitem>
+ 
+ </itemizedlist>
+ </listitem>
+ 
+ <listitem><para> Create a new replication set (<xref
+ linkend="stmtcreateset">) and re-add the table to that set (<xref
+ linkend="stmtsetaddtable">).  </para>
+ 
+ <para> If there are multiple tables, they may be handled via a single
+ replication set.</para>
+ </listitem>
+ 
+ <listitem><para> Subscribe the set (<xref linkend="stmtsubscribeset">)
+ on all the nodes desired. </para> </listitem>
+ 
+ <listitem><para> Once subscriptions are complete, merge the set(s) in,
+ if desired (<xref linkend="stmtmergeset">). </para> </listitem>
+ 
+ </itemizedlist>
+ </sect2>
  </sect1>
  <!-- Keep this comment at the end of the file

Index: bestpractices.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/bestpractices.sgml,v
retrieving revision 1.26
retrieving revision 1.27
diff -C2 -d -r1.26 -r1.27
*** bestpractices.sgml	14 Mar 2007 16:53:51 -0000	1.26
--- bestpractices.sgml	18 Apr 2007 15:03:51 -0000	1.27
***************
*** 224,228 ****
  possibility that updates to this table can fail due to the introduced
  unique index, which means that &slony1; has introduced a new failure
! mode for your application.</para>
  </listitem>
  
--- 224,234 ----
  possibility that updates to this table can fail due to the introduced
  unique index, which means that &slony1; has introduced a new failure
! mode for your application.  
! </para>
! 
! <warning><para> In version 2 of &slony1;, <xref
! linkend="stmttableaddkey"> is no longer supported.  You
! <emphasis>must</emphasis> have either a true primary key or a
! candidate primary key.  </para></warning>
  </listitem>
  

Index: slonik_ref.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/slonik_ref.sgml,v
retrieving revision 1.67
retrieving revision 1.68
diff -C2 -d -r1.67 -r1.68
*** slonik_ref.sgml	14 Mar 2007 16:53:51 -0000	1.67
--- slonik_ref.sgml	18 Apr 2007 15:03:51 -0000	1.68
***************
*** 1010,1020 ****
      </para>
      <para>
!      As a last resort, this command can be used to add such an
!      attribute to a table that does not have a primary key. Since
!      this modification can have unwanted side effects, <emphasis>it is
!       strongly recommended that users add a unique and not null
!       attribute by other means.</emphasis>
      </para>
  
      <variablelist>
       <varlistentry><term><literal> NODE ID = ival </literal></term>
--- 1010,1027 ----
      </para>
      <para>
!      As a last resort, <emphasis>in versions of &slony1; prior to
!      2.0</emphasis>, this command can be used to add such an attribute
!      to a table that does not have a primary key. Since this
!      modification can have unwanted side effects, <emphasis>it is
!      strongly recommended that users add a unique and not null
!      attribute by other means.</emphasis>
      </para>
  
+    <para> If you intend to use &slony1; version 2.0, you
+    <emphasis>must</emphasis> arrange for a more proper primary key.
+    &slony1; will not provide one for you, and if you have cases of
+    keys created via <command>TABLE ADD KEY</command>, you cannot
+    expect &slony1; to function properly. </para>
+ 
      <variablelist>
       <varlistentry><term><literal> NODE ID = ival </literal></term>
***************
*** 1082,1085 ****
--- 1089,1103 ----
     <refsect1> <title> Version Information </title>
      <para> This command was introduced in &slony1; 1.0 </para>
+ 
+     <para> This command is <emphasis> no longer supported </emphasis>
+     as of &slony1; version 2.0.  In version 2, the various
+     <quote>catalogue breakages</quote> done in &postgres; versions
+     prior to 8.3 are being eliminated so that schema dumps may be
+     taken from any node.  That leaves the <quote>kludgy</quote>
+     columns created via <command>TABLE ADD KEY</command> as the only
+     thing that prevents <xref linkend="stmtuninstallnode"> from being
+     comprised of the SQL statement <command>drop schema _ClusterName
+     cascade;</command>.</para>
+ 
     </refsect1>
    </refentry>

Index: faq.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/faq.sgml,v
retrieving revision 1.69
retrieving revision 1.70
diff -C2 -d -r1.69 -r1.70
*** faq.sgml	14 Mar 2007 16:53:51 -0000	1.69
--- faq.sgml	18 Apr 2007 15:03:51 -0000	1.70
***************
*** 15,19 ****
  </question>
  
- 
  <answer><para> <productname>Frotznik Freenix</productname> is new to
  me, so it's a bit dangerous to give really hard-and-fast definitive
--- 15,18 ----

Index: addthings.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/addthings.sgml,v
retrieving revision 1.27
retrieving revision 1.28
diff -C2 -d -r1.27 -r1.28
*** addthings.sgml	13 Mar 2007 21:00:11 -0000	1.27
--- addthings.sgml	18 Apr 2007 15:03:51 -0000	1.28
***************
*** 233,237 ****
  drops the schema and its contents, but also removes any columns
  previously added in using <xref linkend= "stmttableaddkey">.
! </para></listitem>
  </itemizedlist>
  </sect2>
--- 233,243 ----
  drops the schema and its contents, but also removes any columns
  previously added in using <xref linkend= "stmttableaddkey">.
! </para>
! 
! <note><para> In &slony1; version 2.0, <xref linkend=
! "stmttableaddkey"> is <emphasis>no longer supported</emphasis>, and
! thus <xref linkend="stmtuninstallnode"> consists very simply of
! <command>DROP SCHEMA "_ClusterName" CASCADE;</command>.  </para>
! </note></listitem>
  </itemizedlist>
  </sect2>

From cbbrowne at lists.slony.info  Wed Apr 18 08:03:53 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 08:03:59 2007
Subject: [Slony1-commit] slony1-engine/src/backend slony1_base.sql
	slony1_funcs.c slony1_funcs.sql
Message-ID: <20070418150354.4F8CB290BCC@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/backend
In directory main.slony.info:/tmp/cvs-serv3274/src/backend

Modified Files:
	slony1_base.sql slony1_funcs.c slony1_funcs.sql 
Log Message:
Patch that removes TABLE ADD KEY from CVS HEAD.  This supercedes the
previous patch:

http://lists.slony.info/pipermail/slony1-patches/2007-April/000008.html

Per Bill Moran's comments, this also drops out usage of sl_rowid_seq.
http://lists.slony.info/pipermail/slony1-general/2007-April/005883.html

It passes test1, as revised in the patch (e.g. - to remove usage of
TABLE ADD KEY)...

----------------------------------------------------
creating origin DB: cbbrowne -h localhost -U cbbrowne -p 5882 slonyregress1
add plpgsql to Origin
loading origin DB with test1/init_schema.sql
setting up user cbbrowne to have weak access to data
done
creating subscriber 2 DB: cbbrowne -h localhost -U cbbrowne -p 5882
slonyregress2
add plpgsql to subscriber
loading subscriber 2 DB from slonyregress1
done
creating cluster
done
storing nodes
done
Granting weak access on Slony-I schema
done
storing paths
done
launching originnode : /opt/OXRS/dbs/pgsql82/bin/slon -s500 -g10 -d2
slony_regress1 "dbname=slonyregress1 host=localhost user=cbbrowne port=5882"
launching: /opt/OXRS/dbs/pgsql82/bin/slon -s500 -g10 -d2 slony_regress1
"dbname=slonyregress2 host=localhost user=cbbrowne port=5882"
subscribing
done
generating 468 transactions of random data
0 %
5 %
10 %
15 %
20 %
25 %
30 %
35 %
40 %
45 %
50 %
55 %
60 %
65 %
70 %
75 %
80 %
85 %
90 %
95 %
100 %
done
launching polling script
loading data
data load complete
completed generate_sync_event() test
completed make_function_strict() test
done
slony is caught up
getting data from origin DB for diffing
done
getting data from node 2 for diffing against origin
comparing
subscriber node 2 is the same as origin node 1
done
**** killing slon node 1
**** killing slon node 2
waiting for slons to die
done
dropping database
slonyregress1
slonyregress2
done
***************************
test test1 completed successfully
***************************


Index: slony1_funcs.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.c,v
retrieving revision 1.57
retrieving revision 1.58
diff -C2 -d -r1.57 -r1.58
*** slony1_funcs.c	16 Jan 2007 21:40:19 -0000	1.57
--- slony1_funcs.c	18 Apr 2007 15:03:51 -0000	1.58
***************
*** 41,44 ****
--- 41,49 ----
  #endif
  
+ /* -- Change from PostgreSQL Ver 8.3 -- */
+ #ifndef VARATT_SIZEP
+ #define VARATT_SIZEP VARATT_SIZEP_DEPRECATED
+ #endif
+ 
  PG_FUNCTION_INFO_V1(_Slony_I_createEvent);
  PG_FUNCTION_INFO_V1(_Slony_I_getLocalNodeId);
***************
*** 1394,1411 ****
  		 */
  		sprintf(query,
! 				"insert into %s.sl_seqlog "
! 				"(seql_seqid, seql_origin, seql_ev_seqno, seql_last_value) "
! 		   "select seq_id, '%d', currval('%s.sl_event_seq'), seq_last_value "
! 				"from %s.sl_seqlastvalue "
! 				"where seq_origin = '%d'; "
! 				"insert into %s.sl_seqlog "
! 				"(seql_seqid, seql_origin, seql_ev_seqno, seql_last_value) "
! 				"select '0', '%d', currval('%s.sl_event_seq'), "
! 				" last_value from %s.sl_rowid_seq; ",
! 				cs->clusterident,
! 				cs->localNodeId, cs->clusterident,
! 				cs->clusterident, cs->localNodeId,
! 				cs->clusterident, cs->localNodeId,
! 				cs->clusterident, cs->clusterident);
  
  		cs->plan_record_sequences = SPI_saveplan(SPI_prepare(query, 0, NULL));
--- 1399,1415 ----
  		 */
  		sprintf(query,
! 			"insert into %s.sl_seqlog "
! 			"(seql_seqid, seql_origin, seql_ev_seqno, seql_last_value) "
! 			"select seq_id, '%d', currval('%s.sl_event_seq'), seq_last_value "
! 			"from %s.sl_seqlastvalue "
! 			"where seq_origin = '%d'; "
! 			"insert into %s.sl_seqlog "
! 			"(seql_seqid, seql_origin, seql_ev_seqno) "
! 			"select '0', '%d', currval('%s.sl_event_seq'); ",
! 			cs->clusterident,
! 			cs->localNodeId, cs->clusterident,
! 			cs->clusterident, cs->localNodeId,
! 			cs->clusterident, cs->localNodeId,
! 			cs->clusterident);
  
  		cs->plan_record_sequences = SPI_saveplan(SPI_prepare(query, 0, NULL));

Index: slony1_base.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_base.sql,v
retrieving revision 1.32
retrieving revision 1.33
diff -C2 -d -r1.32 -r1.33
*** slony1_base.sql	18 Jul 2006 17:59:59 -0000	1.32
--- slony1_base.sql	18 Apr 2007 15:03:51 -0000	1.33
***************
*** 513,527 ****
  
  
- -- ----------------------------------------------------------------------
- -- SEQUENCE sl_rowid_seq
- --
- --	Application tables that do not have a natural primary key must
- --	be modified and an int8 column added that serves as a rowid for us.
- --	The values are assigned with a default from this sequence.
- -- ----------------------------------------------------------------------
- create sequence @NAMESPACE@.sl_rowid_seq;
- grant select, update on @NAMESPACE@.sl_rowid_seq to public;
- comment on sequence @NAMESPACE@.sl_rowid_seq is 'Application tables that do not have a natural primary key must be modified and an int8 column added that serves as a rowid for us.  The values are assigned with a default from this sequence.';
- 
  
  -- ----------------------------------------------------------------------
--- 513,516 ----

Index: slony1_funcs.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.sql,v
retrieving revision 1.106
retrieving revision 1.107
diff -C2 -d -r1.106 -r1.107
*** slony1_funcs.sql	16 Mar 2007 14:43:29 -0000	1.106
--- slony1_funcs.sql	18 Apr 2007 15:03:51 -0000	1.107
***************
*** 719,724 ****
  	-- ----
  	perform setval(''@NAMESPACE@.sl_local_node_id'', p_local_node_id);
- 	perform setval(''@NAMESPACE@.sl_rowid_seq'', 
- 			p_local_node_id::int8 * ''1000000000000000''::int8);
  	perform @NAMESPACE@.storeNode_int (p_local_node_id, p_comment, false);
  	
--- 719,722 ----
***************
*** 4649,4790 ****
  SYNC.  ';
  
- -- ----------------------------------------------------------------------
- -- FUNCTION tableAddKey (tab_fqname)
- --
- --	If the specified table does not have a column 
- --	"_Slony-I_<clustername>_rowID", then add it as a bigint
- --	with default nextval('"_<clustername>".sl_rowid_seq').
- -- ----------------------------------------------------------------------
- create or replace function @NAMESPACE@.tableAddKey(text) returns text
- as '
- declare
- 	p_tab_fqname	alias for $1;
- 	v_tab_fqname_quoted	text default '''';
- 	v_attkind		text default '''';
- 	v_attrow		record;
- 	v_have_serial	bool default ''f'';
- begin
- 	v_tab_fqname_quoted := @NAMESPACE@.slon_quote_input(p_tab_fqname);
- 	--
- 	-- Loop over the attributes of this relation
- 	-- and add a "v" for every user column, and a "k"
- 	-- if we find the Slony-I special serial column.
- 	--
- 	for v_attrow in select PGA.attnum, PGA.attname
- 			from "pg_catalog".pg_class PGC,
- 			    "pg_catalog".pg_namespace PGN,
- 				"pg_catalog".pg_attribute PGA
- 			where @NAMESPACE@.slon_quote_brute(PGN.nspname) || ''.'' ||
- 			    @NAMESPACE@.slon_quote_brute(PGC.relname) = v_tab_fqname_quoted
- 				and PGN.oid = PGC.relnamespace
- 				and PGA.attrelid = PGC.oid
- 				and not PGA.attisdropped
- 				and PGA.attnum > 0
- 			order by attnum
- 	loop
- 		if v_attrow.attname = ''_Slony-I_@CLUSTERNAME@_rowID'' then
- 		    v_attkind := v_attkind || ''k'';
- 			v_have_serial := ''t'';
- 		else
- 			v_attkind := v_attkind || ''v'';
- 		end if;
- 	end loop;
- 	
- 	--
- 	-- A table must have at least one attribute, so not finding
- 	-- anything means the table does not exist.
- 	--
- 	if not found then
- 		raise exception ''Slony-I: tableAddKey(): table % not found'', v_tab_fqname_quoted;
- 	end if;
- 
- 	--
- 	-- If it does not have the special serial column, we
- 	-- have to add it. This will be only half way done.
- 	-- The function to add the table to the set must finish
- 	-- these definitions with NOT NULL and UNIQUE after
- 	-- updating all existing rows.
- 	--
- 	if not v_have_serial then
- 		execute ''lock table '' || v_tab_fqname_quoted ||
- 			'' in access exclusive mode'';
- 		execute ''alter table only '' || v_tab_fqname_quoted ||
- 			'' add column "_Slony-I_@CLUSTERNAME@_rowID" bigint;'';
- 		execute ''alter table only '' || v_tab_fqname_quoted ||
- 			'' alter column "_Slony-I_@CLUSTERNAME@_rowID" '' ||
- 			'' set default "pg_catalog".nextval(''''@NAMESPACE@.sl_rowid_seq'''');'';
- 
- 		v_attkind := v_attkind || ''k'';
- 	end if;
- 
- 	--
- 	-- Return the resulting Slony-I attkind
- 	--
- 	return v_attkind;
- end;
- ' language plpgsql;
- 
- comment on function @NAMESPACE@.tableAddKey(text) is
- 
- 'tableAddKey (tab_fqname) - if the table has not got a column of the
- form _Slony-I_<clustername>_rowID, then add it as a bigint, defaulted
- to nextval() for a sequence created for the cluster.';
- 
- -- ----------------------------------------------------------------------
- -- FUNCTION tableDropKey (tab_id)
- --
- --	If the specified table has a column 
- --	"_Slony-I_<clustername>_rowID", then drop it.
- -- ----------------------------------------------------------------------
- create or replace function @NAMESPACE@.tableDropKey(int4) returns int4
- as '
- declare
- 	p_tab_id		alias for $1;
- 	v_tab_fqname	text;
- 	v_tab_oid		oid;
- begin
- 	-- ----
- 	-- Grab the central configuration lock
- 	-- ----
- 	lock table @NAMESPACE@.sl_config_lock;
- 
- 	-- ----
- 	-- Construct the tables fully qualified name and get its oid
- 	-- ----
- 	select @NAMESPACE@.slon_quote_brute(PGN.nspname) || ''.'' ||
- 				@NAMESPACE@.slon_quote_brute(PGC.relname),
- 				PGC.oid into v_tab_fqname, v_tab_oid
- 			from @NAMESPACE@.sl_table T,
- 				"pg_catalog".pg_class PGC,
- 				"pg_catalog".pg_namespace PGN
- 			where T.tab_id = p_tab_id
- 				and T.tab_reloid = PGC.oid
- 				and PGC.relnamespace = PGN.oid;
- 	if not found then
- 		raise exception ''Slony-I: tableDropKey(): table with ID % not found'', p_tab_id;
- 	end if;
- 
- 	-- ----
- 	-- Drop the special serial ID column if the table has it
- 	-- ----
- 	if exists (select true from "pg_catalog".pg_attribute
- 			where attrelid = v_tab_oid
- 				and attname = ''_Slony-I_@CLUSTERNAME@_rowID'')
- 	then
- 		execute ''lock table '' || v_tab_fqname ||
- 				'' in access exclusive mode'';
- 		execute ''alter table '' || v_tab_fqname ||
- 				'' drop column "_Slony-I_@CLUSTERNAME@_rowID"'';
- 	end if;
- 
- 	return p_tab_id;
- end;
- ' language plpgsql;
- 
- comment on function @NAMESPACE@.tableDropKey(int4) is
- 'tableDropKey (tab_id)
- 
- If the specified table has a column "_Slony-I_<clustername>_rowID",
- then drop it.';
  
  -- ----------------------------------------------------------------------
--- 4647,4650 ----
***************
*** 5022,5109 ****
  primary key (if indexname is NULL).';
  
- -- ----------------------------------------------------------------------
- -- FUNCTION determineAttKindSerial (tab_fqname)
- --
- --	A table was that was specified without a primary key is added
- --	to the replication. Assume that tableAddKey() was called before
- --	and finish the creation of the serial column. The return an
- --	attkind according to that.
- -- ----------------------------------------------------------------------
- create or replace function @NAMESPACE@.determineAttkindSerial(text)
- returns text
- as '
- declare
- 	p_tab_fqname	alias for $1;
- 	v_tab_fqname_quoted	text default '''';
- 	v_attkind		text default '''';
- 	v_attrow		record;
- 	v_have_serial	bool default ''f'';
- begin
- 	v_tab_fqname_quoted := @NAMESPACE@.slon_quote_input(p_tab_fqname);
- 	--
- 	-- Loop over the attributes of this relation
- 	-- and add a "v" for every user column, and a "k"
- 	-- if we find the Slony-I special serial column.
- 	--
- 	for v_attrow in select PGA.attnum, PGA.attname
- 			from "pg_catalog".pg_class PGC,
- 			    "pg_catalog".pg_namespace PGN,
- 				"pg_catalog".pg_attribute PGA
- 			where @NAMESPACE@.slon_quote_brute(PGN.nspname) || ''.'' ||
- 			    @NAMESPACE@.slon_quote_brute(PGC.relname) = v_tab_fqname_quoted
- 				and PGN.oid = PGC.relnamespace
- 				and PGA.attrelid = PGC.oid
- 				and not PGA.attisdropped
- 				and PGA.attnum > 0
- 			order by attnum
- 	loop
- 		if v_attrow.attname = ''_Slony-I_@CLUSTERNAME@_rowID'' then
- 		    v_attkind := v_attkind || ''k'';
- 			v_have_serial := ''t'';
- 		else
- 			v_attkind := v_attkind || ''v'';
- 		end if;
- 	end loop;
- 	
- 	--
- 	-- A table must have at least one attribute, so not finding
- 	-- anything means the table does not exist.
- 	--
- 	if not found then
- 		raise exception ''Slony-I: table % not found'', v_tab_fqname_quoted;
- 	end if;
- 
- 	--
- 	-- If it does not have the special serial column, we
- 	-- should not have been called in the first place.
- 	--
- 	if not v_have_serial then
- 		raise exception ''Slony-I: table % does not have the serial key'',
- 				v_tab_fqname_quoted;
- 	end if;
- 
- 	execute ''update '' || v_tab_fqname_quoted ||
- 		'' set "_Slony-I_@CLUSTERNAME@_rowID" ='' ||
- 		'' "pg_catalog".nextval(''''@NAMESPACE@.sl_rowid_seq'''');'';
- 	execute ''alter table only '' || v_tab_fqname_quoted ||
- 		'' add unique ("_Slony-I_@CLUSTERNAME@_rowID");'';
- 	execute ''alter table only '' || v_tab_fqname_quoted ||
- 		'' alter column "_Slony-I_@CLUSTERNAME@_rowID" '' ||
- 		'' set not null;'';
- 
- 	--
- 	-- Return the resulting Slony-I attkind
- 	--
- 	return v_attkind;
- end;
- ' language plpgsql;
- 
- comment on function @NAMESPACE@.determineAttkindSerial(text) is
- 'determineAttKindSerial (tab_fqname)
- 
- A table was that was specified without a primary key is added to the
- replication. Assume that tableAddKey() was called before and finish
- the creation of the serial column. The return an attkind according to
- that.';
  
  -- ----------------------------------------------------------------------
--- 4882,4885 ----
***************
*** 5237,5275 ****
  
  -- ----------------------------------------------------------------------
- -- FUNCTION tableHasSerialKey (tab_fqname)
- --
- --	Checks if a table has our special serial key column that is
- --	used if the table has no natural unique constraint.
- -- ----------------------------------------------------------------------
- create or replace function @NAMESPACE@.tableHasSerialKey(text) 
- returns bool
- as '
- declare
- 	p_tab_fqname	alias for $1;
- 	v_tab_fqname_quoted	text default '''';
- 	v_attnum		int2;
- begin
- 	v_tab_fqname_quoted := @NAMESPACE@.slon_quote_input(p_tab_fqname);
- 	select PGA.attnum into v_attnum
- 			from "pg_catalog".pg_class PGC,
- 				"pg_catalog".pg_namespace PGN,
- 				"pg_catalog".pg_attribute PGA
- 			where @NAMESPACE@.slon_quote_brute(PGN.nspname) || ''.'' ||
- 				@NAMESPACE@.slon_quote_brute(PGC.relname) = v_tab_fqname_quoted
- 				and PGC.relnamespace = PGN.oid
- 				and PGA.attrelid = PGC.oid
- 				and PGA.attname = ''_Slony-I_@CLUSTERNAME@_rowID''
- 				and not PGA.attisdropped;
- 	return found;
- end;
- ' language plpgsql;
- 
- comment on function @NAMESPACE@.tableHasSerialKey(text) is
- 'tableHasSerialKey (tab_fqname)
- 
- Checks if a table has our special serial key column that is used if
- the table has no natural unique constraint.';
- 
- -- ----------------------------------------------------------------------
  -- FUNCTION updateRelname (set_id, only_on_node)
  --
--- 5013,5016 ----
***************
*** 5809,5813 ****
  	-- Changes for 1.2
  	-- ----
! 	if p_old IN (''1.0.2'', ''1.0.5'', ''1.0.6'', ''1.1.0'', ''1.1.1'', ''1.1.2'', ''1.1.3'', ''1.1.5'', ''1.1.6'', "1.1.7", "1.1.8") then
  		-- Add new table sl_registry
  		execute ''create table @NAMESPACE@.sl_registry (
--- 5550,5554 ----
  	-- Changes for 1.2
  	-- ----
! 	if p_old IN (''1.0.2'', ''1.0.5'', ''1.0.6'', ''1.1.0'', ''1.1.1'', ''1.1.2'', ''1.1.3'', ''1.1.5'', ''1.1.6'', ''1.1.7'', ''1.1.8'', ''1.1.9'') then
  		-- Add new table sl_registry
  		execute ''create table @NAMESPACE@.sl_registry (

From cbbrowne at lists.slony.info  Wed Apr 18 08:28:18 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 08:28:19 2007
Subject: [Slony1-commit] slony1-engine/doc/adminguide bestpractices.sgml
	maintenance.sgml
Message-ID: <20070418152818.331F82902FE@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv4019

Modified Files:
	bestpractices.sgml maintenance.sgml 
Log Message:
Document how to make autovacuum not bother vacuum Slony-I-managed 
tables, per Richard Yen.


Index: maintenance.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/maintenance.sgml,v
retrieving revision 1.26
retrieving revision 1.27
diff -C2 -d -r1.26 -r1.27
*** maintenance.sgml	17 Apr 2007 21:58:47 -0000	1.26
--- maintenance.sgml	18 Apr 2007 15:28:16 -0000	1.27
***************
*** 59,62 ****
--- 59,113 ----
  </para>
  
+ <sect2 id="maintenance-autovac"> <title> Interaction with &postgres; autovacuum </title>
+ 
+ <indexterm><primary>autovacuum interaction</primary></indexterm>
+ 
+ <para> Recent versions of &postgres; support an <quote/autovacuum/
+ process which notices when tables are modified, thereby creating dead
+ tuples, and vacuums those tables, <quote/on demand./ It has been
+ observed that this can interact somewhat negatively with &slony1;'s
+ own  vacuuming policies on its own tables. </para>
+ 
+ <para> &slony1; requests vacuums on its tables immediately after
+ completing transactions that are expected to clean out old data, which
+ may be expected to be the ideal time to do so.  It appears as though
+ autovacuum may notice the changes a bit earlier, and attempts
+ vacuuming when transactions are not complete, rendering the work
+ pretty useless.  It seems preferable to configure autovacuum to avoid
+ vacuum &slony1;-managed configuration tables. </para>
+ 
+ <para> The following query (change the cluster name to match your
+ local configuration) will identify the tables that autovacuum should
+ be configured not to process: </para>
+ 
+ <programlisting>
+ mycluster=# select oid, relname from pg_class where relnamespace = (select oid from pg_namespace where nspname = '_' || 'MyCluster') and relhasindex;
+   oid  |   relname    
+ -------+--------------
+  17946 | sl_nodelock
+  17963 | sl_setsync
+  17994 | sl_trigger
+  17980 | sl_table
+  18003 | sl_sequence
+  17937 | sl_node
+  18034 | sl_listen
+  18017 | sl_path
+  18048 | sl_subscribe
+  17951 | sl_set
+  18062 | sl_event
+  18069 | sl_confirm
+  18074 | sl_seqlog
+  18078 | sl_log_1
+  18085 | sl_log_2
+ (15 rows)
+ </programlisting>
+ 
+ <para> The following query will populate
+ <envar>pg_catalog.pg_autovacuum</envar> with suitable configuration
+ information: <command> insert into pg_catalog.pg_autovacuum (vacrelid,
+ enabled) select oid, 'f' from pg_catalog.pg_class where relnamespace =
+ (select oid from pg_namespace where nspname = '_' || 'MyCluster') and
+ relhasindex; </command> </para>
+ 
  <sect2><title> Watchdogs: Keeping Slons Running</title>
  
***************
*** 89,92 ****
--- 140,144 ----
  <sect2 id="gensync"><title>Parallel to Watchdog: generate_syncs.sh</title>
  
+ <indexterm><primary>generate SYNCs</primary></indexterm>
  <para>A new script for &slony1; 1.1 is
  <application>generate_syncs.sh</application>, which addresses the following kind of
***************
*** 219,222 ****
--- 271,276 ----
  <sect2><title> Other Replication Tests </title>
  
+ <indexterm><primary>testing replication</primary></indexterm>
+ 
  <para> The methodology of the previous section is designed with a view
  to minimizing the cost of submitting replication test queries; on a
***************
*** 288,291 ****
--- 342,346 ----
  </sect2>
  <sect2><title>mkservice </title>
+ <indexterm><primary>mkservice for BSD </primary></indexterm>
  
  <sect3><title>slon-mkservice.sh</title>

Index: bestpractices.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/bestpractices.sgml,v
retrieving revision 1.27
retrieving revision 1.28
diff -C2 -d -r1.27 -r1.28
*** bestpractices.sgml	18 Apr 2007 15:03:51 -0000	1.27
--- bestpractices.sgml	18 Apr 2007 15:28:16 -0000	1.28
***************
*** 144,147 ****
--- 144,156 ----
  </listitem>
  
+ <listitem><para> If you are using the autovacuum process in recent
+ versions of &postgres;, you may wish to leave &slony1; tables out, as
+ &slony1; is a bit more intelligent about vacuuming when it is expected
+ to be conspicuously useful (<emphasis>e.g.</emphasis> - immediately
+ after purging old data) to do so than autovacuum can be. </para>
+ 
+ <para> See <xref linkend="maintenance-autovac"> for more
+ details. </para> </listitem>
+ 
  <listitem> <para> Running all of the &lslon; daemons on a central
  server for each network has proven preferable. </para>

From cbbrowne at lists.slony.info  Wed Apr 18 12:26:57 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 12:26:59 2007
Subject: [Slony1-commit] slony1-engine/tests/testseqnames init_add_tables.ik
	init_schema.sql
Message-ID: <20070418192657.280AB290436@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testseqnames
In directory main.slony.info:/tmp/cvs-serv18264/tests/testseqnames

Modified Files:
	init_add_tables.ik init_schema.sql 
Log Message:
Applied changes made in 1.2 branch to fix "EXECUTE SCRIPT" only on a specified
node to HEAD.

Also fixed up a bunch of tests that were using TABLE ADD KEY (no longer
supported in HEAD).


Index: init_add_tables.ik
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testseqnames/init_add_tables.ik,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** init_add_tables.ik	15 Nov 2005 21:25:35 -0000	1.1
--- init_add_tables.ik	18 Apr 2007 19:26:54 -0000	1.2
***************
*** 1,6 ****
  set add table (id=1, set id=1, origin=1, fully qualified name = 'public.table1', comment='accounts table');
  set add table (id=2, set id=1, origin=1, fully qualified name = 'public.table2', key='table2_id_key');
! table add key (node id = 1, fully qualified name = 'public.table3');
! set add table (id=3, set id=1, origin=1, fully qualified name = 'public.table3', key = SERIAL);
  set add sequence (set id = 1, origin = 1, id = 1, fully qualified name = 'public."Evil Spacey Sequence Name"');
  
--- 1,5 ----
  set add table (id=1, set id=1, origin=1, fully qualified name = 'public.table1', comment='accounts table');
  set add table (id=2, set id=1, origin=1, fully qualified name = 'public.table2', key='table2_id_key');
! set add table (id=3, set id=1, origin=1, fully qualified name = 'public.table3');
  set add sequence (set id = 1, origin = 1, id = 1, fully qualified name = 'public."Evil Spacey Sequence Name"');
  

Index: init_schema.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testseqnames/init_schema.sql,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** init_schema.sql	15 Nov 2005 21:25:35 -0000	1.1
--- init_schema.sql	18 Apr 2007 19:26:54 -0000	1.2
***************
*** 17,21 ****
    mod_date	TIMESTAMPTZ	NOT NULL DEFAULT now(),
    data		FLOAT		NOT NULL DEFAULT random()
!   CONSTRAINT table3_date_check	CHECK (mod_date <= now())
  ); 
  
--- 17,22 ----
    mod_date	TIMESTAMPTZ	NOT NULL DEFAULT now(),
    data		FLOAT		NOT NULL DEFAULT random()
!   CONSTRAINT table3_date_check	CHECK (mod_date <= now()),
!   primary key (id)
  ); 
  

From cbbrowne at lists.slony.info  Wed Apr 18 12:26:56 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 12:26:59 2007
Subject: [Slony1-commit] slony1-engine/tests/testschemanames
	init_add_tables.ik init_schema.sql
Message-ID: <20070418192657.026732902CF@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testschemanames
In directory main.slony.info:/tmp/cvs-serv18264/tests/testschemanames

Modified Files:
	init_add_tables.ik init_schema.sql 
Log Message:
Applied changes made in 1.2 branch to fix "EXECUTE SCRIPT" only on a specified
node to HEAD.

Also fixed up a bunch of tests that were using TABLE ADD KEY (no longer
supported in HEAD).


Index: init_add_tables.ik
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testschemanames/init_add_tables.ik,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** init_add_tables.ik	15 Nov 2005 21:25:34 -0000	1.1
--- init_add_tables.ik	18 Apr 2007 19:26:54 -0000	1.2
***************
*** 1,6 ****
  set add table (id=1, set id=1, origin=1, fully qualified name = 'public.table1', comment='accounts table');
  set add table (id=2, set id=1, origin=1, fully qualified name = 'public.table2', key='table2_id_key');
! table add key (node id = 1, fully qualified name = 'public.table3');
! set add table (id=3, set id=1, origin=1, fully qualified name = 'public.table3', key = SERIAL);
  
  set add table (set id = 1, origin = 1, id = 6, fully qualified name =
--- 1,5 ----
  set add table (id=1, set id=1, origin=1, fully qualified name = 'public.table1', comment='accounts table');
  set add table (id=2, set id=1, origin=1, fully qualified name = 'public.table2', key='table2_id_key');
! set add table (id=3, set id=1, origin=1, fully qualified name = 'public.table3');
  
  set add table (set id = 1, origin = 1, id = 6, fully qualified name =

Index: init_schema.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testschemanames/init_schema.sql,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** init_schema.sql	15 Nov 2005 21:25:34 -0000	1.1
--- init_schema.sql	18 Apr 2007 19:26:54 -0000	1.2
***************
*** 17,21 ****
    mod_date	TIMESTAMPTZ	NOT NULL DEFAULT now(),
    data		FLOAT		NOT NULL DEFAULT random()
!   CONSTRAINT table3_date_check	CHECK (mod_date <= now())
  ); 
  
--- 17,22 ----
    mod_date	TIMESTAMPTZ	NOT NULL DEFAULT now(),
    data		FLOAT		NOT NULL DEFAULT random()
!   CONSTRAINT table3_date_check	CHECK (mod_date <= now()),
!   primary key (id)
  ); 
  

From cbbrowne at lists.slony.info  Wed Apr 18 12:26:57 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 12:27:00 2007
Subject: [Slony1-commit] slony1-engine/tests/testutf8 init_add_tables.ik
	init_schema.sql
Message-ID: <20070418192657.3EC59290456@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testutf8
In directory main.slony.info:/tmp/cvs-serv18264/tests/testutf8

Modified Files:
	init_add_tables.ik init_schema.sql 
Log Message:
Applied changes made in 1.2 branch to fix "EXECUTE SCRIPT" only on a specified
node to HEAD.

Also fixed up a bunch of tests that were using TABLE ADD KEY (no longer
supported in HEAD).


Index: init_add_tables.ik
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testutf8/init_add_tables.ik,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** init_add_tables.ik	15 Nov 2005 21:25:36 -0000	1.1
--- init_add_tables.ik	18 Apr 2007 19:26:55 -0000	1.2
***************
*** 1,5 ****
  set add table (id=1, set id=1, origin=1, fully qualified name = 'public.table1', comment='accounts table');
  set add table (id=2, set id=1, origin=1, fully qualified name = 'public.table2', key='table2_id_key');
! table add key (node id = 1, fully qualified name = 'public.table3');
! set add table (id=3, set id=1, origin=1, fully qualified name = 'public.table3', key = SERIAL);
  set add table (id=4, set id=1, origin=1, fully qualified name = 'public.utf8table', comment='Test table for multibyte/UTF8');
--- 1,4 ----
  set add table (id=1, set id=1, origin=1, fully qualified name = 'public.table1', comment='accounts table');
  set add table (id=2, set id=1, origin=1, fully qualified name = 'public.table2', key='table2_id_key');
! set add table (id=3, set id=1, origin=1, fully qualified name = 'public.table3');
  set add table (id=4, set id=1, origin=1, fully qualified name = 'public.utf8table', comment='Test table for multibyte/UTF8');

Index: init_schema.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testutf8/init_schema.sql,v
retrieving revision 1.3
retrieving revision 1.4
diff -C2 -d -r1.3 -r1.4
*** init_schema.sql	14 Mar 2007 15:54:07 -0000	1.3
--- init_schema.sql	18 Apr 2007 19:26:55 -0000	1.4
***************
*** 17,21 ****
    mod_date	TIMESTAMPTZ	NOT NULL DEFAULT now(),
    data		FLOAT		NOT NULL DEFAULT random()
!   CONSTRAINT table3_date_check	CHECK (mod_date <= now())
  ); 
  
--- 17,22 ----
    mod_date	TIMESTAMPTZ	NOT NULL DEFAULT now(),
    data		FLOAT		NOT NULL DEFAULT random()
!   CONSTRAINT table3_date_check	CHECK (mod_date <= now()),
!   primary key (id)
  ); 
  

From cbbrowne at lists.slony.info  Wed Apr 18 12:26:57 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 12:27:00 2007
Subject: [Slony1-commit] slony1-engine/tests/testtabnames init_add_tables.ik
	init_schema.sql
Message-ID: <20070418192657.3A75C290440@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testtabnames
In directory main.slony.info:/tmp/cvs-serv18264/tests/testtabnames

Modified Files:
	init_add_tables.ik init_schema.sql 
Log Message:
Applied changes made in 1.2 branch to fix "EXECUTE SCRIPT" only on a specified
node to HEAD.

Also fixed up a bunch of tests that were using TABLE ADD KEY (no longer
supported in HEAD).


Index: init_add_tables.ik
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testtabnames/init_add_tables.ik,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** init_add_tables.ik	15 Nov 2005 21:25:35 -0000	1.1
--- init_add_tables.ik	18 Apr 2007 19:26:55 -0000	1.2
***************
*** 1,6 ****
  set add table (id=1, set id=1, origin=1, fully qualified name = 'public.table1', comment='accounts table');
  set add table (id=2, set id=1, origin=1, fully qualified name = 'public.table2', key='table2_id_key');
! table add key (node id = 1, fully qualified name = 'public.table3');
! set add table (id=3, set id=1, origin=1, fully qualified name = 'public.table3', key = SERIAL);
  
  set add table (set id = 1, origin = 1, id = 5, fully qualified name =
--- 1,5 ----
  set add table (id=1, set id=1, origin=1, fully qualified name = 'public.table1', comment='accounts table');
  set add table (id=2, set id=1, origin=1, fully qualified name = 'public.table2', key='table2_id_key');
! set add table (id=3, set id=1, origin=1, fully qualified name = 'public.table3');
  
  set add table (set id = 1, origin = 1, id = 5, fully qualified name =

Index: init_schema.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testtabnames/init_schema.sql,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** init_schema.sql	15 Nov 2005 21:25:36 -0000	1.1
--- init_schema.sql	18 Apr 2007 19:26:55 -0000	1.2
***************
*** 17,21 ****
    mod_date	TIMESTAMPTZ	NOT NULL DEFAULT now(),
    data		FLOAT		NOT NULL DEFAULT random()
!   CONSTRAINT table3_date_check	CHECK (mod_date <= now())
  ); 
  
--- 17,22 ----
    mod_date	TIMESTAMPTZ	NOT NULL DEFAULT now(),
    data		FLOAT		NOT NULL DEFAULT random()
!   CONSTRAINT table3_date_check	CHECK (mod_date <= now()),
!   primary key (id)
  ); 
  

From cbbrowne at lists.slony.info  Wed Apr 18 12:26:56 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 12:27:00 2007
Subject: [Slony1-commit] slony1-engine/tests/testddl README
	ddl_update_part2.sql generate_dml.sh individual_ddl.sh
	init_add_tables.ik init_schema.sql init_subscribe_set.ik
Message-ID: <20070418192658.35254290456@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testddl
In directory main.slony.info:/tmp/cvs-serv18264/tests/testddl

Modified Files:
	README ddl_update_part2.sql generate_dml.sh individual_ddl.sh 
	init_add_tables.ik init_schema.sql init_subscribe_set.ik 
Log Message:
Applied changes made in 1.2 branch to fix "EXECUTE SCRIPT" only on a specified
node to HEAD.

Also fixed up a bunch of tests that were using TABLE ADD KEY (no longer
supported in HEAD).


Index: ddl_update_part2.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testddl/ddl_update_part2.sql,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** ddl_update_part2.sql	30 Mar 2007 22:44:05 -0000	1.2
--- ddl_update_part2.sql	18 Apr 2007 19:26:54 -0000	1.3
***************
*** 4,5 ****
--- 4,6 ----
  update table1 set seqed = nextval('t1seq');
  alter table table1 add constraint seqed_unique UNIQUE(seqed);
+ 

Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testddl/generate_dml.sh,v
retrieving revision 1.7
retrieving revision 1.8
diff -C2 -d -r1.7 -r1.8
*** generate_dml.sh	30 Mar 2007 22:44:05 -0000	1.7
--- generate_dml.sh	18 Apr 2007 19:26:54 -0000	1.8
***************
*** 23,27 ****
  generate_initdata()
  {
!   numrows=$(random_number 50 1000)
    i=0;
    trippoint=`expr $numrows / 20`
--- 23,27 ----
  generate_initdata()
  {
!   numrows=$(random_number 150 350)
    i=0;
    trippoint=`expr $numrows / 20`
***************
*** 93,97 ****
        init_preamble
        sh ${testname}/individual_ddl.sh ${testname} ${node} >> ${SCRIPT}
!       status "execute DDL script only on node ${node}"
        do_ik
    done
--- 93,97 ----
        init_preamble
        sh ${testname}/individual_ddl.sh ${testname} ${node} >> ${SCRIPT}
!       status "execute DDL script on node ${node}"
        do_ik
    done

Index: individual_ddl.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testddl/individual_ddl.sh,v
retrieving revision 1.3
retrieving revision 1.4
diff -C2 -d -r1.3 -r1.4
*** individual_ddl.sh	2 Apr 2007 18:52:18 -0000	1.3
--- individual_ddl.sh	18 Apr 2007 19:26:54 -0000	1.4
***************
*** 5,9 ****
         SET ID = 1,
         FILENAME = '${testname}/ddl_update_part2.sql',
!        EVENT NODE = 1,
         EXECUTE ONLY ON = ${node}
      );
--- 5,9 ----
         SET ID = 1,
         FILENAME = '${testname}/ddl_update_part2.sql',
!        EVENT NODE = ${node},
         EXECUTE ONLY ON = ${node}
      );

Index: init_add_tables.ik
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testddl/init_add_tables.ik,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** init_add_tables.ik	9 Jan 2006 20:12:56 -0000	1.2
--- init_add_tables.ik	18 Apr 2007 19:26:54 -0000	1.3
***************
*** 1,6 ****
  set add table (id=1, set id=1, origin=1, fully qualified name = 'public.table1', comment='accounts table');
  set add table (id=2, set id=1, origin=1, fully qualified name = 'public.table2', key='table2_id_key');
! table add key (node id = 1, fully qualified name = 'public.table3');
! set add table (id=3, set id=1, origin=1, fully qualified name = 'public.table3', key = SERIAL);
  set add table (id=4, set id=1, origin=1, fully qualified name = 'public.table4');
  set add table (id=5, set id=1, origin=1, fully qualified name = 'public.billing_discount');
--- 1,5 ----
  set add table (id=1, set id=1, origin=1, fully qualified name = 'public.table1', comment='accounts table');
  set add table (id=2, set id=1, origin=1, fully qualified name = 'public.table2', key='table2_id_key');
! set add table (id=3, set id=1, origin=1, fully qualified name = 'public.table3');
  set add table (id=4, set id=1, origin=1, fully qualified name = 'public.table4');
  set add table (id=5, set id=1, origin=1, fully qualified name = 'public.billing_discount');

Index: init_schema.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testddl/init_schema.sql,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** init_schema.sql	9 Jan 2006 20:12:56 -0000	1.2
--- init_schema.sql	18 Apr 2007 19:26:54 -0000	1.3
***************
*** 17,21 ****
    mod_date	TIMESTAMPTZ	NOT NULL DEFAULT now(),
    data		FLOAT		NOT NULL DEFAULT random()
!   CONSTRAINT table3_date_check	CHECK (mod_date <= now())
  ); 
  
--- 17,22 ----
    mod_date	TIMESTAMPTZ	NOT NULL DEFAULT now(),
    data		FLOAT		NOT NULL DEFAULT random()
!   CONSTRAINT table3_date_check	CHECK (mod_date <= now()),
!   primary key(id)
  ); 
  

Index: README
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testddl/README,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** README	18 Nov 2005 17:15:21 -0000	1.1
--- README	18 Apr 2007 19:26:54 -0000	1.2
***************
*** 10,11 ****
--- 10,14 ----
  4.  Inserting new data as part of the script
  5.  Updating data as part of the script
+ 
+ 2007-03-31 - Add in a third node, and add in a series of "EXECUTE ONLY
+ ON" clauses.

Index: init_subscribe_set.ik
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testddl/init_subscribe_set.ik,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** init_subscribe_set.ik	30 Mar 2007 22:44:05 -0000	1.2
--- init_subscribe_set.ik	18 Apr 2007 19:26:54 -0000	1.3
***************
*** 1,2 ****
--- 1,4 ----
+ echo 'sleep a couple seconds';
+ sleep (seconds = 2);
  subscribe set ( id = 1, provider = 1, receiver = 2, forward = no);
  sync(id=1);

From cbbrowne at lists.slony.info  Wed Apr 18 12:26:56 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 12:27:00 2007
Subject: [Slony1-commit] slony1-engine/src/backend slony1_funcs.sql
	slony1_funcs.v73.sql slony1_funcs.v74.sql slony1_funcs.v80.sql
Message-ID: <20070418192658.2A82A290440@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/backend
In directory main.slony.info:/tmp/cvs-serv18264/src/backend

Modified Files:
	slony1_funcs.sql slony1_funcs.v73.sql slony1_funcs.v74.sql 
	slony1_funcs.v80.sql 
Log Message:
Applied changes made in 1.2 branch to fix "EXECUTE SCRIPT" only on a specified
node to HEAD.

Also fixed up a bunch of tests that were using TABLE ADD KEY (no longer
supported in HEAD).


Index: slony1_funcs.v73.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.v73.sql,v
retrieving revision 1.9
retrieving revision 1.10
diff -C2 -d -r1.9 -r1.10
*** slony1_funcs.v73.sql	11 Nov 2005 13:53:24 -0000	1.9
--- slony1_funcs.v73.sql	18 Apr 2007 19:26:54 -0000	1.10
***************
*** 103,104 ****
--- 103,113 ----
  'Returns 1 or 0 based on whether or not the DB is running a
  version earlier than 7.4';
+ 
+ create or replace function @NAMESPACE@.make_function_strict (text, text) returns void as
+ '
+    update "pg_catalog"."pg_proc" set proisstrict = ''t'' where 
+            proname = $1 and pronamespace = (select oid from "pg_catalog"."pg_namespace" where nspname = '_@CLUSTERNAME@') and prolang = (select oid from "pg_catalog"."pg_language" where lanname = ''c'');
+ ' language sql;
+ 
+ comment on function @NAMESPACE@.make_function_strict (text, text) is
+ 'Equivalent to 8.1+ ALTER FUNCTION ... STRICT';

Index: slony1_funcs.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.sql,v
retrieving revision 1.107
retrieving revision 1.108
diff -C2 -d -r1.107 -r1.108
*** slony1_funcs.sql	18 Apr 2007 15:03:51 -0000	1.107
--- slony1_funcs.sql	18 Apr 2007 19:26:54 -0000	1.108
***************
*** 3691,3705 ****
  		raise exception ''Slony-I: set % not found'', p_set_id;
  	end if;
! 	if v_set_origin <> @NAMESPACE@.getLocalNodeId(''_@CLUSTERNAME@'') then
! 		raise exception ''Slony-I: set % does not originate on local node'',
  				p_set_id;
  	end if;
- 
- 	-- ----
- 	-- Create a SYNC event, run the script and generate the DDL_SCRIPT event
- 	-- ----
-     perform @NAMESPACE@.alterTableRestore(tab_id) from @NAMESPACE@.sl_table where tab_set in (select set_id from @NAMESPACE@.sl_set where set_origin = @NAMESPACE@.getLocalNodeId(''_@CLUSTERNAME@''));
- 
- 	perform @NAMESPACE@.createEvent(''_@CLUSTERNAME@'', ''SYNC'', NULL);
  	return 1;
  end;
--- 3691,3710 ----
  		raise exception ''Slony-I: set % not found'', p_set_id;
  	end if;
! 	if p_only_on_node = -1 then
! 		if v_set_origin <> @NAMESPACE@.getLocalNodeId(''_@CLUSTERNAME@'') then
! 			raise exception ''Slony-I: set % does not originate on local node'',
  				p_set_id;
+ 		end if;
+ 		-- ----
+ 		-- Create a SYNC event, run the script and generate the DDL_SCRIPT event
+ 		-- ----
+ 		perform @NAMESPACE@.createEvent(''_@CLUSTERNAME@'', ''SYNC'', NULL);
+ 		perform @NAMESPACE@.alterTableRestore(tab_id) from @NAMESPACE@.sl_table where tab_set in (select set_id from @NAMESPACE@.sl_set where set_origin = @NAMESPACE@.getLocalNodeId(''_@CLUSTERNAME@''));
+ 	else
+ 		-- ----
+ 		-- If doing "only on one node" - restore ALL tables irrespective of set
+ 		-- ----
+ 		perform @NAMESPACE@.alterTableRestore(tab_id) from @NAMESPACE@.sl_table;
  	end if;
  	return 1;
  end;
***************
*** 3726,3732 ****
  begin
  	perform @NAMESPACE@.updateRelname(p_set_id, p_only_on_node);
!     perform @NAMESPACE@.alterTableForReplication(tab_id) from @NAMESPACE@.sl_table where tab_set in (select set_id from @NAMESPACE@.sl_set where set_origin = @NAMESPACE@.getLocalNodeId(''_@CLUSTERNAME@''));
! 	return  @NAMESPACE@.createEvent(''_@CLUSTERNAME@'', ''DDL_SCRIPT'', 
  			p_set_id, p_script, p_only_on_node);
  end;
  ' language plpgsql;
--- 3731,3743 ----
  begin
  	perform @NAMESPACE@.updateRelname(p_set_id, p_only_on_node);
! 	if p_only_on_node = -1 then
! 		perform @NAMESPACE@.alterTableForReplication(tab_id) from @NAMESPACE@.sl_table where tab_set in (select set_id from @NAMESPACE@.sl_set where set_origin = @NAMESPACE@.getLocalNodeId(''_@CLUSTERNAME@''));
! 
! 		return  @NAMESPACE@.createEvent(''_@CLUSTERNAME@'', ''DDL_SCRIPT'', 
  			p_set_id, p_script, p_only_on_node);
+ 	else
+ 		perform @NAMESPACE@.alterTableForReplication(tab_id) from @NAMESPACE@.sl_table;
+ 	end if;
+ 	return NULL;
  end;
  ' language plpgsql;

Index: slony1_funcs.v80.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.v80.sql,v
retrieving revision 1.4
retrieving revision 1.5
diff -C2 -d -r1.4 -r1.5
*** slony1_funcs.v80.sql	8 Feb 2007 18:01:15 -0000	1.4
--- slony1_funcs.v80.sql	18 Apr 2007 19:26:54 -0000	1.5
***************
*** 116,125 ****
  version earlier than 7.4';
  
! create or replace function @NAMESPACE@.make_function_strict (text, text) returns void as
  '
     update "pg_catalog"."pg_proc" set proisstrict = ''t'' where 
!      proname = $1 and pronamespace = (select oid from "pg_catalog"."pg_namespace" where nspname = '@NAMESPACE@') and prolang = (select oid from "pg_catalog"."pg_language" where lanname = ''c'');
! ' language sql;
  
  comment on function @NAMESPACE@.make_function_strict (text, text) is
! 'Equivalent to 8.1+ ALTER FUNCTION ... STRICT';
\ No newline at end of file
--- 116,128 ----
  version earlier than 7.4';
  
! create or replace function @NAMESPACE@.make_function_strict (text, text) returns integer as
  '
+ begin
     update "pg_catalog"."pg_proc" set proisstrict = ''t'' where 
!      proname = $1 and pronamespace = (select oid from "pg_catalog"."pg_namespace" where nspname = ''_@CLUSTERNAME@'') and prolang = (select oid from "pg_catalog"."pg_language" where lanname = ''c'');
!    return 1 ;
! end
! ' language plpgsql;
  
  comment on function @NAMESPACE@.make_function_strict (text, text) is
! 'Equivalent to 8.1+ ALTER FUNCTION ... STRICT';

Index: slony1_funcs.v74.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.v74.sql,v
retrieving revision 1.10
retrieving revision 1.11
diff -C2 -d -r1.10 -r1.11
*** slony1_funcs.v74.sql	14 Mar 2007 15:53:16 -0000	1.10
--- slony1_funcs.v74.sql	18 Apr 2007 19:26:54 -0000	1.11
***************
*** 103,111 ****
  'Returns 1 or 0 based on whether or not the DB is running a
  version earlier than 7.4';
  create or replace function @NAMESPACE@.make_function_strict (text, text) returns void as
  '
     update "pg_catalog"."pg_proc" set proisstrict = ''t'' where 
!            proname = $1 and pronamespace = (select oid from "pg_catalog"."pg_namespace" where nspname = ''_@CLUSTERNAME@'') and prolang = (select oid from "pg_catalog"."pg_language" where lanname = ''c'');
! ' language sql;
  
  comment on function @NAMESPACE@.make_function_strict (text, text) is
--- 103,115 ----
  'Returns 1 or 0 based on whether or not the DB is running a
  version earlier than 7.4';
+ 
  create or replace function @NAMESPACE@.make_function_strict (text, text) returns void as
  '
+ begin
     update "pg_catalog"."pg_proc" set proisstrict = ''t'' where 
!      proname = $1 and pronamespace = (select oid from "pg_catalog"."pg_namespace" where nspname = ''_@CLUSTERNAME@'') and prolang = (select oid from "pg_catalog"."pg_language" where lanname = ''c'');
!    return 1 ;
! end
! ' language plpgsql;
  
  comment on function @NAMESPACE@.make_function_strict (text, text) is

From cbbrowne at lists.slony.info  Wed Apr 18 12:28:29 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 12:28:31 2007
Subject: [Slony1-commit] slony1-engine RELEASE-1.2.10
Message-ID: <20070418192829.55781290436@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine
In directory main.slony.info:/tmp/cvs-serv18573

Modified Files:
      Tag: REL_1_2_STABLE
	RELEASE-1.2.10 
Log Message:
Fixed problem with EXECUTE SCRIPT (EXECUTE ONLY ON = <node>)

  - The script was being executed on too many nodes...


Index: RELEASE-1.2.10
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/Attic/RELEASE-1.2.10,v
retrieving revision 1.1.2.1
retrieving revision 1.1.2.2
diff -C2 -d -r1.1.2.1 -r1.1.2.2
*** RELEASE-1.2.10	3 Apr 2007 21:55:03 -0000	1.1.2.1
--- RELEASE-1.2.10	18 Apr 2007 19:28:27 -0000	1.1.2.2
***************
*** 4,5 ****
--- 4,6 ----
  
    - The script was being executed on too many nodes...
+ 

From cbbrowne at lists.slony.info  Wed Apr 18 12:28:29 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 12:28:31 2007
Subject: [Slony1-commit] slony1-engine/src/backend slony1_funcs.c
	slony1_funcs.sql
Message-ID: <20070418192829.8E89E290440@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/backend
In directory main.slony.info:/tmp/cvs-serv18573/src/backend

Modified Files:
      Tag: REL_1_2_STABLE
	slony1_funcs.c slony1_funcs.sql 
Log Message:
Fixed problem with EXECUTE SCRIPT (EXECUTE ONLY ON = <node>)

  - The script was being executed on too many nodes...


Index: slony1_funcs.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.c,v
retrieving revision 1.53
retrieving revision 1.53.2.1
diff -C2 -d -r1.53 -r1.53.2.1
*** slony1_funcs.c	7 Sep 2006 13:21:16 -0000	1.53
--- slony1_funcs.c	18 Apr 2007 19:28:27 -0000	1.53.2.1
***************
*** 41,44 ****
--- 41,49 ----
  #endif
  
+ /* -- Change from PostgreSQL Ver 8.3 -- */
+ #ifndef VARATT_SIZEP
+ #define VARATT_SIZEP VARATT_SIZEP_DEPRECATED
+ #endif
+ 
  PG_FUNCTION_INFO_V1(_Slony_I_createEvent);
  PG_FUNCTION_INFO_V1(_Slony_I_getLocalNodeId);

Index: slony1_funcs.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.sql,v
retrieving revision 1.98.2.14
retrieving revision 1.98.2.15
diff -C2 -d -r1.98.2.14 -r1.98.2.15
*** slony1_funcs.sql	3 Apr 2007 21:55:03 -0000	1.98.2.14
--- slony1_funcs.sql	18 Apr 2007 19:28:27 -0000	1.98.2.15
***************
*** 3738,3745 ****
  begin
  	perform @NAMESPACE@.updateRelname(p_set_id, p_only_on_node);
- 	perform @NAMESPACE@.alterTableForReplication(tab_id) from @NAMESPACE@.sl_table where tab_set in (select set_id from @NAMESPACE@.sl_set where set_origin = @NAMESPACE@.getLocalNodeId(''_@CLUSTERNAME@''));
  	if p_only_on_node = -1 then
! 	return  @NAMESPACE@.createEvent(''_@CLUSTERNAME@'', ''DDL_SCRIPT'', 
  			p_set_id, p_script, p_only_on_node);
  	end if;
  	return NULL;
--- 3738,3748 ----
  begin
  	perform @NAMESPACE@.updateRelname(p_set_id, p_only_on_node);
  	if p_only_on_node = -1 then
! 		perform @NAMESPACE@.alterTableForReplication(tab_id) from @NAMESPACE@.sl_table where tab_set in (select set_id from @NAMESPACE@.sl_set where set_origin = @NAMESPACE@.getLocalNodeId(''_@CLUSTERNAME@''));
! 
! 		return  @NAMESPACE@.createEvent(''_@CLUSTERNAME@'', ''DDL_SCRIPT'', 
  			p_set_id, p_script, p_only_on_node);
+ 	else
+ 		perform @NAMESPACE@.alterTableForReplication(tab_id) from @NAMESPACE@.sl_table;
  	end if;
  	return NULL;

From cbbrowne at lists.slony.info  Wed Apr 18 12:28:29 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 12:28:31 2007
Subject: [Slony1-commit] slony1-engine/src/slonik dbutil.c slonik.c
Message-ID: <20070418192829.D41D6290436@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slonik
In directory main.slony.info:/tmp/cvs-serv18573/src/slonik

Modified Files:
      Tag: REL_1_2_STABLE
	dbutil.c slonik.c 
Log Message:
Fixed problem with EXECUTE SCRIPT (EXECUTE ONLY ON = <node>)

  - The script was being executed on too many nodes...


Index: dbutil.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/dbutil.c,v
retrieving revision 1.12
retrieving revision 1.12.2.1
diff -C2 -d -r1.12 -r1.12.2.1
*** dbutil.c	28 Jun 2006 04:47:14 -0000	1.12
--- dbutil.c	18 Apr 2007 19:28:27 -0000	1.12.2.1
***************
*** 12,19 ****
  
  
  #include <stdio.h>
  #include <stdlib.h>
  #include <stdarg.h>
- #ifndef WIN32
  #include <unistd.h>
  #include <sys/types.h>
--- 12,19 ----
  
  
+ #ifndef WIN32
  #include <stdio.h>
  #include <stdlib.h>
  #include <stdarg.h>
  #include <unistd.h>
  #include <sys/types.h>

Index: slonik.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/slonik.c,v
retrieving revision 1.67.2.7
retrieving revision 1.67.2.8
diff -C2 -d -r1.67.2.7 -r1.67.2.8
*** slonik.c	3 Apr 2007 21:55:04 -0000	1.67.2.7
--- slonik.c	18 Apr 2007 19:28:27 -0000	1.67.2.8
***************
*** 12,19 ****
  
  
  #include <stdio.h>
  #include <stdlib.h>
  #include <stdarg.h>
- #ifndef WIN32
  #include <unistd.h>
  #include <fcntl.h>
--- 12,19 ----
  
  
+ #ifndef WIN32
  #include <stdio.h>
  #include <stdlib.h>
  #include <stdarg.h>
  #include <unistd.h>
  #include <fcntl.h>
***************
*** 21,31 ****
  #include <sys/types.h>
  #include <sys/wait.h>
  #else
  #define sleep(x) Sleep(x*1000)
  #define vsnprintf _vsnprintf
- #define INT64_FORMAT "%I64d"
  #endif
- #include <errno.h>
- #include <time.h>
  
  #include "postgres.h"
--- 21,30 ----
  #include <sys/types.h>
  #include <sys/wait.h>
+ #include <errno.h>
+ #include <time.h>
  #else
  #define sleep(x) Sleep(x*1000)
  #define vsnprintf _vsnprintf
  #endif
  
  #include "postgres.h"

From cbbrowne at lists.slony.info  Wed Apr 18 14:20:24 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 14:20:24 2007
Subject: [Slony1-commit] slony1-engine/src/backend Makefile slony1_funcs.c
Message-ID: <20070418212024.3D139290453@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/backend
In directory main.slony.info:/tmp/cvs-serv26983

Modified Files:
	Makefile slony1_funcs.c 
Log Message:
Typing changes based on recommendations by splint (splint.org)

- slon_quote_identifier() becomes a static function
- xcnt transforms from int to uint32
- added a number of annotations (C comments) to diminish # of warnings


Index: slony1_funcs.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.c,v
retrieving revision 1.58
retrieving revision 1.59
diff -C2 -d -r1.58 -r1.59
*** slony1_funcs.c	18 Apr 2007 15:03:51 -0000	1.58
--- slony1_funcs.c	18 Apr 2007 21:20:22 -0000	1.59
***************
*** 36,39 ****
--- 36,42 ----
  #include <signal.h>
  #include <errno.h>
+ /*@+matchanyintegral@*/
+ /*@-compmempass@*/
+ /*@-immediatetrans@*/
  
  #ifdef PG_MODULE_MAGIC
***************
*** 73,77 ****
  Datum		_slon_quote_ident(PG_FUNCTION_ARGS);
  
- 
  #ifdef CYGWIN
  extern DLLIMPORT Node *newNodeMacroHolder;
--- 76,79 ----
***************
*** 122,131 ****
  }	Slony_I_ClusterStatus;
  
! 
! static Slony_I_ClusterStatus *clusterStatusList = NULL;
  static Slony_I_ClusterStatus *
  getClusterStatus(Name cluster_name,
  				 int need_plan_mask);
! const char *slon_quote_identifier(const char *ident);
  static char *slon_quote_literal(char *str);
  
--- 124,133 ----
  }	Slony_I_ClusterStatus;
  
! /*@null@*/
! static Slony_I_ClusterStatus *clusterStatusList = NULL;  
  static Slony_I_ClusterStatus *
  getClusterStatus(Name cluster_name,
  				 int need_plan_mask);
! static const char *slon_quote_identifier(const char *ident);
  static char *slon_quote_literal(char *str);
  
***************
*** 143,147 ****
  	size_t		buf_size;
  	int			rc;
! 	int			xcnt;
  	char	   *cp;
  	int			i;
--- 145,149 ----
  	size_t		buf_size;
  	int			rc;
! 	uint32			xcnt;
  	char	   *cp;
  	int			i;
***************
*** 173,179 ****
--- 175,183 ----
  		 * Once per transaction notify on the sl_event relation
  		 */
+ /*@-nullpass@*/
  		if ((rc = SPI_execp(cs->plan_notify_event, NULL, NULL, 0)) < 0)
  			elog(ERROR, "Slony-I: SPI_execp() failed for \"NOTIFY event\"");
  
+ /*@+nullpass@*/
  		cs->currentXid = newXid;
  	}
***************
*** 184,187 ****
--- 188,193 ----
  	 */
  	*(cp = buf) = '\0';
+ 	/*@-nullderef@*/
+ 	/*@-mustfreeonly@*/
  	for (xcnt = 0; xcnt < SerializableSnapshot->xcnt; xcnt++)
  	{
***************
*** 192,199 ****
--- 198,209 ----
  			cp = buf + strlen(buf);
  		}
+ /*@-bufferoverflowhigh@*/
  		sprintf(cp, "%s'%u'", (xcnt > 0) ? "," : "",
  				SerializableSnapshot->xip[xcnt]);
+ /*@+bufferoverflowhigh@*/
  		cp += strlen(cp);
  	}
+ 	/*@+nullderef@*/
+         /*@+mustfreeonly@*/
  	ev_xip = DatumGetTextP(DirectFunctionCall1(textin, PointerGetDatum(buf)));
  
***************
*** 201,206 ****
--- 211,218 ----
  	 * Call the saved INSERT plan
  	 */
+ 	/*@-nullderef@*/
  	argv[0] = TransactionIdGetDatum(SerializableSnapshot->xmin);
  	argv[1] = TransactionIdGetDatum(SerializableSnapshot->xmax);
+ 	/*@+nullderef@*/
  	argv[2] = PointerGetDatum(ev_xip);
  	nulls[0] = ' ';
***************
*** 245,257 ****
  			strcmp(ev_type_c, "ENABLE_SUBSCRIPTION") == 0)
  		{
  			if ((rc = SPI_execp(cs->plan_record_sequences, NULL, NULL, 0)) < 0)
  				elog(ERROR, "Slony-I: SPI_execp() failed for \"INSERT INTO sl_seqlog ...\"");
  		}
  	}
  
! 	SPI_finish();
! 
  	PG_RETURN_INT64(retval);
  }
  
  
--- 257,272 ----
  			strcmp(ev_type_c, "ENABLE_SUBSCRIPTION") == 0)
  		{
+ /*@-nullpass@*/
  			if ((rc = SPI_execp(cs->plan_record_sequences, NULL, NULL, 0)) < 0)
  				elog(ERROR, "Slony-I: SPI_execp() failed for \"INSERT INTO sl_seqlog ...\"");
+ /*@+nullpass@*/
  		}
  	}
  
! 	(void) SPI_finish();
! /*@-mustfreefresh@*/
  	PG_RETURN_INT64(retval);
  }
+ /*@+mustfreefresh@*/
  
  
***************
*** 1135,1139 ****
   * Version: pgsql/src/backend/utils/adt/ruleutils.c,v 1.188 2005/01/13 17:19:10
   */
! const char *
  slon_quote_identifier(const char *ident)
  {
--- 1150,1154 ----
   * Version: pgsql/src/backend/utils/adt/ruleutils.c,v 1.188 2005/01/13 17:19:10
   */
! static const char *
  slon_quote_identifier(const char *ident)
  {
***************
*** 1472,1475 ****
--- 1487,1491 ----
  			elog(ERROR, "Slony-I: SPI_prepare() failed");
  
+ 		/*@-nullderef@*/
  		/*
  		 * Also create the 3 rather static text values for the log_cmdtype
***************
*** 1500,1503 ****
--- 1516,1520 ----
  
  	return cs;
+ 	/*@+nullderef@*/
  }
  

Index: Makefile
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/Makefile,v
retrieving revision 1.23
retrieving revision 1.24
diff -C2 -d -r1.23 -r1.24
*** Makefile	8 Feb 2007 18:01:15 -0000	1.23
--- Makefile	18 Apr 2007 21:20:22 -0000	1.24
***************
*** 59,62 ****
--- 59,65 ----
  	rm -f $(SO_NAME) $(SO_OBJS)
  
+ splint:
+ 	splint -I $(pgincludedir) -I $(pgincludeserverdir) +unixlib -preproc +skip-sys-headers $(wildcard *.c)
+ 
  install: all installdirs
  	$(INSTALL_SCRIPT) $(SO_NAME) $(DESTDIR)$(pgpkglibdir)

From cbbrowne at lists.slony.info  Wed Apr 18 15:19:09 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Apr 18 15:19:11 2007
Subject: [Slony1-commit] slony1-engine/src/slon cleanup_thread.c
	confoptions.c confoptions.h local_listen.c misc.c
	remote_listen.c remote_worker.c slon.c slon.h snmp_thread.c
Message-ID: <20070418221909.ED9D32902FE@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slon
In directory main.slony.info:/tmp/cvs-serv31807

Modified Files:
	cleanup_thread.c confoptions.c confoptions.h local_listen.c 
	misc.c remote_listen.c remote_worker.c slon.c slon.h 
	snmp_thread.c 
Log Message:
A barrel of little type changes based on running Splint analysis on the
code base (e.g. - extended LINT).  Mostly about adding type annotations;
values that need to be cast between int/size_t, annotating that functions
that return values that we ignore pass back (void), and such.


Index: remote_listen.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/remote_listen.c,v
retrieving revision 1.33
retrieving revision 1.34
diff -C2 -d -r1.33 -r1.34
*** remote_listen.c	2 Feb 2007 20:24:16 -0000	1.33
--- remote_listen.c	18 Apr 2007 22:19:07 -0000	1.34
***************
*** 241,245 ****
  			 * register the node connection.
  			 */
! 			slon_mkquery(&query1,
  				     /* "listen \"_%s_Event\"; " */
  				     /*	 skip confirms "listen \"_%s_Confirm\"; " */
--- 241,245 ----
  			 * register the node connection.
  			 */
! 			(void) slon_mkquery(&query1,
  				     /* "listen \"_%s_Event\"; " */
  				     /*	 skip confirms "listen \"_%s_Confirm\"; " */
***************
*** 344,348 ****
  					 node->no_id);
  
! 				slon_mkquery(&query1,
  					     "unlisten \"_%s_Event\"; ",
  					     rtcfg_cluster_name);
--- 344,348 ----
  					 node->no_id);
  
! 				(void) slon_mkquery(&query1,
  					     "unlisten \"_%s_Event\"; ",
  					     rtcfg_cluster_name);
***************
*** 352,356 ****
  					 "remoteListenThread_%d: LISTEN\n",
  					 node->no_id);
! 				slon_mkquery(&query1,
  					     "listen \"_%s_Event\"; ",
  					     rtcfg_cluster_name);
--- 352,356 ----
  					 "remoteListenThread_%d: LISTEN\n",
  					 node->no_id);
! 				(void) slon_mkquery(&query1,
  					     "listen \"_%s_Event\"; ",
  					     rtcfg_cluster_name);
***************
*** 599,603 ****
  	 * the sl_confirm table.
  	 */
! 	slon_mkquery(&query,
  				 "select con_origin, con_received, "
  				 "    max(con_seqno) as con_seqno, "
--- 599,603 ----
  	 * the sl_confirm table.
  	 */
! 	(void) slon_mkquery(&query,
  				 "select con_origin, con_received, "
  				 "    max(con_seqno) as con_seqno, "
***************
*** 675,679 ****
  	 * for here.
  	 */
! 	slon_mkquery(&query,
  				 "select ev_origin, ev_seqno, ev_timestamp, "
  				 "       ev_minxid, ev_maxxid, ev_xip, "
--- 675,679 ----
  	 * for here.
  	 */
! 	(void) slon_mkquery(&query,
  				 "select ev_origin, ev_seqno, ev_timestamp, "
  				 "       ev_minxid, ev_maxxid, ev_xip, "
***************
*** 692,696 ****
  	{
  		dstring_init(&q2);
! 		slon_mkquery(&q2, "where ev_timestamp < now() - '%s'::interval and (", lag_interval);
  		where_or_or = dstring_data(&q2);
  	}
--- 692,696 ----
  	{
  		dstring_init(&q2);
! 		(void) slon_mkquery(&q2, "where ev_timestamp < now() - '%s'::interval and (", lag_interval);
  		where_or_or = dstring_data(&q2);
  	}
***************
*** 731,739 ****
  		return -1;
  	}
! 	time(&timeout);
  	timeout += remote_listen_timeout;
  	while (PQisBusy(conn->dbconn) != 0)
  	{
! 		time(&now);
  		if (now >= timeout)
  		{
--- 731,739 ----
  		return -1;
  	}
! 	(void) time(&timeout);
  	timeout += remote_listen_timeout;
  	while (PQisBusy(conn->dbconn) != 0)
  	{
! 		(void) time(&now);
  		if (now >= timeout)
  		{
***************
*** 780,785 ****
  		int64		ev_seqno;
  
! 		ev_origin = strtol(PQgetvalue(res, tupno, 0), NULL, 10);
! 		slon_scanint64(PQgetvalue(res, tupno, 1), &ev_seqno);
  
  		slon_log(SLON_DEBUG2, "remoteListenThread_%d: "
--- 780,785 ----
  		int64		ev_seqno;
  
! 		ev_origin = (int) strtol(PQgetvalue(res, tupno, 0), NULL, 10);
! 		(void) slon_scanint64(PQgetvalue(res, tupno, 1), &ev_seqno);
  
  		slon_log(SLON_DEBUG2, "remoteListenThread_%d: "

Index: local_listen.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/local_listen.c,v
retrieving revision 1.39
retrieving revision 1.40
diff -C2 -d -r1.39 -r1.40
*** local_listen.c	27 Oct 2006 20:10:57 -0000	1.39
--- local_listen.c	18 Apr 2007 22:19:07 -0000	1.40
***************
*** 67,71 ****
  	 * Listen for local events
  	 */
! 	slon_mkquery(&query1,
  		     /* "listen \"_%s_Event\"; " */
  		     "listen \"_%s_Restart\"; ",
--- 67,71 ----
  	 * Listen for local events
  	 */
! 	(void) slon_mkquery(&query1,
  		     /* "listen \"_%s_Event\"; " */
  		     "listen \"_%s_Restart\"; ",
***************
*** 89,93 ****
  #define NODELOCKERROR "ERROR:  duplicate key violates unique constraint \"sl_nodelock-pkey\""
  
! 	slon_mkquery(&query1,
  				 "select %s.cleanupNodelock(); "
  				 "insert into %s.sl_nodelock values ("
--- 89,93 ----
  #define NODELOCKERROR "ERROR:  duplicate key violates unique constraint \"sl_nodelock-pkey\""
  
! 	(void) slon_mkquery(&query1,
  				 "select %s.cleanupNodelock(); "
  				 "insert into %s.sl_nodelock values ("
***************
*** 148,152 ****
  		 * Drain notifications.
  		 */
! 		PQconsumeInput(dbconn);
  		restart_request = false;
  		while ((notification = PQnotifies(dbconn)) != NULL)
--- 148,152 ----
  		 * Drain notifications.
  		 */
! 		(void) PQconsumeInput(dbconn);
  		restart_request = false;
  		while ((notification = PQnotifies(dbconn)) != NULL)
***************
*** 154,158 ****
  			if (strcmp(restart_notify, notification->relname) == 0)
  				restart_request = true;
! 			PQfreemem(notification);
  		}
  		if (restart_request)
--- 154,158 ----
  			if (strcmp(restart_notify, notification->relname) == 0)
  				restart_request = true;
! 			(void) PQfreemem(notification);
  		}
  		if (restart_request)
***************
*** 172,176 ****
  		 * Query the database for new local events
  		 */
! 		slon_mkquery(&query1,
  					 "select ev_seqno, ev_timestamp, "
  					 "       ev_minxid, ev_maxxid, ev_xip, "
--- 172,176 ----
  		 * Query the database for new local events
  		 */
! 		(void) slon_mkquery(&query1,
  					 "select ev_seqno, ev_timestamp, "
  					 "       ev_minxid, ev_maxxid, ev_xip, "
***************
*** 507,511 ****
  				 */
  				dstring_init(&query2);
! 				slon_mkquery(&query2,
  							 "select sub_provider from %s.sl_subscribe "
  					     "    where sub_receiver = %d and sub_set = %d",
--- 507,511 ----
  				 */
  				dstring_init(&query2);
! 				(void) slon_mkquery(&query2,
  							 "select sub_provider from %s.sl_subscribe "
  					     "    where sub_receiver = %d and sub_set = %d",

Index: misc.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/misc.c,v
retrieving revision 1.23
retrieving revision 1.24
diff -C2 -d -r1.23 -r1.24
*** misc.c	27 Oct 2006 20:10:57 -0000	1.23
--- misc.c	18 Apr 2007 22:19:07 -0000	1.24
***************
*** 77,80 ****
--- 77,81 ----
  	static int	outsize = -1;
  	int			off;
+ 	int  len;
  	char	   *level_c = NULL;
  
***************
*** 141,145 ****
  	{
  		outsize = 8192;
! 		outbuf = malloc(outsize);
  		if (outbuf == NULL)
  		{
--- 142,146 ----
  	{
  		outsize = 8192;
! 		outbuf = malloc((size_t) outsize);
  		if (outbuf == NULL)
  		{
***************
*** 149,153 ****
  		}
  	}
! 	outbuf[0] = 0;
  
  	if (logtimestamp == true && (Use_syslog != 1)
--- 150,154 ----
  		}
  	}
! 	outbuf[0] = (char) 0;
  
  	if (logtimestamp == true && (Use_syslog != 1)
***************
*** 157,161 ****
  		)
  	{
! 		strftime(time_buf, sizeof(time_buf), log_timestamp_format, localtime(&stamp_time));
  		sprintf(outbuf, "%s ", time_buf);
  	}
--- 158,166 ----
  		)
  	{
! 		len = strftime(time_buf, sizeof(time_buf), log_timestamp_format, localtime(&stamp_time));
! 		if (len == 0 && time_buf[0] != '\0') {
! 			perror("slon_log: problem with strftime()");
! 			slon_retry();
! 		}
  		sprintf(outbuf, "%s ", time_buf);
  	}
***************
*** 166,175 ****
  	sprintf(outbuf, "%s%-6.6s ", outbuf, level_c);
  
! 	off = strlen(outbuf);
  
! 	while (vsnprintf(&outbuf[off], outsize - off, fmt, ap) >= outsize - off)
  	{
  		outsize *= 2;
! 		outbuf = realloc(outbuf, outsize);
  		if (outbuf == NULL)
  		{
--- 171,180 ----
  	sprintf(outbuf, "%s%-6.6s ", outbuf, level_c);
  
! 	off = (int) strlen(outbuf);
  
! 	while (vsnprintf(&outbuf[off], (size_t) (outsize - off), fmt, ap) >= outsize - off)
  	{
  		outsize *= 2;
! 		outbuf = realloc(outbuf, (size_t) outsize);
  		if (outbuf == NULL)
  		{
***************
*** 188,193 ****
  		win32_eventlog(level, outbuf);
  #endif
! 	fwrite(outbuf, strlen(outbuf), 1, stdout);
! 	fflush(stdout);
  	pthread_mutex_unlock(&log_mutex);
  
--- 193,198 ----
  		win32_eventlog(level, outbuf);
  #endif
! 	(void) fwrite(outbuf, strlen(outbuf), 1, stdout);
! 	(void) fflush(stdout);
  	pthread_mutex_unlock(&log_mutex);
  
***************
*** 233,237 ****
  		{
  			*result = -INT64CONST(0x7fffffffffffffff) - 1;
! 			return true;
  		}
  #endif
--- 238,242 ----
  		{
  			*result = -INT64CONST(0x7fffffffffffffff) - 1;
! 			return (int) true;
  		}
  #endif
***************
*** 275,279 ****
  	static int	syslog_fac = LOG_LOCAL0;
  
! 	int			len = strlen(line);
  
  	if (Use_syslog == 0)
--- 280,284 ----
  	static int	syslog_fac = LOG_LOCAL0;
  
! 	int			len = (int) strlen(line);
  
  	if (Use_syslog == 0)
***************
*** 333,337 ****
  				*strchr(buf, '\n') = '\0';
  
! 			buflen = strlen(buf);
  
  			if (buflen <= 0)
--- 338,342 ----
  				*strchr(buf, '\n') = '\0';
  
! 			buflen = (int) strlen(buf);
  
  			if (buflen <= 0)

Index: remote_worker.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/remote_worker.c,v
retrieving revision 1.136
retrieving revision 1.137
diff -C2 -d -r1.136 -r1.137
*** remote_worker.c	18 Apr 2007 15:03:51 -0000	1.136
--- remote_worker.c	18 Apr 2007 22:19:07 -0000	1.137
***************
*** 286,294 ****
  	SlonWorkMsg *msg;
  	SlonWorkMsg_event *event;
! 	int			check_config = true;
  	int64		curr_config = -1;
  	char		seqbuf[64];
! 	int			event_ok;
! 	int			need_reloadListen = false;
  	int			rc;
  
--- 286,294 ----
  	SlonWorkMsg *msg;
  	SlonWorkMsg_event *event;
! 	bool			check_config = true;
  	int64		curr_config = -1;
  	char		seqbuf[64];
! 	bool			event_ok;
! 	bool			need_reloadListen = false;
  	int			rc;
  
***************
*** 301,305 ****
  	 */
  	wd = (WorkerGroupData *) malloc(sizeof(WorkerGroupData));
! 	memset(wd, 0, sizeof(WorkerGroupData));
  
  	pthread_mutex_init(&(wd->workdata_lock), NULL);
--- 301,310 ----
  	 */
  	wd = (WorkerGroupData *) malloc(sizeof(WorkerGroupData));
! 	if (wd == 0) {
! 		slon_log(SLON_ERROR, "remoteWorkerThread_%d: could not malloc() space for WorkerGroupData\n");
! 		slon_retry();
! 	} else {
! 		memset(wd, 0, sizeof(WorkerGroupData));
! 	}
  
  	pthread_mutex_init(&(wd->workdata_lock), NULL);
***************
*** 315,319 ****
  	memset(wd->tab_fqname, 0, sizeof(char *) * wd->tab_fqname_size);
  	wd->tab_forward = malloc(wd->tab_fqname_size);
! 	memset(wd->tab_forward, 0, wd->tab_fqname_size);
  
  	dstring_init(&query1);
--- 320,324 ----
  	memset(wd->tab_fqname, 0, sizeof(char *) * wd->tab_fqname_size);
  	wd->tab_forward = malloc(wd->tab_fqname_size);
! 	memset(wd->tab_forward, 0, (size_t) (wd->tab_fqname_size));
  
  	dstring_init(&query1);
***************
*** 331,335 ****
  	 * Put the connection into replication mode
  	 */
! 	slon_mkquery(&query1,
  				 "select %s.setSessionRole('_%s', 'slon'); ",
  				 rtcfg_namespace, rtcfg_cluster_name);
--- 336,340 ----
  	 * Put the connection into replication mode
  	 */
! 	(void) slon_mkquery(&query1,
  				 "select %s.setSessionRole('_%s', 'slon'); ",
  				 rtcfg_namespace, rtcfg_cluster_name);
***************
*** 447,451 ****
  		 * with us as a provider will pick up the news.
  		 */
! 		slon_mkquery(&query1,
  					 "begin transaction; "
  					 "set transaction isolation level serializable; ");
--- 452,456 ----
  		 * with us as a provider will pick up the news.
  		 */
! 		(void) slon_mkquery(&query1,
  					 "begin transaction; "
  					 "set transaction isolation level serializable; ");
***************
*** 577,581 ****
  				 * specified timeout.
  				 */
! 				slon_mkquery(&query2, "rollback transaction");
  				if (query_execute(node, local_dbconn, &query2) < 0)
  					slon_retry();
--- 582,586 ----
  				 * specified timeout.
  				 */
! 				(void) slon_mkquery(&query2, "rollback transaction");
  				if (query_execute(node, local_dbconn, &query2) < 0)
  					slon_retry();
***************
*** 693,702 ****
  						slon_retry();
  
! 					slon_mkquery(&query1, "select %s.uninstallNode(); ",
  								 rtcfg_namespace);
  					if (query_execute(node, local_dbconn, &query1) < 0)
  						slon_retry();
  
! 					slon_mkquery(&query1, "drop schema %s cascade; ",
  								 rtcfg_namespace);
  					query_execute(node, local_dbconn, &query1);
--- 698,707 ----
  						slon_retry();
  
! 					(void) slon_mkquery(&query1, "select %s.uninstallNode(); ",
  								 rtcfg_namespace);
  					if (query_execute(node, local_dbconn, &query1) < 0)
  						slon_retry();
  
! 					(void) slon_mkquery(&query1, "drop schema %s cascade; ",
  								 rtcfg_namespace);
  					query_execute(node, local_dbconn, &query1);
***************
*** 841,845 ****
  				if (archive_dir)
  				{
! 					slon_mkquery(&lsquery,
  								 "delete from %s.sl_setsync_offline "
  								 "  where ssy_setid= %d;",
--- 846,850 ----
  				if (archive_dir)
  				{
! 					(void) slon_mkquery(&lsquery,
  								 "delete from %s.sl_setsync_offline "
  								 "  where ssy_setid= %d;",
***************
*** 1029,1033 ****
  				{
  					slon_log(SLON_DEBUG2, "ACCEPT_SET - node not origin\n");
! 					slon_mkquery(&query2,
  								 "select 1 from %s.sl_event "
  								 "where "
--- 1034,1038 ----
  				{
  					slon_log(SLON_DEBUG2, "ACCEPT_SET - node not origin\n");
! 					(void) slon_mkquery(&query2,
  								 "select 1 from %s.sl_event "
  								 "where "
***************
*** 1125,1129 ****
  					slon_retry();
  
! 				slon_mkquery(&query1,
  							 "select sub_provider from %s.sl_subscribe "
  							 "	where sub_receiver = %d and sub_set = %d",
--- 1130,1134 ----
  					slon_retry();
  
! 				(void) slon_mkquery(&query1,
  							 "select sub_provider from %s.sl_subscribe "
  							 "	where sub_receiver = %d and sub_set = %d",
***************
*** 1214,1218 ****
  					int			sleeptime = 15;
  
! 					slon_mkquery(&query2, "rollback transaction");
  					check_config = true;
  
--- 1219,1223 ----
  					int			sleeptime = 15;
  
! 					(void) slon_mkquery(&query2, "rollback transaction");
  					check_config = true;
  
***************
*** 1263,1267 ****
  						{
  							rtcfg_enableSubscription(sub_set, sub_provider, sub_forward);
! 							slon_mkquery(&query1,
  								"select %s.enableSubscription(%d, %d, %d); ",
  										 rtcfg_namespace,
--- 1268,1272 ----
  						{
  							rtcfg_enableSubscription(sub_set, sub_provider, sub_forward);
! 							(void) slon_mkquery(&query1,
  								"select %s.enableSubscription(%d, %d, %d); ",
  										 rtcfg_namespace,
***************
*** 1325,1329 ****
  				if (archive_dir)
  				{
! 					slon_mkquery(&lsquery,
  								 "delete from %s.sl_setsync_offline "
  								 "  where ssy_setid= %d;",
--- 1330,1334 ----
  				if (archive_dir)
  				{
! 					(void) slon_mkquery(&lsquery,
  								 "delete from %s.sl_setsync_offline "
  								 "  where ssy_setid= %d;",
***************
*** 1383,1387 ****
  					strncpy(dest, ddl_script + startpos, endpos-startpos);
  					dest[STMTS[stmtno]-startpos] = 0;
! 					slon_mkquery(&query1, dest);
  					slon_log(SLON_CONFIG, "remoteWorkerThread_%d: DDL Statement %d: [%s]\n", 
  						 node->no_id, stmtno, dest);						 
--- 1388,1392 ----
  					strncpy(dest, ddl_script + startpos, endpos-startpos);
  					dest[STMTS[stmtno]-startpos] = 0;
! 					(void) slon_mkquery(&query1, dest);
  					slon_log(SLON_CONFIG, "remoteWorkerThread_%d: DDL Statement %d: [%s]\n", 
  						 node->no_id, stmtno, dest);						 
***************
*** 1403,1407 ****
  				}
  	
! 				slon_mkquery(&query1, "select %s.ddlScript_complete_int(%d, %d); ", 
  					     rtcfg_namespace,
  					     ddl_setid,
--- 1408,1412 ----
  				}
  	
! 				(void) slon_mkquery(&query1, "select %s.ddlScript_complete_int(%d, %d); ", 
  					     rtcfg_namespace,
  					     ddl_setid,
***************
*** 1466,1470 ****
  			else
  			{
! 				slon_mkquery(&query1, "rollback transaction;");
  			}
  			if (query_execute(node, local_dbconn, &query1) < 0)
--- 1471,1475 ----
  			else
  			{
! 				(void) slon_mkquery(&query1, "rollback transaction;");
  			}
  			if (query_execute(node, local_dbconn, &query1) < 0)
***************
*** 2314,2318 ****
  			 node->no_id, confirm->con_origin, seqbuf, confirm->con_received);
  
! 	slon_mkquery(&query,
  				 "select %s.forwardConfirm(%d, %d, '%s', '%q'); ",
  				 rtcfg_namespace,
--- 2319,2323 ----
  			 node->no_id, confirm->con_origin, seqbuf, confirm->con_received);
  
! 	(void) slon_mkquery(&query,
  				 "select %s.forwardConfirm(%d, %d, '%s', '%q'); ",
  				 rtcfg_namespace,
***************
*** 2517,2521 ****
  	 * Register this connection in sl_nodelock
  	 */
! 	slon_mkquery(&query1,
  				 "select %s.registerNodeConnection(%d); ",
  				 rtcfg_namespace, rtcfg_nodeid);
--- 2522,2526 ----
  	 * Register this connection in sl_nodelock
  	 */
! 	(void) slon_mkquery(&query1,
  				 "select %s.registerNodeConnection(%d); ",
  				 rtcfg_namespace, rtcfg_nodeid);
***************
*** 2544,2548 ****
  	if (sub_provider == set_origin)
  	{
! 		slon_mkquery(&query1,
  					 "start transaction; "
  					 "set transaction isolation level serializable; "
--- 2549,2553 ----
  	if (sub_provider == set_origin)
  	{
! 		(void) slon_mkquery(&query1,
  					 "start transaction; "
  					 "set transaction isolation level serializable; "
***************
*** 2584,2588 ****
  	else
  	{
! 		slon_mkquery(&query1,
  					 "start transaction; "
  					 "set transaction isolation level serializable; ");
--- 2589,2593 ----
  	else
  	{
! 		(void) slon_mkquery(&query1,
  					 "start transaction; "
  					 "set transaction isolation level serializable; ");
***************
*** 2614,2618 ****
  	 * Select the list of all tables the provider currently has in the set.
  	 */
! 	slon_mkquery(&query1,
  				 "select T.tab_id, "
  				 "    %s.slon_quote_brute(PGN.nspname) || '.' || "
--- 2619,2623 ----
  	 * Select the list of all tables the provider currently has in the set.
  	 */
! 	(void) slon_mkquery(&query1,
  				 "select T.tab_id, "
  				 "    %s.slon_quote_brute(PGN.nspname) || '.' || "
***************
*** 2660,2664 ****
  				 node->no_id, tab_fqname);
  
! 		slon_mkquery(&query3, "select * from %s limit 0;",
  			     tab_fqname);
  		res2 = PQexec(loc_dbconn, dstring_data(&query3));
--- 2665,2669 ----
  				 node->no_id, tab_fqname);
  
! 		(void) slon_mkquery(&query3, "select * from %s limit 0;",
  			     tab_fqname);
  		res2 = PQexec(loc_dbconn, dstring_data(&query3));
***************
*** 2683,2687 ****
  		 */
  		
! 		slon_mkquery(&query3, "lock table %s;\n", tab_fqname);
  		res2 = PQexec(loc_dbconn, dstring_data(&query3));
  		if (PQresultStatus(res2) != PGRES_COMMAND_OK)
--- 2688,2692 ----
  		 */
  		
! 		(void) slon_mkquery(&query3, "lock table %s;\n", tab_fqname);
  		res2 = PQexec(loc_dbconn, dstring_data(&query3));
  		if (PQresultStatus(res2) != PGRES_COMMAND_OK)
***************
*** 2707,2711 ****
  	 * Add in the sequences contained in the set
  	 */
! 	slon_mkquery(&query1,
  				 "select SQ.seq_id, "
  				 "    %s.slon_quote_brute(PGN.nspname) || '.' || "
--- 2712,2716 ----
  	 * Add in the sequences contained in the set
  	 */
! 	(void) slon_mkquery(&query1,
  				 "select SQ.seq_id, "
  				 "    %s.slon_quote_brute(PGN.nspname) || '.' || "
***************
*** 2749,2753 ****
  				 node->no_id, seq_fqname);
  
! 		slon_mkquery(&query1,
  					 "select %s.setAddSequence_int(%d, %s, '%q', '%q')",
  					 rtcfg_namespace, set_id, seq_id,
--- 2754,2758 ----
  				 node->no_id, seq_fqname);
  
! 		(void) slon_mkquery(&query1,
  					 "select %s.setAddSequence_int(%d, %s, '%q', '%q')",
  					 rtcfg_namespace, set_id, seq_id,
***************
*** 2772,2776 ****
  	 * Select the list of all tables the provider currently has in the set.
  	 */
! 	slon_mkquery(&query1,
  				 "select T.tab_id, "
  				 "    %s.slon_quote_brute(PGN.nspname) || '.' || "
--- 2777,2781 ----
  	 * Select the list of all tables the provider currently has in the set.
  	 */
! 	(void) slon_mkquery(&query1,
  				 "select T.tab_id, "
  				 "    %s.slon_quote_brute(PGN.nspname) || '.' || "
***************
*** 2827,2831 ****
  		 * suppressed.
  		 */
! 		slon_mkquery(&query1,
  					 "select %s.setAddTable_int(%d, %d, '%q', '%q', '%q'); ",
  					 rtcfg_namespace,
--- 2832,2836 ----
  		 * suppressed.
  		 */
! 		(void) slon_mkquery(&query1,
  					 "select %s.setAddTable_int(%d, %d, '%q', '%q', '%q'); ",
  					 rtcfg_namespace,
***************
*** 2847,2851 ****
  		 * Copy the content of sl_trigger for this table
  		 */
! 		slon_mkquery(&query1,
  					 "select trig_tgname from %s.sl_trigger "
  					 "where trig_tabid = %d; ",
--- 2852,2856 ----
  		 * Copy the content of sl_trigger for this table
  		 */
! 		(void) slon_mkquery(&query1,
  					 "select trig_tgname from %s.sl_trigger "
  					 "where trig_tabid = %d; ",
***************
*** 2871,2875 ****
  		for (tupno2 = 0; tupno2 < ntuples2; tupno2++)
  		{
! 			slon_mkquery(&query1,
  						 "select %s.storeTrigger(%d, '%q'); ",
  					   rtcfg_namespace, tab_id, PQgetvalue(res2, tupno2, 0));
--- 2876,2880 ----
  		for (tupno2 = 0; tupno2 < ntuples2; tupno2++)
  		{
! 			(void) slon_mkquery(&query1,
  						 "select %s.storeTrigger(%d, '%q'); ",
  					   rtcfg_namespace, tab_id, PQgetvalue(res2, tupno2, 0));
***************
*** 2898,2902 ****
  				 node->no_id, tab_fqname);
  
! 		slon_mkquery(&query2, "select %s.copyFields(%d);",
  					 rtcfg_namespace, tab_id);
  
--- 2903,2907 ----
  				 node->no_id, tab_fqname);
  
! 		(void) slon_mkquery(&query2, "select %s.copyFields(%d);",
  					 rtcfg_namespace, tab_id);
  
***************
*** 2920,2924 ****
  		}
  
! 		slon_mkquery(&query2, "select %s.pre74();",
  					 rtcfg_namespace);
  		res4 = PQexec(loc_dbconn, dstring_data(&query2));
--- 2925,2929 ----
  		}
  
! 		(void) slon_mkquery(&query2, "select %s.pre74();",
  					 rtcfg_namespace);
  		res4 = PQexec(loc_dbconn, dstring_data(&query2));
***************
*** 2949,2953 ****
  				 node->no_id, nodeon73);
  
! 		slon_mkquery(&query1,
  					 "select %s.prepareTableForCopy(%d); "
  					 "copy %s %s from stdin; ",
--- 2954,2958 ----
  				 node->no_id, nodeon73);
  
! 		(void) slon_mkquery(&query1,
  					 "select %s.prepareTableForCopy(%d); "
  					 "copy %s %s from stdin; ",
***************
*** 2976,2980 ****
  		if (archive_dir)
  		{
! 			slon_mkquery(&query1,
  			 "delete from %s;copy %s %s from stdin;", tab_fqname, tab_fqname,
  						 nodeon73 ? "" : PQgetvalue(res3, 0, 0));
--- 2981,2985 ----
  		if (archive_dir)
  		{
! 			(void) slon_mkquery(&query1,
  			 "delete from %s;copy %s %s from stdin;", tab_fqname, tab_fqname,
  						 nodeon73 ? "" : PQgetvalue(res3, 0, 0));
***************
*** 2996,3000 ****
  		 * Begin a COPY to stdout for the table on the provider DB
  		 */
! 		slon_mkquery(&query1,
  			   "copy %s %s to stdout; ", tab_fqname, PQgetvalue(res3, 0, 0));
  		PQclear(res3);
--- 3001,3005 ----
  		 * Begin a COPY to stdout for the table on the provider DB
  		 */
! 		(void) slon_mkquery(&query1,
  			   "copy %s %s to stdout; ", tab_fqname, PQgetvalue(res3, 0, 0));
  		PQclear(res3);
***************
*** 3319,3323 ****
  		 * Analyze the table to update statistics
  		 */
! 		slon_mkquery(&query1, "select %s.finishTableAfterCopy(%d); "
  					 "analyze %s; ",
  					 rtcfg_namespace, tab_id,
--- 3324,3328 ----
  		 * Analyze the table to update statistics
  		 */
! 		(void) slon_mkquery(&query1, "select %s.finishTableAfterCopy(%d); "
  					 "analyze %s; ",
  					 rtcfg_namespace, tab_id,
***************
*** 3357,3361 ****
  	 * ENABLE_SUBSCRIPTION event.
  	 */
! 	slon_mkquery(&query1,
  				 "select SL.seql_seqid, SL.seql_last_value, "
  				 "    %s.slon_quote_brute(PGN.nspname) || '.' || "
--- 3362,3366 ----
  	 * ENABLE_SUBSCRIPTION event.
  	 */
! 	(void) slon_mkquery(&query1,
  				 "select SL.seql_seqid, SL.seql_last_value, "
  				 "    %s.slon_quote_brute(PGN.nspname) || '.' || "
***************
*** 3408,3412 ****
  		if (strtol(seql_seqid, NULL, 10) != 0)
  		{
! 			slon_mkquery(&query1,
  						 "select \"pg_catalog\".setval('%q', '%s'); ",
  						 seq_fqname, seql_last_value);
--- 3413,3417 ----
  		if (strtol(seql_seqid, NULL, 10) != 0)
  		{
! 			(void) slon_mkquery(&query1,
  						 "select \"pg_catalog\".setval('%q', '%s'); ",
  						 seq_fqname, seql_last_value);
***************
*** 3476,3480 ****
  		 * and all
  		 */
! 		slon_mkquery(&query1,
  					 "select max(ev_seqno) as ssy_seqno "
  					 "from %s.sl_event "
--- 3481,3485 ----
  		 * and all
  		 */
! 		(void) slon_mkquery(&query1,
  					 "select max(ev_seqno) as ssy_seqno "
  					 "from %s.sl_event "
***************
*** 3527,3531 ****
  					 node->no_id);
  
! 			slon_mkquery(&query1,
  						 "select log_actionseq "
  						 "from %s.sl_log_1 where log_origin = %d "
--- 3532,3536 ----
  					 node->no_id);
  
! 			(void) slon_mkquery(&query1,
  						 "select log_actionseq "
  						 "from %s.sl_log_1 where log_origin = %d "
***************
*** 3541,3545 ****
  			 * sequence list to all actions after that.
  			 */
! 			slon_mkquery(&query1,
  						 "select ev_seqno, ev_minxid, ev_maxxid, ev_xip "
  						 "from %s.sl_event "
--- 3546,3550 ----
  			 * sequence list to all actions after that.
  			 */
! 			(void) slon_mkquery(&query1,
  						 "select ev_seqno, ev_minxid, ev_maxxid, ev_xip "
  						 "from %s.sl_event "
***************
*** 3583,3587 ****
  			ssy_xip = PQgetvalue(res1, 0, 3);
  
! 			slon_mkquery(&query2,
  						 "log_xid >= '%s' or (log_xid >= '%s'",
  						 ssy_maxxid, ssy_minxid);
--- 3588,3592 ----
  			ssy_xip = PQgetvalue(res1, 0, 3);
  
! 			(void) slon_mkquery(&query2,
  						 "log_xid >= '%s' or (log_xid >= '%s'",
  						 ssy_maxxid, ssy_minxid);
***************
*** 3595,3599 ****
  					 node->no_id, ssy_seqno);
  
! 			slon_mkquery(&query1,
  						 "select log_actionseq "
  						 "from %s.sl_log_1 where log_origin = %d and %s "
--- 3600,3604 ----
  					 node->no_id, ssy_seqno);
  
! 			(void) slon_mkquery(&query1,
  						 "select log_actionseq "
  						 "from %s.sl_log_1 where log_origin = %d and %s "
***************
*** 3649,3653 ****
  		 * setsync from him.
  		 */
! 		slon_mkquery(&query1,
  					 "select ssy_seqno, ssy_minxid, ssy_maxxid, "
  					 "    ssy_xip, ssy_action_list "
--- 3654,3658 ----
  		 * setsync from him.
  		 */
! 		(void) slon_mkquery(&query1,
  					 "select ssy_seqno, ssy_minxid, ssy_maxxid, "
  					 "    ssy_xip, ssy_action_list "
***************
*** 3697,3701 ****
  	 * Create our own initial setsync entry
  	 */
! 	slon_mkquery(&query1,
  		     "delete from %s.sl_setsync where ssy_setid = %d;"
  		     "insert into %s.sl_setsync "
--- 3702,3706 ----
  	 * Create our own initial setsync entry
  	 */
! 	(void) slon_mkquery(&query1,
  		     "delete from %s.sl_setsync where ssy_setid = %d;"
  		     "insert into %s.sl_setsync "
***************
*** 3722,3726 ****
  	if (archive_dir)
  	{
! 		slon_mkquery(&lsquery,
  			     "insert into %s.sl_setsync_offline (ssy_setid, ssy_seqno) "
  			     "values ('%d', '%s');",
--- 3727,3731 ----
  	if (archive_dir)
  	{
! 		(void) slon_mkquery(&lsquery,
  			     "insert into %s.sl_setsync_offline (ssy_setid, ssy_seqno) "
  			     "values ('%d', '%s');",
***************
*** 3769,3773 ****
  	 * database connection.
  	 */
! 	slon_mkquery(&query1, "rollback transaction");
  	if (query_execute(node, pro_dbconn, &query1) < 0)
  	{
--- 3774,3778 ----
  	 * database connection.
  	 */
! 	(void) slon_mkquery(&query1, "rollback transaction");
  	if (query_execute(node, pro_dbconn, &query1) < 0)
  	{
***************
*** 3902,3906 ****
  			 * Listen on the special relation telling our node relationship
  			 */
! 			slon_mkquery(&query,
  						 "select %s.registerNodeConnection(%d); ",
  						 rtcfg_namespace, rtcfg_nodeid);
--- 3907,3911 ----
  			 * Listen on the special relation telling our node relationship
  			 */
! 			(void) slon_mkquery(&query,
  						 "select %s.registerNodeConnection(%d); ",
  						 rtcfg_namespace, rtcfg_nodeid);
***************
*** 3973,3977 ****
  
  	if (strlen(event->ev_xip) != 0)
! 		slon_mkquery(&new_qual,
  					 "(log_xid < '%s' and "
  					 "%s.xxid_lt_snapshot(log_xid, '%s:%s:%q'))",
--- 3978,3982 ----
  
  	if (strlen(event->ev_xip) != 0)
! 		(void) slon_mkquery(&new_qual,
  					 "(log_xid < '%s' and "
  					 "%s.xxid_lt_snapshot(log_xid, '%s:%s:%q'))",
***************
*** 3980,3984 ****
  					 event->ev_minxid_c, event->ev_maxxid_c, event->ev_xip);
  	else
! 		slon_mkquery(&new_qual,
  					 "(log_xid < '%s')",
  					 event->ev_maxxid_c);
--- 3985,3989 ----
  					 event->ev_minxid_c, event->ev_maxxid_c, event->ev_xip);
  	else
! 		(void) slon_mkquery(&new_qual,
  					 "(log_xid < '%s')",
  					 event->ev_maxxid_c);
***************
*** 3996,4000 ****
  		provider_qual = &(provider->helper_qualification);
  		dstring_reset(provider_qual);
! 		slon_mkquery(provider_qual,
  					 "where log_origin = %d and ( ",
  					 node->no_id);
--- 4001,4005 ----
  		provider_qual = &(provider->helper_qualification);
  		dstring_reset(provider_qual);
! 		(void) slon_mkquery(provider_qual,
  					 "where log_origin = %d and ( ",
  					 node->no_id);
***************
*** 4004,4008 ****
  		 * synced better than this SYNC already.
  		 */
! 		slon_mkquery(&query,
  					 "select SSY.ssy_setid, SSY.ssy_seqno, "
  					 "    SSY.ssy_minxid, SSY.ssy_maxxid, SSY.ssy_xip, "
--- 4009,4013 ----
  		 * synced better than this SYNC already.
  		 */
! 		(void) slon_mkquery(&query,
  					 "select SSY.ssy_setid, SSY.ssy_seqno, "
  					 "    SSY.ssy_minxid, SSY.ssy_maxxid, SSY.ssy_xip, "
***************
*** 4054,4058 ****
  			 * Select the tables in that set ...
  			 */
! 			slon_mkquery(&query,
  						 "select T.tab_id, T.tab_set, "
  				 "    %s.slon_quote_brute(PGN.nspname) || '.' || "
--- 4059,4063 ----
  			 * Select the tables in that set ...
  			 */
! 			(void) slon_mkquery(&query,
  						 "select T.tab_id, T.tab_set, "
  				 "    %s.slon_quote_brute(PGN.nspname) || '.' || "
***************
*** 4262,4266 ****
  	 * Get the current sl_log_status
  	 */
! 	slon_mkquery(&query, "select last_value from %s.sl_log_status",
  			rtcfg_namespace);
  	res1 = PQexec(local_dbconn, dstring_data(&query));
--- 4267,4271 ----
  	 * Get the current sl_log_status
  	 */
! 	(void) slon_mkquery(&query, "select last_value from %s.sl_log_status",
  			rtcfg_namespace);
  	res1 = PQexec(local_dbconn, dstring_data(&query));
***************
*** 4526,4530 ****
  		int			tupno1;
  
! 		slon_mkquery(&query,
  					 "select SL.seql_seqid, SL.seql_last_value "
  					 "	from %s.sl_seqlog SL, "
--- 4531,4535 ----
  		int			tupno1;
  
! 		(void) slon_mkquery(&query,
  					 "select SL.seql_seqid, SL.seql_last_value "
  					 "	from %s.sl_seqlog SL, "
***************
*** 4562,4566 ****
  			char	   *seql_last_value = PQgetvalue(res1, tupno1, 1);
  
! 			slon_mkquery(&query,
  						 "select %s.sequenceSetValue(%s,%d,'%s','%s'); ",
  						 rtcfg_namespace,
--- 4567,4571 ----
  			char	   *seql_last_value = PQgetvalue(res1, tupno1, 1);
  
! 			(void) slon_mkquery(&query,
  						 "select %s.sequenceSetValue(%s,%d,'%s','%s'); ",
  						 rtcfg_namespace,
***************
*** 4580,4584 ****
  			if (archive_dir)
  			{
! 				slon_mkquery(&lsquery,
  							 "select %s.sequenceSetValue_offline(%s,'%s');\n",
  							 rtcfg_namespace,
--- 4585,4589 ----
  			if (archive_dir)
  			{
! 				(void) slon_mkquery(&lsquery,
  							 "select %s.sequenceSetValue_offline(%s,'%s');\n",
  							 rtcfg_namespace,
***************
*** 4596,4600 ****
  	 * we've just replicated ...
  	 */
! 	slon_mkquery(&query,
  				 "update %s.sl_setsync set "
  			   "    ssy_seqno = '%s', ssy_minxid = '%s', ssy_maxxid = '%s', "
--- 4601,4605 ----
  	 * we've just replicated ...
  	 */
! 	(void) slon_mkquery(&query,
  				 "update %s.sl_setsync set "
  			   "    ssy_seqno = '%s', ssy_minxid = '%s', ssy_maxxid = '%s', "
***************
*** 4756,4760 ****
  			 * Start a transaction
  			 */
! 			slon_mkquery(&query, "start transaction; "
  						 "set enable_seqscan = off; "
  						 "set enable_indexscan = on; ");
--- 4761,4765 ----
  			 * Start a transaction
  			 */
! 			(void) slon_mkquery(&query, "start transaction; "
  						 "set enable_seqscan = off; "
  						 "set enable_indexscan = on; ");
***************
*** 4768,4772 ****
  			 * Get the current sl_log_status value
  			 */
! 			slon_mkquery(&query, "select last_value from %s.sl_log_status",
  						rtcfg_namespace);
  			res3 = PQexec(dbconn, dstring_data(&query));
--- 4773,4777 ----
  			 * Get the current sl_log_status value
  			 */
! 			(void) slon_mkquery(&query, "select last_value from %s.sl_log_status",
  						rtcfg_namespace);
  			res3 = PQexec(dbconn, dstring_data(&query));
***************
*** 4808,4812 ****
  			{
  				case 0:
! 					slon_mkquery(&query,
  						 "declare LOG cursor for select "
  						 "    log_origin, log_xid, log_tableid, "
--- 4813,4817 ----
  			{
  				case 0:
! 					(void) slon_mkquery(&query,
  						 "declare LOG cursor for select "
  						 "    log_origin, log_xid, log_tableid, "
***************
*** 4823,4827 ****
  
  				case 1:
! 					slon_mkquery(&query,
  						 "declare LOG cursor for select "
  						 "    log_origin, log_xid, log_tableid, "
--- 4828,4832 ----
  
  				case 1:
! 					(void) slon_mkquery(&query,
  						 "declare LOG cursor for select "
  						 "    log_origin, log_xid, log_tableid, "
***************
*** 4839,4843 ****
  				case 2:
  				case 3:
! 					slon_mkquery(&query,
  						 "declare LOG cursor for select * from ("
  						 "  select log_origin, log_xid, log_tableid, "
--- 4844,4848 ----
  				case 2:
  				case 3:
! 					(void) slon_mkquery(&query,
  						 "declare LOG cursor for select * from ("
  						 "  select log_origin, log_xid, log_tableid, "
***************
*** 4885,4889 ****
  			}
  
! 			slon_mkquery(&query, "fetch %d from LOG; ",
  						 SLON_DATA_FETCH_SIZE * SLON_COMMANDS_PER_LINE);
  			data_line_alloc = 0;
--- 4890,4894 ----
  			}
  
! 			(void) slon_mkquery(&query, "fetch %d from LOG; ",
  						 SLON_DATA_FETCH_SIZE * SLON_COMMANDS_PER_LINE);
  			data_line_alloc = 0;
***************
*** 5082,5086 ****
  					if (log_cmdsize >= sync_max_rowsize)
  					{
! 						slon_mkquery(&query2,
  							     "select log_cmddata "
  							     "from %s.sl_log_1 "
--- 5087,5091 ----
  					if (log_cmdsize >= sync_max_rowsize)
  					{
! 						(void) slon_mkquery(&query2,
  							     "select log_cmddata "
  							     "from %s.sl_log_1 "
***************
*** 5267,5274 ****
  		 * Close the cursor and rollback the transaction.
  		 */
! 		slon_mkquery(&query, "close LOG; ");
  		if (query_execute(node, dbconn, &query) < 0)
  			errors++;
! 		slon_mkquery(&query, "rollback transaction; "
  					 "set enable_seqscan = default; "
  					 "set enable_indexscan = default; ");
--- 5272,5279 ----
  		 * Close the cursor and rollback the transaction.
  		 */
! 		(void) slon_mkquery(&query, "close LOG; ");
  		if (query_execute(node, dbconn, &query) < 0)
  			errors++;
! 		(void) slon_mkquery(&query, "rollback transaction; "
  					 "set enable_seqscan = default; "
  					 "set enable_indexscan = default; ");
***************
*** 5647,5651 ****
  	first_subquery = 1;
  	state = START_STATE;
! 	slon_mkquery(action_subquery, " ");
  
  	slon_log(SLON_DEBUG4, "compress_actionseq(list,subquery) Action list: %s\n", ssy_actionlist);
--- 5652,5656 ----
  	first_subquery = 1;
  	state = START_STATE;
! 	(void) slon_mkquery(action_subquery, " ");
  
  	slon_log(SLON_DEBUG4, "compress_actionseq(list,subquery) Action list: %s\n", ssy_actionlist);

Index: slon.h
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/slon.h,v
retrieving revision 1.61
retrieving revision 1.62
diff -C2 -d -r1.61 -r1.62
*** slon.h	27 Oct 2006 20:10:57 -0000	1.61
--- slon.h	18 Apr 2007 22:19:07 -0000	1.62
***************
*** 362,366 ****
  	if (slon_watchdog_pid >= 0) { \
  		slon_log(SLON_DEBUG2, "slon_abort() from pid=%d\n", slon_pid); \
! 		kill(slon_watchdog_pid, SIGTERM); \
  		slon_watchdog_pid = -1; \
  	} \
--- 362,366 ----
  	if (slon_watchdog_pid >= 0) { \
  		slon_log(SLON_DEBUG2, "slon_abort() from pid=%d\n", slon_pid); \
! 		(void) kill(slon_watchdog_pid, SIGTERM);			\
  		slon_watchdog_pid = -1; \
  	} \
***************
*** 373,377 ****
  	if (slon_watchdog_pid >= 0) { \
  		slon_log(SLON_DEBUG2, "slon_restart() from pid=%d\n", slon_pid); \
! 		kill(slon_watchdog_pid, SIGHUP); \
  		slon_watchdog_pid = -1; \
  	} \
--- 373,377 ----
  	if (slon_watchdog_pid >= 0) { \
  		slon_log(SLON_DEBUG2, "slon_restart() from pid=%d\n", slon_pid); \
! 		(void) kill(slon_watchdog_pid, SIGHUP);			\
  		slon_watchdog_pid = -1; \
  	} \
***************
*** 384,388 ****
  	if (slon_watchdog_pid >= 0) { \
  		slon_log(SLON_DEBUG2, "slon_retry() from pid=%d\n", slon_pid); \
! 		kill(slon_watchdog_pid, SIGUSR1); \
  		slon_watchdog_pid = -1; \
  	} \
--- 384,388 ----
  	if (slon_watchdog_pid >= 0) { \
  		slon_log(SLON_DEBUG2, "slon_retry() from pid=%d\n", slon_pid); \
! 		(void) kill(slon_watchdog_pid, SIGUSR1);			\
  		slon_watchdog_pid = -1; \
  	} \

Index: snmp_thread.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/snmp_thread.c,v
retrieving revision 1.4
retrieving revision 1.5
diff -C2 -d -r1.4 -r1.5
*** snmp_thread.c	27 Oct 2006 20:10:57 -0000	1.4
--- snmp_thread.c	18 Apr 2007 22:19:07 -0000	1.5
***************
*** 18,22 ****
  init_nstAgentSubagentObject(void)
  {
! 	static oid	nstAgentSubagentObject_oid[] =
  	{1, 3, 6, 1, 4, 1, 20366, 32, 2, 3, 32, 1};
  
--- 18,22 ----
  init_nstAgentSubagentObject(void)
  {
! 	static oid nstAgentSubagentObject_oid[] =
  	{1, 3, 6, 1, 4, 1, 20366, 32, 2, 3, 32, 1};
  

Index: confoptions.h
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/confoptions.h,v
retrieving revision 1.33
retrieving revision 1.34
diff -C2 -d -r1.33 -r1.34
*** confoptions.h	6 Feb 2007 21:04:27 -0000	1.33
--- confoptions.h	18 Apr 2007 22:19:07 -0000	1.34
***************
*** 1,2 ****
--- 1,3 ----
+ /* $Id$ */
  #ifndef _CONFOPTIONS_H_
  #define _CONFOPTIONS_H_

Index: slon.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/slon.c,v
retrieving revision 1.70
retrieving revision 1.71
diff -C2 -d -r1.70 -r1.71
*** slon.c	13 Dec 2006 14:37:18 -0000	1.70
--- slon.c	18 Apr 2007 22:19:07 -0000	1.71
***************
*** 47,51 ****
  #define		SLON_WATCHDOG_RETRY			2
  #define		SLON_WATCHDOG_SHUTDOWN		3
! int			watchdog_status = SLON_WATCHDOG_NORMAL;
  #endif
  int			sched_wakeuppipe[2];
--- 47,51 ----
  #define		SLON_WATCHDOG_RETRY			2
  #define		SLON_WATCHDOG_SHUTDOWN		3
! static int			watchdog_status = SLON_WATCHDOG_NORMAL;
  #endif
  int			sched_wakeuppipe[2];
***************
*** 80,84 ****
  char	   *pid_file;
  char	   *archive_dir = NULL;
! int			child_status;
  
  
--- 80,84 ----
  char	   *pid_file;
  char	   *archive_dir = NULL;
! static int			child_status;
  
  

Index: cleanup_thread.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/cleanup_thread.c,v
retrieving revision 1.35
retrieving revision 1.36
diff -C2 -d -r1.35 -r1.36
*** cleanup_thread.c	6 Dec 2006 09:36:16 -0000	1.35
--- cleanup_thread.c	18 Apr 2007 22:19:07 -0000	1.36
***************
*** 36,40 ****
  
  /* The list of tables that need to be vacuumed by Slony-I */
! static char *table_list[] = {"%s.sl_event",
  	"%s.sl_confirm",
  	"%s.sl_setsync",
--- 36,42 ----
  
  /* The list of tables that need to be vacuumed by Slony-I */
! /* @-nullassign @*/
! static char *table_list[] = {
! 	"%s.sl_event",
  	"%s.sl_confirm",
  	"%s.sl_setsync",
***************
*** 44,50 ****
  	"pg_catalog.pg_listener",
  	"pg_catalog.pg_statistic",
! 	NULL
  };
! 
  
  static char tstring[255];		/* string used to store table names for the
--- 46,52 ----
  	"pg_catalog.pg_listener",
  	"pg_catalog.pg_statistic",
! 	NULL  
  };
!  /* @end@ */
  
  static char tstring[255];		/* string used to store table names for the
***************
*** 58,64 ****
--- 60,68 ----
   * ----------
   */
+ /* @ -paramuse @ */
  void *
  cleanupThread_main(void *dummy)
  {
+ /* @ +paramuse @ */
  	SlonConn   *conn;
  	SlonDString query1;
***************
*** 95,99 ****
  	{
  #ifndef WIN32
! 		kill(getpid(), SIGTERM);
  		pthread_exit(NULL);
  #else
--- 99,103 ----
  	{
  #ifndef WIN32
! 		(void)	kill(getpid(), SIGTERM);
  		pthread_exit(NULL);
  #else
***************
*** 102,107 ****
  		/* slon_retry(); */
  	}
  	dbconn = conn->dbconn;
- 
  	/*
  	 * Build the query string for calling the cleanupEvent() stored procedure
--- 106,111 ----
  		/* slon_retry(); */
  	}
+ 	
  	dbconn = conn->dbconn;
  	/*
  	 * Build the query string for calling the cleanupEvent() stored procedure

Index: confoptions.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/confoptions.c,v
retrieving revision 1.21
retrieving revision 1.22
diff -C2 -d -r1.21 -r1.22
*** confoptions.c	2 Feb 2007 20:24:16 -0000	1.21
--- confoptions.c	18 Apr 2007 22:19:07 -0000	1.22
***************
*** 101,105 ****
  	num_conf_variables = num_vars;
  	size_conf_variables = size_vars;
! 	qsort((void *)conf_variables, num_conf_variables, sizeof(struct config_generic *), conf_var_compare);
  }
  
--- 101,105 ----
  	num_conf_variables = num_vars;
  	size_conf_variables = size_vars;
! 	qsort((void *)conf_variables, (size_t) num_conf_variables, sizeof(struct config_generic *), conf_var_compare);
  }
  
***************
*** 205,209 ****
  parse_bool(const char *value, bool * result)
  {
! 	size_t		len = strlen(value);
  
  	if (strncasecmp(value, "true", len) == 0)
--- 205,209 ----
  parse_bool(const char *value, bool * result)
  {
! 	int		len = (int)  strlen(value);
  
  	if (strncasecmp(value, "true", len) == 0)
***************
*** 326,330 ****
  		bsearch((void *)&key,
  				(void *)conf_variables,
! 				num_conf_variables,
  				sizeof(struct config_generic *),
  				conf_var_compare);
--- 326,330 ----
  		bsearch((void *)&key,
  				(void *)conf_variables,
! 				(size_t) num_conf_variables,
  				sizeof(struct config_generic *),
  				conf_var_compare);
***************
*** 367,371 ****
  			chb += 'a' - 'A';
  		if (cha != chb)
! 			return cha - chb;
  	}
  	if (*namea)
--- 367,371 ----
  			chb += 'a' - 'A';
  		if (cha != chb)
! 			return (int) (cha - chb);
  	}
  	if (*namea)
***************
*** 394,398 ****
  
  				return (void *)conf->variable;
! 				break;
  			}
  		case SLON_C_INT:
--- 394,398 ----
  
  				return (void *)conf->variable;
! 				/* break; */
  			}
  		case SLON_C_INT:
***************
*** 401,405 ****
  
  				return (void *)conf->variable;
! 				break;
  			}
  		case SLON_C_REAL:
--- 401,405 ----
  
  				return (void *)conf->variable;
! 				/* break; */
  			}
  		case SLON_C_REAL:
***************
*** 408,412 ****
  
  				return (void *)conf->variable;
! 				break;
  			}
  		case SLON_C_STRING:
--- 408,412 ----
  
  				return (void *)conf->variable;
! 				/* break; */
  			}
  		case SLON_C_STRING:
***************
*** 415,419 ****
  
  				return (void *)*conf->variable;
! 				break;
  			}
  	}
--- 415,419 ----
  
  				return (void *)*conf->variable;
! 				/* break; */
  			}
  	}
***************
*** 496,499 ****
--- 496,500 ----
  						return false;
  					}
+ 					/* @ -realcompare @ */
  					if (newval < conf->min || newval > conf->max)
  					{
***************
*** 502,505 ****
--- 503,507 ----
  						return false;
  					}
+ 					/* @ +realcompare @ */
  				}
  				else

From cbbrowne at lists.slony.info  Fri Apr 20 13:21:01 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Apr 20 13:21:03 2007
Subject: [Slony1-commit] slony1-engine/tests/testlogship  - New directory
Message-ID: <20070420202101.B3E282902EA@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testlogship
In directory main.slony.info:/tmp/cvs-serv31432/testlogship

Log Message:
Directory /home/cvsd/slony1/slony1-engine/tests/testlogship added to the repository
--> Using per-directory sticky tag `REL_1_2_STABLE'


From cbbrowne at lists.slony.info  Fri Apr 20 13:51:11 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Apr 20 13:51:13 2007
Subject: [Slony1-commit] slony1-engine RELEASE-1.2.10
Message-ID: <20070420205111.8AD642903B3@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine
In directory main.slony.info:/tmp/cvs-serv31857

Modified Files:
      Tag: REL_1_2_STABLE
	RELEASE-1.2.10 
Log Message:
Add a log shipping regression test.  It is actually not adequate to test
things properly; I start here by adding it in in a form that "seems to
function" before tweaking it further.


Index: RELEASE-1.2.10
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/Attic/RELEASE-1.2.10,v
retrieving revision 1.1.2.2
retrieving revision 1.1.2.3
diff -C2 -d -r1.1.2.2 -r1.1.2.3
*** RELEASE-1.2.10	18 Apr 2007 19:28:27 -0000	1.1.2.2
--- RELEASE-1.2.10	20 Apr 2007 20:51:09 -0000	1.1.2.3
***************
*** 5,6 ****
--- 5,7 ----
    - The script was being executed on too many nodes...
  
+ - Added a test script for log shipping

From cbbrowne at lists.slony.info  Fri Apr 20 13:51:11 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Apr 20 13:51:13 2007
Subject: [Slony1-commit] slony1-engine/doc/adminguide testbed.sgml
Message-ID: <20070420205111.9632829043A@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv31857/doc/adminguide

Modified Files:
      Tag: REL_1_2_STABLE
	testbed.sgml 
Log Message:
Add a log shipping regression test.  It is actually not adequate to test
things properly; I start here by adding it in in a form that "seems to
function" before tweaking it further.


Index: testbed.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/testbed.sgml,v
retrieving revision 1.10.2.1
retrieving revision 1.10.2.2
diff -C2 -d -r1.10.2.1 -r1.10.2.2
*** testbed.sgml	16 Mar 2007 19:01:26 -0000	1.10.2.1
--- testbed.sgml	20 Apr 2007 20:51:09 -0000	1.10.2.2
***************
*** 147,150 ****
--- 147,184 ----
  </glossentry>
  
+ <glossentry>
+ <glossterm><envar>SLTOOLDIR</envar></glossterm>
+ 
+ <glossdef><para> Where to look for &slony1; tools such as
+ <application>slony1_dump.sh</application>.  </para></glossdef>
+ 
+ </glossentry>
+ 
+ <glossentry>
+ <glossterm><envar>ARCHIVE[n]</envar></glossterm>
+ 
+ <glossdef><para> If set to <quote>true</quote>, for a particular node,
+ which will normally get configured out of human sight in the
+ generic-to-a-particular-test file <filename>settings.ik</filename>,
+ then this node will be used as a data source for <xref
+ linkend="logshipping">, and this causes the test tools to set up a
+ directory for the <link linkend="slon-config-archive-dir">
+ archive_dir</link> option.  </para></glossdef>
+ 
+ </glossentry>
+ 
+ <glossentry>
+ <glossterm><envar>LOGSHIP[n]</envar></glossterm>
+ 
+ <glossdef><para> If set to <quote>true</quote>, for a particular node,
+ which will normally get configured out of human sight in
+ <filename>settings.ik</filename> for a particular test, then this
+ indicates that this node is being created via <xref
+ linkend="logshipping">, and a &lslon; is not required for this node.
+ </para></glossdef>
+ 
+ </glossentry>
+ 
+ 
  </glosslist>
  

From cbbrowne at lists.slony.info  Fri Apr 20 13:51:11 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Apr 20 13:51:13 2007
Subject: [Slony1-commit] slony1-engine/tests run_test.sh settings.ik
Message-ID: <20070420205111.BA2E0290449@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests
In directory main.slony.info:/tmp/cvs-serv31857/tests

Modified Files:
      Tag: REL_1_2_STABLE
	run_test.sh settings.ik 
Log Message:
Add a log shipping regression test.  It is actually not adequate to test
things properly; I start here by adding it in in a form that "seems to
function" before tweaking it further.


Index: settings.ik
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/settings.ik,v
retrieving revision 1.4
retrieving revision 1.4.2.1
diff -C2 -d -r1.4 -r1.4.2.1
*** settings.ik	13 Dec 2005 21:45:55 -0000	1.4
--- settings.ik	20 Apr 2007 20:51:09 -0000	1.4.2.1
***************
*** 87,88 ****
--- 87,91 ----
  PORT13=${PORT13:-${PGPORT:-"5432"}}
  PGBINDIR13=${PGBINDIR13:-${PGBINDIR:-"/usr/local/pgsql/bin"}}
+ 
+ # Where to look for tools (e.g. - slony1_dump.sh)
+ SLTOOLDIR=${SLTOOLDIR:-"../tools"}
\ No newline at end of file

Index: run_test.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/run_test.sh,v
retrieving revision 1.11
retrieving revision 1.11.2.1
diff -C2 -d -r1.11 -r1.11.2.1
*** run_test.sh	9 Jun 2006 17:12:20 -0000	1.11
--- run_test.sh	20 Apr 2007 20:51:09 -0000	1.11.2.1
***************
*** 176,183 ****
        eval user=\$USER${alias}
        eval port=\$PORT${alias}
  
        if [ -n "${db}" -a "${host}" -a "${user}" -a "${port}" ]; then
          if [ ${alias} -ne ${originnode} ]; then
!           echo "STORE NODE (id=${alias}, comment='node ${alias}');" >> $mktmp/slonik.script
          fi
          if [ ${alias} -ge ${NUMNODES} ]; then
--- 176,188 ----
        eval user=\$USER${alias}
        eval port=\$PORT${alias}
+       eval logship=\$LOGSHIP${alias}
  
        if [ -n "${db}" -a "${host}" -a "${user}" -a "${port}" ]; then
          if [ ${alias} -ne ${originnode} ]; then
!           if [ "x${logship}" == "xtrue" ]; then    # Don't bother generating nodes used for log shipping
!             status "Node ${alias} is a log shipping node - no need for STORE NODE"
!           else
!             echo "STORE NODE (id=${alias}, comment='node ${alias}');" >> $mktmp/slonik.script
!          fi
          fi
          if [ ${alias} -ge ${NUMNODES} ]; then
***************
*** 203,206 ****
--- 208,212 ----
      eval user=\$USER${i}
      eval port=\$PORT${i}
+     eval logship=\$LOGSHIP${i}
  
      if [ -n "${db}" -a "${host}" -a "${user}" -a "${port}" ]; then
***************
*** 212,217 ****
--- 218,229 ----
            eval buser=\$USER${j}
            eval bport=\$PORT${j}
+           eval blogship=\$LOGSHIP${j}
            if [ -n "${bdb}" -a "${bhost}" -a "${buser}" -a "${bport}" ]; then
+             if [[ "x${logship}" == "xtrue" || "x${blogship}" == "xtrue" ]]; then
+                 # log shipping node - no paths need exist that involve this node
+                 status "log shipping between nodes(${i}/${j}) - ls(${logship}/${blogship}) - omit STORE PATH"
+             else
  	    echo "STORE PATH (SERVER=${i}, CLIENT=${j}, CONNINFO='dbname=${db} host=${host} user=${user} port=${port}');" >> $mktmp/slonik.script
+             fi
            else
              err 3 "No conninfo"
***************
*** 471,474 ****
--- 483,488 ----
        eval port=\$PORT${alias}
        eval cluster=\$CLUSTER1
+       eval archive=\$ARCHIVE{alias}
+       eval logship=\$LOGSHIP${alias}
  
        if [ -n "${db}" -a "${host}" -a "${user}" -a "${port}" ]; then
***************
*** 479,497 ****
          eval slon${alias}_pid=
  
          conninfo="dbname=${db} host=${host} user=${user} port=${port}"
  
!         status "launching: $pgbindir/slon -s500 -g10 -d2 $cluster \"$conninfo\""
  
!         $pgbindir/slon -s500 -g10 -d2 $cluster "$conninfo" 1>> $mktmp/slon_log.${alias} 2>&1 &
!         tmppid=$!
!         tmpppid=$$
!         sleep 1
  
!         foo=$(_check_pid slon ${tmppid} ${tmpppid})
  
  
!         eval slon${alias}_pid=${foo}
!         if [ -z "${foo}" -o "${tmppid}" != "${foo}" ]; then
!           warn 3 "Failed to launch slon on node ${alias} check $mktmp/slon_log.${alias} for details"
          fi
        fi
--- 493,520 ----
          eval slon${alias}_pid=
  
+         if [ "x${archive}" != "xtrue" ]; then
+           status "Creating log shipping directory - $mktmp/archive_logs_3}${alias}"
+           mkdir -p $mktmp/archive_logs_${alias}
+           archiveparm="-a ${mktmp}/archive_logs_${alias}"
+        fi
          conninfo="dbname=${db} host=${host} user=${user} port=${port}"
  
!         if [ "x${logship}" == "xtrue" ]; then
!           status "do not launch slon for node ${alias} - it receives data via log shipping"
!         else
!           status "launching: $pgbindir/slon -s500 -g10 -d2 ${archiveparm} $cluster \"$conninfo\""
  
!           $pgbindir/slon -s500 -g10 -d2 ${archiveparm} $cluster "$conninfo" 1>> $mktmp/slon_log.${alias} 2>&1 &
!           tmppid=$!
!           tmpppid=$$
!           sleep 1
  
!           foo=$(_check_pid slon ${tmppid} ${tmpppid})
  
  
!           eval slon${alias}_pid=${foo}
!           if [ -z "${foo}" -o "${tmppid}" != "${foo}" ]; then
!             warn 3 "Failed to launch slon on node ${alias} check $mktmp/slon_log.${alias} for details"
!           fi
          fi
        fi

From cbbrowne at lists.slony.info  Fri Apr 20 13:51:11 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Apr 20 13:51:14 2007
Subject: [Slony1-commit] slony1-engine/tests/testlogship README
	gen_weak_user.sh generate_dml.sh init_add_tables.ik
	init_cluster.ik init_create_set.ik init_data.sql
	init_schema.sql init_subscribe_set.ik schema.diff settings.ik
Message-ID: <20070420205111.C5013290474@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testlogship
In directory main.slony.info:/tmp/cvs-serv31857/tests/testlogship

Added Files:
      Tag: REL_1_2_STABLE
	README gen_weak_user.sh generate_dml.sh init_add_tables.ik 
	init_cluster.ik init_create_set.ik init_data.sql 
	init_schema.sql init_subscribe_set.ik schema.diff settings.ik 
Log Message:
Add a log shipping regression test.  It is actually not adequate to test
things properly; I start here by adding it in in a form that "seems to
function" before tweaking it further.


--- NEW FILE: settings.ik ---
NUMCLUSTERS=${NUMCLUSTERS:-"1"}
NUMNODES=${NUMNODES:-"3"}
ORIGINNODE=1
WORKERS=${WORKERS:-"1"}
ARCHIVE2=true   # Node #2 needs to run log archiving
LOGSHIP3=true   # Node #3 receives data via log shipping
--- NEW FILE: init_cluster.ik ---
init cluster (id=1, comment = 'Regress test node');
echo 'update functions on node 1 after initializing it';
update functions (id=1);

--- NEW FILE: gen_weak_user.sh ---
weakuser=$1;

for i in 1 2 3 4 5; do
   echo "grant select on table public.table${i} to ${weakuser};"
   echo "grant select on table public.table${i}_id_seq to ${weakuser};"
done
--- NEW FILE: generate_dml.sh ---
. support_funcs.sh

init_dml()
{
  echo "init_dml()"
}

begin()
{
  echo "begin()"
}

rollback()
{
  echo "rollback()"
}

commit()
{
  echo "commit()"
}

generate_initdata()
{
  numrows=$(random_number 50 1000)
  i=0;
  trippoint=`expr $numrows / 20`
  j=0;
  percent=0
  status "generating ${numrows} transactions of random data"
  percent=`expr $j \* 5`
  status "$percent %"
  GENDATA="$mktmp/generate.data"
  echo "" > ${GENDATA}
  while : ; do
    txtalen=$(random_number 1 100)
    txta=$(random_string ${txtalen})
    txta=`echo ${txta} | sed -e "s/\\\\\\\/\\\\\\\\\\\\\\/g" -e "s/'/''/g"`
    txtblen=$(random_number 1 100)
    txtb=$(random_string ${txtblen})
    txtb=`echo ${txtb} | sed -e "s/\\\\\\\/\\\\\\\\\\\\\\/g" -e "s/'/''/g"`
    ra=$(random_number 1 9)
    rb=$(random_number 1 9)
    rc=$(random_number 1 9)
    echo "INSERT INTO table1(data) VALUES ('${txta}');" >> $GENDATA
    echo "INSERT INTO table2(table1_id,data) SELECT id, '${txtb}' FROM table1 WHERE data='${txta}';" >> $GENDATA
    echo "INSERT INTO table3(table2_id) SELECT id FROM table2 WHERE data ='${txtb}';" >> $GENDATA
    echo "INSERT INTO table5(numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol,bitcol) values ('${ra}${rb}.${rc}','${ra}.${rb}${rc}','(${ra},${rb})','((${ra},${ra}),(${rb},${rb}),(${rc},${rc}),(${ra},${rc}))','((${ra},${rb}),(${rc},${ra}),(${rb},${rc}),(${rc},${rb}))','<(${ra},${rb}),${rc}>','192.168.${ra}.${rb}${rc}','08:00:2d:0${ra}:0${rb}:0${rc}',X'${ra}${rb}${rc}');" >> $GENDATA
    if [ ${i} -ge ${numrows} ]; then
      break;
    else
      i=$((${i} +1))
      working=`expr $i % $trippoint`
      if [ $working -eq 0 ]; then
        j=`expr $j + 1`
        percent=`expr $j \* 5`
        status "$percent %"
      fi 
    fi
  done
  status "done"
}

do_initdata()
{
  originnode=${ORIGINNODE:-"1"}
  eval db=\$DB${originnode}
  eval host=\$HOST${originnode}
  eval user=\$USER${originnode}
  eval port=\$PORT${originnode}
  generate_initdata
  launch_poll
  status "loading data"
  $pgbindir/psql -h $host -p $port -d $db -U $user < $mktmp/generate.data 1> $mktmp/initdata.log 2> $mktmp/initdata.log
  if [ $? -ne 0 ]; then
    warn 3 "do_initdata failed, see $mktmp/initdata.log for details"
  fi 
  status "data load complete - nodes are seeded reasonably"

  status "purge archive log files up to present in order to eliminate those that cannot be used"
  for file in `/usr/bin/find ${mktmp}/archive_logs_2 -name "slony1_log_*.sql" -type f`; do
    status "purge ${file}"
    rm ${file}
  done
  sleep 5
  status "pull log shipping dump" 
  PGHOST=${HOST2} PGPORT=${PORT2} PGUSER=${USER2} ${SLTOOLDIR}/slony1_dump.sh ${DB2} ${CLUSTER1} > ${mktmp}/logship_dump.sql
  status "load log shipping dump into node #3"
  ${PGBINDIR3}/psql -h ${HOST3} -p ${PORT3} -U ${USER3} -d ${DB3} -f ${mktmp}/logship_dump.sql
  
  status "generate more data to test log shipping"
  generate_initdata
  launch_poll
  status "loading data"
  $pgbindir/psql -h $host -p $port -d $db -U $user < $mktmp/generate.data 1> $mktmp/moredata.log 2> $mktmp/moredata.log
  if [ $? -ne 0 ]; then
    warn 3 "loading data failed, see $mktmp/moredata.log for details"
  fi 
  wait_for_catchup
  status "second data load complete - now load files into log shipped node"
  for logfile in `/usr/bin/find ${mktmp}/archive_logs_2 -name "slony1_log_*.sql" -type f | sort`; do
    $pgbindir/psql -h ${HOST3} -p ${PORT3} -d ${DB3} -U ${USER3} -f ${logfile} >> $mktmp/logshipping_output.log 2>> $mktmp/logshipping_errors.log
    status "load file ${logfile} - ${?}"
  done
  status "done"
}

--- NEW FILE: init_add_tables.ik ---
set add table (id=1, set id=1, origin=1, fully qualified name = 'public.table1', comment='accounts table');
set add table (id=2, set id=1, origin=1, fully qualified name = 'public.table2', key='table2_id_key');

try {
   set add table (id=3, set id=1, origin=1, fully qualified name = 'public.table3', key = 'no_good_candidate_pk', comment='bad table - table 3');
} on error {
   echo 'Tried to replicate table3 with no good candidate PK - rejected';
} on success {
   echo 'Tried to replicate table3 with no good candidate PK - accepted';
   exit 1;
}

set add table (id=4, set id=1, origin=1, fully qualified name = 'public.table4', comment='a table of many types');
--- NEW FILE: init_create_set.ik ---
create set (id=1, origin=1, comment='All test1 tables');


--- NEW FILE: init_data.sql ---
INSERT INTO table1(data) VALUES ('placeholder 1');
INSERT INTO table1(data) VALUES ('placeholder 2');
INSERT INTO table2(table1_id,data) VALUES (1,'placeholder 1');
INSERT INTO table2(table1_id,data) VALUES (2,'placeholder 2');

INSERT INTO table4(numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol,bitcol) values ('74.0','7.40','(7,4)','((7,7),(4,4),(0,0),(7,0))','((7,4),(0,7),(4,0),(0,4))','<(7,4),0>','192.168.7.40','08:00:2d:07:04:00',X'740');

INSERT INTO table4(numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol,bitcol) values ('93.1','9.31','(9,3)','((9,9),(3,3),(1,1),(9,1))','((9,3),(1,9),(3,1),(1,3))','<(9,3),1>','192.168.9.31','08:00:2d:09:03:01',X'931');
--- NEW FILE: init_schema.sql ---
CREATE TABLE table1(
  id		SERIAL		PRIMARY KEY, 
  data		TEXT
);

CREATE TABLE table2(
  id		SERIAL		UNIQUE NOT NULL, 
  table1_id	INT4		REFERENCES table1(id) 
					ON UPDATE CASCADE ON DELETE CASCADE, 
  data		TEXT
);

create table table3 (
  id serial NOT NULL,
  id2 integer
);

create unique index no_good_candidate_pk on table3 (id, id2);

create table table4 (
  id serial primary key,
  numcol numeric(12,4), -- 1.23
  realcol real,     -- (1.23)
  ptcol point,      -- (1,2)
  pathcol path,     -- ((1,1),(2,2),(3,3),(4,4))
  polycol polygon,  -- ((1,1),(2,2),(3,3),(4,4))
  circcol circle,   -- <(1,2>,3>
  ipcol inet,       -- "192.168.1.1"
  maccol macaddr,   -- "04:05:06:07:08:09"
  bitcol bit varying(20)  -- X'123' 
);

--- NEW FILE: schema.diff ---
SELECT id,data FROM table1 ORDER BY id
SELECT id,table1_id,data FROM table2 ORDER BY id
SELECT id,numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol, bitcol from table4 order by id;

--- NEW FILE: README ---
$Id: README,v 1.1.2.1 2007-04-20 20:51:09 cbbrowne Exp $
  
testlogship is a basic test that replication generally functions with
log shipping.  
  
It creates three simple tables as one replication set, and replicates
them from one database to another.
  
The three tables are of the three interesting types:
  
1.  table1 has a formal primary key

2.  table2 lacks a formal primary key, but has a candidate primary key

It tries replicating a third table, which has an invalid candidate
primary key (columns not defined NOT NULL), which should cause it to
be rejected.  That is done in a slonik TRY {} block.

It also creates...

3.  table3 which has columns of all sorts of vaguely esoteric types to
exercise that points, paths, bitmaps, mac addresses, and inet types
replicate properly.

--- NEW FILE: init_subscribe_set.ik ---
subscribe set (id = 1, provider = 1, receiver = 2, forward = no);
echo 'sleep a couple of seconds...';
sleep (seconds = 2);
echo 'done sleeping...';

From cbbrowne at lists.slony.info  Fri Apr 20 13:53:20 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Apr 20 13:53:22 2007
Subject: [Slony1-commit] slony1-engine/src/slon cleanup_thread.c
	confoptions.c confoptions.h local_listen.c misc.c
	remote_listen.c remote_worker.c slon.c slon.h
Message-ID: <20070420205320.B6BFC2903B3@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slon
In directory main.slony.info:/tmp/cvs-serv31945/slon

Modified Files:
	cleanup_thread.c confoptions.c confoptions.h local_listen.c 
	misc.c remote_listen.c remote_worker.c slon.c slon.h 
Log Message:
Add a whole bunch of type annotations to CVS HEAD in order to silence
Splint (<http://splint.org/>) warnings.


Index: remote_listen.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/remote_listen.c,v
retrieving revision 1.34
retrieving revision 1.35
diff -C2 -d -r1.34 -r1.35
*** remote_listen.c	18 Apr 2007 22:19:07 -0000	1.34
--- remote_listen.c	20 Apr 2007 20:53:18 -0000	1.35
***************
*** 153,157 ****
  			if (node->listen_status == SLON_TSTAT_NONE ||
  				node->listen_status == SLON_TSTAT_SHUTDOWN ||
! 				!node->no_active)
  			{
  				rtcfg_unlock();
--- 153,157 ----
  			if (node->listen_status == SLON_TSTAT_NONE ||
  				node->listen_status == SLON_TSTAT_SHUTDOWN ||
! 			    !((bool) node->no_active))
  			{
  				rtcfg_unlock();
***************
*** 485,489 ****
  				lnode = rtcfg_findNode(listat->li_origin);
  				if (lnode != NULL && lnode->no_active)
! 					found = true;
  				break;
  			}
--- 485,489 ----
  				lnode = rtcfg_findNode(listat->li_origin);
  				if (lnode != NULL && lnode->no_active)
! 					found = (int) true;
  				break;
  			}

Index: local_listen.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/local_listen.c,v
retrieving revision 1.40
retrieving revision 1.41
diff -C2 -d -r1.40 -r1.41
*** local_listen.c	18 Apr 2007 22:19:07 -0000	1.40
--- local_listen.c	20 Apr 2007 20:53:18 -0000	1.41
***************
*** 36,40 ****
   */
  void *
! localListenThread_main(void *dummy)
  {
  	SlonConn   *conn;
--- 36,40 ----
   */
  void *
! localListenThread_main(/* @unused@ */ void *dummy)
  {
  	SlonConn   *conn;

Index: misc.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/misc.c,v
retrieving revision 1.24
retrieving revision 1.25
diff -C2 -d -r1.24 -r1.25
*** misc.c	18 Apr 2007 22:19:07 -0000	1.24
--- misc.c	20 Apr 2007 20:53:18 -0000	1.25
***************
*** 158,162 ****
  		)
  	{
! 		len = strftime(time_buf, sizeof(time_buf), log_timestamp_format, localtime(&stamp_time));
  		if (len == 0 && time_buf[0] != '\0') {
  			perror("slon_log: problem with strftime()");
--- 158,162 ----
  		)
  	{
! 		len = (int) strftime(time_buf, sizeof(time_buf), log_timestamp_format, localtime(&stamp_time));
  		if (len == 0 && time_buf[0] != '\0') {
  			perror("slon_log: problem with strftime()");

Index: remote_worker.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/remote_worker.c,v
retrieving revision 1.137
retrieving revision 1.138
diff -C2 -d -r1.137 -r1.138
*** remote_worker.c	18 Apr 2007 22:19:07 -0000	1.137
--- remote_worker.c	20 Apr 2007 20:53:18 -0000	1.138
***************
*** 213,217 ****
  static struct node_confirm_status *node_confirm_head = NULL;
  static struct node_confirm_status *node_confirm_tail = NULL;
! pthread_mutex_t node_confirm_lock = PTHREAD_MUTEX_INITIALIZER;
  
  int			sync_group_maxsize;
--- 213,217 ----
  static struct node_confirm_status *node_confirm_head = NULL;
  static struct node_confirm_status *node_confirm_tail = NULL;
! static pthread_mutex_t node_confirm_lock = PTHREAD_MUTEX_INITIALIZER;
  
  int			sync_group_maxsize;
***************
*** 219,231 ****
  int			sync_max_largemem;
  
! int			last_sync_group_size;
! int			next_sync_group_size;
  
  int			desired_sync_time;
! int			ideal_sync;
! struct timeval sync_start;
! struct timeval sync_end;
! int			last_sync_length;
! int			max_sync;
  int			min_sync;
  int			quit_sync_provider;
--- 219,231 ----
  int			sync_max_largemem;
  
! static int			last_sync_group_size;
! static int			next_sync_group_size;
  
  int			desired_sync_time;
! static int			ideal_sync;
! static struct timeval sync_start;
! static struct timeval sync_end;
! static int			last_sync_length;
! static  int			max_sync;
  int			min_sync;
  int			quit_sync_provider;

Index: slon.h
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/slon.h,v
retrieving revision 1.62
retrieving revision 1.63
diff -C2 -d -r1.62 -r1.63
*** slon.h	18 Apr 2007 22:19:07 -0000	1.62
--- slon.h	20 Apr 2007 20:53:18 -0000	1.63
***************
*** 410,414 ****
  #endif
  
- extern void slon_exit(int code);
  extern void Usage(char *const argv[]);
  
--- 410,413 ----

Index: confoptions.h
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/confoptions.h,v
retrieving revision 1.34
retrieving revision 1.35
diff -C2 -d -r1.34 -r1.35
*** confoptions.h	18 Apr 2007 22:19:07 -0000	1.34
--- confoptions.h	20 Apr 2007 20:53:18 -0000	1.35
***************
*** 8,13 ****
  void	   *get_config_option(const char *name);
  
- extern double real_placeholder;
- 
  extern char *rtcfg_cluster_name;
  extern char *rtcfg_conninfo;
--- 8,11 ----

Index: slon.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/slon.c,v
retrieving revision 1.71
retrieving revision 1.72
diff -C2 -d -r1.71 -r1.72
*** slon.c	18 Apr 2007 22:19:07 -0000	1.71
--- slon.c	20 Apr 2007 20:53:18 -0000	1.72
***************
*** 58,61 ****
--- 58,62 ----
   * ----------
   */
+ static void slon_exit(int code);
  static pthread_t local_event_thread;
  static pthread_t local_cleanup_thread;
***************
*** 789,793 ****
  #ifndef CYGWIN
  	act.sa_handler = &sighandler;
! 	sigemptyset(&act.sa_mask);
  	act.sa_flags = SA_NODEFER;
  
--- 790,794 ----
  #ifndef CYGWIN
  	act.sa_handler = &sighandler;
! 	(void) sigemptyset(&act.sa_mask);
  	act.sa_flags = SA_NODEFER;
  
***************
*** 852,861 ****
  	slon_log(SLON_DEBUG2, "slon: child terminated status: %d; pid: %d, current worker pid: %d\n", child_status, pid, slon_worker_pid);
  
! 	alarm(0);
  
  	switch (watchdog_status)
  	{
  		case SLON_WATCHDOG_RESTART:
! 			execvp(main_argv[0], main_argv);
  			slon_log(SLON_FATAL, "slon: cannot restart via execvp() - %s\n",
  					 strerror(errno));
--- 853,862 ----
  	slon_log(SLON_DEBUG2, "slon: child terminated status: %d; pid: %d, current worker pid: %d\n", child_status, pid, slon_worker_pid);
  
! 	(void) alarm(0);
  
  	switch (watchdog_status)
  	{
  		case SLON_WATCHDOG_RESTART:
! 			(void) execvp(main_argv[0], main_argv);
  			slon_log(SLON_FATAL, "slon: cannot restart via execvp() - %s\n",
  					 strerror(errno));
***************
*** 869,873 ****
  			{
  				slon_log(SLON_DEBUG1, "slon: restart of worker in 10 seconds\n");
! 				sleep(10);
  			}
  			else
--- 870,874 ----
  			{
  				slon_log(SLON_DEBUG1, "slon: restart of worker in 10 seconds\n");
! 				(void) sleep(10);
  			}
  			else
***************
*** 877,881 ****
  			if (watchdog_status == SLON_WATCHDOG_RETRY)
  			{
! 				execvp(main_argv[0], main_argv);
  				slon_log(SLON_FATAL, "slon: cannot restart via execvp() - %s\n",
  						 strerror(errno));
--- 878,882 ----
  			if (watchdog_status == SLON_WATCHDOG_RETRY)
  			{
! 				(void) execvp(main_argv[0], main_argv);
  				slon_log(SLON_FATAL, "slon: cannot restart via execvp() - %s\n",
  						 strerror(errno));
***************
*** 953,962 ****
  	{
  		slon_log(SLON_FATAL, "main: write to worker pipe failed -(%d) %s\n", errno, strerror(errno));
! 		kill(slon_worker_pid, SIGKILL);
  		slon_exit(-1);
  	}
!         close(sched_wakeuppipe[0]);
! 	close(sched_wakeuppipe[1]);
! 	alarm(20);
  }
  #endif
--- 954,963 ----
  	{
  		slon_log(SLON_FATAL, "main: write to worker pipe failed -(%d) %s\n", errno, strerror(errno));
! 		(void) kill(slon_worker_pid, SIGKILL);
  		slon_exit(-1);
  	}
!         (void) close(sched_wakeuppipe[0]);
! 	(void) close(sched_wakeuppipe[1]);
! 	(void) alarm(20);
  }
  #endif
***************
*** 966,970 ****
   * ----------
   */
! void
  slon_exit(int code)
  {
--- 967,971 ----
   * ----------
   */
! static void
  slon_exit(int code)
  {
***************
*** 977,981 ****
  	{
  		slon_log(SLON_DEBUG2, "slon: remove pid file\n");
! 		unlink(pid_file);
  	}
  
--- 978,982 ----
  	{
  		slon_log(SLON_DEBUG2, "slon: remove pid file\n");
! 		(void) unlink(pid_file);
  	}
  

Index: cleanup_thread.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/cleanup_thread.c,v
retrieving revision 1.36
retrieving revision 1.37
diff -C2 -d -r1.36 -r1.37
*** cleanup_thread.c	18 Apr 2007 22:19:07 -0000	1.36
--- cleanup_thread.c	20 Apr 2007 20:53:18 -0000	1.37
***************
*** 60,68 ****
   * ----------
   */
- /* @ -paramuse @ */
  void *
! cleanupThread_main(void *dummy)
  {
- /* @ +paramuse @ */
  	SlonConn   *conn;
  	SlonDString query1;
--- 60,66 ----
   * ----------
   */
  void *
! cleanupThread_main(/*@unused@*/ void *dummy)
  {
  	SlonConn   *conn;
  	SlonDString query1;
***************
*** 408,412 ****
  
  	dstring_init(&query1);
! 	slon_mkquery(&query1, "select %s.getMinXid();", rtcfg_namespace);
  	res = PQexec(dbconn, dstring_data(&query1));
  	if (PQresultStatus(res) != PGRES_TUPLES_OK)
--- 406,410 ----
  
  	dstring_init(&query1);
! 	(void) slon_mkquery(&query1, "select %s.getMinXid();", rtcfg_namespace);
  	res = PQexec(dbconn, dstring_data(&query1));
  	if (PQresultStatus(res) != PGRES_TUPLES_OK)
***************
*** 415,419 ****
  		PQclear(res);
  		slon_retry();
! 		return -1;
  	}
  	xid = strtoll(PQgetvalue(res, 0, 0), NULL, 10);
--- 413,417 ----
  		PQclear(res);
  		slon_retry();
! 		return (unsigned long) -1;
  	}
  	xid = strtoll(PQgetvalue(res, 0, 0), NULL, 10);

Index: confoptions.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/confoptions.c,v
retrieving revision 1.22
retrieving revision 1.23
diff -C2 -d -r1.22 -r1.23
*** confoptions.c	18 Apr 2007 22:19:07 -0000	1.22
--- confoptions.c	20 Apr 2007 20:53:18 -0000	1.23
***************
*** 18,24 ****
  void	   *get_config_option(const char *name);
  
! bool		bool_placeholder;
! double		real_placeholder;
! char	   *string_placeholder;
  
  
--- 18,24 ----
  void	   *get_config_option(const char *name);
  
! static bool		bool_placeholder;
! static double		real_placeholder;
! static char	   *string_placeholder;
  
  

From cbbrowne at lists.slony.info  Fri Apr 20 13:54:57 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Apr 20 13:54:58 2007
Subject: [Slony1-commit] slony1-engine/tests/testlogship README
	gen_weak_user.sh generate_dml.sh init_add_tables.ik
	init_cluster.ik init_create_set.ik init_data.sql
	init_schema.sql init_subscribe_set.ik schema.diff settings.ik
Message-ID: <20070420205457.6E5BE2903B3@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testlogship
In directory main.slony.info:/tmp/cvs-serv32070/testlogship

Added Files:
	README gen_weak_user.sh generate_dml.sh init_add_tables.ik 
	init_cluster.ik init_create_set.ik init_data.sql 
	init_schema.sql init_subscribe_set.ik schema.diff settings.ik 
Log Message:
Add initial log shipping test to CVS HEAD


--- NEW FILE: settings.ik ---
NUMCLUSTERS=${NUMCLUSTERS:-"1"}
NUMNODES=${NUMNODES:-"3"}
ORIGINNODE=1
WORKERS=${WORKERS:-"1"}
ARCHIVE2=true   # Node #2 needs to run log archiving
LOGSHIP3=true   # Node #3 receives data via log shipping
--- NEW FILE: init_cluster.ik ---
init cluster (id=1, comment = 'Regress test node');
echo 'update functions on node 1 after initializing it';
update functions (id=1);

--- NEW FILE: gen_weak_user.sh ---
weakuser=$1;

for i in 1 2 3 4 5; do
   echo "grant select on table public.table${i} to ${weakuser};"
   echo "grant select on table public.table${i}_id_seq to ${weakuser};"
done
--- NEW FILE: generate_dml.sh ---
. support_funcs.sh

init_dml()
{
  echo "init_dml()"
}

begin()
{
  echo "begin()"
}

rollback()
{
  echo "rollback()"
}

commit()
{
  echo "commit()"
}

generate_initdata()
{
  numrows=$(random_number 50 1000)
  i=0;
  trippoint=`expr $numrows / 20`
  j=0;
  percent=0
  status "generating ${numrows} transactions of random data"
  percent=`expr $j \* 5`
  status "$percent %"
  GENDATA="$mktmp/generate.data"
  echo "" > ${GENDATA}
  while : ; do
    txtalen=$(random_number 1 100)
    txta=$(random_string ${txtalen})
    txta=`echo ${txta} | sed -e "s/\\\\\\\/\\\\\\\\\\\\\\/g" -e "s/'/''/g"`
    txtblen=$(random_number 1 100)
    txtb=$(random_string ${txtblen})
    txtb=`echo ${txtb} | sed -e "s/\\\\\\\/\\\\\\\\\\\\\\/g" -e "s/'/''/g"`
    ra=$(random_number 1 9)
    rb=$(random_number 1 9)
    rc=$(random_number 1 9)
    echo "INSERT INTO table1(data) VALUES ('${txta}');" >> $GENDATA
    echo "INSERT INTO table2(table1_id,data) SELECT id, '${txtb}' FROM table1 WHERE data='${txta}';" >> $GENDATA
    echo "INSERT INTO table3(table2_id) SELECT id FROM table2 WHERE data ='${txtb}';" >> $GENDATA
    echo "INSERT INTO table5(numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol,bitcol) values ('${ra}${rb}.${rc}','${ra}.${rb}${rc}','(${ra},${rb})','((${ra},${ra}),(${rb},${rb}),(${rc},${rc}),(${ra},${rc}))','((${ra},${rb}),(${rc},${ra}),(${rb},${rc}),(${rc},${rb}))','<(${ra},${rb}),${rc}>','192.168.${ra}.${rb}${rc}','08:00:2d:0${ra}:0${rb}:0${rc}',X'${ra}${rb}${rc}');" >> $GENDATA
    if [ ${i} -ge ${numrows} ]; then
      break;
    else
      i=$((${i} +1))
      working=`expr $i % $trippoint`
      if [ $working -eq 0 ]; then
        j=`expr $j + 1`
        percent=`expr $j \* 5`
        status "$percent %"
      fi 
    fi
  done
  status "done"
}

do_initdata()
{
  originnode=${ORIGINNODE:-"1"}
  eval db=\$DB${originnode}
  eval host=\$HOST${originnode}
  eval user=\$USER${originnode}
  eval port=\$PORT${originnode}
  generate_initdata
  launch_poll
  status "loading data"
  $pgbindir/psql -h $host -p $port -d $db -U $user < $mktmp/generate.data 1> $mktmp/initdata.log 2> $mktmp/initdata.log
  if [ $? -ne 0 ]; then
    warn 3 "do_initdata failed, see $mktmp/initdata.log for details"
  fi 
  status "data load complete - nodes are seeded reasonably"

  status "purge archive log files up to present in order to eliminate those that cannot be used"
  for file in `/usr/bin/find ${mktmp}/archive_logs_2 -name "slony1_log_*.sql" -type f`; do
    status "purge ${file}"
    rm ${file}
  done
  sleep 5
  status "pull log shipping dump" 
  PGHOST=${HOST2} PGPORT=${PORT2} PGUSER=${USER2} ${SLTOOLDIR}/slony1_dump.sh ${DB2} ${CLUSTER1} > ${mktmp}/logship_dump.sql
  status "load log shipping dump into node #3"
  ${PGBINDIR3}/psql -h ${HOST3} -p ${PORT3} -U ${USER3} -d ${DB3} -f ${mktmp}/logship_dump.sql
  
  status "generate more data to test log shipping"
  generate_initdata
  launch_poll
  status "loading data"
  $pgbindir/psql -h $host -p $port -d $db -U $user < $mktmp/generate.data 1> $mktmp/moredata.log 2> $mktmp/moredata.log
  if [ $? -ne 0 ]; then
    warn 3 "loading data failed, see $mktmp/moredata.log for details"
  fi 
  wait_for_catchup
  status "second data load complete - now load files into log shipped node"
  for logfile in `/usr/bin/find ${mktmp}/archive_logs_2 -name "slony1_log_*.sql" -type f | sort`; do
    $pgbindir/psql -h ${HOST3} -p ${PORT3} -d ${DB3} -U ${USER3} -f ${logfile} >> $mktmp/logshipping_output.log 2>> $mktmp/logshipping_errors.log
    status "load file ${logfile} - ${?}"
  done
  status "done"
}

--- NEW FILE: init_add_tables.ik ---
set add table (id=1, set id=1, origin=1, fully qualified name = 'public.table1', comment='accounts table');
set add table (id=2, set id=1, origin=1, fully qualified name = 'public.table2', key='table2_id_key');

try {
   set add table (id=3, set id=1, origin=1, fully qualified name = 'public.table3', key = 'no_good_candidate_pk', comment='bad table - table 3');
} on error {
   echo 'Tried to replicate table3 with no good candidate PK - rejected';
} on success {
   echo 'Tried to replicate table3 with no good candidate PK - accepted';
   exit 1;
}

set add table (id=4, set id=1, origin=1, fully qualified name = 'public.table4', comment='a table of many types');
--- NEW FILE: init_create_set.ik ---
create set (id=1, origin=1, comment='All test1 tables');


--- NEW FILE: init_data.sql ---
INSERT INTO table1(data) VALUES ('placeholder 1');
INSERT INTO table1(data) VALUES ('placeholder 2');
INSERT INTO table2(table1_id,data) VALUES (1,'placeholder 1');
INSERT INTO table2(table1_id,data) VALUES (2,'placeholder 2');

INSERT INTO table4(numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol,bitcol) values ('74.0','7.40','(7,4)','((7,7),(4,4),(0,0),(7,0))','((7,4),(0,7),(4,0),(0,4))','<(7,4),0>','192.168.7.40','08:00:2d:07:04:00',X'740');

INSERT INTO table4(numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol,bitcol) values ('93.1','9.31','(9,3)','((9,9),(3,3),(1,1),(9,1))','((9,3),(1,9),(3,1),(1,3))','<(9,3),1>','192.168.9.31','08:00:2d:09:03:01',X'931');
--- NEW FILE: init_schema.sql ---
CREATE TABLE table1(
  id		SERIAL		PRIMARY KEY, 
  data		TEXT
);

CREATE TABLE table2(
  id		SERIAL		UNIQUE NOT NULL, 
  table1_id	INT4		REFERENCES table1(id) 
					ON UPDATE CASCADE ON DELETE CASCADE, 
  data		TEXT
);

create table table3 (
  id serial NOT NULL,
  id2 integer
);

create unique index no_good_candidate_pk on table3 (id, id2);

create table table4 (
  id serial primary key,
  numcol numeric(12,4), -- 1.23
  realcol real,     -- (1.23)
  ptcol point,      -- (1,2)
  pathcol path,     -- ((1,1),(2,2),(3,3),(4,4))
  polycol polygon,  -- ((1,1),(2,2),(3,3),(4,4))
  circcol circle,   -- <(1,2>,3>
  ipcol inet,       -- "192.168.1.1"
  maccol macaddr,   -- "04:05:06:07:08:09"
  bitcol bit varying(20)  -- X'123' 
);

--- NEW FILE: schema.diff ---
SELECT id,data FROM table1 ORDER BY id
SELECT id,table1_id,data FROM table2 ORDER BY id
SELECT id,numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol, bitcol from table4 order by id;

--- NEW FILE: README ---
$Id: README,v 1.2 2007-04-20 20:54:55 cbbrowne Exp $
  
testlogship is a basic test that replication generally functions with
log shipping.  
  
It creates three simple tables as one replication set, and replicates
them from one database to another.
  
The three tables are of the three interesting types:
  
1.  table1 has a formal primary key

2.  table2 lacks a formal primary key, but has a candidate primary key

It tries replicating a third table, which has an invalid candidate
primary key (columns not defined NOT NULL), which should cause it to
be rejected.  That is done in a slonik TRY {} block.

It also creates...

3.  table3 which has columns of all sorts of vaguely esoteric types to
exercise that points, paths, bitmaps, mac addresses, and inet types
replicate properly.

--- NEW FILE: init_subscribe_set.ik ---
subscribe set (id = 1, provider = 1, receiver = 2, forward = no);
echo 'sleep a couple of seconds...';
sleep (seconds = 2);
echo 'done sleeping...';

From cbbrowne at lists.slony.info  Fri Apr 20 13:55:35 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Apr 20 13:55:37 2007
Subject: [Slony1-commit] slony1-engine/doc/adminguide testbed.sgml
Message-ID: <20070420205535.838B52902EA@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv32103

Modified Files:
	testbed.sgml 
Log Message:
Documentation (for CVS HEAD) for new options added to support log shipping
tests


Index: testbed.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/testbed.sgml,v
retrieving revision 1.11
retrieving revision 1.12
diff -C2 -d -r1.11 -r1.12
*** testbed.sgml	1 Mar 2007 21:02:31 -0000	1.11
--- testbed.sgml	20 Apr 2007 20:55:33 -0000	1.12
***************
*** 147,150 ****
--- 147,184 ----
  </glossentry>
  
+ <glossentry>
+ <glossterm><envar>SLTOOLDIR</envar></glossterm>
+ 
+ <glossdef><para> Where to look for &slony1; tools such as
+ <application>slony1_dump.sh</application>.  </para></glossdef>
+ 
+ </glossentry>
+ 
+ <glossentry>
+ <glossterm><envar>ARCHIVE[n]</envar></glossterm>
+ 
+ <glossdef><para> If set to <quote>true</quote>, for a particular node,
+ which will normally get configured out of human sight in the
+ generic-to-a-particular-test file <filename>settings.ik</filename>,
+ then this node will be used as a data source for <xref
+ linkend="logshipping">, and this causes the test tools to set up a
+ directory for the <link linkend="slon-config-archive-dir">
+ archive_dir</link> option.  </para></glossdef>
+ 
+ </glossentry>
+ 
+ <glossentry>
+ <glossterm><envar>LOGSHIP[n]</envar></glossterm>
+ 
+ <glossdef><para> If set to <quote>true</quote>, for a particular node,
+ which will normally get configured out of human sight in
+ <filename>settings.ik</filename> for a particular test, then this
+ indicates that this node is being created via <xref
+ linkend="logshipping">, and a &lslon; is not required for this node.
+ </para></glossdef>
+ 
+ </glossentry>
+ 
+ 
  </glosslist>
  

From cbbrowne at lists.slony.info  Fri Apr 20 14:40:04 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Apr 20 14:40:06 2007
Subject: [Slony1-commit] slony1-engine/tests run_test.sh
Message-ID: <20070420214004.A1527290449@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests
In directory main.slony.info:/tmp/cvs-serv4488/tests

Modified Files:
      Tag: REL_1_2_STABLE
	run_test.sh 
Log Message:
Add a DDL script to the log shipping test in order to point out a bug
in SYNC counting in log shipping.


Index: run_test.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/run_test.sh,v
retrieving revision 1.11.2.1
retrieving revision 1.11.2.2
diff -C2 -d -r1.11.2.1 -r1.11.2.2
*** run_test.sh	20 Apr 2007 20:51:09 -0000	1.11.2.1
--- run_test.sh	20 Apr 2007 21:40:02 -0000	1.11.2.2
***************
*** 494,498 ****
  
          if [ "x${archive}" != "xtrue" ]; then
!           status "Creating log shipping directory - $mktmp/archive_logs_3}${alias}"
            mkdir -p $mktmp/archive_logs_${alias}
            archiveparm="-a ${mktmp}/archive_logs_${alias}"
--- 494,498 ----
  
          if [ "x${archive}" != "xtrue" ]; then
!           status "Creating log shipping directory - $mktmp/archive_logs_${alias}"
            mkdir -p $mktmp/archive_logs_${alias}
            archiveparm="-a ${mktmp}/archive_logs_${alias}"

From cbbrowne at lists.slony.info  Fri Apr 20 14:40:04 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Apr 20 14:40:06 2007
Subject: [Slony1-commit] slony1-engine/tests/testlogship README
	ddl_updates.sql exec_ddl.sh generate_dml.sh schema.diff
Message-ID: <20070420214004.BE4D529046B@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testlogship
In directory main.slony.info:/tmp/cvs-serv4488/tests/testlogship

Modified Files:
      Tag: REL_1_2_STABLE
	README generate_dml.sh schema.diff 
Added Files:
      Tag: REL_1_2_STABLE
	ddl_updates.sql exec_ddl.sh 
Log Message:
Add a DDL script to the log shipping test in order to point out a bug
in SYNC counting in log shipping.


Index: README
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testlogship/README,v
retrieving revision 1.1.2.1
retrieving revision 1.1.2.2
diff -C2 -d -r1.1.2.1 -r1.1.2.2
*** README	20 Apr 2007 20:51:09 -0000	1.1.2.1
--- README	20 Apr 2007 21:40:02 -0000	1.1.2.2
***************
*** 19,23 ****
  It also creates...
  
! 3.  table3 which has columns of all sorts of vaguely esoteric types to
  exercise that points, paths, bitmaps, mac addresses, and inet types
  replicate properly.
--- 19,30 ----
  It also creates...
  
! 3.  table4 which has columns of all sorts of vaguely esoteric types to
  exercise that points, paths, bitmaps, mac addresses, and inet types
  replicate properly.
+ 
+ It then loads data into these tables.
+ 
+ The test proceeds to run a DDL script which alters the schema for
+ table 4, adding two new columns, one to be populated via a default,
+ for new tuples; the other has no default, but we assign the value 42
+ to all tuples existing at the time that the DDL script runs.
\ No newline at end of file

Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testlogship/generate_dml.sh,v
retrieving revision 1.1.2.1
retrieving revision 1.1.2.2
diff -C2 -d -r1.1.2.1 -r1.1.2.2
*** generate_dml.sh	20 Apr 2007 20:51:09 -0000	1.1.2.1
--- generate_dml.sh	20 Apr 2007 21:40:02 -0000	1.1.2.2
***************
*** 65,68 ****
--- 65,69 ----
  {
    originnode=${ORIGINNODE:-"1"}
+   SCRIPT=${mktmp}/slonik.script
    eval db=\$DB${originnode}
    eval host=\$HOST${originnode}
***************
*** 96,100 ****
    if [ $? -ne 0 ]; then
      warn 3 "loading data failed, see $mktmp/moredata.log for details"
!   fi 
    wait_for_catchup
    status "second data load complete - now load files into log shipped node"
--- 97,115 ----
    if [ $? -ne 0 ]; then
      warn 3 "loading data failed, see $mktmp/moredata.log for details"
!   fi
!   wait_for_catchup
! 
!   status "execute DDL script"
!   init_preamble
!   sh ${testname}/exec_ddl.sh ${testname} >> $SCRIPT
!   do_ik
!   status "completed DDL script"
! 
!   status "Generate some more data"
!   generate_initdata
!   eval db=\$DB${originnode}
!   status "loading extra data to node $db"
!   $pgbindir/psql -h $host -p $port -U $user -d $db < $mktmp/generate.data 1> ${mktmp}/even_more_data.log 2> ${mktmp}/even_more_data.log2
! 
    wait_for_catchup
    status "second data load complete - now load files into log shipped node"

--- NEW FILE: ddl_updates.sql ---
alter table table4 add column newcol timestamptz;
alter table table4 alter column newcol set default now();
alter table table4 add column newint integer;
update table4 set newint = 42;

--- NEW FILE: exec_ddl.sh ---
testname=$1
echo "
  EXECUTE SCRIPT (
       SET ID = 1,
       FILENAME = '${testname}/ddl_updates.sql',
       EVENT NODE = 1
    );
"

Index: schema.diff
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testlogship/schema.diff,v
retrieving revision 1.1.2.1
retrieving revision 1.1.2.2
diff -C2 -d -r1.1.2.1 -r1.1.2.2
*** schema.diff	20 Apr 2007 20:51:09 -0000	1.1.2.1
--- schema.diff	20 Apr 2007 21:40:02 -0000	1.1.2.2
***************
*** 1,3 ****
  SELECT id,data FROM table1 ORDER BY id
  SELECT id,table1_id,data FROM table2 ORDER BY id
! SELECT id,numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol, bitcol from table4 order by id;
--- 1,3 ----
  SELECT id,data FROM table1 ORDER BY id
  SELECT id,table1_id,data FROM table2 ORDER BY id
! SELECT id,numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol, bitcol, newcol, newint from table4 order by id;

From cbbrowne at lists.slony.info  Fri Apr 20 14:40:04 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Apr 20 14:40:06 2007
Subject: [Slony1-commit] slony1-engine RELEASE-1.2.10
Message-ID: <20070420214004.930FD29043A@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine
In directory main.slony.info:/tmp/cvs-serv4488

Modified Files:
      Tag: REL_1_2_STABLE
	RELEASE-1.2.10 
Log Message:
Add a DDL script to the log shipping test in order to point out a bug
in SYNC counting in log shipping.


Index: RELEASE-1.2.10
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/Attic/RELEASE-1.2.10,v
retrieving revision 1.1.2.3
retrieving revision 1.1.2.4
diff -C2 -d -r1.1.2.3 -r1.1.2.4
*** RELEASE-1.2.10	20 Apr 2007 20:51:09 -0000	1.1.2.3
--- RELEASE-1.2.10	20 Apr 2007 21:40:02 -0000	1.1.2.4
***************
*** 6,7 ****
--- 6,10 ----
  
  - Added a test script for log shipping
+ 
+   ... And alter it to add invocation of a DDL script.  This
+   allows testing for an event-counting problem in log shipping.

From cbbrowne at lists.slony.info  Fri Apr 20 14:43:16 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Apr 20 14:43:18 2007
Subject: [Slony1-commit] slony1-engine/tests/testlogship README
	ddl_updates.sql exec_ddl.sh generate_dml.sh schema.diff
Message-ID: <20070420214316.F1E0429043A@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests/testlogship
In directory main.slony.info:/tmp/cvs-serv4869/testlogship

Modified Files:
	README generate_dml.sh schema.diff 
Added Files:
	ddl_updates.sql exec_ddl.sh 
Log Message:
Log shipping test added to HEAD which tweaks a problem found in 1.2
where DDL statements (and other non-SYNC events) are not loaded
properly on a log shipped node.


Index: generate_dml.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testlogship/generate_dml.sh,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** generate_dml.sh	20 Apr 2007 20:54:55 -0000	1.2
--- generate_dml.sh	20 Apr 2007 21:43:14 -0000	1.3
***************
*** 65,68 ****
--- 65,69 ----
  {
    originnode=${ORIGINNODE:-"1"}
+   SCRIPT=${mktmp}/slonik.script
    eval db=\$DB${originnode}
    eval host=\$HOST${originnode}
***************
*** 96,100 ****
    if [ $? -ne 0 ]; then
      warn 3 "loading data failed, see $mktmp/moredata.log for details"
!   fi 
    wait_for_catchup
    status "second data load complete - now load files into log shipped node"
--- 97,115 ----
    if [ $? -ne 0 ]; then
      warn 3 "loading data failed, see $mktmp/moredata.log for details"
!   fi
!   wait_for_catchup
! 
!   status "execute DDL script"
!   init_preamble
!   sh ${testname}/exec_ddl.sh ${testname} >> $SCRIPT
!   do_ik
!   status "completed DDL script"
! 
!   status "Generate some more data"
!   generate_initdata
!   eval db=\$DB${originnode}
!   status "loading extra data to node $db"
!   $pgbindir/psql -h $host -p $port -U $user -d $db < $mktmp/generate.data 1> ${mktmp}/even_more_data.log 2> ${mktmp}/even_more_data.log2
! 
    wait_for_catchup
    status "second data load complete - now load files into log shipped node"

--- NEW FILE: ddl_updates.sql ---
alter table table4 add column newcol timestamptz;
alter table table4 alter column newcol set default now();
alter table table4 add column newint integer;
update table4 set newint = 42;

Index: schema.diff
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testlogship/schema.diff,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** schema.diff	20 Apr 2007 20:54:55 -0000	1.2
--- schema.diff	20 Apr 2007 21:43:14 -0000	1.3
***************
*** 1,3 ****
  SELECT id,data FROM table1 ORDER BY id
  SELECT id,table1_id,data FROM table2 ORDER BY id
! SELECT id,numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol, bitcol from table4 order by id;
--- 1,3 ----
  SELECT id,data FROM table1 ORDER BY id
  SELECT id,table1_id,data FROM table2 ORDER BY id
! SELECT id,numcol,realcol,ptcol,pathcol,polycol,circcol,ipcol,maccol, bitcol, newcol, newint from table4 order by id;

Index: README
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/testlogship/README,v
retrieving revision 1.2
retrieving revision 1.3
diff -C2 -d -r1.2 -r1.3
*** README	20 Apr 2007 20:54:55 -0000	1.2
--- README	20 Apr 2007 21:43:14 -0000	1.3
***************
*** 19,23 ****
  It also creates...
  
! 3.  table3 which has columns of all sorts of vaguely esoteric types to
  exercise that points, paths, bitmaps, mac addresses, and inet types
  replicate properly.
--- 19,30 ----
  It also creates...
  
! 3.  table4 which has columns of all sorts of vaguely esoteric types to
  exercise that points, paths, bitmaps, mac addresses, and inet types
  replicate properly.
+ 
+ It then loads data into these tables.
+ 
+ The test proceeds to run a DDL script which alters the schema for
+ table 4, adding two new columns, one to be populated via a default,
+ for new tuples; the other has no default, but we assign the value 42
+ to all tuples existing at the time that the DDL script runs.
\ No newline at end of file

--- NEW FILE: exec_ddl.sh ---
testname=$1
echo "
  EXECUTE SCRIPT (
       SET ID = 1,
       FILENAME = '${testname}/ddl_updates.sql',
       EVENT NODE = 1
    );
"

From cbbrowne at lists.slony.info  Fri Apr 20 14:43:16 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Apr 20 14:43:18 2007
Subject: [Slony1-commit] slony1-engine/tests run_test.sh settings.ik
Message-ID: <20070420214316.C2BD52902EA@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests
In directory main.slony.info:/tmp/cvs-serv4869

Modified Files:
	run_test.sh settings.ik 
Log Message:
Log shipping test added to HEAD which tweaks a problem found in 1.2
where DDL statements (and other non-SYNC events) are not loaded
properly on a log shipped node.


Index: settings.ik
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/settings.ik,v
retrieving revision 1.5
retrieving revision 1.6
diff -C2 -d -r1.5 -r1.6
*** settings.ik	1 Mar 2007 21:02:31 -0000	1.5
--- settings.ik	20 Apr 2007 21:43:14 -0000	1.6
***************
*** 100,101 ****
--- 100,104 ----
  PORT13=${PORT13:-${PGPORT:-"5432"}}
  PGBINDIR13=${PGBINDIR13:-${PGBINDIR:-"/usr/local/pgsql/bin"}}
+ 
+ # Where to look for tools (e.g. - slony1_dump.sh)
+ SLTOOLDIR=${SLTOOLDIR:-"../tools"}
\ No newline at end of file

Index: run_test.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/run_test.sh,v
retrieving revision 1.13
retrieving revision 1.14
diff -C2 -d -r1.13 -r1.14
*** run_test.sh	18 Apr 2007 15:03:51 -0000	1.13
--- run_test.sh	20 Apr 2007 21:43:14 -0000	1.14
***************
*** 176,183 ****
        eval user=\$USER${alias}
        eval port=\$PORT${alias}
  
        if [ -n "${db}" -a "${host}" -a "${user}" -a "${port}" ]; then
          if [ ${alias} -ne ${originnode} ]; then
!           echo "STORE NODE (id=${alias}, comment='node ${alias}');" >> $mktmp/slonik.script
          fi
          if [ ${alias} -ge ${NUMNODES} ]; then
--- 176,188 ----
        eval user=\$USER${alias}
        eval port=\$PORT${alias}
+       eval logship=\$LOGSHIP${alias}
  
        if [ -n "${db}" -a "${host}" -a "${user}" -a "${port}" ]; then
          if [ ${alias} -ne ${originnode} ]; then
!           if [ "x${logship}" == "xtrue" ]; then    # Don't bother generating nodes used for log shipping
!             status "Node ${alias} is a log shipping node - no need for STORE NODE"
!           else
!             echo "STORE NODE (id=${alias}, comment='node ${alias}');" >> $mktmp/slonik.script
!          fi
          fi
          if [ ${alias} -ge ${NUMNODES} ]; then
***************
*** 203,206 ****
--- 208,212 ----
      eval user=\$USER${i}
      eval port=\$PORT${i}
+     eval logship=\$LOGSHIP${i}
  
      if [ -n "${db}" -a "${host}" -a "${user}" -a "${port}" ]; then
***************
*** 212,217 ****
--- 218,229 ----
            eval buser=\$WEAKUSER${j}
            eval bport=\$PORT${j}
+           eval blogship=\$LOGSHIP${j}
            if [ -n "${bdb}" -a "${bhost}" -a "${buser}" -a "${bport}" ]; then
+              if [[ "x${logship}" == "xtrue" || "x${blogship}" == "xtrue" ]]; then
+                  # log shipping node - no paths need exist that involve this node
+                  status "log shipping between nodes(${i}/${j}) - ls(${logship}/${blogship}) - omit STORE PATH"
+              else
  	    echo "STORE PATH (SERVER=${i}, CLIENT=${j}, CONNINFO='dbname=${db} host=${host} user=${buser} port=${port}');" >> $mktmp/slonik.script
+              fi
            else
              err 3 "No conninfo"
***************
*** 514,517 ****
--- 526,531 ----
        eval port=\$PORT${alias}
        eval cluster=\$CLUSTER1
+       eval archive=\$ARCHIVE{alias}
+       eval logship=\$LOGSHIP${alias}
  
        if [ -n "${db}" -a "${host}" -a "${user}" -a "${port}" ]; then
***************
*** 522,540 ****
          eval slon${alias}_pid=
  
          conninfo="dbname=${db} host=${host} user=${user} port=${port}"
  
!         status "launching: $pgbindir/slon -s500 -g10 -d2 $cluster \"$conninfo\""
  
!         $pgbindir/slon -s500 -g10 -d2 $cluster "$conninfo" 1>> $mktmp/slon_log.${alias} 2>&1 &
!         tmppid=$!
!         tmpppid=$$
!         sleep 1
  
!         foo=$(_check_pid slon ${tmppid} ${tmpppid})
  
  
!         eval slon${alias}_pid=${foo}
!         if [ -z "${foo}" -o "${tmppid}" != "${foo}" ]; then
!           warn 3 "Failed to launch slon on node ${alias} check $mktmp/slon_log.${alias} for details"
          fi
        fi
--- 536,563 ----
          eval slon${alias}_pid=
  
+         if [ "x${archive}" != "xtrue" ]; then
+           status "Creating log shipping directory - $mktmp/archive_logs_${alias}"
+           mkdir -p $mktmp/archive_logs_${alias}
+           archiveparm="-a ${mktmp}/archive_logs_${alias}"
+        fi
          conninfo="dbname=${db} host=${host} user=${user} port=${port}"
  
!         if [ "x${logship}" == "xtrue" ]; then
!           status "do not launch slon for node ${alias} - it receives data via log shipping"
!         else
!           status "launching: $pgbindir/slon -s500 -g10 -d2 ${archiveparm} $cluster \"$conninfo\""
  
!           $pgbindir/slon -s500 -g10 -d2 ${archiveparm} $cluster "$conninfo" 1>> $mktmp/slon_log.${alias} 2>&1 &
!           tmppid=$!
!           tmpppid=$$
!           sleep 1
  
!           foo=$(_check_pid slon ${tmppid} ${tmpppid})
  
  
!           eval slon${alias}_pid=${foo}
!           if [ -z "${foo}" -o "${tmppid}" != "${foo}" ]; then
!             warn 3 "Failed to launch slon on node ${alias} check $mktmp/slon_log.${alias} for details"
!           fi
          fi
        fi

From cbbrowne at lists.slony.info  Tue Apr 24 09:27:03 2007
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Apr 24 09:27:04 2007
Subject: [Slony1-commit] slony1-engine/doc/adminguide slonik_ref.sgml
Message-ID: <20070424162703.5B39A290440@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv26686

Modified Files:
	slonik_ref.sgml 
Log Message:
Document a clever usage of STORE TRIGGER when the trigger is not yet
installed.

Per email from Andrew Sullivan...


Index: slonik_ref.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/slonik_ref.sgml,v
retrieving revision 1.68
retrieving revision 1.69
diff -C2 -d -r1.68 -r1.69
*** slonik_ref.sgml	18 Apr 2007 15:03:51 -0000	1.68
--- slonik_ref.sgml	24 Apr 2007 16:27:01 -0000	1.69
***************
*** 1833,1837 ****
       </variablelist>
      </para>
!     <para> This uses &funstoretrigger;. </para>
     </refsect1>
     <refsect1><title>Example</title>
--- 1833,1850 ----
       </variablelist>
      </para>
!     <note><para> A nifty trick is that you can run <command>STORE
!     TRIGGER</command> <emphasis>before the trigger is
!     installed;</emphasis> that will not cause any errors.  You could
!     thus add &slony1;'s handling of the trigger
!     <emphasis>before</emphasis> it is installed.  That allows you to
!     be certain that it becomes active on all nodes immediately upon
!     its installation via <xref linkend="stmtddlscript">; there is no
!     risk of events getting through in between the <command>EXECUTE
!     SCRIPT</command> and <command>STORE TRIGGER</command>
!     events. </para>
!     </note>
! 
!    <para> This uses
!     &funstoretrigger;. </para>
     </refsect1>
     <refsect1><title>Example</title>
***************
*** 1847,1851 ****
      <para> This operation will need to acquire an exclusive lock on
      the specified table on each node to which it applies in order to
!     alter table schemas to add back the trigger. </para>
     </refsect1>
     <refsect1> <title> Version Information </title>
--- 1860,1864 ----
      <para> This operation will need to acquire an exclusive lock on
      the specified table on each node to which it applies in order to
!     alter table schemas to add back the trigger, but only briefly. </para>
     </refsect1>
     <refsect1> <title> Version Information </title>

