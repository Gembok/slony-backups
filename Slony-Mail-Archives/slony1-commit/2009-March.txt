From cbbrowne at lists.slony.info  Tue Mar  3 14:19:49 2009
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Mar  3 14:19:51 2009
Subject: [Slony1-commit] slony1-engine/tools slony-cluster-analysis-mass.sh
	slony-cluster-analysis.sh
Message-ID: <20090303221949.B14BB290153@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tools
In directory main.slony.info:/tmp/cvs-serv10631

Modified Files:
	slony-cluster-analysis-mass.sh slony-cluster-analysis.sh 
Log Message:
Change "/usr/bin/sh" to "/bin/sh", per bug #74 per Rod Taylor


Index: slony-cluster-analysis.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tools/slony-cluster-analysis.sh,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** slony-cluster-analysis.sh	19 May 2006 20:43:48 -0000	1.1
--- slony-cluster-analysis.sh	3 Mar 2009 22:19:47 -0000	1.2
***************
*** 1,3 ****
! #!/usr/bin/sh
  # $Id$
  # Analyze Slony-I Configuration
--- 1,3 ----
! #!/bin/sh
  # $Id$
  # Analyze Slony-I Configuration

Index: slony-cluster-analysis-mass.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tools/slony-cluster-analysis-mass.sh,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** slony-cluster-analysis-mass.sh	19 May 2006 20:43:48 -0000	1.1
--- slony-cluster-analysis-mass.sh	3 Mar 2009 22:19:47 -0000	1.2
***************
*** 1,3 ****
! #!/usr/bin/sh
  # $Id$
  # Do cluster analyses
--- 1,3 ----
! #!/bin/sh
  # $Id$
  # Do cluster analyses

From cbbrowne at lists.slony.info  Tue Mar  3 14:20:46 2009
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Mar  3 14:20:48 2009
Subject: [Slony1-commit] slony1-engine/tools slony-cluster-analysis-mass.sh
	slony-cluster-analysis.sh
Message-ID: <20090303222046.DC5852902C0@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tools
In directory main.slony.info:/tmp/cvs-serv10986

Modified Files:
      Tag: REL_2_0_STABLE
	slony-cluster-analysis-mass.sh slony-cluster-analysis.sh 
Log Message:
Change /usr/bin/sh to /bin/sh per bug#74 per Rod Taylor


Index: slony-cluster-analysis.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tools/slony-cluster-analysis.sh,v
retrieving revision 1.1
retrieving revision 1.1.4.1
diff -C2 -d -r1.1 -r1.1.4.1
*** slony-cluster-analysis.sh	19 May 2006 20:43:48 -0000	1.1
--- slony-cluster-analysis.sh	3 Mar 2009 22:20:44 -0000	1.1.4.1
***************
*** 1,3 ****
! #!/usr/bin/sh
  # $Id$
  # Analyze Slony-I Configuration
--- 1,3 ----
! #!/bin/sh
  # $Id$
  # Analyze Slony-I Configuration

Index: slony-cluster-analysis-mass.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tools/slony-cluster-analysis-mass.sh,v
retrieving revision 1.1
retrieving revision 1.1.4.1
diff -C2 -d -r1.1 -r1.1.4.1
*** slony-cluster-analysis-mass.sh	19 May 2006 20:43:48 -0000	1.1
--- slony-cluster-analysis-mass.sh	3 Mar 2009 22:20:44 -0000	1.1.4.1
***************
*** 1,3 ****
! #!/usr/bin/sh
  # $Id$
  # Do cluster analyses
--- 1,3 ----
! #!/bin/sh
  # $Id$
  # Do cluster analyses

From wieck at lists.slony.info  Thu Mar  5 14:37:25 2009
From: wieck at lists.slony.info (Jan Wieck)
Date: Thu Mar  5 14:37:28 2009
Subject: [Slony1-commit] slony1-engine/src/ducttape test_2_pgbench.in
Message-ID: <20090305223725.99F8E290400@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/ducttape
In directory main.slony.info:/tmp/cvs-serv5923/src/ducttape

Modified Files:
      Tag: REL_1_2_STABLE
	test_2_pgbench.in 
Log Message:
Fixed a problem where slonik cannot perform a failover if multiple origins
fail at once. 

Jan


Index: test_2_pgbench.in
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/ducttape/test_2_pgbench.in,v
retrieving revision 1.2
retrieving revision 1.2.2.1
diff -C2 -d -r1.2 -r1.2.2.1
*** test_2_pgbench.in	1 Jun 2006 12:35:52 -0000	1.2
--- test_2_pgbench.in	5 Mar 2009 22:37:23 -0000	1.2.2.1
***************
*** 13,19 ****
  #	- starts the second replication daemon
  #	- creates the pgbench tables (schema only)
! #	- subscribes the replication set from the primary node
! #	- creates a 3rd database node similar that replicates
  #	  cascaded from the second node.
  #	- waits until the pgbench is terminated and the user
  #	  stopped all 3 replication engines.
--- 13,20 ----
  #	- starts the second replication daemon
  #	- creates the pgbench tables (schema only)
! #	- creates a 3rd database node similar that will replicate
  #	  cascaded from the second node.
+ #	- subscribes the replication set to node 2
+ #	- subscribes the replication set to node 3
  #	- waits until the pgbench is terminated and the user
  #	  stopped all 3 replication engines.
***************
*** 30,36 ****
  DB2=slony_test2
  DB3=slony_test3
  DEBUG_LEVEL=2
  
! PGBENCH_SCALE=10
  PGBENCH_CLIENTS=5
  PGBENCH_TRANS=`expr 50000 / $PGBENCH_CLIENTS`
--- 31,38 ----
  DB2=slony_test2
  DB3=slony_test3
+ DB4=slony_test4
  DEBUG_LEVEL=2
  
! PGBENCH_SCALE=2
  PGBENCH_CLIENTS=5
  PGBENCH_TRANS=`expr 50000 / $PGBENCH_CLIENTS`
***************
*** 55,58 ****
--- 57,64 ----
  		kill -15 $slon3_pid
  	fi
+ 	if [ ! -z $slon4_pid ] ; then
+ 		echo "**** killing node daemon 4"
+ 		kill -15 $slon4_pid
+ 	fi
  	exit -1
  ' 2 15
***************
*** 94,97 ****
--- 100,105 ----
  dropdb $DB3 || echo "**** ignored"
  sleep 1
+ dropdb $DB4 || echo "**** ignored"
+ sleep 1
  
  #####
***************
*** 129,134 ****
  	cluster name = T1;
  	node 1 admin conninfo = 'dbname=$DB1';
! 	node 2 admin conninfo = 'dbname=$DB1';
! 	node 3 admin conninfo = 'dbname=$DB1';
  	init cluster (id = 1, comment = 'Node 1');
  	echo 'Database $DB1 initialized as Node 1';
--- 137,144 ----
  	cluster name = T1;
  	node 1 admin conninfo = 'dbname=$DB1';
! 	node 2 admin conninfo = 'dbname=$DB2';
! 	node 3 admin conninfo = 'dbname=$DB3';
! 	node 4 admin conninfo = 'dbname=$DB4';
! 
  	init cluster (id = 1, comment = 'Node 1');
  	echo 'Database $DB1 initialized as Node 1';
***************
*** 150,153 ****
--- 160,164 ----
  	node 2 admin conninfo = 'dbname=$DB2';
  	node 3 admin conninfo = 'dbname=$DB3';
+ 	node 4 admin conninfo = 'dbname=$DB4';
  
  	try {
***************
*** 219,222 ****
--- 230,234 ----
  	node 2 admin conninfo = 'dbname=$DB2';
  	node 3 admin conninfo = 'dbname=$DB3';
+ 	node 4 admin conninfo = 'dbname=$DB4';
  
  	try {
***************
*** 257,282 ****
  	cat pgbench_schema.sql
  ) | psql -q $DB2
- slonik <<_EOF_
- 	cluster name = T1;
- 	node 1 admin conninfo = 'dbname=$DB1';
- 	node 2 admin conninfo = 'dbname=$DB2';
- 	node 3 admin conninfo = 'dbname=$DB3';
- 
- 	subscribe set ( id = 1, provider = 1, receiver = 2, forward = yes );
- _EOF_
- 
- echo ""
- echo "**********************************************************************"
- echo "**** $DB2 should now be copying data and attempting to catch up."
- echo "**********************************************************************"
- echo ""
  
  ######################################################################
! # Setup DB2 as a subscriber node and let it subscribe the replication
  # set of the running pgbench
  ######################################################################
- # echo -n "**** Hit enter when node 2 is done with copy set"
- # read line
- sleep `expr 60 \* $PGBENCH_SCALE`
  echo "**** creating database for Node 3"
  if ! createdb $DB3 ; then
--- 269,277 ----
  	cat pgbench_schema.sql
  ) | psql -q $DB2
  
  ######################################################################
! # Setup DB3 as a subscriber node and let it subscribe the replication
  # set of the running pgbench
  ######################################################################
  echo "**** creating database for Node 3"
  if ! createdb $DB3 ; then
***************
*** 301,305 ****
  	store path (server = 3, client = 2, conninfo = 'dbname=$DB3');
  
! 	echo 'Database $DB2 added as Node 2';
  _EOF_
  
--- 296,300 ----
  	store path (server = 3, client = 2, conninfo = 'dbname=$DB3');
  
! 	echo 'Database $DB3 added as Node 3';
  _EOF_
  
***************
*** 328,335 ****
  # Create the pgbench tables and subscribe to set 1
  ######################################################################
! echo "**** creating pgbench tables and subscribing Node 2 to set 1"
  (
  	cat pgbench_schema.sql
  ) | psql -q $DB3
  slonik <<_EOF_
  	cluster name = T1;
--- 323,345 ----
  # Create the pgbench tables and subscribe to set 1
  ######################################################################
! echo "**** creating pgbench tables and subscribing Node 3 to set 1"
  (
  	cat pgbench_schema.sql
  ) | psql -q $DB3
+ 
+ 
+ ######################################################################
+ # Setup DB4 as a subscriber node and let it subscribe the replication
+ # set of the running pgbench
+ ######################################################################
+ echo "**** creating database for Node 4"
+ if ! createdb $DB4 ; then
+ 	kill $pgbench_pid 2>/dev/null
+ 	kill $slon1_pid 2>/dev/null
+ 	kill $slon2_pid 2>/dev/null
+ 	exit -1
+ fi
+ 
+ echo "**** initializing $DB4 as Node 4 of Slony-I cluster T1"
  slonik <<_EOF_
  	cluster name = T1;
***************
*** 337,352 ****
  	node 2 admin conninfo = 'dbname=$DB2';
  	node 3 admin conninfo = 'dbname=$DB3';
  
! 	subscribe set ( id = 1, provider = 2, receiver = 3, forward = no );
  _EOF_
  
  echo ""
  echo "**********************************************************************"
! echo "**** $DB3 should now be copying data and attempting to catch up."
  echo "**********************************************************************"
  echo ""
  
  
- 
  echo -n "**** waiting for pgbench to finish "
  while kill -0 $pgbench_pid 2>/dev/null ; do
--- 347,437 ----
  	node 2 admin conninfo = 'dbname=$DB2';
  	node 3 admin conninfo = 'dbname=$DB3';
+ 	node 4 admin conninfo = 'dbname=$DB4';
  
! 	store node (id = 4, comment = 'Node 4');
! 
! 	store path (server = 1, client = 4, conninfo = 'dbname=$DB1');
! 	store path (server = 3, client = 4, conninfo = 'dbname=$DB3');
! 	store path (server = 4, client = 1, conninfo = 'dbname=$DB4');
! 	store path (server = 4, client = 3, conninfo = 'dbname=$DB4');
! 
! 	echo 'Database $DB4 added as Node 4';
! _EOF_
! 
! if [ $? -ne 0 ] ; then
! 	kill $pgbench_pid 2>/dev/null
! 	kill $slon1_pid 2>/dev/null
! 	kill $slon2_pid 2>/dev/null
! 	kill $slon3_pid 2>/dev/null
! 	exit -1
! fi
! 
! echo "**** starting the Slony-I node daemon for $DB4"
! $TERMPROG -title "Slon node 4" -e sh -c "slon -d$DEBUG_LEVEL T1 dbname=$DB4; echo -n 'Enter>'; read line" &
! slon4_pid=$!
! echo "slon[$slon4_pid] on dbname=$DB4"
! 
! #####
! # Check that pgbench is still running
! #####
! if ! kill -0 $pgbench_pid 2>/dev/null ; then
! 	echo "**** pgbench terminated ???"
! 	kill $slon1_pid 2>/dev/null
! 	kill $slon2_pid 2>/dev/null
! 	kill $slon3_pid 2>/dev/null
! 	kill $slon4_pid 2>/dev/null
! 	exit -1
! fi
! 
! ######################################################################
! # Create the pgbench tables and subscribe to set 1
! ######################################################################
! echo "**** creating pgbench tables and subscribing Node 4 to set 1"
! (
! 	cat pgbench_schema.sql
! ) | psql -q $DB4
! 
! 
! ######################################################################
! # Now subscribe both nodes and use WAIT properly
! ######################################################################
! slonik <<_EOF_
! 	cluster name = T1;
! 	node 1 admin conninfo = 'dbname=$DB1';
! 	node 2 admin conninfo = 'dbname=$DB2';
! 	node 3 admin conninfo = 'dbname=$DB3';
! 	node 4 admin conninfo = 'dbname=$DB4';
! 
! 	echo 'Subscribing node 2';
! 	subscribe set ( id = 1, provider = 1, receiver = 2, forward = yes );
! 	echo 'Generating SYNC on Origin and waiting for that';
! 	sync (id = 1);
! 	wait for event (origin = 1, confirmed = all, wait on = 2, timeout = 0);
! 	echo 'Subscription of node 2 complete';
! 
! 	echo 'Subscribing node 3';
! 	subscribe set ( id = 1, provider = 2, receiver = 3, forward = yes );
! 	echo 'Waiting for SUBSCRIBE_SET to arrive at 1';
! 	wait for event (origin = 2, confirmed = all, wait on = 1, timeout = 0);
! 	echo 'Generating SYNC on Origin and waiting for that';
! 	sync (id = 1);
! 	wait for event (origin = 1, confirmed = all, wait on = 3, timeout = 0);
! 
! 	echo 'Subscribing node 4';
! 	subscribe set ( id = 1, provider = 3, receiver = 4, forward = no );
! 	echo 'Waiting for SUBSCRIBE_SET to arrive at 1';
! 	wait for event (origin = 3, confirmed = all, wait on = 1, timeout = 0);
! 	echo 'Generating SYNC on Origin and waiting for that';
! 	sync (id = 1);
! 	wait for event (origin = 1, confirmed = all, wait on = 4, timeout = 0);
  _EOF_
  
  echo ""
  echo "**********************************************************************"
! echo "**** $DB3 should now be done copying data and attempting to catch up."
  echo "**********************************************************************"
  echo ""
  
  
  echo -n "**** waiting for pgbench to finish "
  while kill -0 $pgbench_pid 2>/dev/null ; do
***************
*** 355,362 ****
  done
  echo "**** pgbench finished"
! echo "**** please terminate the replication engines when caught up."
  wait $slon1_pid
  wait $slon2_pid
  wait $slon3_pid
  
  kill $pgbench_pid 2>/dev/null
--- 440,460 ----
  done
  echo "**** pgbench finished"
! sleep 5
! slonik <<_EOF_
! 	cluster name = T1;
! 	node 1 admin conninfo = 'dbname=$DB1';
! 	node 2 admin conninfo = 'dbname=$DB2';
! 	node 3 admin conninfo = 'dbname=$DB3';
! 	node 4 admin conninfo = 'dbname=$DB4';
! 
! 	echo 'Generating final SYNC on Origin and waiting for that';
! 	sync (id = 1);
! 	wait for event (origin = 1, confirmed = all, wait on = 1, timeout = 0);
! _EOF_
! echo "**** please terminate the replication engines."
  wait $slon1_pid
  wait $slon2_pid
  wait $slon3_pid
+ wait $slon4_pid
  
  kill $pgbench_pid 2>/dev/null
***************
*** 364,367 ****
--- 462,466 ----
  kill $slon2_pid 2>/dev/null
  kill $slon3_pid 2>/dev/null
+ kill $slon4_pid 2>/dev/null
  
  echo -n "**** comparing databases ... "
***************
*** 396,399 ****
--- 495,508 ----
  			"_Slony-I_T1_rowID" from history order by "_Slony-I_T1_rowID";
  _EOF_
+ psql $DB4 -o dump.tmp.4.$$ <<_EOF_
+ 	select 'accounts:'::text, aid, bid, abalance, filler
+ 			from accounts order by aid;
+ 	select 'branches:'::text, bid, bbalance, filler
+ 			from branches order by bid;
+ 	select 'tellers:'::text, tid, bid, tbalance, filler
+ 			from tellers order by tid;
+ 	select 'history:'::text, tid, bid, aid, delta, mtime, filler,
+ 			"_Slony-I_T1_rowID" from history order by "_Slony-I_T1_rowID";
+ _EOF_
  
  if diff dump.tmp.1.$$ dump.tmp.2.$$ >test_2.1-2.diff ; then
***************
*** 408,414 ****
  	echo "success - databases 1 and 3 are equal."
  	rm dump.tmp.3.$$
- 	rm dump.tmp.1.$$
  	rm test_2.1-3.diff
  else
  	echo "FAILED - see test_2.1-3.diff for database differences"
  fi
--- 517,531 ----
  	echo "success - databases 1 and 3 are equal."
  	rm dump.tmp.3.$$
  	rm test_2.1-3.diff
  else
  	echo "FAILED - see test_2.1-3.diff for database differences"
  fi
+ echo -n "**** comparing databases ... "
+ if diff dump.tmp.1.$$ dump.tmp.4.$$ >test_2.1-4.diff ; then
+ 	echo "success - databases 1 and 4 are equal."
+ 	rm dump.tmp.4.$$
+ 	rm dump.tmp.1.$$
+ 	rm test_2.1-4.diff
+ else
+ 	echo "FAILED - see test_2.1-4.diff for database differences"
+ fi

From wieck at lists.slony.info  Thu Mar  5 14:37:25 2009
From: wieck at lists.slony.info (Jan Wieck)
Date: Thu Mar  5 14:37:28 2009
Subject: [Slony1-commit] slony1-engine/src/backend slony1_funcs.sql
Message-ID: <20090305223725.92ED72903FF@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/backend
In directory main.slony.info:/tmp/cvs-serv5923/src/backend

Modified Files:
      Tag: REL_1_2_STABLE
	slony1_funcs.sql 
Log Message:
Fixed a problem where slonik cannot perform a failover if multiple origins
fail at once. 

Jan


Index: slony1_funcs.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.sql,v
retrieving revision 1.98.2.31
retrieving revision 1.98.2.32
diff -C2 -d -r1.98.2.31 -r1.98.2.32
*** slony1_funcs.sql	12 Sep 2008 17:37:48 -0000	1.98.2.31
--- slony1_funcs.sql	5 Mar 2009 22:37:23 -0000	1.98.2.32
***************
*** 1141,1145 ****
  								and PP.pa_client = P.pa_client)
  	loop
! 		raise exception ''Slony-I: cannot failover - node % has no path to the backup node'',
  				v_row.pa_client;
  	end loop;
--- 1141,1145 ----
  								and PP.pa_client = P.pa_client)
  	loop
! 		raise notice ''Slony-I: Warning: node % has no path to the backup node'',
  				v_row.pa_client;
  	end loop;

From wieck at lists.slony.info  Thu Mar  5 14:37:25 2009
From: wieck at lists.slony.info (Jan Wieck)
Date: Thu Mar  5 14:37:28 2009
Subject: [Slony1-commit] slony1-engine/src/slonik slonik.c
Message-ID: <20090305223725.BC297148204@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slonik
In directory main.slony.info:/tmp/cvs-serv5923/src/slonik

Modified Files:
      Tag: REL_1_2_STABLE
	slonik.c 
Log Message:
Fixed a problem where slonik cannot perform a failover if multiple origins
fail at once. 

Jan


Index: slonik.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/slonik.c,v
retrieving revision 1.67.2.17
retrieving revision 1.67.2.18
diff -C2 -d -r1.67.2.17 -r1.67.2.18
*** slonik.c	3 Jun 2008 18:54:42 -0000	1.67.2.17
--- slonik.c	5 Mar 2009 22:37:23 -0000	1.67.2.18
***************
*** 2642,2645 ****
--- 2642,2655 ----
  	{
  		nodeinfo[i].no_id = (int)strtol(PQgetvalue(res1, i, 0), NULL, 10);
+ 
+ 		if (get_adminfo((SlonikStmt *) stmt, nodeinfo[i].no_id) == NULL)
+ 		{
+ 			printf("WARNING: no admin conninfo for node %d - node will not be considered\n",
+ 				nodeinfo[i].no_id);
+ 			nodeinfo[i].adminfo = NULL;
+ 			nodeinfo[i].has_slon = false;
+ 			continue;
+ 		}
+ 
  		nodeinfo[i].adminfo = get_active_adminfo((SlonikStmt *) stmt,
  												 nodeinfo[i].no_id);
***************
*** 2759,2762 ****
--- 2769,2775 ----
  			continue;
  
+ 		if (nodeinfo[i].adminfo == NULL)
+ 			continue;
+ 
  		if (db_exec_command((SlonikStmt *) stmt, nodeinfo[i].adminfo, &query) < 0)
  		{
***************
*** 2772,2775 ****
--- 2785,2791 ----
  	for (i = 0; i < num_nodes; i++)
  	{
+ 		if (nodeinfo[i].adminfo == NULL)
+ 			continue;
+ 
  		if (db_commit_xact((SlonikStmt *) stmt, nodeinfo[i].adminfo) < 0)
  		{
***************
*** 2838,2841 ****
--- 2854,2860 ----
  	for (i = 0; i < num_nodes; i++)
  	{
+ 		if (nodeinfo[i].adminfo == NULL)
+ 			continue;
+ 
  		res1 = db_exec_select((SlonikStmt *) stmt, nodeinfo[i].adminfo, &query);
  		if (res1 != NULL)
***************
*** 2906,2909 ****
--- 2925,2931 ----
  			int64		ssy_seqno;
  
+ 			if (setinfo[i].subscribers[j]->adminfo == NULL)
+ 				continue;
+ 
  			res1 = db_exec_select((SlonikStmt *) stmt,
  								  setinfo[i].subscribers[j]->adminfo, &query);
***************
*** 3033,3036 ****
--- 3055,3061 ----
  	for (i = 0; i < num_nodes; i++)
  	{
+ 		if (nodeinfo[i].adminfo == NULL)
+ 			continue;
+ 
  		if (db_commit_xact((SlonikStmt *) stmt, nodeinfo[i].adminfo) < 0)
  			rc = -1;

From cbbrowne at lists.slony.info  Fri Mar  6 10:53:56 2009
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Mar  6 10:53:57 2009
Subject: [Slony1-commit] slony1-engine/doc/adminguide firstdb.sgml
Message-ID: <20090306185356.0B6C029039A@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv9160

Modified Files:
	firstdb.sgml 
Log Message:
Bastian Voigt pointed out some 1.2-isms in the documentation of
"how to replicate your first database."  Fixed!


Index: firstdb.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/firstdb.sgml,v
retrieving revision 1.29
retrieving revision 1.30
diff -C2 -d -r1.29 -r1.30
*** firstdb.sgml	18 Sep 2008 21:30:08 -0000	1.29
--- firstdb.sgml	6 Mar 2009 18:53:53 -0000	1.30
***************
*** 107,111 ****
  
  <programlisting>
! psql -U $PGBENCH_USER -h $HOST1 -d $DBNAME1 -c "begin; alter table
  history add column id serial; update history set id =
  nextval('history_id_seq'); alter table history add primary key(id);
--- 107,111 ----
  
  <programlisting>
! psql -U $PGBENCHUSER -h $HOST1 -d $MASTERDBNAME -c "begin; alter table
  history add column id serial; update history set id =
  nextval('history_id_seq'); alter table history add primary key(id);
***************
*** 214,228 ****
   
  	#--
- 	# Because the history table does not have a primary key or other unique
- 	# constraint that could be used to identify a row, we need to add one.
- 	# The following command adds a bigint column named
- 	# _Slony-I_$CLUSTERNAME_rowID to the table.  It will have a default value
- 	# of nextval('_$CLUSTERNAME.s1_rowid_seq'), and have UNIQUE and NOT NULL
- 	# constraints applied.  All existing rows will be initialized with a
- 	# number
- 	#--
- 	table add key (node id = 1, fully qualified name = 'public.history');
- 
- 	#--
  	# Slony-I organizes tables into sets.  The smallest unit a node can
  	# subscribe is a set.  The following commands create one set containing
--- 214,217 ----
***************
*** 240,244 ****
  	#--
  
! 	store node (id=2, comment = 'Slave node');
  	store path (server = 1, client = 2, conninfo='dbname=$MASTERDBNAME host=$MASTERHOST user=$REPLICATIONUSER');
  	store path (server = 2, client = 1, conninfo='dbname=$SLAVEDBNAME host=$SLAVEHOST user=$REPLICATIONUSER');
--- 229,233 ----
  	#--
  
! 	store node (id=2, comment = 'Slave node', event node=1);
  	store path (server = 1, client = 2, conninfo='dbname=$MASTERDBNAME host=$MASTERHOST user=$REPLICATIONUSER');
  	store path (server = 2, client = 1, conninfo='dbname=$SLAVEDBNAME host=$SLAVEHOST user=$REPLICATIONUSER');

From cbbrowne at lists.slony.info  Fri Mar  6 10:54:12 2009
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Mar  6 10:54:13 2009
Subject: [Slony1-commit] slony1-engine/doc/adminguide firstdb.sgml
Message-ID: <20090306185412.09CB1290412@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv9202

Modified Files:
      Tag: REL_2_0_STABLE
	firstdb.sgml 
Log Message:
Bastian Voigt pointed out some 1.2-isms in the documentation of
"how to replicate your first database."  Fixed!



Index: firstdb.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/firstdb.sgml,v
retrieving revision 1.29
retrieving revision 1.29.2.1
diff -C2 -d -r1.29 -r1.29.2.1
*** firstdb.sgml	18 Sep 2008 21:30:08 -0000	1.29
--- firstdb.sgml	6 Mar 2009 18:54:09 -0000	1.29.2.1
***************
*** 107,111 ****
  
  <programlisting>
! psql -U $PGBENCH_USER -h $HOST1 -d $DBNAME1 -c "begin; alter table
  history add column id serial; update history set id =
  nextval('history_id_seq'); alter table history add primary key(id);
--- 107,111 ----
  
  <programlisting>
! psql -U $PGBENCHUSER -h $HOST1 -d $MASTERDBNAME -c "begin; alter table
  history add column id serial; update history set id =
  nextval('history_id_seq'); alter table history add primary key(id);
***************
*** 214,228 ****
   
  	#--
- 	# Because the history table does not have a primary key or other unique
- 	# constraint that could be used to identify a row, we need to add one.
- 	# The following command adds a bigint column named
- 	# _Slony-I_$CLUSTERNAME_rowID to the table.  It will have a default value
- 	# of nextval('_$CLUSTERNAME.s1_rowid_seq'), and have UNIQUE and NOT NULL
- 	# constraints applied.  All existing rows will be initialized with a
- 	# number
- 	#--
- 	table add key (node id = 1, fully qualified name = 'public.history');
- 
- 	#--
  	# Slony-I organizes tables into sets.  The smallest unit a node can
  	# subscribe is a set.  The following commands create one set containing
--- 214,217 ----
***************
*** 240,244 ****
  	#--
  
! 	store node (id=2, comment = 'Slave node');
  	store path (server = 1, client = 2, conninfo='dbname=$MASTERDBNAME host=$MASTERHOST user=$REPLICATIONUSER');
  	store path (server = 2, client = 1, conninfo='dbname=$SLAVEDBNAME host=$SLAVEHOST user=$REPLICATIONUSER');
--- 229,233 ----
  	#--
  
! 	store node (id=2, comment = 'Slave node', event node=1);
  	store path (server = 1, client = 2, conninfo='dbname=$MASTERDBNAME host=$MASTERHOST user=$REPLICATIONUSER');
  	store path (server = 2, client = 1, conninfo='dbname=$SLAVEDBNAME host=$SLAVEHOST user=$REPLICATIONUSER');

From wieck at lists.slony.info  Mon Mar 23 08:30:20 2009
From: wieck at lists.slony.info (Jan Wieck)
Date: Mon Mar 23 08:30:22 2009
Subject: [Slony1-commit] slony1-engine/src/backend slony1_funcs.sql
Message-ID: <20090323153020.8FC0A290058@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/backend
In directory main.slony.info:/tmp/cvs-serv30765/src/backend

Modified Files:
      Tag: REL_1_2_STABLE
	slony1_funcs.sql 
Log Message:
Fixed some broken logic in failedNode()

It apparently never supported failing over to nodes that are not
a direct subscriber to the failed node. 


Jan


Index: slony1_funcs.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.sql,v
retrieving revision 1.98.2.32
retrieving revision 1.98.2.33
diff -C2 -d -r1.98.2.32 -r1.98.2.33
*** slony1_funcs.sql	5 Mar 2009 22:37:23 -0000	1.98.2.32
--- slony1_funcs.sql	23 Mar 2009 15:30:18 -0000	1.98.2.33
***************
*** 1243,1249 ****
  			raise notice ''failedNode: set % has other direct receivers - change providers only'', v_row.set_id;
  			-- ----
! 			-- Backup node is not the only direct subscriber. This
! 			-- means that at this moment, we redirect all direct
! 			-- subscribers to receive from the backup node, and the
  			-- backup node itself to receive from another one.
  			-- The admin utility will wait for the slon engine to
--- 1243,1250 ----
  			raise notice ''failedNode: set % has other direct receivers - change providers only'', v_row.set_id;
  			-- ----
! 			-- Backup node is not the only direct subscriber or not
! 			-- a direct subscriber at all. 
! 			-- This means that at this moment, we redirect all possible
! 			-- direct subscribers to receive from the backup node, and the
  			-- backup node itself to receive from another one.
  			-- The admin utility will wait for the slon engine to
***************
*** 1256,1269 ****
  							from @NAMESPACE@.sl_subscribe SS
  							where SS.sub_set = v_row.set_id
- 								and SS.sub_provider = p_failed_node
  								and SS.sub_receiver <> p_backup_node
! 								and SS.sub_forward)
  					where sub_set = v_row.set_id
  						and sub_receiver = p_backup_node;
  			update @NAMESPACE@.sl_subscribe
! 					set sub_provider = p_backup_node
  					where sub_set = v_row.set_id
- 						and sub_provider = p_failed_node
  						and sub_receiver <> p_backup_node;
  		end if;
  	end loop;
--- 1257,1291 ----
  							from @NAMESPACE@.sl_subscribe SS
  							where SS.sub_set = v_row.set_id
  								and SS.sub_receiver <> p_backup_node
! 								and SS.sub_forward
! 								and exists (
! 									select 1 from @NAMESPACE@.sl_path
! 										where pa_server = SS.sub_receiver
! 										  and pa_client = p_backup_node
! 								))
  					where sub_set = v_row.set_id
  						and sub_receiver = p_backup_node;
  			update @NAMESPACE@.sl_subscribe
! 					set sub_provider = (select min(SS.sub_receiver)
! 							from @NAMESPACE@.sl_subscribe SS
! 							where SS.sub_set = v_row.set_id
! 								and SS.sub_receiver <> p_failed_node
! 								and SS.sub_forward
! 								and exists (
! 									select 1 from @NAMESPACE@.sl_path
! 										where pa_server = SS.sub_receiver
! 										  and pa_client = @NAMESPACE@.sl_subscribe.sub_receiver
! 								))
  					where sub_set = v_row.set_id
  						and sub_receiver <> p_backup_node;
+ 			update @NAMESPACE@.sl_subscribe
+ 					set sub_provider = p_backup_node
+ 					where sub_set = v_row.set_id
+ 						and sub_receiver <> p_backup_node
+ 						and exists (
+ 							select 1 from @NAMESPACE@.sl_path
+ 								where pa_server = p_backup_node
+ 								  and pa_client = @NAMESPACE@.sl_subscribe.sub_receiver
+ 						);
  		end if;
  	end loop;

