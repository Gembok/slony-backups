From cbbrowne at lists.slony.info  Fri Aug  1 12:25:43 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Aug  1 12:25:45 2008
Subject: [Slony1-commit] slony1-engine/doc/adminguide locking.sgml
	slonik_ref.sgml
Message-ID: <20080801192543.726D2290176@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv21755

Modified Files:
	locking.sgml slonik_ref.sgml 
Log Message:
Add some caveats about locking.  There's some locking that shouldn't
normally be visible, except when it is ;-).   Or rather, except when
someone runs a database-wide vacuum or something like that which hits
Slony-I-specific objects.


Index: slonik_ref.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/slonik_ref.sgml,v
retrieving revision 1.88
retrieving revision 1.89
diff -C2 -d -r1.88 -r1.89
*** slonik_ref.sgml	27 Jun 2008 20:17:17 -0000	1.88
--- slonik_ref.sgml	1 Aug 2008 19:25:41 -0000	1.89
***************
*** 1287,1294 ****
      <variablelist>
       <varlistentry><term><literal> ID = ival </literal></term>
!       <listitem><para> Unique ID of the set to contain the union of the two separate sets.</para></listitem>
       </varlistentry>
       <varlistentry><term><literal> ADD ID = ival </literal></term>
!       <listitem><para> Unique ID of the set whose objects should be transferred.  </para></listitem>
       </varlistentry>
       <varlistentry><term><literal> ORIGIN = ival </literal></term>
--- 1287,1294 ----
      <variablelist>
       <varlistentry><term><literal> ID = ival </literal></term>
!       <listitem><para> Unique ID of the set to contain the union of the two formerly separate sets.</para></listitem>
       </varlistentry>
       <varlistentry><term><literal> ADD ID = ival </literal></term>
!       <listitem><para> Unique ID of the set whose objects should be transferred into the above set.  </para></listitem>
       </varlistentry>
       <varlistentry><term><literal> ORIGIN = ival </literal></term>

Index: locking.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/locking.sgml,v
retrieving revision 1.10
retrieving revision 1.11
diff -C2 -d -r1.10 -r1.11
*** locking.sgml	2 Aug 2006 18:34:59 -0000	1.10
--- locking.sgml	1 Aug 2008 19:25:41 -0000	1.11
***************
*** 14,18 ****
  can access <quote>old tuples.</quote> Most of the time, this allows
  the gentle user of &postgres; to not need to worry very much about
! locks. </para>
  
  <para> Unfortunately, there are several sorts of &slony1; events that
--- 14,21 ----
  can access <quote>old tuples.</quote> Most of the time, this allows
  the gentle user of &postgres; to not need to worry very much about
! locks.  &slony1; configuration events normally grab locks on an
! internal table, <envar>sl_config_lock</envar>, which should not be
! visible to applications unless they are performing actions on &slony1;
! components.  </para>
  
  <para> Unfortunately, there are several sorts of &slony1; events that

From cbbrowne at lists.slony.info  Fri Aug  1 12:32:26 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Aug  1 12:32:28 2008
Subject: [Slony1-commit] slony1-engine/doc/adminguide adminscripts.sgml
Message-ID: <20080801193226.AFFA82902E9@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv22485

Modified Files:
	adminscripts.sgml 
Log Message:
Add in documentation for start_slon.sh script


Index: adminscripts.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/adminscripts.sgml,v
retrieving revision 1.50
retrieving revision 1.51
diff -C2 -d -r1.50 -r1.51
*** adminscripts.sgml	27 Feb 2008 19:37:03 -0000	1.50
--- adminscripts.sgml	1 Aug 2008 19:32:24 -0000	1.51
***************
*** 362,365 ****
--- 362,409 ----
  </sect2>
  
+ <sect2 id="startslon"> <title>start_slon.sh</title>
+ 
+ <para> This <filename>rc.d</filename>-style script was introduced in
+ &slony1; version 2.0; it provides automatable ways of:</para>
+ 
+ <itemizedlist>
+ <listitem><para>Starting the &lslon;, via <command> start_slon.sh start </command> </para> 
+ 
+ <para> Attempts to start the &lslon;, checking first to verify that it
+ is not already running, that configuration exists, and that the log
+ file location is writable.  Failure cases include:</para>
+ 
+ <itemizedlist>
+ <listitem><para> No <link linkend="runtime-config"> slon runtime configuration file </link> exists, </para></listitem>
+ <listitem><para> A &lslon; is found with the PID indicated via the runtime configuration, </para></listitem>
+ <listitem><para> The specified <envar>SLON_LOG</envar> location is not writable. </para></listitem>
+ </listitem>
+ <listitem><para>Stopping the &lslon;, via <command> start_slon.sh stop </command> </para> 
+ <para> This fails (doing nothing) if the PID (indicated via the runtime configuration file) does not exist; </para> </listitem>
+ <listitem><para>Monitoring the status of the &lslon;, via <command> start_slon.sh status </command> </para> 
+ <para> This indicates whether or not the &lslon; is running, and, if so, prints out the process ID. </para> </listitem>
+ 
+ </itemizedlist>
+ 
+ <para> The following environment variables are used to control &lslon; configuration:</para>
+ 
+ <glosslist>
+ <glossentry><glossterm> <envar> SLON_BIN_PATH </envar> </glossterm>
+ <glossdef><para> This indicates where the &lslon; binary program is found. </para> </glossdef> </glossentry>
+ <glossentry><glossterm> <envar> SLON_CONF </envar> </glossterm>
+ <glossdef><para> This indicates the location of the <link linkend="runtime-config"> slon runtime configuration file </link> that controls how the &lslon; behaves. </para> 
+ <para> Note that this file is <emphasis>required</emphasis> to contain a value for <link linkend="slon-config-logging-pid-file">log_pid_file</link>; that is necessary to allow this script to detect whether the &lslon; is running or not. </para>
+ </glossdef> </glossentry>
+ <glossentry><glossterm> <envar> SLON_LOG </envar> </glossterm>
+ <glossdef><para> This file is the location where &lslon; log files are to be stored, if need be.  There is an option <xref linkend ="slon-config-logging-syslog"> for &lslon; to use <application>syslog</application> to manage logging; in that case, you may prefer to set <envar>SLON_LOG</envar> to <filename>/dev/null</filename>.  </para> </glossdef> </glossentry>
+ </glosslist>
+ 
+ <para> Note that these environment variables may either be set, in the
+ script, or overridden by values passed in from the environment.  The
+ latter usage makes it easy to use this script in conjunction with the
+ <xref linkend="testbed"> so that it is regularly tested. </para>
+ 
+ </sect2>
+ 
  <sect2 id="launchclusters"><title> launch_clusters.sh </title>
  
***************
*** 367,374 ****
  
  <para> This is another shell script which uses the configuration as
! set up by <filename>mkslonconf.sh</filename> and is intended to either
! be run at system boot time, as an addition to the
! <filename>rc.d</filename> processes, or regularly, as a cron process,
! to ensure that &lslon; processes are running.</para>
  
  <para> It uses the following environment variables:</para>
--- 411,418 ----
  
  <para> This is another shell script which uses the configuration as
! set up by <filename>mkslonconf.sh</filename> and is intended to
! support an approach to running &slony1; involving regularly
! (<emphasis>e.g.</emphasis> via a cron process) checking to ensure that
! &lslon; processes are running.</para>
  
  <para> It uses the following environment variables:</para>

From cbbrowne at lists.slony.info  Fri Aug  1 12:33:26 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Aug  1 12:33:28 2008
Subject: [Slony1-commit] slony1-engine/tools start_slon.sh
Message-ID: <20080801193327.011E8290D6D@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tools
In directory main.slony.info:/tmp/cvs-serv22539

Modified Files:
	start_slon.sh 
Log Message:
Regularize the styles of output messages.

Add a check that the log file location is writable.


Index: start_slon.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tools/start_slon.sh,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** start_slon.sh	15 Jul 2008 22:28:23 -0000	1.1
--- start_slon.sh	1 Aug 2008 19:33:24 -0000	1.2
***************
*** 8,12 ****
  SLON_BIN_PATH=${SLON_BUILD:-"/home/chris/dbs/postgresql-8.3.3/bin"}
  SLON_CONF=${SLON_CONF:-"${HOME}/test/slon-conf.1"}
! SLON_LOG=${SLON_LOG:-"${HOME}/test/slon.1.log"}
  
  # shouldn't need to edit anything below this
--- 8,12 ----
  SLON_BIN_PATH=${SLON_BUILD:-"/home/chris/dbs/postgresql-8.3.3/bin"}
  SLON_CONF=${SLON_CONF:-"${HOME}/test/slon-conf.1"}
! SLON_LOG=${SLON_LOG:-"${HOME}/test/slon.1.log"}    # If you use syslog, then this may use /dev/null
  
  # shouldn't need to edit anything below this
***************
*** 30,36 ****
    start)
          if [ ! -z "$FINDPID" ]; then
! 	    echo "**** slon still running - PID $PID ****"
  	    exit 1
  	fi
          echo "Starting slon: $SLON_BIN_PATH/slon -f ${SLON_CONF} 1>> ${SLON_LOG} 2>>1" &
  	$SLON_BIN_PATH/slon -f ${SLON_CONF} 1>> ${SLON_LOG} 2>>1 &
--- 30,38 ----
    start)
          if [ ! -z "$FINDPID" ]; then
! 	    echo "**** slon already running - PID $PID ****"
  	    exit 1
  	fi
+ 	touch $SLON_LOG
+ 	test -w "$SLON_LOG" || (echo "**** SLON_LOG not writable - $SLON_LOG ****"; exit 1)
          echo "Starting slon: $SLON_BIN_PATH/slon -f ${SLON_CONF} 1>> ${SLON_LOG} 2>>1" &
  	$SLON_BIN_PATH/slon -f ${SLON_CONF} 1>> ${SLON_LOG} 2>>1 &
***************
*** 40,45 ****
  	if [ ! -z "$FINDPID" ]; then
  	    kill -15 ${PID}
  	else
! 	    echo "slon with PID ${PID} not found"
          fi
          ;;
--- 42,48 ----
  	if [ ! -z "$FINDPID" ]; then
  	    kill -15 ${PID}
+ 	    echo "Killed slon at PID ${PID}"
  	else
! 	    echo "**** slon with PID ${PID} not found ****"
          fi
          ;;
***************
*** 49,58 ****
  	if [ -f $PID_FILE ]; then
  	    if [ ! -z "$FINDPID" ]; then
! 		echo "Slon running as PID:$PID"
  	    else
! 		echo "Slon not running - PID:$PID - ${FINDPID}"
  	    fi
  	else
! 	    echo "Slon not running - no PID file ${PID_FILE}"
  	fi
  	;;
--- 52,61 ----
  	if [ -f $PID_FILE ]; then
  	    if [ ! -z "$FINDPID" ]; then
! 		echo "**** Slon running as PID:$PID ****"
  	    else
! 		echo "**** Slon not running - PID:$PID - ${FINDPID} ****"
  	    fi
  	else
! 	    echo "**** Slon not running - no PID file ${PID_FILE} ****"
  	fi
  	;;

From cbbrowne at lists.slony.info  Fri Aug  1 12:49:41 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Aug  1 12:49:42 2008
Subject: [Slony1-commit] slony1-engine/src/slon slon.c
Message-ID: <20080801194941.9C6B82902E9@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slon
In directory main.slony.info:/tmp/cvs-serv23657

Modified Files:
	slon.c 
Log Message:
a) Make slon_terminate_worker() visible so that remote_worker.c can
   reference it

b) Revise various logging that used SLON_DEBUG1 and SLON_DEBUG2 to 
   instead log at SLON_INFO and SLON_CONFIG levels


Index: slon.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/slon.c,v
retrieving revision 1.77
retrieving revision 1.78
diff -C2 -d -r1.77 -r1.78
*** slon.c	3 Jan 2008 15:47:21 -0000	1.77
--- slon.c	1 Aug 2008 19:49:39 -0000	1.78
***************
*** 76,80 ****
  static void SlonWatchdog(void);
  static void sighandler(int signo);
! static void slon_terminate_worker(void);
  #endif
  
--- 76,80 ----
  static void SlonWatchdog(void);
  static void sighandler(int signo);
! void slon_terminate_worker(void);
  #endif
  
***************
*** 658,662 ****
  	PQclear(res);
  	dstring_free(&query);
! 	slon_log(SLON_DEBUG2,
  			 "main: last local event sequence = %s\n",
  			 rtcfg_lastevent);
--- 658,662 ----
  	PQclear(res);
  	dstring_free(&query);
! 	slon_log(SLON_CONFIG,
  			 "main: last local event sequence = %s\n",
  			 rtcfg_lastevent);
***************
*** 737,741 ****
  	 * Wait until the scheduler has shut down all remote connections
  	 */
! 	slon_log(SLON_DEBUG1, "main: running scheduler mainloop\n");
  	if (sched_wait_mainloop() < 0)
  	{
--- 737,741 ----
  	 * Wait until the scheduler has shut down all remote connections
  	 */
! 	slon_log(SLON_INFO, "main: running scheduler mainloop\n");
  	if (sched_wait_mainloop() < 0)
  	{
***************
*** 743,747 ****
  		slon_retry();
  	}
! 	slon_log(SLON_DEBUG1, "main: scheduler mainloop returned\n");
  
  	/*
--- 743,747 ----
  		slon_retry();
  	}
! 	slon_log(SLON_INFO, "main: scheduler mainloop returned\n");
  
  	/*
***************
*** 750,754 ****
  	main_thread = pthread_self();
  
! 	slon_log(SLON_DEBUG2, "main: wait for remote threads\n");
  	rtcfg_joinAllRemoteThreads();
  
--- 750,754 ----
  	main_thread = pthread_self();
  
! 	slon_log(SLON_CONFIG, "main: wait for remote threads\n");
  	rtcfg_joinAllRemoteThreads();
  
***************
*** 774,778 ****
  #endif
  
! 	slon_log(SLON_DEBUG1, "main: done\n");
  
  	exit(0);
--- 774,778 ----
  #endif
  
! 	slon_log(SLON_CONFIG, "main: done\n");
  
  	exit(0);
***************
*** 858,864 ****
  			continue;
  
! 		slon_log(SLON_DEBUG2, "slon: child terminated status: %d; pid: %d, current worker pid: %d\n", child_status, pid, slon_worker_pid);
  	}
! 	slon_log(SLON_DEBUG2, "slon: child terminated status: %d; pid: %d, current worker pid: %d\n", child_status, pid, slon_worker_pid);
  
  	(void) alarm(0);
--- 858,864 ----
  			continue;
  
! 		slon_log(SLON_CONFIG, "slon: child terminated status: %d; pid: %d, current worker pid: %d\n", child_status, pid, slon_worker_pid);
  	}
! 	slon_log(SLON_CONFIG, "slon: child terminated status: %d; pid: %d, current worker pid: %d\n", child_status, pid, slon_worker_pid);
  
  	(void) alarm(0);
***************
*** 878,887 ****
  			if (child_status != 0)
  			{
! 				slon_log(SLON_DEBUG1, "slon: restart of worker in 10 seconds\n");
  				(void) sleep(10);
  			}
  			else
  			{
! 				slon_log(SLON_DEBUG1, "slon: restart of worker\n");
  			}
  			if (watchdog_status == SLON_WATCHDOG_RETRY)
--- 878,887 ----
  			if (child_status != 0)
  			{
! 				slon_log(SLON_CONFIG, "slon: restart of worker in 10 seconds\n");
  				(void) sleep(10);
  			}
  			else
  			{
! 				slon_log(SLON_CONFIG, "slon: restart of worker\n");
  			}
  			if (watchdog_status == SLON_WATCHDOG_RETRY)
***************
*** 898,902 ****
  	}
  
! 	slon_log(SLON_DEBUG1, "slon: done\n");
  
  	/*
--- 898,902 ----
  	}
  
! 	slon_log(SLON_INFO, "slon: done\n");
  
  	/*
***************
*** 917,921 ****
  	{
  		case SIGALRM:
! 			slon_log(SLON_DEBUG1, "slon: child termination timeout - kill child\n");
  			kill(slon_worker_pid, SIGKILL);
  			break;
--- 917,921 ----
  	{
  		case SIGALRM:
! 			slon_log(SLON_INFO, "slon: child termination timeout - kill child\n");
  			kill(slon_worker_pid, SIGKILL);
  			break;
***************
*** 925,929 ****
  
  		case SIGHUP:
! 			slon_log(SLON_DEBUG1, "slon: restart requested\n");
  			watchdog_status = SLON_WATCHDOG_RESTART;
  			slon_terminate_worker();
--- 925,929 ----
  
  		case SIGHUP:
! 			slon_log(SLON_INFO, "slon: restart requested\n");
  			watchdog_status = SLON_WATCHDOG_RESTART;
  			slon_terminate_worker();
***************
*** 931,935 ****
  
  		case SIGUSR1:
! 			slon_log(SLON_DEBUG1, "slon: retry requested\n");
  			watchdog_status = SLON_WATCHDOG_RETRY;
  			slon_terminate_worker();
--- 931,935 ----
  
  		case SIGUSR1:
! 			slon_log(SLON_INFO, "slon: retry requested\n");
  			watchdog_status = SLON_WATCHDOG_RETRY;
  			slon_terminate_worker();
***************
*** 938,942 ****
  		case SIGINT:
  		case SIGTERM:
! 			slon_log(SLON_DEBUG1, "slon: shutdown requested\n");
  			watchdog_status = SLON_WATCHDOG_SHUTDOWN;
  			slon_terminate_worker();
--- 938,942 ----
  		case SIGINT:
  		case SIGTERM:
! 			slon_log(SLON_INFO, "slon: shutdown requested\n");
  			watchdog_status = SLON_WATCHDOG_SHUTDOWN;
  			slon_terminate_worker();
***************
*** 944,948 ****
  
  		case SIGQUIT:
! 			slon_log(SLON_DEBUG1, "slon: shutdown now requested\n");
  			kill(slon_worker_pid, SIGKILL);
  			slon_exit(-1);
--- 944,948 ----
  
  		case SIGQUIT:
! 			slon_log(SLON_INFO, "slon: shutdown now requested\n");
  			kill(slon_worker_pid, SIGKILL);
  			slon_exit(-1);
***************
*** 958,962 ****
  slon_terminate_worker()
  {
! 	slon_log(SLON_DEBUG2, "slon: notify worker process to shutdown\n");
  
  	if (pipewrite(sched_wakeuppipe[1], "p", 1) != 1)
--- 958,962 ----
  slon_terminate_worker()
  {
! 	slon_log(SLON_INFO, "slon: notify worker process to shutdown\n");
  
  	if (pipewrite(sched_wakeuppipe[1], "p", 1) != 1)

From cbbrowne at lists.slony.info  Fri Aug  1 13:18:19 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Aug  1 13:18:22 2008
Subject: [Slony1-commit] slony1-engine/src/slon remote_worker.c
Message-ID: <20080801201819.2CD8B290E14@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slon
In directory main.slony.info:/tmp/cvs-serv25935

Modified Files:
	remote_worker.c 
Log Message:
Add in additional performance monitoring, indicating how much time
and how many queries are being spent against the provider + subscriber.

This should enable people to more readily infer where bottlenecks are
themselves, rather than having to go to Jan & Chris and have them infer
things *really* indirectly.


Index: remote_worker.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/remote_worker.c,v
retrieving revision 1.173
retrieving revision 1.174
diff -C2 -d -r1.173 -r1.174
*** remote_worker.c	27 Jun 2008 20:16:04 -0000	1.173
--- remote_worker.c	1 Aug 2008 20:18:17 -0000	1.174
***************
*** 28,33 ****
  
  #define MAXGROUPSIZE 10000		/* What is the largest number of SYNCs we'd
! 								 * want to group together??? */
  
  /* ----------
   * Local definitions
--- 28,35 ----
  
  #define MAXGROUPSIZE 10000		/* What is the largest number of SYNCs we'd
! 					 * want to group together??? */
! 
  
+ void slon_terminate_worker(void);
  /* ----------
   * Local definitions
***************
*** 139,142 ****
--- 141,161 ----
  } WorkGroupLineCode;
  
+ typedef struct PerfMon_s PerfMon;   /* Structure for doing performance monitoring */
+ struct PerfMon_s 
+ {
+ 	struct timeval prev_t;
+ 	struct timeval now_t;
+ 	double prov_query_t;      /* Time spent running queries against the provider */
+ 	int prov_query_c;         /* Number of queries run against the provider */
+ 	double subscr_query_t;      /* Time spent running prep queries against the subscriber */
+ 	int subscr_query_c;         /* Number of prep queries run against the subscriber */
+ 	double subscr_iud__t;    /* Time spent running IUD against subscriber */
+ 	int subscr_iud__c;       /* Number of IUD requests run against subscriber */
+ 	double large_tuples_t;          /* Number of large tuples processed */
+ 	int large_tuples_c;          /* Number of large tuples processed */
+ 	int num_inserts;
+ 	int num_updates;
+ 	int num_deletes;
+ };
  
  struct ProviderInfo_s
***************
*** 241,244 ****
--- 260,274 ----
   * ----------
   */
+ /*
+  * Monitoring data structure
+  */
+ 
+ static void init_perfmon(PerfMon *pm);
+ static void start_monitored_event(PerfMon *pm);
+ static void monitor_provider_query(PerfMon *pm);
+ static void monitor_subscriber_query(PerfMon *pm);
+ static void monitor_subscriber_iud(PerfMon *pm);
+ static void monitor_largetuples(PerfMon *pm);
+ 
  static void adjust_provider_info(SlonNode *node,
  					 WorkerGroupData *wd, int cleanup);
***************
*** 2103,2112 ****
  					 * Move the message to the head of the queue
  					 */
! 					if (oldmsg != node->message_head)
  					{
  						DLLIST_REMOVE(node->message_head,
! 								node->message_tail, oldmsg);
  						DLLIST_ADD_HEAD(node->message_head,
! 								node->message_tail, oldmsg);
  					}
  				}
--- 2133,2142 ----
  					 * Move the message to the head of the queue
  					 */
! 					if ((SlonWorkMsg *) oldmsg != node->message_head)
  					{
  						DLLIST_REMOVE(node->message_head,
! 									  node->message_tail, (SlonWorkMsg *) oldmsg);
  						DLLIST_ADD_HEAD(node->message_head,
! 								node->message_tail, (SlonWorkMsg *) oldmsg);
  					}
  				}
***************
*** 2420,2423 ****
--- 2450,2470 ----
  		}
  	}
+ 	if (sub_provider < 0) {
+ 		rtcfg_unlock();
+ 		slon_log(SLON_ERROR, "remoteWorkerThread_%d: provider node for set %"
+ 				 "not found in runtime configuration\n",
+ 				 set_id);
+ 		slon_terminate_worker();
+ 		return -1;
+ 		
+ 	}
+ 	if (set_origin < 0) {
+ 		rtcfg_unlock();
+ 		slon_log(SLON_ERROR, "remoteWorkerThread_%d: origin node for set %"
+ 				 "not found in runtime configuration\n",
+ 				 set_id);
+ 		slon_terminate_worker();
+ 		return -1;
+ 	}
  	if (set == NULL)
  	{
***************
*** 2431,2435 ****
  	{
  		rtcfg_unlock();
! 		slon_log(SLON_ERROR, "remoteWorkerThread_%d: node %d "
  				 "not found in runtime configuration\n",
  				 node->no_id, sub_provider);
--- 2478,2482 ----
  	{
  		rtcfg_unlock();
! 		slon_log(SLON_ERROR, "remoteWorkerThread_%d: provider node %d "
  				 "not found in runtime configuration\n",
  				 node->no_id, sub_provider);
***************
*** 3499,3502 ****
--- 3546,3550 ----
  	slon_log(SLON_INFO, "copy_set %d done in %.3f seconds\n", set_id,
  			 TIMEVAL_DIFF(&tv_start, &tv_now));
+ 
  	return 0;
  }
***************
*** 3537,3542 ****
--- 3585,3592 ----
  	int			actionlist_len;
  	int64		min_ssy_seqno;
+ 	PerfMon pm;
  
  	gettimeofday(&tv_start, NULL);
+ 
  	slon_log(SLON_DEBUG2, "remoteWorkerThread_%d: SYNC " INT64_FORMAT
  			 " processing\n",
***************
*** 3547,3550 ****
--- 3597,3602 ----
  	dstring_init(&lsquery);
  
+ 	init_perfmon(&pm);
+ 
  	/*
  	 * If this slon is running in log archiving mode, open a temporary file
***************
*** 3590,3593 ****
--- 3642,3647 ----
  			sprintf(conn_symname, "subscriber_%d_provider_%d",
  					node->no_id, provider->no_id);
+ 
+ 
  			provider->conn = slon_connectdb(provider->pa_conninfo,
  											conn_symname);
***************
*** 3610,3613 ****
--- 3664,3668 ----
  								"select %s.registerNodeConnection(%d); ",
  								rtcfg_namespace, rtcfg_nodeid);
+ 			start_monitored_event(&pm);
  			if (query_execute(node, provider->conn->dbconn, &query) < 0)
  			{
***************
*** 3619,3622 ****
--- 3674,3679 ----
  				return provider->pa_connretry;
  			}
+ 			monitor_provider_query(&pm);
+ 
  			slon_log(SLON_DEBUG1, "remoteWorkerThread_%d: "
  					 "connected to data provider %d on '%s'\n",
***************
*** 3718,3722 ****
--- 3775,3783 ----
  		slon_appendquery(&query, "); ");
  
+ 		start_monitored_event(&pm);
  		res1 = PQexec(local_dbconn, dstring_data(&query));
+ 		slon_log(SLON_INFO, "about to monitor_subscriber_query - pulling big actionid list %d\n", provider);
+ 		monitor_subscriber_query(&pm);
+ 
  		if (PQresultStatus(res1) != PGRES_TUPLES_OK)
  		{
***************
*** 3773,3777 ****
--- 3834,3840 ----
  								rtcfg_namespace,
  								sub_set);
+ 			start_monitored_event(&pm);
  			res2 = PQexec(local_dbconn, dstring_data(&query));
+ 			monitor_subscriber_query(&pm);
  			if (PQresultStatus(res2) != PGRES_TUPLES_OK)
  			{
***************
*** 3946,3949 ****
--- 4009,4013 ----
  						rtcfg_namespace);
  	res1 = PQexec(local_dbconn, dstring_data(&query));
+ 
  	if (PQresultStatus(res1) != PGRES_TUPLES_OK)
  	{
***************
*** 4031,4035 ****
--- 4095,4102 ----
  					if (wgline->log.n_used > 0)
  					{
+ 					  start_monitored_event(&pm);
  						res1 = PQexec(local_dbconn, dstring_data(&(wgline->log)));
+ 
+ 					  monitor_subscriber_iud(&pm);
  						if (PQresultStatus(res1) == PGRES_EMPTY_QUERY)
  						{
***************
*** 4049,4053 ****
--- 4116,4122 ----
  					}
  
+ 					start_monitored_event(&pm);
  					res1 = PQexec(local_dbconn, dstring_data(&(wgline->data)));
+ 					monitor_subscriber_iud(&pm);
  					if (PQresultStatus(res1) == PGRES_EMPTY_QUERY)
  					{
***************
*** 4228,4232 ****
--- 4297,4303 ----
  						 "  group by 1; ");
  
+ 		start_monitored_event(&pm);
  		res1 = PQexec(provider->conn->dbconn, dstring_data(&query));
+ 		monitor_subscriber_iud(&pm);
  		if (PQresultStatus(res1) != PGRES_TUPLES_OK)
  		{
***************
*** 4252,4255 ****
--- 4323,4327 ----
  								rtcfg_namespace,
  						   seql_seqid, node->no_id, seqbuf, seql_last_value);
+ 			start_monitored_event(&pm);
  			if (query_execute(node, local_dbconn, &query) < 0)
  			{
***************
*** 4260,4263 ****
--- 4332,4336 ----
  				return 60;
  			}
+ 			monitor_subscriber_iud(&pm);
  
  			/*
***************
*** 4344,4347 ****
--- 4417,4432 ----
  			 node->no_id, event->ev_seqno,
  			 TIMEVAL_DIFF(&tv_start, &tv_now));
+ 
+  	slon_log(SLON_INFO, 
+ 			 "remoteWorkerThread_%d: SYNC " INT64_FORMAT " Timing: " 
+ 			 " pqexec (s/count)" 
+ 			 "- provider %.3f/%d " 
+ 			 "- subscriber %.3f/%d " 
+ 			 "- IUD %.3f/%d\n",
+ 			 node->no_id, event->ev_seqno, 
+ 			 pm.prov_query_t, pm.prov_query_c, 
+ 			 pm.subscr_query_t, pm.prov_query_c,
+ 			 pm.subscr_iud__t, pm.subscr_iud__c);
+ 
  	return 0;
  }
***************
*** 4383,4393 ****
  	int			line_ncmds;
  
! 	int			num_inserts,
! 				num_deletes,
! 				num_updates;
  
! 	num_inserts = 0;
! 	num_deletes = 0;
! 	num_updates = 0;
  
  	dstring_init(&query);
--- 4468,4474 ----
  	int			line_ncmds;
  
! 	PerfMon pm;
  
! 	init_perfmon(&pm);
  
  	dstring_init(&query);
***************
*** 4618,4622 ****
  					/*
  					 * First make sure that the overall memory usage is inside
! 					 * bouds.
  					 */
  					if (wd->workdata_largemem > sync_max_largemem)
--- 4699,4703 ----
  					/*
  					 * First make sure that the overall memory usage is inside
! 					 * bounds.
  					 */
  					if (wd->workdata_largemem > sync_max_largemem)
***************
*** 4759,4762 ****
--- 4840,4844 ----
  					if (log_cmdsize >= sync_max_rowsize)
  					{
+ 						start_monitored_event(&pm);
  						(void) slon_mkquery(&query2,
  											"select log_cmddata "
***************
*** 4776,4779 ****
--- 4858,4862 ----
  										log_origin, log_txid, log_actionseq);
  						res2 = PQexec(dbconn, dstring_data(&query2));
+ 						monitor_largetuples(&pm);
  						if (PQresultStatus(res2) != PGRES_TUPLES_OK)
  						{
***************
*** 4839,4843 ****
  											 wd->tab_fqname[log_tableid],
  											 log_cmddata);
! 							num_inserts++;
  							break;
  
--- 4922,4926 ----
  											 wd->tab_fqname[log_tableid],
  											 log_cmddata);
! 							pm.num_inserts++;
  							break;
  
***************
*** 4847,4851 ****
  											 wd->tab_fqname[log_tableid],
  											 log_cmddata);
! 							num_updates++;
  							break;
  
--- 4930,4934 ----
  											 wd->tab_fqname[log_tableid],
  											 log_cmddata);
! 							pm.num_updates++;
  							break;
  
***************
*** 4855,4859 ****
  											 wd->tab_fqname[log_tableid],
  											 log_cmddata);
! 							num_deletes++;
  							break;
  					}
--- 4938,4942 ----
  											 wd->tab_fqname[log_tableid],
  											 log_cmddata);
! 							pm.num_deletes++;
  							break;
  					}
***************
*** 4959,4967 ****
  
  		slon_log(SLON_INFO, "remoteHelperThread_%d_%d: inserts=%d updates=%d deletes=%d\n",
! 		node->no_id, provider->no_id, num_inserts, num_updates, num_deletes);
  
! 		num_inserts = 0;
! 		num_deletes = 0;
! 		num_updates = 0;
  
  		/*
--- 5042,5052 ----
  
  		slon_log(SLON_INFO, "remoteHelperThread_%d_%d: inserts=%d updates=%d deletes=%d\n",
! 				 node->no_id, provider->no_id, pm.num_inserts, pm.num_updates, pm.num_deletes);
  
!  	slon_log(SLON_INFO, 
! 			 "remoteWorkerThread_%d: sync_helper timing: " 
! 			 " large tuples %.3f/%d\n", 
! 			 node->no_id, 
! 			 pm.large_tuples_t, pm.large_tuples_c);
  
  		/*
***************
*** 5650,5655 ****
  check_set_subscriber(int set_id, int node_id, PGconn *local_dbconn)
  {
- 
- 
  	SlonDString query1;
  	PGresult   *res;
--- 5735,5738 ----
***************
*** 5675,5679 ****
--- 5758,5806 ----
  	PQclear(res);
  	return 1;
+ }
  
+ static void init_perfmon(PerfMon *perf_info) {
+   perf_info->prov_query_t = 0.0;
+   perf_info->prov_query_c = 0;
+   perf_info->subscr_query_t = 0.0;
+   perf_info->subscr_query_c = 0;
+   perf_info->subscr_iud__t = 0.0;
+   perf_info->subscr_iud__c = 0;
+   perf_info->large_tuples_t = 0;
+   perf_info->large_tuples_c = 0;
+   perf_info->num_inserts = 0;
+   perf_info->num_updates = 0;
+   perf_info->num_deletes = 0;
+ }
+ static void start_monitored_event(PerfMon *perf_info) {
+   gettimeofday(&(perf_info->prev_t), NULL);
+ }
+ static void monitor_subscriber_query(PerfMon *perf_info) {
+   double diff;
+   gettimeofday(&(perf_info->now_t), NULL);
+   diff = TIMEVAL_DIFF(&(perf_info->prev_t), &(perf_info->now_t)); 
+   (perf_info->subscr_query_t) += diff;
+   (perf_info->subscr_query_c) ++;
+ }
+ static void monitor_provider_query(PerfMon *perf_info) {
+   double diff;
+   gettimeofday(&(perf_info->now_t), NULL);
+   diff = TIMEVAL_DIFF(&(perf_info->prev_t), &(perf_info->now_t)); 
+   (perf_info->prov_query_t) += diff;
+   (perf_info->prov_query_c) ++;
+ }
+ static void monitor_subscriber_iud(PerfMon *perf_info) {
+   double diff;
+   gettimeofday(&(perf_info->now_t), NULL);
+   diff = TIMEVAL_DIFF(&(perf_info->prev_t), &(perf_info->now_t)); 
+   (perf_info->subscr_iud__t) += diff;
+   (perf_info->subscr_iud__c) ++;
+ }
  
+ static void monitor_largetuples(PerfMon *perf_info) {
+   double diff;
+   gettimeofday(&(perf_info->now_t), NULL);
+   diff = TIMEVAL_DIFF(&(perf_info->prev_t), &(perf_info->now_t)); 
+   (perf_info->large_tuples_t) += diff;
+   (perf_info->large_tuples_c) ++;
  }

From cbbrowne at lists.slony.info  Fri Aug  1 15:45:20 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Aug  1 15:45:22 2008
Subject: [Slony1-commit] slony1-engine/doc/adminguide monitoring.sgml
Message-ID: <20080801224520.7FB35290DDD@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv3690/doc/adminguide

Modified Files:
	monitoring.sgml 
Log Message:
Add in more detailed monitoring of numbers and timings of queries against
providers and subscribers, along with some documentation on how the log
output may be interpreted (at least, at the more technical level).



Index: monitoring.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/monitoring.sgml,v
retrieving revision 1.44
retrieving revision 1.45
diff -C2 -d -r1.44 -r1.45
*** monitoring.sgml	19 Jun 2008 20:35:39 -0000	1.44
--- monitoring.sgml	1 Aug 2008 22:45:18 -0000	1.45
***************
*** 345,348 ****
--- 345,424 ----
  
  </sect2>
+ 
+ <sect2> <title> Analysis of a SYNC </title>
+ 
+ <para> The following is (as of 2.0) an extract from the &lslon; log for node
+ #2 in a run of <quote>test1</test> from the <xref linkend="testbed">. </para>
+ 
+ <screen>
+ DEBUG2 remoteWorkerThread_1: SYNC 19 processing
+ INFO   about to monitor_subscriber_query - pulling big actionid list 134885072
+ INFO   remoteWorkerThread_1: syncing set 1 with 4 table(s) from provider 1
+ DEBUG2  ssy_action_list length: 0
+ DEBUG2 remoteWorkerThread_1: current local log_status is 0
+ DEBUG2 remoteWorkerThread_1_1: current remote log_status = 0
+ DEBUG1 remoteHelperThread_1_1: 0.028 seconds delay for first row
+ DEBUG1 remoteHelperThread_1_1: 0.978 seconds until close cursor
+ INFO   remoteHelperThread_1_1: inserts=144 updates=1084 deletes=0
+ INFO   remoteWorkerThread_1: sync_helper timing:  pqexec (s/count)- provider 0.063/6 - subscriber 0.000/6
+ INFO   remoteWorkerThread_1: sync_helper timing:  large tuples 0.315/288
+ DEBUG2 remoteWorkerThread_1: cleanup
+ INFO   remoteWorkerThread_1: SYNC 19 done in 1.272 seconds
+ INFO   remoteWorkerThread_1: SYNC 19 sync_event timing:  pqexec (s/count)- provider 0.001/1 - subscriber 0.004/1 - IUD 0.972/248
+ </screen>
+ 
+ <para> Here are some notes to interpret this output: </para>
+ 
+ <itemizedlist>
+ <listitem><para> Note the line that indicates <screen>inserts=144 updates=1084 deletes=0</screen> </para> 
+ <para> This indicates how many tuples were affected by this particular SYNC. </para></listitem>
+ <listitem><para> Note the line indicating <screen>0.028 seconds delay for first row</screen></para> 
+ 
+ <para> This indicates the time it took for the <screen>LOG
+ cursor</screen> to get to the point of processing the first row of
+ data.  Normally, this takes a long time if the SYNC is a large one,
+ and one requiring sorting of a sizable result set.</para></listitem>
+ 
+ <listitem><para> Note the line indicating <screen>0.978 seconds until
+ close cursor</screen></para> 
+ 
+ <para> This indicates how long processing took against the
+ provider.</para></listitem>
+ 
+ <listitem><para> sync_helper timing:  large tuples 0.315/288 </para> 
+ 
+ <para> This breaks off, as a separate item, the number of large tuples
+ (<emphasis>e.g.</emphasis> - where size exceeded the configuration
+ parameter <xref linkend="slon-config-max-rowsize">) and where the
+ tuples had to be processed individually. </para></listitem>
+ 
+ <listitem><para> <screen>SYNC 19 done in 1.272 seconds</screen></para> 
+ 
+ <para> This indicates that it took 0.108 seconds, in total, to process
+ this set of SYNCs. </para>
+ </listitem>
+ 
+ <listitem><para> <screen>SYNC 19 sync_event timing:  pqexec (s/count)- provider 0.001/1 - subscriber 0.004/0 - IUD 0.972/248</screen></para> 
+ 
+ <para> This records information about how many queries were issued
+ against providers and subscribers in function
+ <function>sync_event()</function>, and how long they took. </para>
+ 
+ <para> Note that 248 does not match against the numbers of inserts,
+ updates, and deletes, described earlier, as I/U/D requests are
+ clustered into groups of queries that are submitted via a single
+ <function>pqexec()</function> call on the
+ subscriber. </para></listitem>
+ 
+ <listitem><para> <screen>sync_helper timing:  pqexec (s/count)- provider 0.063/6 - subscriber 0.000/6</screen></para>
+ 
+ <para> This records information about how many queries were issued
+ against providers and subscribers in function
+ <function>sync_helper()</function>, and how long they took.
+ </para></listitem>
+ 
+ </itemizedlist>
+ 
+ </sect2>
  </sect1>
  <!-- Keep this comment at the end of the file

From cbbrowne at lists.slony.info  Fri Aug  1 15:45:20 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Aug  1 15:45:23 2008
Subject: [Slony1-commit] slony1-engine/src/slon remote_worker.c
Message-ID: <20080801224520.70690290176@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slon
In directory main.slony.info:/tmp/cvs-serv3690/src/slon

Modified Files:
	remote_worker.c 
Log Message:
Add in more detailed monitoring of numbers and timings of queries against
providers and subscribers, along with some documentation on how the log
output may be interpreted (at least, at the more technical level).



Index: remote_worker.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/remote_worker.c,v
retrieving revision 1.174
retrieving revision 1.175
diff -C2 -d -r1.174 -r1.175
*** remote_worker.c	1 Aug 2008 20:18:17 -0000	1.174
--- remote_worker.c	1 Aug 2008 22:45:18 -0000	1.175
***************
*** 3777,3783 ****
  		start_monitored_event(&pm);
  		res1 = PQexec(local_dbconn, dstring_data(&query));
- 		slon_log(SLON_INFO, "about to monitor_subscriber_query - pulling big actionid list %d\n", provider);
  		monitor_subscriber_query(&pm);
  
  		if (PQresultStatus(res1) != PGRES_TUPLES_OK)
  		{
--- 3777,3784 ----
  		start_monitored_event(&pm);
  		res1 = PQexec(local_dbconn, dstring_data(&query));
  		monitor_subscriber_query(&pm);
  
+ 		slon_log(SLON_INFO, "about to monitor_subscriber_query - pulling big actionid list %d\n", provider);
+ 
  		if (PQresultStatus(res1) != PGRES_TUPLES_OK)
  		{
***************
*** 3834,3840 ****
--- 3835,3843 ----
  								rtcfg_namespace,
  								sub_set);
+ 
  			start_monitored_event(&pm);
  			res2 = PQexec(local_dbconn, dstring_data(&query));
  			monitor_subscriber_query(&pm);
+ 
  			if (PQresultStatus(res2) != PGRES_TUPLES_OK)
  			{
***************
*** 4008,4012 ****
--- 4011,4017 ----
  	(void) slon_mkquery(&query, "select last_value from %s.sl_log_status",
  						rtcfg_namespace);
+ 	start_monitored_event(&pm);
  	res1 = PQexec(local_dbconn, dstring_data(&query));
+ 	monitor_subscriber_query(&pm);
  
  	if (PQresultStatus(res1) != PGRES_TUPLES_OK)
***************
*** 4095,4102 ****
  					if (wgline->log.n_used > 0)
  					{
! 					  start_monitored_event(&pm);
  						res1 = PQexec(local_dbconn, dstring_data(&(wgline->log)));
  
- 					  monitor_subscriber_iud(&pm);
  						if (PQresultStatus(res1) == PGRES_EMPTY_QUERY)
  						{
--- 4100,4107 ----
  					if (wgline->log.n_used > 0)
  					{
! 						start_monitored_event(&pm);
  						res1 = PQexec(local_dbconn, dstring_data(&(wgline->log)));
+ 						monitor_subscriber_iud(&pm);
  
  						if (PQresultStatus(res1) == PGRES_EMPTY_QUERY)
  						{
***************
*** 4119,4122 ****
--- 4124,4128 ----
  					res1 = PQexec(local_dbconn, dstring_data(&(wgline->data)));
  					monitor_subscriber_iud(&pm);
+ 
  					if (PQresultStatus(res1) == PGRES_EMPTY_QUERY)
  					{
***************
*** 4299,4303 ****
  		start_monitored_event(&pm);
  		res1 = PQexec(provider->conn->dbconn, dstring_data(&query));
! 		monitor_subscriber_iud(&pm);
  		if (PQresultStatus(res1) != PGRES_TUPLES_OK)
  		{
--- 4305,4310 ----
  		start_monitored_event(&pm);
  		res1 = PQexec(provider->conn->dbconn, dstring_data(&query));
! 		monitor_provider_query(&pm);
! 
  		if (PQresultStatus(res1) != PGRES_TUPLES_OK)
  		{
***************
*** 4379,4383 ****
--- 4386,4394 ----
  		 */
  		slon_appendquery(&query, ") and ssy_seqno < '%s'; ", seqbuf);
+ 		
+ 		start_monitored_event(&pm);
  		res1 = PQexec(local_dbconn, dstring_data(&query));
+ 		monitor_subscriber_query (&pm);
+ 
  		if (PQresultStatus(res1) != PGRES_COMMAND_OK)
  		{
***************
*** 4419,4423 ****
  
   	slon_log(SLON_INFO, 
! 			 "remoteWorkerThread_%d: SYNC " INT64_FORMAT " Timing: " 
  			 " pqexec (s/count)" 
  			 "- provider %.3f/%d " 
--- 4430,4434 ----
  
   	slon_log(SLON_INFO, 
! 			 "remoteWorkerThread_%d: SYNC " INT64_FORMAT " sync_event timing: " 
  			 " pqexec (s/count)" 
  			 "- provider %.3f/%d " 
***************
*** 4470,4475 ****
  	PerfMon pm;
  
- 	init_perfmon(&pm);
- 
  	dstring_init(&query);
  	dstring_init(&query2);
--- 4481,4484 ----
***************
*** 4513,4522 ****
--- 4522,4536 ----
  		do
  		{
+ 			init_perfmon(&pm);
  			/*
  			 * Start a transaction
  			 */
+ 
  			(void) slon_mkquery(&query, "start transaction; "
  								"set enable_seqscan = off; "
  								"set enable_indexscan = on; ");
+ 
+ 			start_monitored_event(&pm);
+ 
  			if (query_execute(node, dbconn, &query) < 0)
  			{
***************
*** 4524,4527 ****
--- 4538,4542 ----
  				break;
  			}
+ 			monitor_subscriber_query (&pm);
  
  			/*
***************
*** 4530,4534 ****
--- 4545,4553 ----
  			(void) slon_mkquery(&query, "select last_value from %s.sl_log_status",
  								rtcfg_namespace);
+ 
+ 			start_monitored_event(&pm);
  			res3 = PQexec(dbconn, dstring_data(&query));
+ 			monitor_provider_query(&pm);
+ 
  			rc = PQresultStatus(res3);
  			if (rc != PGRES_TUPLES_OK)
***************
*** 4639,4642 ****
--- 4658,4662 ----
  			res = NULL;
  
+ 			start_monitored_event(&pm);
  			if (query_execute(node, dbconn, &query) < 0)
  			{
***************
*** 4644,4647 ****
--- 4664,4668 ----
  				break;
  			}
+ 			monitor_provider_query(&pm);
  
  			(void) slon_mkquery(&query, "fetch %d from LOG; ",
***************
*** 4775,4779 ****
--- 4796,4803 ----
  						PQclear(res);
  
+ 					start_monitored_event(&pm);
  					res = PQexec(dbconn, dstring_data(&query));
+ 					monitor_provider_query(&pm);
+ 
  					if (PQresultStatus(res) != PGRES_TUPLES_OK)
  					{
***************
*** 4840,4844 ****
  					if (log_cmdsize >= sync_max_rowsize)
  					{
- 						start_monitored_event(&pm);
  						(void) slon_mkquery(&query2,
  											"select log_cmddata "
--- 4864,4867 ----
***************
*** 4857,4862 ****
--- 4880,4887 ----
  											rtcfg_namespace,
  										log_origin, log_txid, log_actionseq);
+ 						start_monitored_event(&pm);
  						res2 = PQexec(dbconn, dstring_data(&query2));
  						monitor_largetuples(&pm);
+ 
  						if (PQresultStatus(res2) != PGRES_TUPLES_OK)
  						{
***************
*** 5044,5052 ****
  				 node->no_id, provider->no_id, pm.num_inserts, pm.num_updates, pm.num_deletes);
  
!  	slon_log(SLON_INFO, 
! 			 "remoteWorkerThread_%d: sync_helper timing: " 
! 			 " large tuples %.3f/%d\n", 
! 			 node->no_id, 
! 			 pm.large_tuples_t, pm.large_tuples_c);
  
  		/*
--- 5069,5086 ----
  				 node->no_id, provider->no_id, pm.num_inserts, pm.num_updates, pm.num_deletes);
  
! 		slon_log(SLON_INFO, 
! 				 "remoteWorkerThread_%d: sync_helper timing: " 
! 				 " pqexec (s/count)" 
! 				 "- provider %.3f/%d " 
! 				 "- subscriber %.3f/%d\n",
! 				 node->no_id, 
! 				 pm.prov_query_t, pm.prov_query_c, 
! 				 pm.subscr_query_t, pm.prov_query_c);
! 
! 		slon_log(SLON_INFO, 
! 				 "remoteWorkerThread_%d: sync_helper timing: " 
! 				 " large tuples %.3f/%d\n", 
! 				 node->no_id, 
! 				 pm.large_tuples_t, pm.large_tuples_c);
  
  		/*

From cbbrowne at lists.slony.info  Wed Aug  6 15:05:48 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Aug  6 15:05:49 2008
Subject: [Slony1-commit] slony1-engine/src/backend slony1_funcs.sql
Message-ID: <20080806220548.1327D290D91@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/backend
In directory main.slony.info:/tmp/cvs-serv5574

Modified Files:
	slony1_funcs.sql 
Log Message:
Changed stored functions to use dollar quoting (e.g. - $$ / $$) rather than
the 7.4-and-before ''double quotes''.

In the few places where we need to use quotes, generate that via 
pg_catalog.quote_literal().

I ran through test1 and testddl, which look to exercise enough logic to
cover everything, as, between them, these tests:
 a) Load all stored procs, to make sure we have them all
    syntax-correct;
 b) Create logtriggers + denyaccess triggers on replicated tables;
 c) Drop & recreate those triggers.

That would seem to represent the 3 scenarios where quoting might be
expected to break.


Index: slony1_funcs.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.sql,v
retrieving revision 1.141
retrieving revision 1.142
diff -C2 -d -r1.141 -r1.142
*** slony1_funcs.sql	15 Jul 2008 22:25:44 -0000	1.141
--- slony1_funcs.sql	6 Aug 2008 22:05:45 -0000	1.142
***************
*** 180,194 ****
    'Returns the compiled-in version number of the Slony-I shared object';
  
! create or replace function @NAMESPACE@.checkmoduleversion () returns text as '
  declare
    moduleversion	text;
  begin
    select into moduleversion @NAMESPACE@.getModuleVersion();
!   if moduleversion <> ''@MODULEVERSION@'' then
!       raise exception ''Slonik version: @MODULEVERSION@ != Slony-I version in PG build %'',
               moduleversion;
[...5912 lines suppressed...]
  
***************
*** 5840,5844 ****
     return @NAMESPACE@.add_empty_table_to_replication(v_set_id, p_tab_id, p_nspname, p_tabname, p_idxname, p_comment);
  end
! ' language plpgsql;
  
  comment on function @NAMESPACE@.replicate_partition(int4, text, text, text, text) is
--- 5839,5843 ----
     return @NAMESPACE@.add_empty_table_to_replication(v_set_id, p_tab_id, p_nspname, p_tabname, p_idxname, p_comment);
  end
! $$ language plpgsql;
  
  comment on function @NAMESPACE@.replicate_partition(int4, text, text, text, text) is
***************
*** 5846,5848 ****
  tab_idxname is optional - if NULL, then we use the primary key.
  This function looks up replication configuration via the parent table.';
- 
--- 5845,5846 ----

From wieck at lists.slony.info  Fri Aug  8 10:52:50 2008
From: wieck at lists.slony.info (Jan Wieck)
Date: Fri Aug  8 10:52:52 2008
Subject: [Slony1-commit] slony1-engine/src/backend slony1_funcs.sql
Message-ID: <20080808175250.8A390290E3B@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/backend
In directory main.slony.info:/tmp/cvs-serv12320

Modified Files:
	slony1_funcs.sql 
Log Message:
Dollar quoting broke sequenceSetValue().

Jan


Index: slony1_funcs.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.sql,v
retrieving revision 1.142
retrieving revision 1.143
diff -C2 -d -r1.142 -r1.143
*** slony1_funcs.sql	6 Aug 2008 22:05:45 -0000	1.142
--- slony1_funcs.sql	8 Aug 2008 17:52:48 -0000	1.143
***************
*** 3544,3549 ****
  	-- Update it to the new value
  	-- ----
! 	execute 'select setval('' || v_fqname ||
! 			'', '' || p_last_value || '')';
  
  	insert into @NAMESPACE@.sl_seqlog
--- 3544,3549 ----
  	-- Update it to the new value
  	-- ----
! 	execute 'select setval(''' || v_fqname ||
! 			''', ' || p_last_value || ')';
  
  	insert into @NAMESPACE@.sl_seqlog

From cbbrowne at lists.slony.info  Fri Aug 22 15:38:21 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Aug 22 15:38:22 2008
Subject: [Slony1-commit] slony1-engine/doc/adminguide bestpractices.sgml
Message-ID: <20080822223821.90B18290008@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv28770

Modified Files:
	bestpractices.sgml 
Log Message:
Revise WAN discussion to make it clearer, as there was some confusion
expressed on the mailing list.


Index: bestpractices.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/bestpractices.sgml,v
retrieving revision 1.34
retrieving revision 1.35
diff -C2 -d -r1.34 -r1.35
*** bestpractices.sgml	21 Apr 2008 21:31:23 -0000	1.34
--- bestpractices.sgml	22 Aug 2008 22:38:19 -0000	1.35
***************
*** 177,187 ****
  for managing so that the connection to that node is a
  <quote>local</quote> one.  Do <emphasis>not</emphasis> run such links
! across a WAN. </para>
  
! <para> A WAN outage can leave database connections
! <quote>zombied</quote>, and typical TCP/IP behaviour <link
! linkend="multipleslonconnections"> will allow those connections to
! persist, preventing a slon restart for around two hours. </link>
! </para>
  
  <para> It is not difficult to remedy this; you need only <command>kill
--- 177,189 ----
  for managing so that the connection to that node is a
  <quote>local</quote> one.  Do <emphasis>not</emphasis> run such links
! across a WAN.  Thus, if you have nodes in London and nodes in New
! York, the &lslon;s managing London nodes should run in London, and the
! &lslon;s managing New York nodes should run in New York.</para>
  
! <para> A WAN outage (or flakiness of the WAN in general) can leave
! database connections <quote>zombied</quote>, and typical TCP/IP
! behaviour <link linkend="multipleslonconnections"> will allow those
! connections to persist, preventing a slon restart for around two
! hours. </link> </para>
  
  <para> It is not difficult to remedy this; you need only <command>kill
***************
*** 206,213 ****
  scratch.</para>
  
! <para> The exception, where it is undesirable to restart a &lslon;, is
! where a <command>COPY_SET</command> is running on a large replication
! set, such that stopping the &lslon; may discard several hours worth of
! load work. </para>
  
  <para> In early versions of &slony1;, it was frequently the case that
--- 208,215 ----
  scratch.</para>
  
! <para> The exception scenario where it is undesirable to restart a
! &lslon; is where a <command>COPY_SET</command> is running on a large
! replication set, such that stopping the &lslon; may discard several
! hours worth of load work. </para>
  
  <para> In early versions of &slony1;, it was frequently the case that

From cbbrowne at lists.slony.info  Fri Aug 29 14:06:47 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Aug 29 14:06:49 2008
Subject: [Slony1-commit] slony1-engine/src/slon remote_worker.c
Message-ID: <20080829210647.BD6D8290ED7@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slon
In directory main.slony.info:/tmp/cvs-serv7163/src/slon

Modified Files:
	remote_worker.c 
Log Message:
Fix format strings in log messages


Index: remote_worker.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/remote_worker.c,v
retrieving revision 1.175
retrieving revision 1.176
diff -C2 -d -r1.175 -r1.176
*** remote_worker.c	1 Aug 2008 22:45:18 -0000	1.175
--- remote_worker.c	29 Aug 2008 21:06:45 -0000	1.176
***************
*** 2452,2458 ****
  	if (sub_provider < 0) {
  		rtcfg_unlock();
! 		slon_log(SLON_ERROR, "remoteWorkerThread_%d: provider node for set %"
! 				 "not found in runtime configuration\n",
! 				 set_id);
  		slon_terminate_worker();
  		return -1;
--- 2452,2460 ----
  	if (sub_provider < 0) {
  		rtcfg_unlock();
! 		slon_log(SLON_ERROR, "remoteWorkerThread_%d: provider node %d for set %d"
! 			 "not found in runtime configuration\n",
! 			 node->no_id,
! 			 sub_provider,
! 			 set_id);
  		slon_terminate_worker();
  		return -1;
***************
*** 2461,2467 ****
  	if (set_origin < 0) {
  		rtcfg_unlock();
! 		slon_log(SLON_ERROR, "remoteWorkerThread_%d: origin node for set %"
! 				 "not found in runtime configuration\n",
! 				 set_id);
  		slon_terminate_worker();
  		return -1;
--- 2463,2471 ----
  	if (set_origin < 0) {
  		rtcfg_unlock();
! 		slon_log(SLON_ERROR, "remoteWorkerThread_%d: origin node %d for set %d "
! 			 "not found in runtime configuration\n",
! 			 node->no_id, 
! 			 set_origin,
! 			 set_id);
  		slon_terminate_worker();
  		return -1;

