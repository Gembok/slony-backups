From cbbrowne at lists.slony.info  Wed Jan  2 11:00:29 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Jan  2 11:00:31 2008
Subject: [Slony1-commit] slony1-engine RELEASE-2.0
Message-ID: <20080102190029.AA8ED290C52@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine
In directory main.slony.info:/tmp/cvs-serv29551

Modified Files:
	RELEASE-2.0 
Log Message:
Apply changes to cleanup thread:

- by default, use TRUNCATE rather than DELETE to trim sl_log_? tables

- moved substantially all logic for trimming of old data out of the
  C code, and into cleanupThread() stored function

  The stored function now can be used to request cleanup, so that we
  might force it to happen at an unexpected time, such as manually,
  for testing, as part of a test script.

- added parameters to slon to support this:

  interval - cleanup_interval (default '10 minutes')

     This controls how quickly old events are trimmed out.  It used to
     be a hard-coded value.

     Old events are trimmed out once the confirmations are aged by
     (cleanup_interval).

     This then controls when the data in sl_log_1/sl_log_2 can be dropped.

     Data in *those* tables is deleted when it is older than the
     earliest XID still captured in sl_event.

  boolean - cleanup_deletelogs (default 'false')

     This controls whether or not we DELETE data from sl_log_1/sl_log_2

     By default, we now NEVER delete data from the log tables; we
     instead use TRUNCATE.


Index: RELEASE-2.0
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/RELEASE-2.0,v
retrieving revision 1.10
retrieving revision 1.11
diff -C2 -d -r1.10 -r1.11
*** RELEASE-2.0	11 Dec 2007 20:27:11 -0000	1.10
--- RELEASE-2.0	2 Jan 2008 19:00:27 -0000	1.11
***************
*** 112,113 ****
--- 112,159 ----
    Note that this affects the structure of sl_event, and leads to some
    changes in the coding of the regression tests.
+ 
+ - All of the interesting cleanup work is now done in the stored
+   function, cleanupEvent(interval, boolean).
+ 
+   Interesting side-effect: You can now induce a cleanup manually,
+   which will be useful for testing.
+ 
+ - cleanupEvent now has two parameters, passed in from slon config
+   parameters:
+ 
+   interval - cleanup_interval (default '10 minutes')
+ 
+    This controls how quickly old events are trimmed out.  It used to
+    be a hard-coded value.
+ 
+    Old events are trimmed out once the confirmations are aged by
+    (cleanup_interval).
+ 
+    This then controls when the data in sl_log_1/sl_log_2 can be
+    dropped.
+ 
+    Data in *those* tables is deleted when it is older than the
+    earliest XID still captured in sl_event.
+ 
+   boolean - cleanup_deletelogs (default 'false')
+ 
+    This controls whether or not we DELETE data from sl_log_1/sl_log_2
+ 
+    By default, we now NEVER delete data from the log tables; we
+    instead use TRUNCATE.
+ 
+ - We now consider initiating a log switch every time cleanupEvent()
+   runs.
+ 
+   If the call to logswitch_finish() indicates that there was no log
+   switch in progress, we initiate one.
+ 
+   This means that log switches will be initiated almost as often as
+   possible.  That's a policy well worth debating :-).
+ 
+ - logswitch_finish() changes a fair bit...
+ 
+   It uses the same logic as in cleanupEvent() to determine if there
+   are any *relevant* tuples left in sl_log_[whatever], rather than
+   (potentially) scanning the table to see if there are any undeleted
+   tuples left.

From cbbrowne at lists.slony.info  Wed Jan  2 11:00:29 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Jan  2 11:00:32 2008
Subject: [Slony1-commit] slony1-engine/src/backend slony1_funcs.sql
Message-ID: <20080102190029.C858E290C67@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/backend
In directory main.slony.info:/tmp/cvs-serv29551/src/backend

Modified Files:
	slony1_funcs.sql 
Log Message:
Apply changes to cleanup thread:

- by default, use TRUNCATE rather than DELETE to trim sl_log_? tables

- moved substantially all logic for trimming of old data out of the
  C code, and into cleanupThread() stored function

  The stored function now can be used to request cleanup, so that we
  might force it to happen at an unexpected time, such as manually,
  for testing, as part of a test script.

- added parameters to slon to support this:

  interval - cleanup_interval (default '10 minutes')

     This controls how quickly old events are trimmed out.  It used to
     be a hard-coded value.

     Old events are trimmed out once the confirmations are aged by
     (cleanup_interval).

     This then controls when the data in sl_log_1/sl_log_2 can be dropped.

     Data in *those* tables is deleted when it is older than the
     earliest XID still captured in sl_event.

  boolean - cleanup_deletelogs (default 'false')

     This controls whether or not we DELETE data from sl_log_1/sl_log_2

     By default, we now NEVER delete data from the log tables; we
     instead use TRUNCATE.


Index: slony1_funcs.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.sql,v
retrieving revision 1.126
retrieving revision 1.127
diff -C2 -d -r1.126 -r1.127
*** slony1_funcs.sql	11 Dec 2007 20:39:41 -0000	1.126
--- slony1_funcs.sql	2 Jan 2008 19:00:27 -0000	1.127
***************
*** 1959,1963 ****
  	-- ----
  	update @NAMESPACE@.sl_set
! 			set set_locked = "pg_catalog".txid_snapshot_xmax("public".txid_current_snapshot())
  			where set_id = p_set_id;
  
--- 1959,1963 ----
  	-- ----
  	update @NAMESPACE@.sl_set
! 			set set_locked = "pg_catalog".txid_snapshot_xmax("pg_catalog".txid_current_snapshot())
  			where set_id = p_set_id;
  
***************
*** 2081,2085 ****
  		raise exception ''Slony-I: set % is not locked'', p_set_id;
  	end if;
! 	if v_set_row.set_locked > "pg_catalog".txid_snapshot_xmin("public".txid_current_snapshot()) then
  		raise exception ''Slony-I: cannot move set % yet, transactions < % are still in progress'',
  				p_set_id, v_set_row.set_locked;
--- 2081,2085 ----
  		raise exception ''Slony-I: set % is not locked'', p_set_id;
  	end if;
! 	if v_set_row.set_locked > "pg_catalog".txid_snapshot_xmin("pg_catalog".txid_current_snapshot()) then
  		raise exception ''Slony-I: cannot move set % yet, transactions < % are still in progress'',
  				p_set_id, v_set_row.set_locked;
***************
*** 4383,4396 ****
  
  -- ----------------------------------------------------------------------
! -- FUNCTION cleanupEvent ()
  --
  -- ----------------------------------------------------------------------
! create or replace function @NAMESPACE@.cleanupEvent ()
  returns int4
  as '
  declare
  	v_max_row	record;
  	v_min_row	record;
  	v_max_sync	int8;
  begin
  	-- ----
--- 4383,4402 ----
  
  -- ----------------------------------------------------------------------
! -- FUNCTION cleanupEvent (interval, deletelogs)
  --
  -- ----------------------------------------------------------------------
! create or replace function @NAMESPACE@.cleanupEvent (interval, boolean)
  returns int4
  as '
  declare
+ 	p_interval alias for $1;
+ 	p_deletelogs alias for $2;
  	v_max_row	record;
  	v_min_row	record;
  	v_max_sync	int8;
+ 	v_origin	int8;
+ 	v_seqno		int8;
+ 	v_xmin		bigint;
+ 	v_rc            int8;
  begin
  	-- ----
***************
*** 4467,4478 ****
  	perform @NAMESPACE@.cleanupNodelock();
  
  	return 0;
  end;
  ' language plpgsql;
! comment on function @NAMESPACE@.cleanupEvent () is
  'cleaning old data out of sl_confirm, sl_event.  Removes all but the
  last sl_confirm row per (origin,receiver), and then removes all events
  that are confirmed by all nodes in the whole cluster up to the last
! SYNC.  ';
  
  
--- 4473,4503 ----
  	perform @NAMESPACE@.cleanupNodelock();
  
+ 	-- ----
+ 	-- Find the eldest event left, for each origin
+ 	-- ----
+         for v_origin, v_seqno, v_xmin in
+ 	  select ev_origin, ev_seqno, "pg_catalog".txid_snapshot_xmin(ev_snapshot) from @NAMESPACE@.sl_event
+           where (ev_origin, ev_seqno) in (select ev_origin, min(ev_seqno) from @NAMESPACE@.sl_event where ev_type = ''SYNC'' group by ev_origin)
+ 	loop
+ 		if p_deletelogs then
+ 			delete from @NAMESPACE@.sl_log_1 where log_origin = v_origin and log_txid < v_xmin;		
+ 			delete from @NAMESPACE@.sl_log_2 where log_origin = v_origin and log_txid < v_xmin;		
+ 		end if;
+ 		delete from @NAMESPACE@.sl_seqlog where seql_origin = v_origin and seql_ev_seqno < v_seqno;
+         end loop;
+ 	
+ 	v_rc := @NAMESPACE@.logswitch_finish();
+ 	if v_rc = 0 then   -- no switch in progress
+ 		perform @NAMESPACE@.logswitch_start();
+ 	end if;
+ 
  	return 0;
  end;
  ' language plpgsql;
! comment on function @NAMESPACE@.cleanupEvent (interval, boolean) is
  'cleaning old data out of sl_confirm, sl_event.  Removes all but the
  last sl_confirm row per (origin,receiver), and then removes all events
  that are confirmed by all nodes in the whole cluster up to the last
! SYNC.  Deletes now-orphaned entries from sl_log_* if delete_logs parameter is set';
  
  
***************
*** 5081,5084 ****
--- 5106,5113 ----
  	v_current_status	int4;
  	v_dummy				record;
+ 	v_origin	int8;
+ 	v_seqno		int8;
+ 	v_xmin		bigint;
+ 	v_purgeable boolean;
  BEGIN
  	-- ----
***************
*** 5104,5107 ****
--- 5133,5137 ----
  	-- ----
  	if v_current_status = 2 then
+ 		v_purgeable := ''true'';
  		-- ----
  		-- The cleanup thread calls us after it did the delete and
***************
*** 5109,5113 ****
  		-- can truncate it and the log switch is done.
  		-- ----
! 		for v_dummy in select 1 from @NAMESPACE@.sl_log_2 loop
  			-- ----
  			-- Found a row ... log switch is still in progress.
--- 5139,5153 ----
  		-- can truncate it and the log switch is done.
  		-- ----
! 		
! 	        for v_origin, v_seqno, v_xmin in
! 		  select ev_origin, ev_seqno, "pg_catalog".txid_snapshot_xmin(ev_snapshot) from @NAMESPACE@.sl_event
! 	          where (ev_origin, ev_seqno) in (select ev_origin, min(ev_seqno) from @NAMESPACE@.sl_event where ev_type = ''SYNC'' group by ev_origin)
! 		loop
! 			select 1 from @NAMESPACE@.sl_log_2 where log_origin = v_origin and log_txid < v_xmin limit 1;		
! 			if exists then
! 				v_purgeable := ''false'';
! 			end if;
! 	        end loop;
! 		if not v_purgeable then
  			-- ----
  			-- Found a row ... log switch is still in progress.
***************
*** 5115,5119 ****
  			raise notice ''Slony-I: log switch to sl_log_1 still in progress - sl_log_2 not truncated'';
  			return -1;
! 		end loop;
  
  		raise notice ''Slony-I: log switch to sl_log_1 complete - truncate sl_log_2'';
--- 5155,5159 ----
  			raise notice ''Slony-I: log switch to sl_log_1 still in progress - sl_log_2 not truncated'';
  			return -1;
! 		end if;
  
  		raise notice ''Slony-I: log switch to sl_log_1 complete - truncate sl_log_2'';
***************
*** 5133,5136 ****
--- 5173,5177 ----
  	-- ----
  	if v_current_status = 3 then
+ 		v_purgeable := ''true'';
  		-- ----
  		-- The cleanup thread calls us after it did the delete and
***************
*** 5138,5142 ****
  		-- can truncate it and the log switch is done.
  		-- ----
! 		for v_dummy in select 1 from @NAMESPACE@.sl_log_1 loop
  			-- ----
  			-- Found a row ... log switch is still in progress.
--- 5179,5192 ----
  		-- can truncate it and the log switch is done.
  		-- ----
! 	        for v_origin, v_seqno, v_xmin in
! 		  select ev_origin, ev_seqno, "pg_catalog".txid_snapshot_xmin(ev_snapshot) from @NAMESPACE@.sl_event
! 	          where (ev_origin, ev_seqno) in (select ev_origin, min(ev_seqno) from @NAMESPACE@.sl_event where ev_type = ''SYNC'' group by ev_origin)
! 		loop
! 			select 1 from @NAMESPACE@.sl_log_1 where log_origin = v_origin and log_txid < v_xmin limit 1;		
! 			if exists then
! 				v_purgeable := ''false'';
! 			end if;
! 	        end loop;
! 		if not v_purgeable then
  			-- ----
  			-- Found a row ... log switch is still in progress.
***************
*** 5144,5148 ****
  			raise notice ''Slony-I: log switch to sl_log_2 still in progress - sl_log_1 not truncated'';
  			return -1;
! 		end loop;
  
  		raise notice ''Slony-I: log switch to sl_log_2 complete - truncate sl_log_1'';
--- 5194,5198 ----
  			raise notice ''Slony-I: log switch to sl_log_2 still in progress - sl_log_1 not truncated'';
  			return -1;
! 		end if;
  
  		raise notice ''Slony-I: log switch to sl_log_2 complete - truncate sl_log_1'';
***************
*** 5161,5165 ****
  'logswitch_finish()
  
! Attempt to finalize a log table switch in progress';
  
  
--- 5211,5221 ----
  'logswitch_finish()
  
! Attempt to finalize a log table switch in progress
! return values:
!   -1 if switch in progress, but not complete
!    0 if no switch in progress
!    1 if performed truncate on sl_log_2
!    2 if performed truncate on sl_log_1
! ';
  
  

From cbbrowne at lists.slony.info  Wed Jan  2 11:00:29 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Jan  2 11:00:32 2008
Subject: [Slony1-commit] slony1-engine/src/slon cleanup_thread.c
	confoptions.c confoptions.h slon.h
Message-ID: <20080102190029.D785E290C8D@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slon
In directory main.slony.info:/tmp/cvs-serv29551/src/slon

Modified Files:
	cleanup_thread.c confoptions.c confoptions.h slon.h 
Log Message:
Apply changes to cleanup thread:

- by default, use TRUNCATE rather than DELETE to trim sl_log_? tables

- moved substantially all logic for trimming of old data out of the
  C code, and into cleanupThread() stored function

  The stored function now can be used to request cleanup, so that we
  might force it to happen at an unexpected time, such as manually,
  for testing, as part of a test script.

- added parameters to slon to support this:

  interval - cleanup_interval (default '10 minutes')

     This controls how quickly old events are trimmed out.  It used to
     be a hard-coded value.

     Old events are trimmed out once the confirmations are aged by
     (cleanup_interval).

     This then controls when the data in sl_log_1/sl_log_2 can be dropped.

     Data in *those* tables is deleted when it is older than the
     earliest XID still captured in sl_event.

  boolean - cleanup_deletelogs (default 'false')

     This controls whether or not we DELETE data from sl_log_1/sl_log_2

     By default, we now NEVER delete data from the log tables; we
     instead use TRUNCATE.


Index: slon.h
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/slon.h,v
retrieving revision 1.65
retrieving revision 1.66
diff -C2 -d -r1.65 -r1.66
*** slon.h	19 Oct 2007 18:38:35 -0000	1.65
--- slon.h	2 Jan 2008 19:00:27 -0000	1.66
***************
*** 473,476 ****
--- 473,478 ----
  
  extern int	vac_frequency;
+ extern char *cleanup_interval;
+ extern bool cleanup_deletelogs;
  
  /* ----------

Index: confoptions.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/confoptions.c,v
retrieving revision 1.25
retrieving revision 1.26
diff -C2 -d -r1.25 -r1.26
*** confoptions.c	30 Jul 2007 22:34:33 -0000	1.25
--- confoptions.c	2 Jan 2008 19:00:27 -0000	1.26
***************
*** 721,724 ****
--- 721,735 ----
  	},
  
+ 	{
+ 		{
+ 			(const char *)"cleanup_deletelogs",
+ 			gettext_noop("Should the cleanup thread DELETE sl_log_? entries or not"),
+ 			gettext_noop("Should the cleanup thread DELETE sl_log_? entries or not"),
+ 			SLON_C_BOOL
+ 		},
+ 		&cleanup_deletelogs,
+ 		false
+ 	},
+ 
  	{{0}}
  };
***************
*** 860,863 ****
--- 871,886 ----
  	},
  #endif
+ 	{
+ 		{
+ 			(const char *)"cleanup_interval",
+ 			gettext_noop("A PostgreSQL value compatible with ::interval "
+ 						 "which indicates what aging interval should be used "
+ 						 "for deleting old events, and hence for purging sl_log_* tables."),
+ 			NULL,
+ 			SLON_C_STRING
+ 		},
+ 		&cleanup_interval,
+ 		"10 minutes"
+ 	},
  	{{0}}
  };

Index: cleanup_thread.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/cleanup_thread.c,v
retrieving revision 1.40
retrieving revision 1.41
diff -C2 -d -r1.40 -r1.41
*** cleanup_thread.c	19 Oct 2007 18:38:35 -0000	1.40
--- cleanup_thread.c	2 Jan 2008 19:00:27 -0000	1.41
***************
*** 31,34 ****
--- 31,37 ----
   */
  int			vac_frequency = SLON_VACUUM_FREQUENCY;
+ char *cleanup_interval;
+ bool cleanup_deletelogs;
+ 
  static int	vac_bias = 0;
  static unsigned long earliest_xid = 0;
***************
*** 46,52 ****
  {
  	SlonConn   *conn;
! 	SlonDString query1;
  	SlonDString query2;
! 	SlonDString query3;
  
  	PGconn	   *dbconn;
--- 49,55 ----
  {
  	SlonConn   *conn;
! 	SlonDString query_baseclean;
  	SlonDString query2;
! 	SlonDString query_pertbl;
  
  	PGconn	   *dbconn;
***************
*** 55,60 ****
  	struct timeval tv_start;
  	struct timeval tv_end;
! 	int			n,
! 				t;
  	int			vac_count = 0;
  	int			vac_enable = SLON_VACUUM_FREQUENCY;
--- 58,62 ----
  	struct timeval tv_start;
  	struct timeval tv_end;
! 	int			t;
  	int			vac_count = 0;
  	int			vac_enable = SLON_VACUUM_FREQUENCY;
***************
*** 92,97 ****
  	 * Build the query string for calling the cleanupEvent() stored procedure
  	 */
! 	dstring_init(&query1);
! 	slon_mkquery(&query1, "select %s.cleanupEvent(); ", rtcfg_namespace);
  	dstring_init(&query2);
  
--- 94,103 ----
  	 * Build the query string for calling the cleanupEvent() stored procedure
  	 */
! 	dstring_init(&query_baseclean);
! 	slon_mkquery(&query_baseclean, "select %s.cleanupEvent('%s'::interval, '%s'::boolean); ", 
! 		     rtcfg_namespace, 
! 		     cleanup_interval,
! 		     cleanup_deletelogs ? "true" : "false"
! 		);
  	dstring_init(&query2);
  
***************
*** 110,119 ****
  		 */
  		gettimeofday(&tv_start, NULL);
! 		res = PQexec(dbconn, dstring_data(&query1));
  		if (PQresultStatus(res) != PGRES_TUPLES_OK)
  		{
  			slon_log(SLON_FATAL,
  					 "cleanupThread: \"%s\" - %s",
! 					 dstring_data(&query1), PQresultErrorMessage(res));
  			PQclear(res);
  			slon_retry();
--- 116,125 ----
  		 */
  		gettimeofday(&tv_start, NULL);
! 		res = PQexec(dbconn, dstring_data(&query_baseclean));
  		if (PQresultStatus(res) != PGRES_TUPLES_OK)
  		{
  			slon_log(SLON_FATAL,
  					 "cleanupThread: \"%s\" - %s",
! 					 dstring_data(&query_baseclean), PQresultErrorMessage(res));
  			PQclear(res);
  			slon_retry();
***************
*** 126,210 ****
  				 TIMEVAL_DIFF(&tv_start, &tv_end));
  
- 		/*
- 		 * Clean up the logs and eventually finish switching logs
- 		 */
- 		gettimeofday(&tv_start, NULL);
  		slon_mkquery(&query2,
! 					 "select ev_origin, ev_seqno, "
! 					 "  \"public\".txid_snapshot_xmin(ev_snapshot) "
! 					 "from %s.sl_event "
! 					 "where (ev_origin, ev_seqno) in "
! 					 "    (select ev_origin, min(ev_seqno) "
! 					 "     from %s.sl_event "
! 					 "     where ev_type = 'SYNC' "
! 					 "     group by ev_origin); ",
! 					 rtcfg_namespace, rtcfg_namespace);
! 		res = PQexec(dbconn, dstring_data(&query2));
! 		if (PQresultStatus(res) != PGRES_TUPLES_OK)
! 		{
! 			slon_log(SLON_FATAL,
! 					 "cleanupThread: \"%s\" - %s",
! 					 dstring_data(&query2), PQresultErrorMessage(res));
! 			PQclear(res);
! 			slon_retry();
! 			break;
! 		}
! 		n = PQntuples(res);
! 		for (t = 0; t < n; t++)
  		{
! 			slon_mkquery(&query2,
! 						 "delete from %s.sl_log_1 "
! 						 "where log_origin = '%s' "
! 						 "and log_txid < '%s'; "
! 						 "delete from %s.sl_log_2 "
! 						 "where log_origin = '%s' "
! 						 "and log_txid < '%s'; "
! 						 "delete from %s.sl_seqlog "
! 						 "where seql_origin = '%s' "
! 						 "and seql_ev_seqno < '%s'; "
! 						 "select %s.logswitch_finish(); ",
! 						 rtcfg_namespace, PQgetvalue(res, t, 0),
! 						 PQgetvalue(res, t, 2),
! 						 rtcfg_namespace, PQgetvalue(res, t, 0),
! 						 PQgetvalue(res, t, 2),
! 						 rtcfg_namespace, PQgetvalue(res, t, 0),
! 						 PQgetvalue(res, t, 1),
! 						 rtcfg_namespace);
! 			res2 = PQexec(dbconn, dstring_data(&query2));
! 			if (PQresultStatus(res2) != PGRES_TUPLES_OK)
! 			{
! 				slon_log(SLON_FATAL,
! 						 "cleanupThread: \"%s\" - %s",
! 						 dstring_data(&query2), PQresultErrorMessage(res2));
! 				PQclear(res);
! 				PQclear(res2);
! 				slon_retry();
! 				break;
! 			}
! 			PQclear(res2);
! 
! 			/*
! 			 * Eventually kick off a logswitch. This might fail,
! 			 * but this is not really slon's problem, so we just
! 			 * shrug and move on if it does.
! 			 */
! 			slon_mkquery(&query2,
! 						 "select %s.logswitch_weekly(); ",
! 						 rtcfg_namespace);
! 			res2 = PQexec(dbconn, dstring_data(&query2));
! 			if (PQresultStatus(res2) != PGRES_TUPLES_OK)
! 			{
! 				slon_log(SLON_WARN,
! 						 "cleanupThread: \"%s\" - %s",
! 						 dstring_data(&query2), PQresultErrorMessage(res2));
! 			}
! 			PQclear(res2);
  		}
! 		PQclear(res);
! 		gettimeofday(&tv_end, NULL);
! 		slon_log(SLON_INFO,
! 				 "cleanupThread: %8.3f seconds for delete logs\n",
! 				 TIMEVAL_DIFF(&tv_start, &tv_end));
! 
  		/*
  		 * Detain the usual suspects (vacuum event and log data)
--- 132,146 ----
  				 TIMEVAL_DIFF(&tv_start, &tv_end));
  
  		slon_mkquery(&query2,
! 			     "select %s.logswitch_weekly(); ",
! 			     rtcfg_namespace);
! 		res2 = PQexec(dbconn, dstring_data(&query2));
! 		if (PQresultStatus(res2) != PGRES_TUPLES_OK)
  		{
! 			slon_log(SLON_WARN,
! 				 "cleanupThread: \"%s\" - %s",
! 				 dstring_data(&query2), PQresultErrorMessage(res2));
  		}
! 		PQclear(res2);
  		/*
  		 * Detain the usual suspects (vacuum event and log data)
***************
*** 267,279 ****
  				slon_log (SLON_DEBUG1, "cleanupThread: %s analyze \"%s\".%s;\n",
  					      vacuum_action, tab_nspname, tab_relname);
! 				dstring_init(&query3);
! 				slon_mkquery (&query3, "%s analyze \"%s\".%s;",
  					      vacuum_action, tab_nspname, tab_relname);
! 				res2 = PQexec(dbconn, dstring_data(&query3));
  				if (PQresultStatus(res) != PGRES_COMMAND_OK)  /* query error */
                                  {
                   	                slon_log(SLON_ERROR,
  	                                        "cleanupThread: \"%s\" - %s",
!                                                 dstring_data(&query3), PQresultErrorMessage(res2));
                                                  /*
                                                   * slon_retry(); break;
--- 203,215 ----
  				slon_log (SLON_DEBUG1, "cleanupThread: %s analyze \"%s\".%s;\n",
  					      vacuum_action, tab_nspname, tab_relname);
! 				dstring_init(&query_pertbl);
! 				slon_mkquery (&query_pertbl, "%s analyze \"%s\".%s;",
  					      vacuum_action, tab_nspname, tab_relname);
! 				res2 = PQexec(dbconn, dstring_data(&query_pertbl));
  				if (PQresultStatus(res) != PGRES_COMMAND_OK)  /* query error */
                                  {
                   	                slon_log(SLON_ERROR,
  	                                        "cleanupThread: \"%s\" - %s",
!                                                 dstring_data(&query_pertbl), PQresultErrorMessage(res2));
                                                  /*
                                                   * slon_retry(); break;
***************
*** 281,285 ****
                                  }
  				PQclear(res2);
! 				dstring_reset(&query3);
  			}
  			gettimeofday(&tv_end, NULL);
--- 217,221 ----
                                  }
  				PQclear(res2);
! 				dstring_reset(&query_pertbl);
  			}
  			gettimeofday(&tv_end, NULL);
***************
*** 291,295 ****
  			 * Free Resources
  			 */
! 			dstring_free(&query3);
  
  		}
--- 227,231 ----
  			 * Free Resources
  			 */
! 			dstring_free(&query_pertbl);
  
  		}
***************
*** 299,303 ****
  	 * Free Resources
  	 */
! 	dstring_free(&query1);
  	dstring_free(&query2);
  
--- 235,239 ----
  	 * Free Resources
  	 */
! 	dstring_free(&query_baseclean);
  	dstring_free(&query2);
  
***************
*** 331,339 ****
  	long long	xid;
  	PGresult   *res;
! 	SlonDString query1;
  
! 	dstring_init(&query1);
! 	(void) slon_mkquery(&query1, "select %s.getMinXid();", rtcfg_namespace);
! 	res = PQexec(dbconn, dstring_data(&query1));
  	if (PQresultStatus(res) != PGRES_TUPLES_OK)
  	{
--- 267,275 ----
  	long long	xid;
  	PGresult   *res;
! 	SlonDString query;
  
! 	dstring_init(&query);
! 	(void) slon_mkquery(&query, "select %s.getMinXid();", rtcfg_namespace);
! 	res = PQexec(dbconn, dstring_data(&query));
  	if (PQresultStatus(res) != PGRES_TUPLES_OK)
  	{
***************
*** 346,350 ****
  	slon_log(SLON_DEBUG1, "cleanupThread: minxid: %d\n", xid);
  	PQclear(res);
! 	dstring_free(&query1);
  	return (unsigned long)xid;
  }
--- 282,286 ----
  	slon_log(SLON_DEBUG1, "cleanupThread: minxid: %d\n", xid);
  	PQclear(res);
! 	dstring_free(&query);
  	return (unsigned long)xid;
  }

Index: confoptions.h
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/confoptions.h,v
retrieving revision 1.35
retrieving revision 1.36
diff -C2 -d -r1.35 -r1.36
*** confoptions.h	20 Apr 2007 20:53:18 -0000	1.35
--- confoptions.h	2 Jan 2008 19:00:27 -0000	1.36
***************
*** 14,18 ****
  extern char *archive_dir;
  
- extern int	vac_frequency;
  extern int	slon_log_level;
  extern int	sync_interval;
--- 14,17 ----
***************
*** 28,31 ****
--- 27,40 ----
  extern int	quit_sync_finalsync;
  
+ /*
+  * ----------
+  * Global variables in cleanup_thread.c
+  * ----------
+  */
+ 
+ extern int	vac_frequency;
+ extern char *cleanup_interval;
+ extern bool cleanup_deletelogs;
+ 
  char	   *Syslog_ident;
  char	   *Syslog_facility;

From cbbrowne at lists.slony.info  Wed Jan  2 11:12:44 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Jan  2 11:12:46 2008
Subject: [Slony1-commit] slony1-engine/doc/adminguide bestpractices.sgml
	maintenance.sgml slonconf.sgml
Message-ID: <20080102191244.7B48C290C52@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv29791

Modified Files:
	bestpractices.sgml maintenance.sgml slonconf.sgml 
Log Message:
Indicate changes to slon policy resulting from the recent patch that allows
configuring slon to never DELETE from sl_log_? tables, but to rather solely
trim data via TRUNCATE.

Notably this includes:
- Documenting the new slon parameters
- Changing wordings in several places to indicate the behaviour change


Index: slonconf.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/slonconf.sgml,v
retrieving revision 1.18
retrieving revision 1.19
diff -C2 -d -r1.18 -r1.19
*** slonconf.sgml	27 Aug 2007 22:23:29 -0000	1.18
--- slonconf.sgml	2 Jan 2008 19:12:42 -0000	1.19
***************
*** 336,339 ****
--- 336,370 ----
        </listitem>
      </varlistentry>
+ 
+ 
+     <varlistentry id="slon-config-cleanup-interval" xreflabel="slon_config_cleanup_interval">
+       <term><varname>cleanup_interval</varname> (<type>interval</type>)</term>
+       <indexterm>
+         <primary><varname>cleanup_interval</varname> configuration parameter</primary>
+       </indexterm>
+       <listitem>
+         <para>
+           Controls how quickly old events are trimmed out.  That
+           subsequently controls when the data in the log tables,
+           <envar>sl_log_1</envar> and <envar>sl_log_2</envar>, get
+           trimmed out.  Default: '10 minutes'.
+         </para>
+       </listitem>
+     </varlistentry>
+ 
+     <varlistentry id="slon-config-cleanup-deletelogs" xreflabel="slon_conf_cleanup_deletelogs">
+       <term><varname>cleanup_deletelogs</varname> (<type>boolean</type>)</term>
+       <indexterm>
+         <primary><varname>cleanup_deletelogs</varname> configuration parameter</primary>
+       </indexterm>
+       <listitem>
+         <para>
+           Controls whether or not we use DELETE to trim old data from the log tables,
+           <envar>sl_log_1</envar> and <envar>sl_log_2</envar>.
+           Default: false
+         </para>
+       </listitem>
+     </varlistentry>
+ 
      <varlistentry id="slon-config-desired-sync-time" xreflabel="desired_sync_time">
        <term><varname>desired_sync_time</varname>  (<type>integer</type>)</term>

Index: maintenance.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/maintenance.sgml,v
retrieving revision 1.27
retrieving revision 1.28
diff -C2 -d -r1.27 -r1.28
*** maintenance.sgml	18 Apr 2007 15:28:16 -0000	1.27
--- maintenance.sgml	2 Jan 2008 19:12:42 -0000	1.28
***************
*** 12,16 ****
  <productname>Slony-I</productname> cluster's namespace, notably
  entries in <xref linkend="table.sl-log-1">, <xref
! linkend="table.sl-log-2"> (not yet used), and <xref
  linkend="table.sl-seqlog">.</para></listitem>
  
--- 12,16 ----
  <productname>Slony-I</productname> cluster's namespace, notably
  entries in <xref linkend="table.sl-log-1">, <xref
! linkend="table.sl-log-2">, and <xref
  linkend="table.sl-seqlog">.</para></listitem>
  
***************
*** 26,30 ****
  vacuuming of these tables.  Unfortunately, it has been quite possible
  for <application>pg_autovacuum</application> to not vacuum quite
! frequently enough, so you probably want to use the internal vacuums.
  Vacuuming &pglistener; <quote>too often</quote> isn't nearly as
  hazardous as not vacuuming it frequently enough.</para>
--- 26,30 ----
  vacuuming of these tables.  Unfortunately, it has been quite possible
  for <application>pg_autovacuum</application> to not vacuum quite
! frequently enough, so you may prefer to use the internal vacuums.
  Vacuuming &pglistener; <quote>too often</quote> isn't nearly as
  hazardous as not vacuuming it frequently enough.</para>
***************
*** 59,71 ****
  </para>
  
! <sect2 id="maintenance-autovac"> <title> Interaction with &postgres; autovacuum </title>
  
  <indexterm><primary>autovacuum interaction</primary></indexterm>
  
! <para> Recent versions of &postgres; support an <quote/autovacuum/
! process which notices when tables are modified, thereby creating dead
! tuples, and vacuums those tables, <quote/on demand./ It has been
! observed that this can interact somewhat negatively with &slony1;'s
! own  vacuuming policies on its own tables. </para>
  
  <para> &slony1; requests vacuums on its tables immediately after
--- 59,73 ----
  </para>
  
! <sect2 id="maintenance-autovac"> <title> Interaction with &postgres;
! autovacuum </title>
  
  <indexterm><primary>autovacuum interaction</primary></indexterm>
  
! <para> Recent versions of &postgres; support an
! <quote>autovacuum</quote> process which notices when tables are
! modified, thereby creating dead tuples, and vacuums those tables,
! <quote>on demand.</quote> It has been observed that this can interact
! somewhat negatively with &slony1;'s own vacuuming policies on its own
! tables. </para>
  
  <para> &slony1; requests vacuums on its tables immediately after

Index: bestpractices.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/bestpractices.sgml,v
retrieving revision 1.28
retrieving revision 1.29
diff -C2 -d -r1.28 -r1.29
*** bestpractices.sgml	18 Apr 2007 15:28:16 -0000	1.28
--- bestpractices.sgml	2 Jan 2008 19:12:42 -0000	1.29
***************
*** 614,620 ****
  the exact form that the index setup should take. </para> 
  
! <para> In 1.2, there is a process that runs automatically to add
! partial indexes by origin node number, which should be the optimal
! form for such an index to take.  </para>
  </listitem>
  
--- 614,620 ----
  the exact form that the index setup should take. </para> 
  
! <para> In 1.2 and later versions, there is a process that runs
! automatically to add partial indexes by origin node number, which
! should be the optimal form for such an index to take.  </para>
  </listitem>
  

From cbbrowne at lists.slony.info  Thu Jan  3 07:47:23 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Thu Jan  3 07:47:24 2008
Subject: [Slony1-commit] slony1-engine/src/slon confoptions.c confoptions.h
	slon.c
Message-ID: <20080103154723.D3C9B290C47@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slon
In directory main.slony.info:/tmp/cvs-serv14474

Modified Files:
	confoptions.c confoptions.h slon.c 
Log Message:
Added dump_configuration() function that dumps (to Slony-I log), at start
time, the values of all configuration parameters.

Example:

2008-01-03 15:44:55 UTC CONFIG main: Integer option vac_frequency = 3
2008-01-03 15:44:55 UTC CONFIG main: Integer option log_level = 0
2008-01-03 15:44:55 UTC CONFIG main: Integer option sync_interval = 2000
2008-01-03 15:44:55 UTC CONFIG main: Integer option sync_interval_timeout = 10000
2008-01-03 15:44:55 UTC CONFIG main: Integer option sync_group_maxsize = 20
2008-01-03 15:44:55 UTC CONFIG main: Integer option desired_sync_time = 60000
2008-01-03 15:44:55 UTC CONFIG main: Integer option syslog = 0
2008-01-03 15:44:55 UTC CONFIG main: Integer option quit_sync_provider = 0
2008-01-03 15:44:55 UTC CONFIG main: Integer option quit_sync_finalsync = 0
2008-01-03 15:44:55 UTC CONFIG main: Integer option sync_max_rowsize = 8192
2008-01-03 15:44:55 UTC CONFIG main: Integer option sync_max_largemem = 5242880
2008-01-03 15:44:55 UTC CONFIG main: Integer option remote_listen_timeout = 300
2008-01-03 15:44:55 UTC CONFIG main: Boolean option log_pid = 0
2008-01-03 15:44:55 UTC CONFIG main: Boolean option log_timestamp = 1
2008-01-03 15:44:55 UTC CONFIG main: Boolean option cleanup_deletelogs = 0
2008-01-03 15:44:55 UTC CONFIG main: Real option real_placeholder = 0.000000
2008-01-03 15:44:55 UTC CONFIG main: String option cluster_name = dw
2008-01-03 15:44:55 UTC CONFIG main: String option conn_info = dbname=dw
2008-01-03 15:44:55 UTC CONFIG main: String option pid_file = (null)
2008-01-03 15:44:55 UTC CONFIG main: String option log_timestamp_format = %Y-%m-%d %H:%M:%S %Z
2008-01-03 15:44:55 UTC CONFIG main: String option archive_dir = (null)
2008-01-03 15:44:55 UTC CONFIG main: String option sql_on_connection = (null)
2008-01-03 15:44:55 UTC CONFIG main: String option lag_interval = (null)
2008-01-03 15:44:55 UTC CONFIG main: String option command_on_logarchive = (null)
2008-01-03 15:44:55 UTC CONFIG main: String option syslog_facility = LOCAL0
2008-01-03 15:44:55 UTC CONFIG main: String option syslog_ident = slon
2008-01-03 15:44:55 UTC CONFIG main: String option cleanup_interval = 10 minutes
2008-01-03 15:44:55 UTC CONFIG slon: worker process created - pid = 28482



Index: slon.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/slon.c,v
retrieving revision 1.76
retrieving revision 1.77
diff -C2 -d -r1.76 -r1.77
*** slon.c	20 Jul 2007 19:59:54 -0000	1.76
--- slon.c	3 Jan 2008 15:47:21 -0000	1.77
***************
*** 392,395 ****
--- 392,400 ----
  	}
  
+ 
+ 	/* 
+ 	 * Dump out current configuration - all elements of the various arrays...
+ 	 */
+ 	dump_configuration();
  	/*
  	 * Connect to the local database to read the initial configuration

Index: confoptions.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/confoptions.c,v
retrieving revision 1.26
retrieving revision 1.27
diff -C2 -d -r1.26 -r1.27
*** confoptions.c	2 Jan 2008 19:00:27 -0000	1.26
--- confoptions.c	3 Jan 2008 15:47:21 -0000	1.27
***************
*** 22,25 ****
--- 22,49 ----
  static char	   *string_placeholder;
  
+ void dump_configuration(void);
+ 
+ void dump_configuration(void)
+ {
+ 	int i;
+ 	for (i = 0; ConfigureNamesInt[i].gen.name; i++) {
+ 		slon_log(SLON_CONFIG, "main: Integer option %s = %d\n",
+ 			 ConfigureNamesInt[i].gen.name, *(ConfigureNamesInt[i].variable));
+ 	}
+ 	for (i = 0; ConfigureNamesBool[i].gen.name; i++) {
+ 		slon_log(SLON_CONFIG, "main: Boolean option %s = %d\n",
+ 			 ConfigureNamesBool[i].gen.name, *(ConfigureNamesBool[i].variable));
+ 	}
+ 	for (i = 0; ConfigureNamesReal[i].gen.name; i++) {
+ 		slon_log(SLON_CONFIG, "main: Real option %s = %f\n",
+ 			 ConfigureNamesReal[i].gen.name, *(ConfigureNamesReal[i].variable));
+ 	}
+ 	for (i = 0; ConfigureNamesString[i].gen.name; i++) {
+ 		slon_log(SLON_CONFIG, "main: String option %s = %s\n",
+ 			 ConfigureNamesString[i].gen.name, *(ConfigureNamesString[i].variable));
+ 	}
+ 	
+ 
+ }
  
  

Index: confoptions.h
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/confoptions.h,v
retrieving revision 1.36
retrieving revision 1.37
diff -C2 -d -r1.36 -r1.37
*** confoptions.h	2 Jan 2008 19:00:27 -0000	1.36
--- confoptions.h	3 Jan 2008 15:47:21 -0000	1.37
***************
*** 8,11 ****
--- 8,13 ----
  void	   *get_config_option(const char *name);
  
+ void dump_configuration(void);
+ 
  extern char *rtcfg_cluster_name;
  extern char *rtcfg_conninfo;

From cbbrowne at lists.slony.info  Thu Jan  3 07:48:51 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Thu Jan  3 07:48:52 2008
Subject: [Slony1-commit] slony1-engine RELEASE-2.0
Message-ID: <20080103154851.6A7A7290C6F@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine
In directory main.slony.info:/tmp/cvs-serv14512

Modified Files:
	RELEASE-2.0 
Log Message:
Note logging of configuration in release notes


Index: RELEASE-2.0
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/RELEASE-2.0,v
retrieving revision 1.11
retrieving revision 1.12
diff -C2 -d -r1.11 -r1.12
*** RELEASE-2.0	2 Jan 2008 19:00:27 -0000	1.11
--- RELEASE-2.0	3 Jan 2008 15:48:49 -0000	1.12
***************
*** 158,159 ****
--- 158,164 ----
    (potentially) scanning the table to see if there are any undeleted
    tuples left.
+ 
+ - At slon startup time, it logs (at SLON_CONFIG level) all of the
+   parameter values.  Per Bugzilla entry #21.
+ 
+   http://www.slony.info/bugzilla/show_bug.cgi?id=21

From cbbrowne at lists.slony.info  Thu Jan  3 14:42:49 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Thu Jan  3 14:42:51 2008
Subject: [Slony1-commit] slony1-engine/doc/adminguide schemadoc.xml
Message-ID: <20080103224249.1ED64290273@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv23112

Modified Files:
      Tag: REL_1_2_STABLE
	schemadoc.xml 
Log Message:
Bring schemadoc.xml up to date, and address bug #29
http://www.slony.info/bugzilla/show_bug.cgi?id=29


Index: schemadoc.xml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/schemadoc.xml,v
retrieving revision 1.7.2.1
retrieving revision 1.7.2.2
diff -C2 -d -r1.7.2.1 -r1.7.2.2
*** schemadoc.xml	28 Aug 2007 19:18:12 -0000	1.7.2.1
--- schemadoc.xml	3 Jan 2008 22:42:46 -0000	1.7.2.2
***************
*** 3034,3037 ****
--- 3034,3126 ----
  
  
+ <!-- Function add_empty_table_to_replication( integer, integer, text, text, text, text ) -->
+     <section id="function.add-empty-table-to-replication-integer-integer-text-text-text-text"
+              xreflabel="schemadocadd_empty_table_to_replication( integer, integer, text, text, text, text )">
+       <title id="function.add-empty-table-to-replication-integer-integer-text-text-text-text-title">
+        add_empty_table_to_replication( integer, integer, text, text, text, text )
+       </title>
+       <titleabbrev id="function.add-empty-table-to-replication-integer-integer-text-text-text-text-titleabbrev">
+        add_empty_table_to_replication( integer, integer, text, text, text, text )
+       </titleabbrev>
+ 
+       <para>
+        <segmentedlist>
+         <title>Function Properties</title>
+         <?dbhtml list-presentation="list"?>
+         <segtitle>Language</segtitle>
+         <segtitle>Return Type</segtitle>
+         <seglistitem>
+          <seg>PLPGSQL</seg>
+          <seg>bigint</seg>
+         </seglistitem>
+        </segmentedlist>
+  
+        Verify that a table is empty, and add it to replication.  
+ tab_idxname is optional - if NULL, then we use the primary key.
+         <programlisting>
+ declare
+   p_set_id alias for $1;
+   p_tab_id alias for $2;
+   p_nspname alias for $3;
+   p_tabname alias for $4;
+   p_idxname alias for $5;
+   p_comment alias for $6;
+ 
+   prec record;
+   v_origin int4;
+   v_isorigin boolean;
+   v_fqname text;
+   v_query text;
+   v_rows integer;
+   v_idxname text;
+ 
+ begin
+ -- Need to validate that the set exists; the set will tell us if this is the origin
+   select set_origin into v_origin from sl_set where set_id = p_set_id;
+   if not found then
+ 	raise exception &#39;add_empty_table_to_replication: set % not found!&#39;, p_set_id;
+   end if;
+ 
+ -- Need to be aware of whether or not this node is origin for the set
+    v_isorigin := ( v_origin = getLocalNodeId(&#39;_schemadoc&#39;) );
+ 
+    v_fqname := &#39;&quot;&#39; || p_nspname || &#39;&quot;.&quot;&#39; || p_tabname || &#39;&quot;&#39;;
+ -- Take out a lock on the table
+    v_query := &#39;lock &#39; || v_fqname || &#39;;&#39;;
+    execute v_query;
+ 
+    if v_isorigin then
+ 	-- On the origin, verify that the table is empty, failing if it has any tuples
+         v_query := &#39;select 1 as tuple from &#39; || v_fqname || &#39; limit 1;&#39;;
+ 	execute v_query into prec;
+         GET DIAGNOSTICS v_rows = ROW_COUNT;
+ 	if v_rows = 0 then
+ 		raise notice &#39;add_empty_table_to_replication: table % empty on origin - OK&#39;, v_fqname;
+ 	else
+ 		raise exception &#39;add_empty_table_to_replication: table % contained tuples on origin node %&#39;, v_fqname, v_origin;
+ 	end if;
+    else
+ 	-- On other nodes, TRUNCATE the table
+         v_query := &#39;truncate &#39; || v_fqname || &#39;;&#39;;
+ 	execute v_query;
+    end if;
+ -- If p_idxname is NULL, then look up the PK index, and RAISE EXCEPTION if one does not exist
+    if p_idxname is NULL then
+ 	select c2.relname into prec from pg_catalog.pg_index i, pg_catalog.pg_class c1, pg_catalog.pg_class c2, pg_catalog.pg_namespace n where i.indrelid = c1.oid and i.indexrelid = c2.oid and c1.relname = p_tabname and i.indisprimary and n.nspname = p_nspname and n.oid = c1.relnamespace;
+ 	if not found then
+ 		raise exception &#39;add_empty_table_to_replication: table % has no primary key and no candidate specified!&#39;, v_fqname;
+ 	else
+ 		v_idxname := prec.relname;
+ 	end if;
+    else
+ 	v_idxname := p_idxname;
+    end if;
+    perform setAddTable_int(p_set_id, p_tab_id, v_fqname, v_idxname, p_comment);
+    return alterTableRestore(p_tab_id);
+ end
+ </programlisting>
+       </para>
+     </section>
+ 
  <!-- Function add_missing_table_field( text, text, text, text ) -->
      <section id="function.add-missing-table-field-text-text-text-text"
***************
*** 6677,6681 ****
  <!-- Function logtrigger(  ) -->
      <section id="function.logtrigger"
!              xreflabel="schemadoclogtrigger(  )">
        <title id="function.logtrigger-title">
         logtrigger(  )
--- 6766,6770 ----
  <!-- Function logtrigger(  ) -->
      <section id="function.logtrigger"
!              xreflabel="schemadoc.logtrigger(  )">
        <title id="function.logtrigger-title">
         logtrigger(  )
***************
*** 7286,7321 ****
          <programlisting>
  declare
! 	v_receiver record ;
! 	v_provider record ;
! 	v_origin record ;
! 	v_reachable int4[] ;
  begin
  	-- First remove the entire configuration
  	delete from sl_listen;
  
! 	-- Loop over every possible pair of receiver and provider
! 	for v_receiver in select no_id from sl_node loop
! 		for v_provider in select pa_server as no_id from sl_path where pa_client = v_receiver.no_id loop
  
! 			-- Find all nodes that v_provider.no_id can receiver events from without using v_receiver.no_id			
! 			for v_origin in select * from ReachableFromNode(v_provider.no_id, array[v_receiver.no_id]) as r(no_id) loop
  
! 				-- If v_receiver.no_id subscribes a set from v_provider.no_id, events have to travel the same
! 				-- path as the data. Ignore possible sl_listen that would break that rule.
! 				perform 1 from sl_subscribe
! 					join sl_set on sl_set.set_id = sl_subscribe.sub_set
! 		 			where
! 						sub_receiver = v_receiver.no_id and
! 						sub_provider != v_provider.no_id and
! 						set_origin = v_origin.no_id ;
! 				if not found then
! 					insert into sl_listen (li_receiver, li_provider, li_origin)
! 						values (v_receiver.no_id, v_provider.no_id, v_origin.no_id) ;
! 				end if ;
  
  
! 			end loop ;
  
- 		end loop ;
  	end loop ;
  
--- 7375,7461 ----
          <programlisting>
  declare
! 	v_row	record;
! 	skip    boolean;
  begin
  	-- First remove the entire configuration
  	delete from sl_listen;
  
! 	-- Second populate the sl_listen configuration with a full
! 	-- network of all possible paths.
! 	insert into sl_listen
! 				(li_origin, li_provider, li_receiver)
! 			select pa_server, pa_server, pa_client from sl_path;
! 	while true loop
! 		insert into sl_listen
! 					(li_origin, li_provider, li_receiver)
! 			select distinct li_origin, pa_server, pa_client
! 				from sl_listen, sl_path
! 				where li_receiver = pa_server
! 				  and li_origin &lt;&gt; pa_client
! 			except
! 			select li_origin, li_provider, li_receiver
! 				from sl_listen;
  
! 		if not found then
! 			exit;
! 		end if;
! 	end loop;
  
! 	-- We now replace specific event-origin,receiver combinations
! 	-- with a configuration that tries to avoid events arriving at
! 	-- a node before the data provider actually has the data ready.
  
+ 	-- Loop over every possible pair of receiver and event origin
+ 	for v_row in select N1.no_id as receiver, N2.no_id as origin
+ 			from sl_node as N1, sl_node as N2
+ 			where N1.no_id &lt;&gt; N2.no_id
+ 	loop
+ 		skip := &#39;f&#39;;
+ 		-- 1st choice:
+ 		-- If we use the event origin as a data provider for any
+ 		-- set that originates on that very node, we are a direct
+ 		-- subscriber to that origin and listen there only.
+ 		if exists (select true from sl_set, sl_subscribe
+ 				where set_origin = v_row.origin
+ 				  and sub_set = set_id
+ 				  and sub_provider = v_row.origin
+ 				  and sub_receiver = v_row.receiver
+ 				  and sub_active)
+ 		then
+ 			delete from sl_listen
+ 				where li_origin = v_row.origin
+ 				  and li_receiver = v_row.receiver;
+ 			insert into sl_listen (li_origin, li_provider, li_receiver)
+ 				values (v_row.origin, v_row.origin, v_row.receiver);
+ 			skip := &#39;t&#39;;
+ 		end if;
  
! 		if skip then
! 			skip := &#39;f&#39;;
! 		else
! 		-- 2nd choice:
! 		-- If we are subscribed to any set originating on this
! 		-- event origin, we want to listen on all data providers
! 		-- we use for this origin. We are a cascaded subscriber
! 		-- for sets from this node.
! 			if exists (select true from sl_set, sl_subscribe
! 						where set_origin = v_row.origin
! 						  and sub_set = set_id
! 						  and sub_receiver = v_row.receiver
! 						  and sub_active)
! 			then
! 				delete from sl_listen
! 					where li_origin = v_row.origin
! 					  and li_receiver = v_row.receiver;
! 				insert into sl_listen (li_origin, li_provider, li_receiver)
! 					select distinct set_origin, sub_provider, v_row.receiver
! 						from sl_set, sl_subscribe
! 						where set_origin = v_row.origin
! 						  and sub_set = set_id
! 						  and sub_receiver = v_row.receiver
! 						  and sub_active;
! 			end if;
! 		end if;
  
  	end loop ;
  
***************
*** 7661,7664 ****
--- 7801,7863 ----
      </section>
  
+ <!-- Function replicate_partition( integer, text, text, text, text ) -->
+     <section id="function.replicate-partition-integer-text-text-text-text"
+              xreflabel="schemadocreplicate_partition( integer, text, text, text, text )">
+       <title id="function.replicate-partition-integer-text-text-text-text-title">
+        replicate_partition( integer, text, text, text, text )
+       </title>
+       <titleabbrev id="function.replicate-partition-integer-text-text-text-text-titleabbrev">
+        replicate_partition( integer, text, text, text, text )
+       </titleabbrev>
+ 
+       <para>
+        <segmentedlist>
+         <title>Function Properties</title>
+         <?dbhtml list-presentation="list"?>
+         <segtitle>Language</segtitle>
+         <segtitle>Return Type</segtitle>
+         <seglistitem>
+          <seg>PLPGSQL</seg>
+          <seg>bigint</seg>
+         </seglistitem>
+        </segmentedlist>
+  
+        Add a partition table to replication.
+ tab_idxname is optional - if NULL, then we use the primary key.
+ This function looks up replication configuration via the parent table.
+         <programlisting>
+ declare
+   p_tab_id alias for $1;
+   p_nspname alias for $2;
+   p_tabname alias for $3;
+   p_idxname alias for $4;
+   p_comment alias for $5;
+ 
+   prec record;
+   prec2 record;
+   v_set_id int4;
+ 
+ begin
+ -- Look up the parent table; fail if it does not exist
+    select c1.oid into prec from pg_catalog.pg_class c1, pg_catalog.pg_class c2, pg_catalog.pg_inherits i, pg_catalog.pg_namespace n where c1.oid = i.inhparent  and c2.oid = i.inhrelid and n.oid = c2.relnamespace and n.nspname = p_nspname and c2.relname = p_tabname;
+    if not found then
+ 	raise exception &#39;replicate_partition: No parent table found for %.%!&#39;, p_nspname, p_tabname;
+    end if;
+ 
+ -- The parent table tells us what replication set to use
+    select tab_set into prec2 from sl_table where tab_reloid = prec.oid;
+    if not found then
+ 	raise exception &#39;replicate_partition: Parent table % for new partition %.% is not replicated!&#39;, prec.oid, p_nspname, p_tabname;
+    end if;
+ 
+    v_set_id := prec2.tab_set;
+ 
+ -- Now, we have all the parameters necessary to run add_empty_table_to_replication...
+    return add_empty_table_to_replication(v_set_id, p_tab_id, p_nspname, p_tabname, p_idxname, p_comment);
+ end
+ </programlisting>
+       </para>
+     </section>
+ 
  <!-- Function sequencelastvalue( text ) -->
      <section id="function.sequencelastvalue-text"
***************
*** 9092,9096 ****
          <programlisting>
  begin
! 	return 11;
  end;
  </programlisting>
--- 9291,9295 ----
          <programlisting>
  begin
! 	return 12;
  end;
  </programlisting>

From cbbrowne at lists.slony.info  Fri Jan  4 11:52:37 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Jan  4 11:52:39 2008
Subject: [Slony1-commit] slony1-engine/tools/altperl
	slony_show_configuration.pl
Message-ID: <20080104195237.7DD17290C25@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tools/altperl
In directory main.slony.info:/tmp/cvs-serv11366

Modified Files:
	slony_show_configuration.pl 
Log Message:
Per Bugzilla bug #30 observed by Peter Eisentraut <peter_e@gmx.net>

It says

Slony Binaries in: @@PGBINDIR@@

but it should say

Slony Binaries in: @@SLONBINDIR@@

because that is where they are actually installed.



Index: slony_show_configuration.pl
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tools/altperl/slony_show_configuration.pl,v
retrieving revision 1.1
retrieving revision 1.2
diff -C2 -d -r1.1 -r1.2
*** slony_show_configuration.pl	31 May 2005 16:11:05 -0000	1.1
--- slony_show_configuration.pl	4 Jan 2008 19:52:35 -0000	1.2
***************
*** 41,45 ****
  Slony-I Cluster: $CLUSTER_NAME
  Logs stored under $LOGDIR
! Slony Binaries in: @@PGBINDIR@@
  };
  if ($APACHE_ROTATOR) {
--- 41,45 ----
  Slony-I Cluster: $CLUSTER_NAME
  Logs stored under $LOGDIR
! Slony Binaries in: @@SLONBINDIR@@
  };
  if ($APACHE_ROTATOR) {

From cbbrowne at lists.slony.info  Fri Jan  4 11:55:20 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Jan  4 11:55:21 2008
Subject: [Slony1-commit] slony1-engine/tools/altperl
	slony_show_configuration.pl
Message-ID: <20080104195520.E175729024C@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tools/altperl
In directory main.slony.info:/tmp/cvs-serv11447

Modified Files:
      Tag: REL_1_2_STABLE
	slony_show_configuration.pl 
Log Message:
Fix to bug #30 - altperl script references PGBINDIR rather than SLONBINDIR


Index: slony_show_configuration.pl
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tools/altperl/slony_show_configuration.pl,v
retrieving revision 1.1
retrieving revision 1.1.4.1
diff -C2 -d -r1.1 -r1.1.4.1
*** slony_show_configuration.pl	31 May 2005 16:11:05 -0000	1.1
--- slony_show_configuration.pl	4 Jan 2008 19:55:18 -0000	1.1.4.1
***************
*** 41,45 ****
  Slony-I Cluster: $CLUSTER_NAME
  Logs stored under $LOGDIR
! Slony Binaries in: @@PGBINDIR@@
  };
  if ($APACHE_ROTATOR) {
--- 41,45 ----
  Slony-I Cluster: $CLUSTER_NAME
  Logs stored under $LOGDIR
! Slony Binaries in: @@SLONBINDIR@@
  };
  if ($APACHE_ROTATOR) {

From cbbrowne at lists.slony.info  Fri Jan  4 11:56:21 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Jan  4 11:56:22 2008
Subject: [Slony1-commit] slony1-engine RELEASE
Message-ID: <20080104195621.C4B9A290C78@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine
In directory main.slony.info:/tmp/cvs-serv11477

Modified Files:
      Tag: REL_1_2_STABLE
	RELEASE 
Log Message:
Add to RELEASE notes fix for bug #30


Index: RELEASE
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/RELEASE,v
retrieving revision 1.1.2.23
retrieving revision 1.1.2.24
diff -C2 -d -r1.1.2.23 -r1.1.2.24
*** RELEASE	13 Dec 2007 17:22:48 -0000	1.1.2.23
--- RELEASE	4 Jan 2008 19:56:19 -0000	1.1.2.24
***************
*** 8,11 ****
--- 8,14 ----
    actually constrained by config parameter sync_group_maxsize.
  
+ - Fix to show_slony_configuration - point to proper directory where
+   slon/slonik are actually installed.
+ 
  RELEASE 1.2.12
  - Fixed problem with DDL SCRIPT parser where C-style comments were

From cbbrowne at lists.slony.info  Fri Jan  4 15:05:52 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Fri Jan  4 15:05:54 2008
Subject: [Slony1-commit] slony1-engine/doc/adminguide faq.sgml
Message-ID: <20080104230552.60674290C50@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv13699

Modified Files:
	faq.sgml 
Log Message:
Add notes to FAQ concerning findings for bug #27


Index: faq.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/faq.sgml,v
retrieving revision 1.72
retrieving revision 1.73
diff -C2 -d -r1.72 -r1.73
*** faq.sgml	11 Dec 2007 21:27:31 -0000	1.72
--- faq.sgml	4 Jan 2008 23:05:50 -0000	1.73
***************
*** 1374,1377 ****
--- 1374,1396 ----
  </answer> </qandaentry>
  
+ <qandaentry>
+ 
+ <question><para> I'm noticing in the logs that a &lslon; is frequently
+ switching in and out of <quote>polling</quote> mode as it is
+ frequently reporting <quote>LISTEN - switch from polling mode to use
+ LISTEN</quote> and <quote>UNLISTEN - switch into polling
+ mode</quote>. </para> </question>
+ 
+ <answer><para> The thresholds for switching between these modes are
+ controlled by the configuration parameters <xref
+ linkend="slon-config-sync-interval"> and <xref
+ linkend="slon-config-sync-interval-timeout">; if the timeout value
+ (which defaults to 10000, implying 10s) is kept low, that makes it
+ easy for the &lslon; to decide to return to <quote>listening</quote>
+ mode.  You may want to increase the value of the timeout
+ parameter. </para>
+ </answer>
+ </qandaentry>
+ 
  </qandadiv>
  <qandadiv id="faqbugs"> <title> &slony1; FAQ: &slony1; Bugs in Elder Versions </title>

From cbbrowne at lists.slony.info  Tue Jan  8 12:42:43 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Jan  8 12:42:45 2008
Subject: [Slony1-commit] slony1-engine/src/slonik Makefile slonik.c
Message-ID: <20080108204243.C92112900A7@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slonik
In directory main.slony.info:/tmp/cvs-serv25562

Modified Files:
      Tag: REL_1_2_STABLE
	Makefile slonik.c 
Log Message:
Change slonik build to query Postgres for the share directory at runtime

Per Dave Page


Index: slonik.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/slonik.c,v
retrieving revision 1.67.2.12
retrieving revision 1.67.2.13
diff -C2 -d -r1.67.2.12 -r1.67.2.13
*** slonik.c	20 Jul 2007 19:55:57 -0000	1.67.2.12
--- slonik.c	8 Jan 2008 20:42:41 -0000	1.67.2.13
***************
*** 43,49 ****
  int			current_try_level;
  
- #ifdef WIN32
  static char myfull_path[MAXPGPATH];
- #endif
  static char share_path[MAXPGPATH];
  
--- 43,47 ----
***************
*** 105,111 ****
  	if (parser_errors)
  		usage();
! #ifndef WIN32
!         strcpy(share_path, PGSHARE);
! #else
  	/*
  	 * We need to find a share directory like PostgreSQL. 
--- 103,107 ----
  	if (parser_errors)
  		usage();
! 
  	/*
  	 * We need to find a share directory like PostgreSQL. 
***************
*** 113,118 ****
  	if (find_my_exec(argv[0],myfull_path) < 0)
  	{
! 		printf("full path was unacquirable. '%s'\n", argv[0]);
! 		return -1;
  	}
  	else
--- 109,113 ----
  	if (find_my_exec(argv[0],myfull_path) < 0)
  	{
! 		strcpy(share_path, PGSHARE);
  	}
  	else
***************
*** 120,124 ****
  		get_share_path(myfull_path, share_path);
  	}
! #endif
  	if (optind < argc)
  	{
--- 115,119 ----
  		get_share_path(myfull_path, share_path);
  	}
! 
  	if (optind < argc)
  	{

Index: Makefile
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/Makefile,v
retrieving revision 1.24
retrieving revision 1.24.2.1
diff -C2 -d -r1.24 -r1.24.2.1
*** Makefile	2 Aug 2006 15:32:02 -0000	1.24
--- Makefile	8 Jan 2008 20:42:41 -0000	1.24.2.1
***************
*** 18,21 ****
--- 18,22 ----
  
  CFLAGS += -I$(slony_top_builddir) -DPGSHARE="\"$(pgsharedir)\"" 
+ LDFLAGS += -lpgport
  
  PROG		= slonik
***************
*** 23,32 ****
  ifeq ($(PORTNAME), win)
  PROG            = slonik.exe
- LDFLAG		= $(LDFLAG) -lpgport
  endif
  ifeq ($(PORTNAME), win32)
  PROG            = slonik.exe
- LDFLAG		= $(LDFLAG) -lpgport
  endif
  
  OBJS		= 			\
--- 24,32 ----
  ifeq ($(PORTNAME), win)
  PROG            = slonik.exe
  endif
  ifeq ($(PORTNAME), win32)
  PROG            = slonik.exe
  endif
+ LDFLAGS += $(filter -lcrypto -lz, $(LIBS))
  
  OBJS		= 			\

From cbbrowne at lists.slony.info  Tue Jan  8 12:43:26 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Jan  8 12:43:28 2008
Subject: [Slony1-commit] slony1-engine/src/slonik Makefile slonik.c
Message-ID: <20080108204326.486762903A1@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slonik
In directory main.slony.info:/tmp/cvs-serv25617

Modified Files:
	Makefile slonik.c 
Log Message:
Change slonik build to query Postgres for the share directory at runtime

Per Dave Page


Index: slonik.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/slonik.c,v
retrieving revision 1.83
retrieving revision 1.84
diff -C2 -d -r1.83 -r1.84
*** slonik.c	11 Dec 2007 19:30:30 -0000	1.83
--- slonik.c	8 Jan 2008 20:43:24 -0000	1.84
***************
*** 43,49 ****
  int			current_try_level;
  
- #ifdef WIN32
  static char myfull_path[MAXPGPATH];
- #endif
  static char share_path[MAXPGPATH];
  
--- 43,47 ----
***************
*** 105,111 ****
  	if (parser_errors)
  		usage();
! #ifndef WIN32
!         strcpy(share_path, PGSHARE);
! #else
  	/*
  	 * We need to find a share directory like PostgreSQL. 
--- 103,107 ----
  	if (parser_errors)
  		usage();
! 
  	/*
  	 * We need to find a share directory like PostgreSQL. 
***************
*** 113,118 ****
  	if (find_my_exec(argv[0],myfull_path) < 0)
  	{
! 		printf("full path was unacquirable. '%s'\n", argv[0]);
! 		return -1;
  	}
  	else
--- 109,113 ----
  	if (find_my_exec(argv[0],myfull_path) < 0)
  	{
! 		strcpy(share_path, PGSHARE);
  	}
  	else
***************
*** 120,124 ****
  		get_share_path(myfull_path, share_path);
  	}
! #endif
  	if (optind < argc)
  	{
--- 115,119 ----
  		get_share_path(myfull_path, share_path);
  	}
! 
  	if (optind < argc)
  	{

Index: Makefile
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/Makefile,v
retrieving revision 1.25
retrieving revision 1.26
diff -C2 -d -r1.25 -r1.26
*** Makefile	2 Jan 2007 23:34:35 -0000	1.25
--- Makefile	8 Jan 2008 20:43:24 -0000	1.26
***************
*** 18,21 ****
--- 18,22 ----
  
  CFLAGS += -I$(slony_top_builddir) -DPGSHARE="\"$(pgsharedir)\"" 
+ LDFLAGS += -lpgport
  
  PROG		= slonik
***************
*** 23,31 ****
  ifeq ($(PORTNAME), win)
  PROG            = slonik.exe
- LDFLAG		= $(LDFLAG) -lpgport
  endif
  ifeq ($(PORTNAME), win32)
  PROG            = slonik.exe
- LDFLAG		= $(LDFLAG) -lpgport
  endif
  
--- 24,30 ----

From cbbrowne at lists.slony.info  Tue Jan  8 12:46:32 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Jan  8 12:46:34 2008
Subject: [Slony1-commit] slony1-engine RELEASE
Message-ID: <20080108204632.5EAEB290263@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine
In directory main.slony.info:/tmp/cvs-serv25666

Modified Files:
      Tag: REL_1_2_STABLE
	RELEASE 
Log Message:
Release notes for Dave Page's patch


Index: RELEASE
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/RELEASE,v
retrieving revision 1.1.2.24
retrieving revision 1.1.2.25
diff -C2 -d -r1.1.2.24 -r1.1.2.25
*** RELEASE	4 Jan 2008 19:56:19 -0000	1.1.2.24
--- RELEASE	8 Jan 2008 20:46:30 -0000	1.1.2.25
***************
*** 11,14 ****
--- 11,17 ----
    slon/slonik are actually installed.
  
+ - Fix to slonik Makefile + slonik.c - Change slonik build to query
+   Postgres for the share directory at runtime - per Dave Page
+ 
  RELEASE 1.2.12
  - Fixed problem with DDL SCRIPT parser where C-style comments were

From cbbrowne at lists.slony.info  Tue Jan  8 12:48:31 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Jan  8 12:48:33 2008
Subject: [Slony1-commit] slony1-engine config.h.in
Message-ID: <20080108204831.74DC5290263@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine
In directory main.slony.info:/tmp/cvs-serv25713

Modified Files:
      Tag: REL_1_2_STABLE
	config.h.in 
Log Message:
Update version numbers to 1.2.13


Index: config.h.in
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/config.h.in,v
retrieving revision 1.17.2.10
retrieving revision 1.17.2.11
diff -C2 -d -r1.17.2.10 -r1.17.2.11
*** config.h.in	22 Nov 2007 22:51:04 -0000	1.17.2.10
--- config.h.in	8 Jan 2008 20:48:29 -0000	1.17.2.11
***************
*** 13,18 ****
  #define SLONY_I_CONFIG_H
  
! #define SLONY_I_VERSION_STRING	"1.2.12"
! #define SLONY_I_VERSION_STRING_DEC 1,2,12
  
  #ifndef PG_VERSION_MAJOR
--- 13,18 ----
  #define SLONY_I_CONFIG_H
  
! #define SLONY_I_VERSION_STRING	"1.2.13"
! #define SLONY_I_VERSION_STRING_DEC 1,2,13
  
  #ifndef PG_VERSION_MAJOR

From cbbrowne at lists.slony.info  Tue Jan  8 12:48:31 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Jan  8 12:48:33 2008
Subject: [Slony1-commit] slony1-engine/src/backend slony1_funcs.sql
Message-ID: <20080108204831.A1B8A2903A1@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/backend
In directory main.slony.info:/tmp/cvs-serv25713/src/backend

Modified Files:
      Tag: REL_1_2_STABLE
	slony1_funcs.sql 
Log Message:
Update version numbers to 1.2.13


Index: slony1_funcs.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.sql,v
retrieving revision 1.98.2.25
retrieving revision 1.98.2.26
diff -C2 -d -r1.98.2.25 -r1.98.2.26
*** slony1_funcs.sql	22 Oct 2007 15:19:50 -0000	1.98.2.25
--- slony1_funcs.sql	8 Jan 2008 20:48:29 -0000	1.98.2.26
***************
*** 431,435 ****
  as '
  begin
! 	return 12;
  end;
  ' language plpgsql;
--- 431,435 ----
  as '
  begin
! 	return 13;
  end;
  ' language plpgsql;

From cbbrowne at lists.slony.info  Wed Jan  9 12:47:50 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Jan  9 12:47:51 2008
Subject: [Slony1-commit] slony1-engine/tests run_test.sh
Message-ID: <20080109204750.3EDE1290CB0@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests
In directory main.slony.info:/tmp/cvs-serv17569

Modified Files:
	run_test.sh 
Log Message:
Minor spelling fix


Index: run_test.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/run_test.sh,v
retrieving revision 1.19
retrieving revision 1.20
diff -C2 -d -r1.19 -r1.20
*** run_test.sh	19 Oct 2007 15:21:46 -0000	1.19
--- run_test.sh	9 Jan 2008 20:47:48 -0000	1.20
***************
*** 652,656 ****
  {
    node=1
!   status "waiting for nodes to catchup"
  
    poll_cluster
--- 652,656 ----
  {
    node=1
!   status "waiting for nodes to catch up"
  
    poll_cluster
***************
*** 725,729 ****
      done
      if [ -z $tmpdir ]; then
!        err 3 "unable to create tmp dir"
         exit
      fi
--- 725,729 ----
      done
      if [ -z $tmpdir ]; then
!        err 3 "unable to create temp dir"
         exit
      fi

From cbbrowne at lists.slony.info  Wed Jan  9 12:49:35 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Jan  9 12:49:35 2008
Subject: [Slony1-commit] slony1-engine/tests support_funcs.sh
Message-ID: <20080109204935.34A21290C76@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/tests
In directory main.slony.info:/tmp/cvs-serv17611

Modified Files:
	support_funcs.sh 
Log Message:
Fix usability of C-based random data generators:

1.  Change existence test to use -x (is program executable) rather than -e
    (merely checking for existence)

2.  Typo in string handling was throwing away string values when we ran
    the C-based "random_string" program


Index: support_funcs.sh
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/tests/support_funcs.sh,v
retrieving revision 1.7
retrieving revision 1.8
diff -C2 -d -r1.7 -r1.8
*** support_funcs.sh	4 Oct 2007 15:32:31 -0000	1.7
--- support_funcs.sh	9 Jan 2008 20:49:33 -0000	1.8
***************
*** 90,94 ****
    _upperbound=$2
  
!   if [ -e ./random_number ] ; then
      rannum=`./random_number ${_lowerbound} ${_upperbound}`
    else
--- 90,94 ----
    _upperbound=$2
  
!   if [ -x ./random_number ] ; then
      rannum=`./random_number ${_lowerbound} ${_upperbound}`
    else
***************
*** 129,134 ****
  
    _length=$1
!   if [ -e ./random_string ] ; then
!     rannum=`./random_string ${_length}`
    else
    case `uname` in
--- 129,134 ----
  
    _length=$1
!   if [ -x ./random_string ] ; then
!     ranstring=`./random_string ${_length}`
    else
    case `uname` in

From cbbrowne at lists.slony.info  Wed Jan  9 12:55:49 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Wed Jan  9 12:55:50 2008
Subject: [Slony1-commit] slony1-engine/doc/adminguide testbed.sgml
Message-ID: <20080109205549.1DAFB290CBB@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv17724

Modified Files:
	testbed.sgml 
Log Message:
Add in notes about C programs random_number, random_string


Index: testbed.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/testbed.sgml,v
retrieving revision 1.14
retrieving revision 1.15
diff -C2 -d -r1.14 -r1.15
*** testbed.sgml	7 Sep 2007 19:51:28 -0000	1.14
--- testbed.sgml	9 Jan 2008 20:55:47 -0000	1.15
***************
*** 207,210 ****
--- 207,222 ----
  </glossentry>
  
+ 
+ <glossterm><envar><filename>random_number</filename> and <filename>random_string</filename> </envar></glossterm>
+ 
+ <glossdef><para> If you run <command>make</command> in the
+ <filename>test</filename> directory, C programs
+ <application>random_number</application> and
+ <application>random_string</application> will be built which will then
+ be used when generating random data in lieu of using shell/SQL
+ capabilities that are much slower than the C programs.  </para>
+ </glossdef>
+ </glossentry>
+ 
  </glosslist>
  

From devrim at lists.slony.info  Wed Jan 16 19:34:24 2008
From: devrim at lists.slony.info (Devrim GUNDUZ)
Date: Wed Jan 16 19:34:26 2008
Subject: [Slony1-commit] slony1-engine postgresql-slony1.spec.in
Message-ID: <20080117033424.A5EEF290C34@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine
In directory main.slony.info:/tmp/cvs-serv24671

Modified Files:
      Tag: REL_1_2_STABLE
	postgresql-slony1.spec.in 
Log Message:
Fix RPM docs installation and dependency issue.



Index: postgresql-slony1.spec.in
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/Attic/postgresql-slony1.spec.in,v
retrieving revision 1.1.2.2
retrieving revision 1.1.2.3
diff -C2 -d -r1.1.2.2 -r1.1.2.3
*** postgresql-slony1.spec.in	17 Dec 2007 19:42:23 -0000	1.1.2.2
--- postgresql-slony1.spec.in	17 Jan 2008 03:34:22 -0000	1.1.2.3
***************
*** 36,40 ****
  Summary:	Documentation for Slony-I
  Group:		Applications/Databases
! Requires:	@PACKAGE_NAME@-@PACKAGE_VERSION@-%{release}
  BuildRequires:	libjpeg, netpbm-progs, groff, docbook-style-dsssl, ghostscript
  
--- 36,40 ----
  Summary:	Documentation for Slony-I
  Group:		Applications/Databases
! Requires:	@PACKAGE_NAME@
  BuildRequires:	libjpeg, netpbm-progs, groff, docbook-style-dsssl, ghostscript
  
***************
*** 49,53 ****
  %build
  
! # Temporary measure for 1.2.10
  %if %docs
  find doc/ -type f -exec chmod 600 {} \;
--- 49,53 ----
  %build
  
! # Temporary measure for 1.2.X
  %if %docs
  find doc/ -type f -exec chmod 600 {} \;
***************
*** 87,91 ****
  install -m 755 redhat/postgresql-slony1.init %{buildroot}%{_initrddir}/postgresql-slony1
  
! # Temporary measure for 1.2.10
  %if %docs
  	rm -f doc/implementation/.cvsignore
--- 87,91 ----
  install -m 755 redhat/postgresql-slony1.init %{buildroot}%{_initrddir}/postgresql-slony1
  
! # Temporary measure for 1.2.X
  %if %docs
  	rm -f doc/implementation/.cvsignore

From wieck at lists.slony.info  Mon Jan 21 10:54:13 2008
From: wieck at lists.slony.info (Jan Wieck)
Date: Mon Jan 21 10:54:14 2008
Subject: [Slony1-commit] slony1-engine/src/backend slony1_funcs.sql
Message-ID: <20080121185413.39AAC29006B@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/backend
In directory main.slony.info:/tmp/cvs-serv28743/backend

Modified Files:
	slony1_funcs.sql 
Log Message:
New even type CLONE_NODE.

This will allow to duplicate an existing subscriber using pg_dump or
even filesystem level of copying the database cluster. See the script
src/ducttape/test_1_clonenode for an example of how this works.

Documentation still to be done.

Jan


Index: slony1_funcs.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.sql,v
retrieving revision 1.127
retrieving revision 1.128
diff -C2 -d -r1.127 -r1.128
*** slony1_funcs.sql	2 Jan 2008 19:00:27 -0000	1.127
--- slony1_funcs.sql	21 Jan 2008 18:54:11 -0000	1.128
***************
*** 1462,1465 ****
--- 1462,1588 ----
  
  -- ----------------------------------------------------------------------
+ -- FUNCTION cloneNodePrepare ()
+ --
+ --	Duplicate a nodes configuration under a different no_id in
+ --	preparation for the node to be copied with standard DB tools.
+ -- ----------------------------------------------------------------------
+ create or replace function @NAMESPACE@.cloneNodePrepare (int4, int4, text)
+ returns int4
+ as '
+ declare
+ 	p_no_id			alias for $1;
+ 	p_no_provider	alias for $2;
+ 	p_no_comment	alias for $3;
+ begin
+ 	-- ----
+ 	-- Grab the central configuration lock
+ 	-- ----
+ 	lock table @NAMESPACE@.sl_config_lock;
+ 
+ 	perform @NAMESPACE@.cloneNodePrepare_int (p_no_id, p_no_provider, p_no_comment);
+ 	return  @NAMESPACE@.createEvent(''_@CLUSTERNAME@'', ''CLONE_NODE'',
+ 									p_no_id::text, p_no_provider::text,
+ 									p_no_comment::text);
+ end;
+ ' language plpgsql;
+ 
+ comment on function @NAMESPACE@.cloneNodePrepare(int4, int4, text) is
+ 'Prepare for cloning a node.';
+ 
+ -- ----------------------------------------------------------------------
+ -- FUNCTION cloneNodePrepare_int ()
+ --
+ --	Internal part of cloneNodePrepare()
+ -- ----------------------------------------------------------------------
+ create or replace function @NAMESPACE@.cloneNodePrepare_int (int4, int4, text)
+ returns int4
+ as '
+ declare
+ 	p_no_id			alias for $1;
+ 	p_no_provider	alias for $2;
+ 	p_no_comment	alias for $3;
+ begin
+ 	insert into @NAMESPACE@.sl_node
+ 		(no_id, no_active, no_comment, no_spool)
+ 		select p_no_id, no_active, p_no_comment, no_spool
+ 		from @NAMESPACE@.sl_node
+ 		where no_id = p_no_provider;
+ 
+ 	insert into @NAMESPACE@.sl_path
+ 		(pa_server, pa_client, pa_conninfo, pa_connretry)
+ 		select pa_server, p_no_id, ''Event pending'', pa_connretry
+ 		from @NAMESPACE@.sl_path
+ 		where pa_client = p_no_provider;
+ 	insert into @NAMESPACE@.sl_path
+ 		(pa_server, pa_client, pa_conninfo, pa_connretry)
+ 		select p_no_id, pa_client, ''Event pending'', pa_connretry
+ 		from @NAMESPACE@.sl_path
+ 		where pa_server = p_no_provider;
+ 
+ 	insert into @NAMESPACE@.sl_subscribe
+ 		(sub_set, sub_provider, sub_receiver, sub_forward, sub_active)
+ 		select sub_set, sub_provider, p_no_id, sub_forward, sub_active
+ 		from @NAMESPACE@.sl_subscribe
+ 		where sub_receiver = p_no_provider;
+ 
+ 	insert into @NAMESPACE@.sl_confirm
+ 		(con_origin, con_received, con_seqno, con_timestamp)
+ 		select con_origin, p_no_id, con_seqno, con_timestamp
+ 		from @NAMESPACE@.sl_confirm
+ 		where con_received = p_no_provider;
+ 
+ 	perform @NAMESPACE@.RebuildListenEntries();
+ 
+ 	return 0;
+ end;
+ ' language plpgsql;
+ 
+ comment on function @NAMESPACE@.cloneNodePrepare_int(int4, int4, text) is
+ 'Internal part of cloneNodePrepare().';
+ 
+ -- ----------------------------------------------------------------------
+ -- FUNCTION cloneNodeFinish ()
+ --
+ --	Finish the cloning process on the new node.
+ -- ----------------------------------------------------------------------
+ create or replace function @NAMESPACE@.cloneNodeFinish (int4, int4)
+ returns int4
+ as '
+ declare
+ 	p_no_id			alias for $1;
+ 	p_no_provider	alias for $2;
+ 	v_row			record;
+ begin
+ 	perform "pg_catalog".setval(''@NAMESPACE@.sl_local_node_id'', p_no_id);
+ 
+ 	for v_row in select sub_set from @NAMESPACE@.sl_subscribe
+ 			where sub_receiver = p_no_id
+ 	loop
+ 		perform @NAMESPACE@.updateReloid(v_row.sub_set, p_no_id);
+ 	end loop;
+ 
+ 	perform @NAMESPACE@.RebuildListenEntries();
+ 
+ 	delete from @NAMESPACE@.sl_confirm
+ 		where con_received = p_no_id;
+ 	insert into @NAMESPACE@.sl_confirm
+ 		(con_origin, con_received, con_seqno, con_timestamp)
+ 		select con_origin, p_no_id, con_seqno, con_timestamp
+ 		from @NAMESPACE@.sl_confirm
+ 		where con_received = p_no_provider;
+ 	insert into @NAMESPACE@.sl_confirm
+ 		(con_origin, con_received, con_seqno, con_timestamp)
+ 		select p_no_provider, p_no_id, 
+ 				(select max(ev_seqno) from @NAMESPACE@.sl_event
+ 					where ev_origin = p_no_provider), current_timestamp;
+ 
+ 	return 0;
+ end;
+ ' language plpgsql;
+ 
+ comment on function @NAMESPACE@.cloneNodeFinish(int4, int4) is
+ 'Internal part of cloneNodePrepare().';
+ 
+ -- ----------------------------------------------------------------------
  -- FUNCTION storePath (pa_server, pa_client, pa_conninfo, pa_connretry)
  --

From wieck at lists.slony.info  Mon Jan 21 10:54:13 2008
From: wieck at lists.slony.info (Jan Wieck)
Date: Mon Jan 21 10:54:15 2008
Subject: [Slony1-commit] slony1-engine/src/ducttape test_1_clonenode
	test_1_pgbench.in
Message-ID: <20080121185413.44389290321@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/ducttape
In directory main.slony.info:/tmp/cvs-serv28743/ducttape

Modified Files:
	test_1_pgbench.in 
Added Files:
	test_1_clonenode 
Log Message:
New even type CLONE_NODE.

This will allow to duplicate an existing subscriber using pg_dump or
even filesystem level of copying the database cluster. See the script
src/ducttape/test_1_clonenode for an example of how this works.

Documentation still to be done.

Jan


--- NEW FILE: test_1_clonenode ---
#!/bin/sh

# **********
# test_1_clonenode
#
#	Script to clone node 2 into node 3
# **********

export PATH
TMPOUT=/tmp/output.$$
DB1=slony_test1
DB2=slony_test2
DB3=slony_test3

######################################################################
# Move set 1 to node 2
######################################################################

echo "**** Prepare to clone node 22"
slonik <<_EOF_
	cluster name = T1;
	node 11 admin conninfo = 'dbname=$DB1';
	node 22 admin conninfo = 'dbname=$DB2';
	node 33 admin conninfo = 'dbname=$DB3';

	clone prepare (id = 33, provider = 22, comment = 'Clone 33');
	wait for event (origin = 22, confirmed = all, wait on = 22);
	sync (id = 11);
	wait for event (origin = 11, confirmed = 22, wait on = 22);
_EOF_

echo "**** Create database $DB3"
createdb $DB3 || exit
echo "**** Copy $DB2 to $DB3"
pg_dump $DB2 | psql -q $DB3

echo "**** Finish clone 33"
slonik <<_EOF_
	cluster name = T1;
	node 11 admin conninfo = 'dbname=$DB1';
	node 22 admin conninfo = 'dbname=$DB2';
	node 33 admin conninfo = 'dbname=$DB3';

	clone finish (id = 33, provider = 22);
	store path (server = 11, client = 33, conninfo = 'dbname=$DB1');
	store path (server = 33, client = 11, conninfo = 'dbname=$DB3');
_EOF_


Index: test_1_pgbench.in
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/ducttape/test_1_pgbench.in,v
retrieving revision 1.5
retrieving revision 1.6
diff -C2 -d -r1.5 -r1.6
*** test_1_pgbench.in	5 Jul 2007 18:19:04 -0000	1.5
--- test_1_pgbench.in	21 Jan 2008 18:54:11 -0000	1.6
***************
*** 23,26 ****
--- 23,27 ----
  DB1=slony_test1
  DB2=slony_test2
+ DB3=slony_test3
  DEBUG_LEVEL=2
  
***************
*** 81,84 ****
--- 82,87 ----
  dropdb $DB2 || echo "**** ignored"
  sleep 1
+ dropdb $DB3 || echo "**** ignored"
+ sleep 1
  
  #####

From wieck at lists.slony.info  Mon Jan 21 10:54:13 2008
From: wieck at lists.slony.info (Jan Wieck)
Date: Mon Jan 21 10:54:15 2008
Subject: [Slony1-commit] slony1-engine/src/slon local_listen.c
	remote_worker.c
Message-ID: <20080121185413.597D6290468@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slon
In directory main.slony.info:/tmp/cvs-serv28743/slon

Modified Files:
	local_listen.c remote_worker.c 
Log Message:
New even type CLONE_NODE.

This will allow to duplicate an existing subscriber using pg_dump or
even filesystem level of copying the database cluster. See the script
src/ducttape/test_1_clonenode for an example of how this works.

Documentation still to be done.

Jan


Index: remote_worker.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/remote_worker.c,v
retrieving revision 1.162
retrieving revision 1.163
diff -C2 -d -r1.162 -r1.163
*** remote_worker.c	13 Dec 2007 17:19:28 -0000	1.162
--- remote_worker.c	21 Jan 2008 18:54:11 -0000	1.163
***************
*** 787,790 ****
--- 787,803 ----
  				need_reloadListen = true;
  			}
+ 			if (strcmp(event->ev_type, "CLONE_NODE") == 0)
+ 			{
+ 				int			no_id = (int)strtol(event->ev_data1, NULL, 10);
+ 				int			no_provider = (int)strtol(event->ev_data2, NULL, 10);
+ 				char	   *no_comment = event->ev_data3;
+ 
+ 				rtcfg_storeNode(no_id, no_comment);
+ 
+ 				slon_appendquery(&query1,
+ 								 "select %s.cloneNodePrepare_int(%d, %d, '%q'); ",
+ 								 rtcfg_namespace,
+ 								 no_id, no_provider, no_comment);
+ 			}
  			else if (strcmp(event->ev_type, "STORE_PATH") == 0)
  			{

Index: local_listen.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/local_listen.c,v
retrieving revision 1.44
retrieving revision 1.45
diff -C2 -d -r1.44 -r1.45
*** local_listen.c	19 Oct 2007 18:38:35 -0000	1.44
--- local_listen.c	21 Jan 2008 18:54:11 -0000	1.45
***************
*** 285,288 ****
--- 285,303 ----
  				rtcfg_reloadListen(dbconn);
  			}
+ 			else if (strcmp(ev_type, "CLONE_NODE") == 0)
+ 			{
+ 				/*
+ 				 * CLONE_NODE
+ 				 */
+ 				int			no_id;
+ 				int			no_provider;
+ 				char	   *no_comment;
+ 
+ 				no_id = (int)strtol(PQgetvalue(res, tupno, 6), NULL, 10);
+ 				no_provider = (int)strtol(PQgetvalue(res, tupno, 7), NULL, 10);
+ 				no_comment = PQgetvalue(res, tupno, 8);
+ 
+ 				rtcfg_storeNode(no_id, no_comment);
+ 			}
  			else if (strcmp(ev_type, "STORE_PATH") == 0)
  			{

From wieck at lists.slony.info  Mon Jan 21 10:54:13 2008
From: wieck at lists.slony.info (Jan Wieck)
Date: Mon Jan 21 10:54:15 2008
Subject: [Slony1-commit] slony1-engine/src/slonik parser.y scan.l slonik.c
	slonik.h
Message-ID: <20080121185413.BD706290CBB@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slonik
In directory main.slony.info:/tmp/cvs-serv28743/slonik

Modified Files:
	parser.y scan.l slonik.c slonik.h 
Log Message:
New even type CLONE_NODE.

This will allow to duplicate an existing subscriber using pg_dump or
even filesystem level of copying the database cluster. See the script
src/ducttape/test_1_clonenode for an example of how this works.

Documentation still to be done.

Jan


Index: slonik.h
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/slonik.h,v
retrieving revision 1.32
retrieving revision 1.33
diff -C2 -d -r1.32 -r1.33
*** slonik.h	5 Jul 2007 18:19:04 -0000	1.32
--- slonik.h	21 Jan 2008 18:54:11 -0000	1.33
***************
*** 25,28 ****
--- 25,30 ----
  typedef struct SlonikStmt_failed_node_s SlonikStmt_failed_node;
  typedef struct SlonikStmt_uninstall_node_s SlonikStmt_uninstall_node;
+ typedef struct SlonikStmt_clone_prepare_s SlonikStmt_clone_prepare;
+ typedef struct SlonikStmt_clone_finish_s SlonikStmt_clone_finish;
  typedef struct SlonikStmt_store_path_s SlonikStmt_store_path;
  typedef struct SlonikStmt_drop_path_s SlonikStmt_drop_path;
***************
*** 53,56 ****
--- 55,60 ----
  {
  	STMT_TRY = 1,
+ 	STMT_CLONE_FINISH,
+ 	STMT_CLONE_PREPARE,
  	STMT_CREATE_SET,
  	STMT_DDL_SCRIPT,
***************
*** 60,63 ****
--- 64,68 ----
  	STMT_DROP_SET,
  	STMT_ECHO,
+ 	STMT_ERROR,
  	STMT_EXIT,
  	STMT_FAILED_NODE,
***************
*** 66,71 ****
  	STMT_MERGE_SET,
  	STMT_MOVE_SET,
- 	STMT_RESTART_NODE,
  	STMT_REPAIR_CONFIG,
  	STMT_SET_ADD_SEQUENCE,
  	STMT_SET_ADD_TABLE,
--- 71,76 ----
  	STMT_MERGE_SET,
  	STMT_MOVE_SET,
  	STMT_REPAIR_CONFIG,
+ 	STMT_RESTART_NODE,
  	STMT_SET_ADD_SEQUENCE,
  	STMT_SET_ADD_TABLE,
***************
*** 74,90 ****
  	STMT_SET_MOVE_SEQUENCE,
  	STMT_SET_MOVE_TABLE,
  	STMT_STORE_LISTEN,
  	STMT_STORE_NODE,
  	STMT_STORE_PATH,
  	STMT_SUBSCRIBE_SET,
  	STMT_UNINSTALL_NODE,
  	STMT_UNLOCK_SET,
  	STMT_UNSUBSCRIBE_SET,
  	STMT_UPDATE_FUNCTIONS,
! 	STMT_WAIT_EVENT,
! 	STMT_SWITCH_LOG,
! 	STMT_ERROR,
! 	STMT_SYNC,
! 	STMT_SLEEP
  }	Slonik_stmttype;
  
--- 79,94 ----
  	STMT_SET_MOVE_SEQUENCE,
  	STMT_SET_MOVE_TABLE,
+ 	STMT_SLEEP,
  	STMT_STORE_LISTEN,
  	STMT_STORE_NODE,
  	STMT_STORE_PATH,
  	STMT_SUBSCRIBE_SET,
+ 	STMT_SWITCH_LOG,
+ 	STMT_SYNC,
  	STMT_UNINSTALL_NODE,
  	STMT_UNLOCK_SET,
  	STMT_UNSUBSCRIBE_SET,
  	STMT_UPDATE_FUNCTIONS,
! 	STMT_WAIT_EVENT
  }	Slonik_stmttype;
  
***************
*** 205,208 ****
--- 209,229 ----
  
  
+ struct SlonikStmt_clone_prepare_s
+ {
+ 	SlonikStmt	hdr;
+ 	int			no_id;
+ 	int			no_provider;
+ 	char	   *no_comment;
+ };
+ 
+ 
+ struct SlonikStmt_clone_finish_s
+ {
+ 	SlonikStmt	hdr;
+ 	int			no_id;
+ 	int			no_provider;
+ };
+ 
+ 
  struct SlonikStmt_store_path_s
  {
***************
*** 513,516 ****
--- 534,539 ----
  extern int	slonik_failed_node(SlonikStmt_failed_node * stmt);
  extern int	slonik_uninstall_node(SlonikStmt_uninstall_node * stmt);
+ extern int	slonik_clone_prepare(SlonikStmt_clone_prepare * stmt);
+ extern int	slonik_clone_finish(SlonikStmt_clone_finish * stmt);
  extern int	slonik_store_path(SlonikStmt_store_path * stmt);
  extern int	slonik_drop_path(SlonikStmt_drop_path * stmt);

Index: parser.y
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/parser.y,v
retrieving revision 1.29
retrieving revision 1.30
diff -C2 -d -r1.29 -r1.30
*** parser.y	5 Jul 2007 18:19:04 -0000	1.29
--- parser.y	21 Jan 2008 18:54:11 -0000	1.30
***************
*** 46,50 ****
  	O_TAB_ID,
  	O_TIMEOUT,
- 	O_TRIG_NAME,
  	O_USE_KEY,
  	O_WAIT_CONFIRMED,
--- 46,49 ----
***************
*** 135,138 ****
--- 134,139 ----
  %type <statement>	stmt_failed_node
  %type <statement>	stmt_uninstall_node
+ %type <statement>	stmt_clone_prepare
+ %type <statement>	stmt_clone_finish
  %type <statement>	stmt_store_path
  %type <statement>	stmt_drop_path
***************
*** 176,179 ****
--- 177,181 ----
  %token	K_BACKUP
  %token	K_CLIENT
+ %token	K_CLONE
  %token	K_CLUSTER
  %token	K_CLUSTERNAME
***************
*** 193,196 ****
--- 195,199 ----
  %token	K_FALSE
  %token	K_FILENAME
+ %token	K_FINISH
  %token	K_FOR
  %token	K_FORWARD
***************
*** 215,218 ****
--- 218,222 ----
  %token	K_ORIGIN
  %token	K_PATH
+ %token	K_PREPARE
  %token	K_PROVIDER
  %token	K_QUALIFIED
***************
*** 429,432 ****
--- 433,440 ----
  					| stmt_uninstall_node
  						{ $$ = $1; }
+ 					| stmt_clone_prepare
+ 						{ $$ = $1; }
+ 					| stmt_clone_finish
+ 						{ $$ = $1; }
  					| stmt_store_path
  						{ $$ = $1; }
***************
*** 698,701 ****
--- 706,767 ----
  					;
  
+ stmt_clone_prepare	: lno K_CLONE K_PREPARE option_list
+ 					{
+ 						SlonikStmt_clone_prepare *new;
+ 						statement_option opt[] = {
+ 							STMT_OPTION_INT( O_ID, -1 ),
+ 							STMT_OPTION_INT( O_PROVIDER, -1 ),
+ 							STMT_OPTION_STR( O_COMMENT, NULL ),
+ 							STMT_OPTION_END
+ 						};
+ 
+ 						new = (SlonikStmt_clone_prepare *)
+ 								malloc(sizeof(SlonikStmt_clone_prepare));
+ 						memset(new, 0, sizeof(SlonikStmt_clone_prepare));
+ 						new->hdr.stmt_type		= STMT_CLONE_PREPARE;
+ 						new->hdr.stmt_filename	= current_file;
+ 						new->hdr.stmt_lno		= $1;
+ 
+ 						if (assign_options(opt, $4) == 0)
+ 						{
+ 							new->no_id			= opt[0].ival;
+ 							new->no_provider	= opt[1].ival;
+ 							new->no_comment		= opt[2].str;
+ 						}
+ 						else
+ 							parser_errors++;
+ 
+ 						$$ = (SlonikStmt *)new;
+ 					}
+ 					;
+ 
+ stmt_clone_finish	: lno K_CLONE K_FINISH option_list
+ 					{
+ 						SlonikStmt_clone_finish *new;
+ 						statement_option opt[] = {
+ 							STMT_OPTION_INT( O_ID, -1 ),
+ 							STMT_OPTION_INT( O_PROVIDER, -1 ),
+ 							STMT_OPTION_END
+ 						};
+ 
+ 						new = (SlonikStmt_clone_finish *)
+ 								malloc(sizeof(SlonikStmt_clone_finish));
+ 						memset(new, 0, sizeof(SlonikStmt_clone_finish));
+ 						new->hdr.stmt_type		= STMT_CLONE_FINISH;
+ 						new->hdr.stmt_filename	= current_file;
+ 						new->hdr.stmt_lno		= $1;
+ 
+ 						if (assign_options(opt, $4) == 0)
+ 						{
+ 							new->no_id			= opt[0].ival;
+ 							new->no_provider	= opt[1].ival;
+ 						}
+ 						else
+ 							parser_errors++;
+ 
+ 						$$ = (SlonikStmt *)new;
+ 					}
+ 					;
+ 
  stmt_store_path		: lno K_STORE K_PATH option_list
  					{

Index: slonik.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/slonik.c,v
retrieving revision 1.84
retrieving revision 1.85
diff -C2 -d -r1.84 -r1.85
*** slonik.c	8 Jan 2008 20:43:24 -0000	1.84
--- slonik.c	21 Jan 2008 18:54:11 -0000	1.85
***************
*** 380,383 ****
--- 380,411 ----
  				break;
  
+ 			case STMT_CLONE_PREPARE:
+ 				{
+ 					SlonikStmt_clone_prepare *stmt =
+ 					(SlonikStmt_clone_prepare *) hdr;
+ 
+ 					if (stmt->no_id < 0)
+ 					{
+ 						printf("%s:%d: Error: "
+ 							   "new node ID must be specified\n",
+ 							   hdr->stmt_filename, hdr->stmt_lno);
+ 						errors++;
+ 					}
+ 
+ 					if (script_check_adminfo(hdr, stmt->no_provider) < 0)
+ 						errors++;
+ 				}
+ 				break;
+ 
+ 			case STMT_CLONE_FINISH:
+ 				{
+ 					SlonikStmt_clone_finish *stmt =
+ 					(SlonikStmt_clone_finish *) hdr;
+ 
+ 					if (script_check_adminfo(hdr, stmt->no_id) < 0)
+ 						errors++;
+ 				}
+ 				break;
+ 
  			case STMT_STORE_PATH:
  				{
***************
*** 1212,1215 ****
--- 1240,1263 ----
  				break;
  
+ 			case STMT_CLONE_PREPARE:
+ 				{
+ 					SlonikStmt_clone_prepare *stmt =
+ 					(SlonikStmt_clone_prepare *) hdr;
+ 
+ 					if (slonik_clone_prepare(stmt) < 0)
+ 						errors++;
+ 				}
+ 				break;
+ 
+ 			case STMT_CLONE_FINISH:
+ 				{
+ 					SlonikStmt_clone_finish *stmt =
+ 					(SlonikStmt_clone_finish *) hdr;
+ 
+ 					if (slonik_clone_finish(stmt) < 0)
+ 						errors++;
+ 				}
+ 				break;
+ 
  			case STMT_STORE_PATH:
  				{
***************
*** 1871,1874 ****
--- 1919,1923 ----
  }
  
+ 
  static int
  slonik_repair_config(SlonikStmt_repair_config * stmt)
***************
*** 2873,2876 ****
--- 2922,2987 ----
  
  int
+ slonik_clone_prepare(SlonikStmt_clone_prepare * stmt)
+ {
+ 	SlonikAdmInfo *adminfo1;
+ 	SlonDString query;
+ 
+ 	adminfo1 = get_active_adminfo((SlonikStmt *) stmt, stmt->no_provider);
+ 	if (adminfo1 == NULL)
+ 		return -1;
+ 
+ 	dstring_init(&query);
+ 
+ 	if (stmt->no_comment == NULL)
+ 		slon_mkquery(&query,
+ 				 "select \"_%s\".cloneNodePrepare(%d, %d, 'Node %d'); ",
+ 				 stmt->hdr.script->clustername,
+ 				 stmt->no_id, stmt->no_provider,
+ 				 stmt->no_id);
+ 	else
+ 		slon_mkquery(&query,
+ 				 "select \"_%s\".cloneNodePrepare(%d, %d, '%q'); ",
+ 				 stmt->hdr.script->clustername,
+ 				 stmt->no_id, stmt->no_provider,
+ 				 stmt->no_comment);
+ 	if (db_exec_evcommand((SlonikStmt *) stmt, adminfo1, &query) < 0)
+ 	{
+ 		dstring_free(&query);
+ 		return -1;
+ 	}
+ 
+ 	dstring_free(&query);
+ 	return 0;
+ }
+ 
+ 
+ int
+ slonik_clone_finish(SlonikStmt_clone_finish * stmt)
+ {
+ 	SlonikAdmInfo *adminfo1;
+ 	SlonDString query;
+ 
+ 	adminfo1 = get_active_adminfo((SlonikStmt *) stmt, stmt->no_id);
+ 	if (adminfo1 == NULL)
+ 		return -1;
+ 
+ 	dstring_init(&query);
+ 
+ 	slon_mkquery(&query,
+ 				 "select \"_%s\".cloneNodeFinish(%d, %d); ",
+ 				 stmt->hdr.script->clustername,
+ 				 stmt->no_id, stmt->no_provider);
+ 	if (db_exec_command((SlonikStmt *) stmt, adminfo1, &query) < 0)
+ 	{
+ 		dstring_free(&query);
+ 		return -1;
+ 	}
+ 
+ 	dstring_free(&query);
+ 	return 0;
+ }
+ 
+ 
+ int
  slonik_store_path(SlonikStmt_store_path * stmt)
  {

Index: scan.l
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slonik/scan.l,v
retrieving revision 1.28
retrieving revision 1.29
diff -C2 -d -r1.28 -r1.29
*** scan.l	5 Jul 2007 18:19:04 -0000	1.28
--- scan.l	21 Jan 2008 18:54:11 -0000	1.29
***************
*** 72,75 ****
--- 72,76 ----
  backup			{ return K_BACKUP;			}
  client			{ return K_CLIENT;			}
+ clone			{ return K_CLONE;			}
  cluster			{ return K_CLUSTER;			}
  comment			{ return K_COMMENT;			}
***************
*** 88,91 ****
--- 89,93 ----
  false			{ return K_FALSE;			}
  filename		{ return K_FILENAME;		}
+ finish			{ return K_FINISH;			}
  for				{ return K_FOR;				}
  forward			{ return K_FORWARD;			}
***************
*** 111,114 ****
--- 113,117 ----
  origin			{ return K_ORIGIN;			}
  path			{ return K_PATH;			}
+ prepare			{ return K_PREPARE;			}
  provider		{ return K_PROVIDER;		}
  qualified		{ return K_QUALIFIED;		}

From cbbrowne at lists.slony.info  Mon Jan 21 11:27:39 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Jan 21 11:27:42 2008
Subject: [Slony1-commit] slony1-engine RELEASE-2.0
Message-ID: <20080121192739.5980F290055@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine
In directory main.slony.info:/tmp/cvs-serv29342

Modified Files:
	RELEASE-2.0 
Log Message:
Add basic documentation, release notes on CLONE PREPARE/CLONE FINISH


Index: RELEASE-2.0
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/RELEASE-2.0,v
retrieving revision 1.12
retrieving revision 1.13
diff -C2 -d -r1.12 -r1.13
*** RELEASE-2.0	3 Jan 2008 15:48:49 -0000	1.12
--- RELEASE-2.0	21 Jan 2008 19:27:37 -0000	1.13
***************
*** 163,164 ****
--- 163,168 ----
  
    http://www.slony.info/bugzilla/show_bug.cgi?id=21
+ 
+ - New slonik "CLONE PREPARE" and "CLONE FINISH" command to assist in
+   creating duplicate nodes based on taking a copy of some existing
+   subscriber node.
\ No newline at end of file

From cbbrowne at lists.slony.info  Mon Jan 21 11:27:39 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Jan 21 11:27:42 2008
Subject: [Slony1-commit] slony1-engine/doc/adminguide slonik_ref.sgml
Message-ID: <20080121192739.6AE0A29006B@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv29342/doc/adminguide

Modified Files:
	slonik_ref.sgml 
Log Message:
Add basic documentation, release notes on CLONE PREPARE/CLONE FINISH


Index: slonik_ref.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/slonik_ref.sgml,v
retrieving revision 1.76
retrieving revision 1.77
diff -C2 -d -r1.76 -r1.77
*** slonik_ref.sgml	20 Dec 2007 00:56:46 -0000	1.76
--- slonik_ref.sgml	21 Jan 2008 19:27:37 -0000	1.77
***************
*** 2649,2655 ****
     <refsect1> <title> Locking Behaviour </title>
  
!     <para> Each replicated table receives an exclusive lock, on the
!     origin node, in order to remove the replication triggers; after
!     the DDL script completes, those locks will be cleared. </para>
  
      <para> After the DDL script has run on the origin node, it will
--- 2649,2656 ----
     <refsect1> <title> Locking Behaviour </title>
  
!     <para> Up until the 2.0 branch, each replicated table received an
!     exclusive lock, on the origin node, in order to remove the
!     replication triggers; after the DDL script completes, those locks
!     will be cleared. </para>
  
      <para> After the DDL script has run on the origin node, it will
***************
*** 2659,2662 ****
--- 2660,2670 ----
      turn. </para>
  
+     <para> As of the 2.0 branch, &slony1; uses a GUC that controls
+     trigger behaviour, which allows deactivating the &slony1;-created
+     triggers during this operation <emphasis>without</emphasis> the
+     need to take out exclusive locks on all tables.  Now, the only
+     tables requiring exclusive locks are those tables that are
+     actually altered as a part of the DDL script. </para>
+ 
     </refsect1>
     <refsect1> <title> Version Information </title>
***************
*** 2994,2997 ****
--- 3002,3082 ----
    </Refentry>
  
+ 
+   <refentry id ="stmtcloneprepare"><refmeta><refentrytitle>CLONE PREPARE</refentrytitle>
+    <manvolnum>7</manvolnum></refmeta>
+    
+    <refnamediv><refname>CLONE PREPARE</refname>
+     
+     <refpurpose> Prepare for cloning a node. </refpurpose>
+    </refnamediv>
+    <refsynopsisdiv>
+     <cmdsynopsis>
+      <command>clone prepare </command>
+      <arg><replaceable class="parameter"> id</replaceable></arg>
+      <arg><replaceable class="parameter"> provider</replaceable></arg>
+      <arg><replaceable class="parameter"> comment</replaceable></arg>
+     </cmdsynopsis>
+    </refsynopsisdiv>
+    <refsect1>
+     <title>Description</title>
+     <para>
+      Prepares for cloning a specified node.
+     </para>
+ 
+     <para>
+      This duplicates the <quote>provider</quote> node's configuration
+      under a new node ID in preparation for the node to be copied via
+      standard database tools.
+     </para>
+    </Refsect1>
+    <Refsect1><Title>Example</Title>
+     <Programlisting>
+      clone prepare (id = 33, provider = 22, comment='Clone 33');
+     </Programlisting>
+    </Refsect1>
+    <refsect1> <title> Version Information </title>
+     <para> This command was introduced in &slony1; 1.2.0. </para>
+    </refsect1>
+   </Refentry>
+ 
+ 
+   <refentry id ="stmtclonefinish"><refmeta><refentrytitle>CLONE FINISH</refentrytitle>
+    <manvolnum>7</manvolnum></refmeta>
+    
+    <refnamediv><refname>CLONE FINISH</refname>
+     
+     <refpurpose> Complete cloning a node. </refpurpose>
+    </refnamediv>
+    <refsynopsisdiv>
+     <cmdsynopsis>
+      <command>clone prepare </command>
+      <arg><replaceable class="parameter"> id</replaceable></arg>
+      <arg><replaceable class="parameter"> provider</replaceable></arg>
+     </cmdsynopsis>
+    </refsynopsisdiv>
+    <refsect1>
+     <title>Description</title>
+     <para>
+      Finishes cloning a specified node.
+     </para>
+ 
+     <para>
+      This completes the work done by <xref
+      linkend="stmtcloneprepare">, establishing confirmation data for
+      the new <quote>clone</quote> based on the status found for the
+      <quote>provider</quote> node.
+     </para>
+    </Refsect1>
+    <Refsect1><Title>Example</Title>
+     <Programlisting>
+      clone finish (id = 33, provider = 22);
+     </Programlisting>
+    </Refsect1>
+    <refsect1> <title> Version Information </title>
+     <para> This command was introduced in &slony1; 1.2.0. </para>
+    </refsect1>
+   </Refentry>
+ 
+ 
   </reference>
  <!-- Keep this comment at the end of the file

From JanWieck at Yahoo.com  Mon Jan 21 12:28:25 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon Jan 21 12:30:56 2008
Subject: [Slony1-commit] slony1-engine/doc/adminguide slonik_ref.sgml
In-Reply-To: <20080121192739.6AE0A29006B@main.slony.info>
References: <20080121192739.6AE0A29006B@main.slony.info>
Message-ID: <47950069.803@Yahoo.com>

On 1/21/2008 2:27 PM, Chris Browne wrote:
> Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
> In directory main.slony.info:/tmp/cvs-serv29342/doc/adminguide

You probably want to add some very important detail information to that.

It is very essential that the slonik script not only does a "wait for 
event" for the original "clone prepare". But it also must issue a "sync" 
on every existing node other than the provider and wait at least until 
all those sync events have been confirmed by the provider. Otherwise it 
is possible for the clone to miss some events.


Jan


> 
> Modified Files:
> 	slonik_ref.sgml 
> Log Message:
> Add basic documentation, release notes on CLONE PREPARE/CLONE FINISH
> 
> 
> Index: slonik_ref.sgml
> ===================================================================
> RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/slonik_ref.sgml,v
> retrieving revision 1.76
> retrieving revision 1.77
> diff -C2 -d -r1.76 -r1.77
> *** slonik_ref.sgml	20 Dec 2007 00:56:46 -0000	1.76
> --- slonik_ref.sgml	21 Jan 2008 19:27:37 -0000	1.77
> ***************
> *** 2649,2655 ****
>      <refsect1> <title> Locking Behaviour </title>
>   
> !     <para> Each replicated table receives an exclusive lock, on the
> !     origin node, in order to remove the replication triggers; after
> !     the DDL script completes, those locks will be cleared. </para>
>   
>       <para> After the DDL script has run on the origin node, it will
> --- 2649,2656 ----
>      <refsect1> <title> Locking Behaviour </title>
>   
> !     <para> Up until the 2.0 branch, each replicated table received an
> !     exclusive lock, on the origin node, in order to remove the
> !     replication triggers; after the DDL script completes, those locks
> !     will be cleared. </para>
>   
>       <para> After the DDL script has run on the origin node, it will
> ***************
> *** 2659,2662 ****
> --- 2660,2670 ----
>       turn. </para>
>   
> +     <para> As of the 2.0 branch, &slony1; uses a GUC that controls
> +     trigger behaviour, which allows deactivating the &slony1;-created
> +     triggers during this operation <emphasis>without</emphasis> the
> +     need to take out exclusive locks on all tables.  Now, the only
> +     tables requiring exclusive locks are those tables that are
> +     actually altered as a part of the DDL script. </para>
> + 
>      </refsect1>
>      <refsect1> <title> Version Information </title>
> ***************
> *** 2994,2997 ****
> --- 3002,3082 ----
>     </Refentry>
>   
> + 
> +   <refentry id ="stmtcloneprepare"><refmeta><refentrytitle>CLONE PREPARE</refentrytitle>
> +    <manvolnum>7</manvolnum></refmeta>
> +    
> +    <refnamediv><refname>CLONE PREPARE</refname>
> +     
> +     <refpurpose> Prepare for cloning a node. </refpurpose>
> +    </refnamediv>
> +    <refsynopsisdiv>
> +     <cmdsynopsis>
> +      <command>clone prepare </command>
> +      <arg><replaceable class="parameter"> id</replaceable></arg>
> +      <arg><replaceable class="parameter"> provider</replaceable></arg>
> +      <arg><replaceable class="parameter"> comment</replaceable></arg>
> +     </cmdsynopsis>
> +    </refsynopsisdiv>
> +    <refsect1>
> +     <title>Description</title>
> +     <para>
> +      Prepares for cloning a specified node.
> +     </para>
> + 
> +     <para>
> +      This duplicates the <quote>provider</quote> node's configuration
> +      under a new node ID in preparation for the node to be copied via
> +      standard database tools.
> +     </para>
> +    </Refsect1>
> +    <Refsect1><Title>Example</Title>
> +     <Programlisting>
> +      clone prepare (id = 33, provider = 22, comment='Clone 33');
> +     </Programlisting>
> +    </Refsect1>
> +    <refsect1> <title> Version Information </title>
> +     <para> This command was introduced in &slony1; 1.2.0. </para>
> +    </refsect1>
> +   </Refentry>
> + 
> + 
> +   <refentry id ="stmtclonefinish"><refmeta><refentrytitle>CLONE FINISH</refentrytitle>
> +    <manvolnum>7</manvolnum></refmeta>
> +    
> +    <refnamediv><refname>CLONE FINISH</refname>
> +     
> +     <refpurpose> Complete cloning a node. </refpurpose>
> +    </refnamediv>
> +    <refsynopsisdiv>
> +     <cmdsynopsis>
> +      <command>clone prepare </command>
> +      <arg><replaceable class="parameter"> id</replaceable></arg>
> +      <arg><replaceable class="parameter"> provider</replaceable></arg>
> +     </cmdsynopsis>
> +    </refsynopsisdiv>
> +    <refsect1>
> +     <title>Description</title>
> +     <para>
> +      Finishes cloning a specified node.
> +     </para>
> + 
> +     <para>
> +      This completes the work done by <xref
> +      linkend="stmtcloneprepare">, establishing confirmation data for
> +      the new <quote>clone</quote> based on the status found for the
> +      <quote>provider</quote> node.
> +     </para>
> +    </Refsect1>
> +    <Refsect1><Title>Example</Title>
> +     <Programlisting>
> +      clone finish (id = 33, provider = 22);
> +     </Programlisting>
> +    </Refsect1>
> +    <refsect1> <title> Version Information </title>
> +     <para> This command was introduced in &slony1; 1.2.0. </para>
> +    </refsect1>
> +   </Refentry>
> + 
> + 
>    </reference>
>   <!-- Keep this comment at the end of the file
> 
> _______________________________________________
> Slony1-commit mailing list
> Slony1-commit@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-commit


-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From wieck at lists.slony.info  Wed Jan 23 09:12:36 2008
From: wieck at lists.slony.info (Jan Wieck)
Date: Wed Jan 23 09:12:38 2008
Subject: [Slony1-commit] slony1-engine/src/ducttape test_2_clonenode
	test_2_pgbench.in
Message-ID: <20080123171236.C8BAC290321@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/ducttape
In directory main.slony.info:/tmp/cvs-serv18613

Modified Files:
	test_2_pgbench.in 
Added Files:
	test_2_clonenode 
Log Message:


Index: test_2_pgbench.in
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/ducttape/test_2_pgbench.in,v
retrieving revision 1.4
retrieving revision 1.5
diff -C2 -d -r1.4 -r1.5
*** test_2_pgbench.in	29 Jul 2007 20:15:41 -0000	1.4
--- test_2_pgbench.in	23 Jan 2008 17:12:34 -0000	1.5
***************
*** 30,33 ****
--- 30,34 ----
  DB2=slony_test2
  DB3=slony_test3
+ DB4=slony_test4
  DEBUG_LEVEL=2
  
***************
*** 94,97 ****
--- 95,100 ----
  dropdb $DB3 || echo "**** ignored"
  sleep 1
+ dropdb $DB4 || echo "**** ignored"
+ sleep 1
  
  #####

--- NEW FILE: test_2_clonenode ---
#!/bin/sh

# **********
# test_2_clonenode
#
#	Script to clone node 2 into node 4
# **********

export PATH
TMPOUT=/tmp/output.$$
DB1=slony_test1
DB2=slony_test2
DB3=slony_test3
DB4=slony_test4

######################################################################
# Move set 1 to node 2
######################################################################

echo "**** Prepare to clone node 2"
slonik <<_EOF_
	cluster name = T1;
	node 1 admin conninfo = 'dbname=$DB1';
	node 2 admin conninfo = 'dbname=$DB2';
	node 3 admin conninfo = 'dbname=$DB3';
	node 4 admin conninfo = 'dbname=$DB4';

	clone prepare (id = 4, provider = 2, comment = 'Clone 4');
	wait for event (origin = 2, confirmed = all, wait on = 2);
	sync (id = 1);
	wait for event (origin = 1, confirmed = 2, wait on = 2);
	sync (id = 3);
	wait for event (origin = 3, confirmed = 2, wait on = 2);
_EOF_

echo "**** Create database $DB4"
createdb $DB4 || exit
echo "**** Copy $DB2 to $DB4"
pg_dump $DB2 | psql -q $DB4

echo "**** Finish clone 4"
slonik <<_EOF_
	cluster name = T1;
	node 1 admin conninfo = 'dbname=$DB1';
	node 2 admin conninfo = 'dbname=$DB2';
	node 3 admin conninfo = 'dbname=$DB3';
	node 4 admin conninfo = 'dbname=$DB4';

	clone finish (id = 4, provider = 2);
	store path (server = 1, client = 4, conninfo = 'dbname=$DB1');
	store path (server = 4, client = 1, conninfo = 'dbname=$DB4');
	store path (server = 2, client = 4, conninfo = 'dbname=$DB2');
	store path (server = 4, client = 2, conninfo = 'dbname=$DB4');
	store path (server = 3, client = 4, conninfo = 'dbname=$DB3');
	store path (server = 4, client = 3, conninfo = 'dbname=$DB4');
_EOF_


From cbbrowne at lists.slony.info  Mon Jan 28 11:33:12 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Jan 28 11:33:14 2008
Subject: [Slony1-commit] slony1-engine/doc/adminguide testbed.sgml
Message-ID: <20080128193312.C77A5290456@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv5859

Modified Files:
	testbed.sgml 
Log Message:
SGML cleanup


Index: testbed.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/testbed.sgml,v
retrieving revision 1.15
retrieving revision 1.16
diff -C2 -d -r1.15 -r1.16
*** testbed.sgml	9 Jan 2008 20:55:47 -0000	1.15
--- testbed.sgml	28 Jan 2008 19:33:10 -0000	1.16
***************
*** 207,212 ****
  </glossentry>
  
! 
! <glossterm><envar><filename>random_number</filename> and <filename>random_string</filename> </envar></glossterm>
  
  <glossdef><para> If you run <command>make</command> in the
--- 207,212 ----
  </glossentry>
  
! <glossentry>
! <glossterm><filename>random_number</filename> and <filename>random_string</filename> </glossterm>
  
  <glossdef><para> If you run <command>make</command> in the

From cbbrowne at lists.slony.info  Mon Jan 28 11:34:27 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Jan 28 11:34:28 2008
Subject: [Slony1-commit] slony1-engine/doc/adminguide slonik_ref.sgml
Message-ID: <20080128193427.2421C290C2A@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv5899

Modified Files:
	slonik_ref.sgml 
Log Message:
Add notes regarding requirement for CLONE NODE that there be
synchronization against SYNCs against other nodes


Index: slonik_ref.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/slonik_ref.sgml,v
retrieving revision 1.77
retrieving revision 1.78
diff -C2 -d -r1.77 -r1.78
*** slonik_ref.sgml	21 Jan 2008 19:27:37 -0000	1.77
--- slonik_ref.sgml	28 Jan 2008 19:34:25 -0000	1.78
***************
*** 3029,3037 ****
       standard database tools.
      </para>
     </Refsect1>
     <Refsect1><Title>Example</Title>
      <Programlisting>
       clone prepare (id = 33, provider = 22, comment='Clone 33');
!     </Programlisting>
     </Refsect1>
     <refsect1> <title> Version Information </title>
--- 3029,3047 ----
       standard database tools.
      </para>
+ 
+     <para> Note that in order that we be certain that this new node be
+     consistent with all nodes, it is important to issue a SYNC event
+     against every node aside from the provider and wait to start
+     copying the provider database at least until all those SYNC events
+     have been confirmed by the provider.  Otherwise, it is possible
+     for the clone to miss some events. </para>
+ 
     </Refsect1>
     <Refsect1><Title>Example</Title>
      <Programlisting>
       clone prepare (id = 33, provider = 22, comment='Clone 33');
!      sync (id=11);
!      sync (id=22);
!      </Programlisting>
     </Refsect1>
     <refsect1> <title> Version Information </title>

From cbbrowne at lists.slony.info  Mon Jan 28 11:35:23 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Mon Jan 28 11:35:24 2008
Subject: [Slony1-commit] slony1-engine/doc/adminguide slony.sgml
	filelist.sgml triggers.sgml
Message-ID: <20080128193523.401E4290D0B@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/doc/adminguide
In directory main.slony.info:/tmp/cvs-serv5927

Modified Files:
	slony.sgml filelist.sgml 
Added Files:
	triggers.sgml 
Log Message:
Add a new discussion of how triggers function, as this changes pretty
massively in the next Slony-I release


Index: filelist.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/filelist.sgml,v
retrieving revision 1.20
retrieving revision 1.21
diff -C2 -d -r1.20 -r1.21
*** filelist.sgml	5 Sep 2007 21:35:25 -0000	1.20
--- filelist.sgml	28 Jan 2008 19:35:21 -0000	1.21
***************
*** 47,50 ****
--- 47,51 ----
  <!entity raceconditions     SYSTEM "raceconditions.sgml">
  <!entity partitioning       SYSTEM "partitioning.sgml">
+ <!entity triggers           SYSTEM "triggers.sgml">
  
  <!-- back matter -->

Index: slony.sgml
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/doc/adminguide/slony.sgml,v
retrieving revision 1.38
retrieving revision 1.39
diff -C2 -d -r1.38 -r1.39
*** slony.sgml	5 Sep 2007 21:35:25 -0000	1.38
--- slony.sgml	28 Jan 2008 19:35:21 -0000	1.39
***************
*** 94,97 ****
--- 94,98 ----
   &listenpaths;
   &plainpaths;
+  &triggers;
   &locking;
   &raceconditions;

--- NEW FILE: triggers.sgml ---
<!-- $Id: triggers.sgml,v 1.1 2008-01-28 19:35:21 cbbrowne Exp $ --> 
<sect1 id="triggers"><title>&slony1; Trigger Handling</title>

<indexterm><primary>trigger handling</primary></indexterm>

<para> &slony1; has had two <quote>flavours</quote> of trigger
handling:
<itemizedlist>

<listitem><para> In versions up to 1.2, &postgres; had no awareness of
replication, with the result that &slony1; needed to
<quote>hack</quote> on the system catalog in order to deactivate, on
subscribers, triggers that ought not to run.</para>

<para> This has had a number of somewhat painful side-effects including:</para> 
<itemizedlist>

<listitem><para> Corruption of the system catalog on subscribers, as
existing triggers, that generally need to be hidden, are
<quote>hacked</quote>, via <envar>pg_catalog.pg_trigger</envar>, to
point to the index being used by &slony1; as its <quote>primary
key</quote>.</para>

<para> The very same thing was true for rules. </para>

<para> This had the side-effect that
<application>pg_dump</application> could not be used to pull proper
schemas from subscriber nodes.</para></listitem>

<listitem><para> It introduced the need to take out exclusive locks on
<emphasis>all replicated tables</emphasis> when processing
&rddlchanges; as triggers on each replicated table would need to be
dropped and re-added during the course of
processing.</para></listitem>

</itemizedlist>

<listitem><para> In &postgres; version 8.3, there is new functionality
where triggers and rules can have their behaviour altered via
<command>ALTER TABLE</command>, and specify any of the following
further trigger-related options:</para>

<itemizedlist>

<listitem><para> <command> DISABLE TRIGGER trigger_name</command>  </para></listitem>
<listitem><para> <command> ENABLE TRIGGER trigger_name</command>  </para></listitem>
<listitem><para> <command> ENABLE REPLICA TRIGGER trigger_name</command>  </para></listitem>
<listitem><para> <command> ENABLE ALWAYS TRIGGER trigger_name</command>  </para></listitem>
<listitem><para> <command> DISABLE RULE rewrite_rule_name</command>  </para></listitem>
<listitem><para> <command> ENABLE RULE rewrite_rule_name</command>  </para></listitem>
<listitem><para> <command> ENABLE REPLICA RULE rewrite_rule_name</command>  </para></listitem>
<listitem><para> <command> ENABLE ALWAYS RULE rewrite_rule_name</command>  </para></listitem>

</itemizedlist>

<para> A new GUC variable, <envar>session_replication_role</envar>
controls whether the session is in origin, replica, or local mode,
which then, in combination with the above enabling/disabling options,
controls whether or not the trigger function actually runs. </para>

<para> We may characterize when triggers fire, under &slony1;
replication, based on the following table; the same rules apply to
&postgres; rules.

<table id="triggerbehaviour"> <title> Trigger Behaviour </title>
<tgroup cols="7">
<thead>
 <row> <entry>Trigger Form</entry> <entry>When Established</entry>  <entry>Log Trigger</entry> <entry>denyaccess Trigger</entry>  <entry>Action - origin</entry> <entry>Action - replica</entry>  <entry> Action - local</entry> </row>
</thead>
<tbody>
<row> <entry>DISABLE TRIGGER</entry> <entry>User request</entry> <entry>disabled on subscriber</entry> <entry>enabled on subscriber</entry> <entry>does not fire</entry>  <entry>does not fire</entry>  <entry>does not fire</entry> </row>
<row> <entry>ENABLE TRIGGER</entry> <entry>Default</entry> <entry>enabled on subscriber</entry> <entry>disabled on subscriber</entry> <entry>fires</entry>  <entry>does not fire</entry>  <entry>fires</entry> </row>
<row> <entry>ENABLE REPLICA TRIGGER</entry> <entry>User request</entry> <entry>inappropriate</entry> <entry>inappropriate</entry> <entry>does not fire</entry>  <entry>fires</entry>  <entry>does not fire</entry> </row>
<row> <entry>ENABLE ALWAYS TRIGGER</entry> <entry>User request</entry> <entry>inappropriate</entry> <entry>inappropriate</entry> <entry>fires</entry>  <entry>fires</entry>  <entry>fires</entry> </row>
</tbody>

<para> There are, correspondingly, now, several ways in which &slony1;
interacts with this.  Let us outline those times that are interesting:
</para>

<itemizedlist>

<listitem><para> Before replication is set up,
<emphasis>every</emphasis> database starts out in
<quote>origin</quote> status, and, by default, all triggers are of the
<command>ENABLE TRIGGER</command> form, so they all run, as is normal
in a system uninvolved in replication. </para> </listitem>

<listitem><para> When a &slony1; subscription is set up, on the origin
node, both the <function>logtrigger</function> and
<function>denyaccess</function> triggers are added, the former being
enabled, and running, the latter being disabled, so it does not
run. </para>

<para> From a locking perspective, each <xref
linkend="stmtsetaddtable"> request will need to briefly take out an
exclusive lock on each table as it attaches these triggers, which is
much the same as has always been the case with &slony1;. </para>
</listitem>

<listitem><para> On the subscriber, the subscription process will add
the same triggers, but with the polarities <quote>reversed</quote>, to
protect data from accidental corruption on subscribers.  </para>

<para> From a locking perspective, again, there is not much difference
from earlier &slony1; behaviour, as the subscription process, due to
running <command>TRUNCATE</command>, copying data, and altering table
schemas, requires <emphasis>extensive</emphasis> exclusive table
locks, and the changes in trigger behaviour do not change those
requirements.  </para>

<para> However, note that the ability to enable and disable triggers
in a &postgres;-supported fashion means that we have had no need to
<quote>corrupt</quote> the system catalog, so we have the considerable
advantage that <application>pg_dump</application> may be used to draw
a completely consistent backup against any node in a &slony1;
cluster.</para>

</listitem>

<listitem><para> If you take a <application>pg_dump</application> of a
&slony1; node, and drop out the &slony1; namespace, this now cleanly
removes <emphasis>all</emphasis> &slony1; components, leaving the
database, <emphasis>including its schema,</emphasis> in a
<quote>pristine</quote>, consistent fashion, ready for whatever use
may be desired. </para> </listitem>

<listitem><para> &rddlchanges; is now performed in quite a different
way: rather than altering each replicated table to <quote>take it out
of replicated mode</quote>, &slony1; instead simply shifts into the
<command>local</command> status for the duration of this event.  </para>

<para> On the origin, this deactivates the
<function>logtrigger</function> trigger. </para>

<para> On each subscriber, this deactivates the
<function>denyaccess</function> trigger. </para>

<para> This may be expected to allow DDL changes to become
<emphasis>enormously</emphasis> less expensive, since, rather than needing to
take out exclusive locks on <emphasis>all</emphasis> replicated tables (as used
to be mandated by the action of dropping and adding back the
&slony1;-created triggers), the only tables that are locked are those
ones that the DDL script was specifically acting on.  </para>

</listitem>

<listitem><para> At the time of invoking <xref linkend="stmtmoveset">
against the former origin, &slony1; must transform that node into a
subscriber, which requires dropping the <function>lockset</function>
triggers, disabling the <function>logtrigger</function> triggers, and
enabling the <function>denyaccess</function> triggers. </para>

<para> At about the same time, when processing <xref
linkend="stmtmoveset"> against the new origin, &slony1; must transform
that node into an origin, which requires disabling the formerly active
<function>denyaccess</function> triggers, and enabling the
<function>logtrigger</function> triggers. </para>

<para> From a locking perspective, this will not behave differently
from older versions of &slony1;; the locking that takes place here is
quite necessary. </para>

</listitem>

<listitem><para> Similarly to <xref linkend="stmtmoveset">, <xref
linkend="stmtfailover"> transforms a subscriber node into an origin,
which requires disabling the formerly active
<function>denyaccess</function> triggers, and enabling the
<function>logtrigger</function> triggers.  The locking implications
are again, much the same.  </para> </listitem>

</itemizedlist>

</sect1>
<!-- Keep this comment at the end of the file
Local variables:
mode:sgml
sgml-omittag:nil
sgml-shorttag:t
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:1
sgml-indent-data:t
sgml-parent-document:"slony.sgml"
sgml-exposed-tags:nil
sgml-local-catalogs:("/usr/lib/sgml/catalog")
sgml-local-ecat-files:nil
End:
-->

From cbbrowne at lists.slony.info  Tue Jan 29 07:54:48 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Jan 29 07:54:49 2008
Subject: [Slony1-commit] slony1-engine config.h.in
Message-ID: <20080129155448.281DC29024C@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine
In directory main.slony.info:/tmp/cvs-serv28334

Modified Files:
	config.h.in 
Log Message:
Add #define to support 8.3 behaviour where typenameTypeId() takes 3 args


Index: config.h.in
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/config.h.in,v
retrieving revision 1.19
retrieving revision 1.20
diff -C2 -d -r1.19 -r1.20
*** config.h.in	31 May 2007 16:46:17 -0000	1.19
--- config.h.in	29 Jan 2008 15:54:46 -0000	1.20
***************
*** 88,91 ****
--- 88,94 ----
  #undef HAVE_TYPENAMETYPEID_2
  
+ /* Set to 1 if typenameTypeId() takes 3 args */
+ #undef HAVE_TYPENAMETYPEID_3
+ 
  /* Set to 1 if standard_conforming_strings available */
  #undef HAVE_STANDARDCONFORMINGSTRINGS

From cbbrowne at lists.slony.info  Tue Jan 29 07:56:17 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Jan 29 07:56:17 2008
Subject: [Slony1-commit] slony1-engine/src/slon remote_listen.c
Message-ID: <20080129155617.37D0F290CEB@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/slon
In directory main.slony.info:/tmp/cvs-serv28405/slon

Modified Files:
	remote_listen.c 
Log Message:
Improve logging on mode switching between polling/listening


Index: remote_listen.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/remote_listen.c,v
retrieving revision 1.38
retrieving revision 1.39
diff -C2 -d -r1.38 -r1.39
*** remote_listen.c	3 Dec 2007 23:53:42 -0000	1.38
--- remote_listen.c	29 Jan 2008 15:56:15 -0000	1.39
***************
*** 341,345 ****
  			case SLON_POLLSTATE_POLL:
  				slon_log(SLON_DEBUG2, 
! 					 "remoteListenThread_%d: UNLISTEN\n",
  					 node->no_id);
  
--- 341,345 ----
  			case SLON_POLLSTATE_POLL:
  				slon_log(SLON_DEBUG2, 
! 					 "remoteListenThread_%d: UNLISTEN - switch into polling mode\n",
  					 node->no_id);
  
***************
*** 350,354 ****
  			case SLON_POLLSTATE_LISTEN:
  				slon_log(SLON_DEBUG2, 
! 					 "remoteListenThread_%d: LISTEN\n",
  					 node->no_id);
  				(void) slon_mkquery(&query1,
--- 350,354 ----
  			case SLON_POLLSTATE_LISTEN:
  				slon_log(SLON_DEBUG2, 
! 					 "remoteListenThread_%d: LISTEN - switch from polling mode to use LISTEN\n",
  					 node->no_id);
  				(void) slon_mkquery(&query1,

From cbbrowne at lists.slony.info  Tue Jan 29 07:57:19 2008
From: cbbrowne at lists.slony.info (Chris Browne)
Date: Tue Jan 29 07:57:20 2008
Subject: [Slony1-commit] slony1-engine/src/backend slony1_funcs.sql
Message-ID: <20080129155719.69F94290D0B@main.slony.info>

Update of /home/cvsd/slony1/slony1-engine/src/backend
In directory main.slony.info:/tmp/cvs-serv28427/backend

Modified Files:
	slony1_funcs.sql 
Log Message:
updateReloid() does not need to raise an event


Index: slony1_funcs.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/backend/slony1_funcs.sql,v
retrieving revision 1.128
retrieving revision 1.129
diff -C2 -d -r1.128 -r1.129
*** slony1_funcs.sql	21 Jan 2008 18:54:11 -0000	1.128
--- slony1_funcs.sql	29 Jan 2008 15:57:17 -0000	1.129
***************
*** 5078,5083 ****
  			and @NAMESPACE@.slon_quote_brute(PGN.nspname) = @NAMESPACE@.slon_quote_brute(@NAMESPACE@.sl_sequence.seq_nspname);
  
!         return  @NAMESPACE@.createEvent(''_@CLUSTERNAME@'', ''RESET_CONFIG'',
!                         p_set_id::text, p_only_on_node::text);
  end;
  ' language plpgsql;
--- 5078,5082 ----
  			and @NAMESPACE@.slon_quote_brute(PGN.nspname) = @NAMESPACE@.slon_quote_brute(@NAMESPACE@.sl_sequence.seq_nspname);
  
! 	return 1;
  end;
  ' language plpgsql;

