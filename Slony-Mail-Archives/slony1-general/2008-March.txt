From jason at buberel.org  Sun Mar  2 22:51:41 2008
From: jason at buberel.org (Jason L. Buberel)
Date: Sun Mar  2 22:52:03 2008
Subject: [Slony1-general] After patching, still getting compilation error
Message-ID: <47CB9FFD.5050106@buberel.org>

I found the exchange between CB and Mike C on Dec 5, 2007 in which the =

following compilation error was discussed:

slony1_funcs.c: In function slon_quote_literal:
slony1_funcs.c:1106: warning: pointer targets in passing argument 1 of =

pg_mblen differ in signedness
slony1_funcs.c: In function getClusterStatus:
slony1_funcs.c:1357: warning: passing argument 1 of typenameTypeId from =

incompatible pointer type
slony1_funcs.c:1357: error: too few arguments to function typenameTypeId
slony1_funcs.c:1440: warning: passing argument 1 of typenameTypeId from =

incompatible pointer type
slony1_funcs.c:1440: error: too few arguments to function typenameTypeId
make[2]: *** [slony1_funcs.o] Error 1
make[2]: Leaving directory `/home/jason/downloads/slony1-1.2.12/src/backend'
make[1]: *** [all] Error 2
make[1]: Leaving directory `/home/jason/downloads/slony1-1.2.12/src'
make: *** [all] Error 2

I also need to build slony1-1.2.12 against postgres-8.3 on a x86_65 =

RHEL5 server. I found the patch from the slony-commit mailinglist, and =

applied it to config/acx_libpq.m4 successfully (no errors or warnings). =

I then re-ran autoconf and ./configure. This was all done on a pristine =

copy of the source tree (had just been un-tarred).

However, after re-running configure, I still get the same error. The =

diff looks like this:

 > diff acx_libpq.m4 acx_libpq.m4.orig
354,361c354
< AC_MSG_CHECKING(for typenameTypeId)
< if test -z "$ac_cv_typenameTypeId_args"; then
<   AC_TRY_COMPILE(
<     [#include "postgres.h"
<      #include "parser/parse_type.h"],
<     [typenameTypeId(NULL, NULL, NULL); ],
<     ac_cv_typenameTypeId_args=3D3)
< fi
---
 >
380,382c373
<   if test "$ac_cv_typenameTypeId_args" =3D 3; then
<     AC_DEFINE(HAVE_TYPENAMETYPEID_3)
<   elif test "$ac_cv_typenameTypeId_args" =3D 2; then
---
 >   if test "$ac_cv_typenameTypeId_args" =3D 2; then

Any suggestions?

-jason



-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080302/=
0a846c18/attachment.htm
From y-mori at sraoss.co.jp  Mon Mar  3 05:02:20 2008
From: y-mori at sraoss.co.jp (Yoshiharu Mori)
Date: Mon Mar  3 05:02:50 2008
Subject: [Slony1-general] failover problems with 3 nodes
In-Reply-To: <60tzjuc7ss.fsf@dba2.int.libertyrms.com>
References: <20080226230734.de0f09d5.y-mori@sraoss.co.jp>
	<20080227170349.c9c17a42.y-mori@sraoss.co.jp>
	<47C52C89.7080307@postgresqlfr.org>
	<20080227201129.27ae8860.y-mori@sraoss.co.jp>
	<47C546FE.5090706@postgresqlfr.org>
	<20080227132917.4489327c@amilo.home>
	<60tzjuc7ss.fsf@dba2.int.libertyrms.com>
Message-ID: <20080303220220.41450d92.y-mori@sraoss.co.jp>

Hello.

> >> [...]
> >> Hey, I should test failover before updating to 1.2.13...
> >
> > I have some strange periodic problems with 'ACCEPT_SET - MOVE_SET or
> > FAILOVER_SET not received yet - sleep' on 1.2.12 and 1.2.13. Looks
> > similar to this one.
> >
> > I should try to downgrade to 1.2.11 and try if my 'move set' problems
> > will disappear. Here is the initial problem description:
> > http://lists.slony.info/pipermail/slony1-general/2008-February/007445.html
> 
> There's something about this that isn't making sense...
> 
> I just did a CVS diff between 1.2.11 and REL_1_2_STABLE, and didn't
> see anything that ought to have anything to do with this.
> 
> I haven't yet done any testing of this case, out of the samples
> described; I intend to do so; but it's not making sense that changing
> between 1.2.11 and 1.2.13 should make any difference in this...

Sorry,I should have checked more carefully.

I think this problem is not the difference of the version but "remoteWorkerThread"

When the problem of 'ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep' occurs,
the pg_lock table is as following.

----
testdb=# SELECT relname,granted,pid,mode from pg_locks as l , pg_class as c where c.oid = l.relation and locktype='relation';
          relname           | granted |  pid  |        mode
----------------------------+---------+-------+---------------------
 pg_class_oid_index         | t       | 15778 | AccessShareLock
 pg_class_relname_nsp_index | t       | 15778 | AccessShareLock
 pg_locks                   | t       | 15778 | AccessShareLock
 pg_class                   | t       | 15778 | AccessShareLock
 sl_event                   | t       | 15771 | AccessShareLock
 sl_event-pkey              | t       | 15771 | AccessShareLock
 sl_config_lock             | f       | 15770 | AccessExclusiveLock <-- attention!
 sl_config_lock             | t       | 15771 | AccessExclusiveLock
----

Next,I examined why two lock table sl_config_lock was executed.

In the case of failover or move set, two events are generated.
The one is "FAILOVER/MOVE_SET",the other is "ACCEPT_SET".
Furthermore, "FAILOVER/MOVE_SET" event is executed by remoteWorkerThread_1 which INSERT INTO sl_event table.
and "ACCEPT_SET" event is executed by remoteWorkerThread_2 which SELECT ev_type FROM sl_event.

Both events lock sl_config_lock table as following.
---
"begin transaction; set transaction isolation level serializable; lock table "_testdbcluster".sl_config_lock;
---

if it is executed in order of remoteWorkerThread_1(INSERT) and remoteWorkerThread_2(SELECT), the problem doesn't occur as following.

----this is postgresql SQL-log SUCCESS  CASE: attention pid=15407 ---
2008-03-03 18:56:15 JST[15407]LOG:  statement: begin transaction; set transaction isolation level serializable; /* FAILOVER_SET */ lock table "_testdbcluster".sl_config_lock;
2008-03-03 18:56:15 JST[15408]LOG:  statement: begin transaction; set transaction isolation level serializable; /* ACCEPT_SET */ lock table "_testdbcluster".sl_config_lock;
2008-03-03 18:56:15 JST[15407]LOG:  statement: select "_testdbcluster".failoverSet_int(1, 2, 1, 16); notify "_testdbcluster_Event"; insert into "_testdbcluster".sl_event     (ev_origin, ev_seqno, ev_timestamp,      ev_minxid, ev_maxxid, ev_xip, ev_type , ev_data1, ev_data2, ev_data3    ) values ('1', '16', '2008-03-03 18:56:14.173481', '798269', '798271', '''798270''', 'FAILOVER_SET', '1', '2', '1'); insert into "_testdbcluster".sl_confirm   (con_origin, con_received, con_seqno, con_timestamp)    values (1, 3, '16', now()); commit transaction;
-------------------------------

But, if it is executed in order of remoteWorkerThread_2(SELECT) and remoteWorkerThread_2(INSERT),
we have  'ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep' loops.

-- this is postgresql SQL-log FAILED CASE: attention pid = 15771 ---
2008-03-03 19:13:51 JST[15771]LOG:  statement: begin transaction; set transaction isolation level serializable; /* ACCEPT_SET */ lock table "_testdbcluster".sl_config_lock;
2008-03-03 19:13:51 JST[15770]LOG:  statement: begin transaction; set transaction isolation level serializable; /* FAILOVER_SET */ lock table "_testdbcluster".sl_config_lock;
2008-03-03 19:13:51 JST[15771]LOG:  statement: select 1 from "_testdbcluster".sl_event where      (ev_origin = 1 and       ev_seqno = 22 and       ev_type = 'MOVE_SET' and       ev_data1 = '1' and      ev_data2 = '1' and       ev_data3 = '2') or      (ev_origin = 1 and       ev_seqno = 22 and       ev_type = 'FAILOVER_SET' and       ev_data1 = '1' and       ev_data2 = '2' and       ev_data3 = '1');
----------------------------------------------

Because of "lock table sl_config_lock", remoteWorkerThread_1 cannot insert "FAILOVER/MOVE_SET" event into sl_event!!

I think this is big bug.

my env is Cent OS x86_64, DUAL-CORE cpu.

Regards,

-- 
SRA OSS, Inc. Japan
Yoshiharu Mori <y-mori@sraoss.co.jp>
http://www.sraoss.co.jp/
From victor.aluko at gmail.com  Tue Mar  4 01:26:53 2008
From: victor.aluko at gmail.com (ajcity)
Date: Tue Mar  4 01:27:19 2008
Subject: [Slony1-general] Subscribe set problem
Message-ID: <15823349.post@talk.nabble.com>


 Hi All,
  I trying to replicate a large database (2GB) from the master to a slave.
The master has about 60 tables. When I added the smaller tables (<100000
rows) it replicates but it queues up the sync events from the master and I
don't see any data in the slave tables until its done. 
  When I added a large table to another replication set, it takes like
forever (which isn't the problem) but whenever there is a break in the
internet connection, there is no data in the table when I it (after stopping
the daemons) and when I restart daemon, it gives a "ERROR: Slony-I
setAddTable_int(): table id 1 has already been assigned!" even though I am
adding any table to the replication set.
  Also I noticed that if I add the command:
    sync (id=1);
     wait for event (origin=1, confirmed=2, wait on=1, timeout=0);
  to the subscribe command, it doesn't return until all the data has been
copied (for small tables) or it never does (for a large table).
   Does any one have an idea what could be wrong? I am following the
commands as specified in the "Replicating Your First Database" section of
the Slony help.
   Also, when replicating a table which is always updated every 3 minutes,
does slony added just the new data or does it start copying all the data
again?

     Victor
-- 
View this message in context: http://www.nabble.com/Subscribe-set-problem-tp15823349p15823349.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From stephane.schildknecht at postgresqlfr.org  Tue Mar  4 01:57:19 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Tue Mar  4 01:57:49 2008
Subject: [Slony1-general] Subscribe set problem
In-Reply-To: <15823349.post@talk.nabble.com>
References: <15823349.post@talk.nabble.com>
Message-ID: <47CD1CFF.8060308@postgresqlfr.org>

ajcity a ?crit :
>  Hi All,
>   I trying to replicate a large database (2GB) from the master to a slave.
> The master has about 60 tables. When I added the smaller tables (<100000
> rows) it replicates but it queues up the sync events from the master and I
> don't see any data in the slave tables until its done. 
>   When I added a large table to another replication set, it takes like
> forever (which isn't the problem) but whenever there is a break in the
> internet connection, there is no data in the table when I it (after stopping
> the daemons) and when I restart daemon, it gives a "ERROR: Slony-I
> setAddTable_int(): table id 1 has already been assigned!" even though I am
> adding any table to the replication set.
>   Also I noticed that if I add the command:
>     sync (id=1);
>      wait for event (origin=1, confirmed=2, wait on=1, timeout=0);
>   to the subscribe command, it doesn't return until all the data has been
> copied (for small tables) or it never does (for a large table).
>    Does any one have an idea what could be wrong? I am following the
> commands as specified in the "Replicating Your First Database" section of
> the Slony help.
>    Also, when replicating a table which is always updated every 3 minutes,
> does slony added just the new data or does it start copying all the data
> again?
> 
>      Victor

Hi,

When you add a table to replication, first operation is to synchronize data.
The only way Slony can do it is to delete (or truncate when possible) all data
on the target table (slave) and copy all data from the source (master).

Therefore, if you table contains many Gb of data, and the net link is leak, it
certainly can take a very long time.

What's more, if any error occurs while synchronizing data set for the first
time, all synchronized data are lost, and the whole synchronisation processus
has to be processed again.

When the first synchronisation is OK, then only the diff is transfered from
master to slave.

Regards,

-- 
St?phane SCHILDKNECHT
Pr?sident de PostgreSQLFr
T?l. 09 53 69 97 12
http://www.postgresqlfr.org
From victor.aluko at gmail.com  Tue Mar  4 02:46:55 2008
From: victor.aluko at gmail.com (ajcity)
Date: Tue Mar  4 02:47:23 2008
Subject: [Slony1-general] Subscribe set problem
In-Reply-To: <47CD1CFF.8060308@postgresqlfr.org>
References: <15823349.post@talk.nabble.com> <47CD1CFF.8060308@postgresqlfr.org>
Message-ID: <15824694.post@talk.nabble.com>


Hi,
 Thanks for the info.
 But am I supposed to only see "remoteListenThread: queue event 1,400 SYNC"
(and the event keeps growing) while its replicating or isn't there supposed
to be another log info from the remoteWorkerThread showing that its still
copying?
  And on the slave node, there is no record of "ENABLE_SUBSCRIPTION" in the
sl_event table and if I use a "WAIT FOR EVENT" command with timeout=0, it
waits endlessly. Is the slave suppose to confirm after replication or before
copying begins?
  
  Victor


&quot;St?phane A. Schildknecht&quot; wrote:
> 
> Hi,
> 
> When you add a table to replication, first operation is to synchronize
> data.
> The only way Slony can do it is to delete (or truncate when possible) all
> data
> on the target table (slave) and copy all data from the source (master).
> 
> Therefore, if you table contains many Gb of data, and the net link is
> leak, it
> certainly can take a very long time.
> 
> What's more, if any error occurs while synchronizing data set for the
> first
> time, all synchronized data are lost, and the whole synchronisation
> processus
> has to be processed again.
> 
> When the first synchronisation is OK, then only the diff is transfered
> from
> master to slave.
> 
> Regards,
> 
> -- 
> St?phane SCHILDKNECHT
> Pr?sident de PostgreSQLFr
> T?l. 09 53 69 97 12
> http://www.postgresqlfr.org
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/Subscribe-set-problem-tp15823349p15824694.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From liobod.slony at gmail.com  Tue Mar  4 07:13:25 2008
From: liobod.slony at gmail.com (lio bod)
Date: Tue Mar  4 07:13:35 2008
Subject: [Slony1-general] mkslonconf.sh and pg service
Message-ID: <d4f444290803040713p4ac08f99ufbcc771ccca69e79@mail.gmail.com>

hello world,

i'm playin with mkslonconf.sh and i'm referring to
http://slony.info/documentation/adminscripts.html
it's said set several variables and i'm lookin for what value i should use
for PGSERVICE
i tried some but the scripts says :

psql: ERROR: Service file '/etc/sysconfig/pgsql/pg_service.conf'

of course i don't have this file on my system (RHEL4 / postgres 7.4.19).
should i?

i just understand it's something relative to run postgres as a service.

any clue?

thx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080304/=
3c314b39/attachment.htm
From y-mori at sraoss.co.jp  Tue Mar  4 07:13:36 2008
From: y-mori at sraoss.co.jp (Yoshiharu Mori)
Date: Tue Mar  4 07:13:42 2008
Subject: [Slony1-general] failover problems with 3 nodes
In-Reply-To: <20080303220220.41450d92.y-mori@sraoss.co.jp>
References: <20080226230734.de0f09d5.y-mori@sraoss.co.jp>
	<20080227170349.c9c17a42.y-mori@sraoss.co.jp>
	<47C52C89.7080307@postgresqlfr.org>
	<20080227201129.27ae8860.y-mori@sraoss.co.jp>
	<47C546FE.5090706@postgresqlfr.org>
	<20080227132917.4489327c@amilo.home>
	<60tzjuc7ss.fsf@dba2.int.libertyrms.com>
	<20080303220220.41450d92.y-mori@sraoss.co.jp>
Message-ID: <20080305001336.6f96361a.y-mori@sraoss.co.jp>

Hi

I send a small patch for REL_1_2_STABLE branch.

When this patch was applied, the problem of "FAILOVER/MOVE_SET" was solved.

This patch only move the

  "begin transaction; set transaction isolation level serializable; lock table "_testdbcluster".sl_config_lock;"

after

  the processing of 'ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep' in remote_worker.c.

This "ACCEPT_SET" loops used only SELECT QUERY. I don't know why it was used in
islocation-level-serializable and why "lock table" is necessary.

This patch doesn't care for "archive log" and take care,please.

-------------------
Index: remote_worker.c
===================================================================
RCS file: /slony1/slony1-engine/src/slon/remote_worker.c,v
retrieving revision 1.124.2.31
diff -u -r1.124.2.31 remote_worker.c
--- remote_worker.c 6 Feb 2008 20:23:52 -0000 1.124.2.31
+++ remote_worker.c 4 Mar 2008 02:42:30 -0000
@@ -677,9 +677,12 @@
      slon_appendquery(&query1,
               "lock table %s.sl_config_lock; ",
               rtcfg_namespace);
-     if (query_execute(node, local_dbconn, &query1) < 0)
-       slon_retry();
-     dstring_reset(&query1);
+     if (strcmp(event->ev_type, "ACCEPT_SET") != 0)
+     {
+       if (query_execute(node, local_dbconn, &query1) < 0)
+         slon_retry();
+       dstring_reset(&query1);
+     }

      /*
       * For all non-SYNC events, we write at least a standard
@@ -1017,6 +1020,10 @@
          PQclear(res);
          slon_log(SLON_DEBUG2, "ACCEPT_SET - MOVE_SET or FAILOVER_SET exists - adjusting setsync status\n");

+         if (query_execute(node, local_dbconn, &query1) < 0)
+           slon_retry();
+         dstring_reset(&query1);
+
          /*
           * Finalize the setsync status to mave the ACCEPT_SET's
           * seqno and snapshot info.
@@ -1056,6 +1063,10 @@
        else
        {
          slon_log(SLON_DEBUG2, "ACCEPT_SET - on origin node...\n");
+
+         if (query_execute(node, local_dbconn, &query1) < 0)
+           slon_retry();
+         dstring_reset(&query1);
        }

      }
--------------------
> Hello.
> 
> > >> [...]
> > >> Hey, I should test failover before updating to 1.2.13...
> > >
> > > I have some strange periodic problems with 'ACCEPT_SET - MOVE_SET or
> > > FAILOVER_SET not received yet - sleep' on 1.2.12 and 1.2.13. Looks
> > > similar to this one.
> > >
> > > I should try to downgrade to 1.2.11 and try if my 'move set' problems
> > > will disappear. Here is the initial problem description:
> > > http://lists.slony.info/pipermail/slony1-general/2008-February/007445.html
> > 
> > There's something about this that isn't making sense...
> > 
> > I just did a CVS diff between 1.2.11 and REL_1_2_STABLE, and didn't
> > see anything that ought to have anything to do with this.
> > 
> > I haven't yet done any testing of this case, out of the samples

 
> I think this problem is not the difference of the version but "remoteWorkerThread"
> 
> When the problem of 'ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep' occurs,
> the pg_lock table is as following.
> 
> ----
> testdb=# SELECT relname,granted,pid,mode from pg_locks as l , pg_class as c where c.oid = l.relation and locktype='relation';
>           relname           | granted |  pid  |        mode
> ----------------------------+---------+-------+---------------------
>  pg_class_oid_index         | t       | 15778 | AccessShareLock
>  pg_class_relname_nsp_index | t       | 15778 | AccessShareLock
>  pg_locks                   | t       | 15778 | AccessShareLock
>  pg_class                   | t       | 15778 | AccessShareLock
>  sl_event                   | t       | 15771 | AccessShareLock
>  sl_event-pkey              | t       | 15771 | AccessShareLock
>  sl_config_lock             | f       | 15770 | AccessExclusiveLock <-- attention!
>  sl_config_lock             | t       | 15771 | AccessExclusiveLock
> ----
> 
> Next,I examined why two lock table sl_config_lock was executed.
> 
> In the case of failover or move set, two events are generated.
> The one is "FAILOVER/MOVE_SET",the other is "ACCEPT_SET".
> Furthermore, "FAILOVER/MOVE_SET" event is executed by remoteWorkerThread_1 which INSERT INTO sl_event table.
> and "ACCEPT_SET" event is executed by remoteWorkerThread_2 which SELECT ev_type FROM sl_event.
> 
> Both events lock sl_config_lock table as following.
> ---
> "begin transaction; set transaction isolation level serializable; lock table "_testdbcluster".sl_config_lock;
> ---
> 
> if it is executed in order of remoteWorkerThread_1(INSERT) and remoteWorkerThread_2(SELECT), the problem doesn't occur as following.
> 
> ----this is postgresql SQL-log SUCCESS  CASE: attention pid=15407 ---
> 2008-03-03 18:56:15 JST[15407]LOG:  statement: begin transaction; set transaction isolation level serializable; /* FAILOVER_SET */ lock table "_testdbcluster".sl_config_lock;
> 2008-03-03 18:56:15 JST[15408]LOG:  statement: begin transaction; set transaction isolation level serializable; /* ACCEPT_SET */ lock table "_testdbcluster".sl_config_lock;
> 2008-03-03 18:56:15 JST[15407]LOG:  statement: select "_testdbcluster".failoverSet_int(1, 2, 1, 16); notify "_testdbcluster_Event"; insert into "_testdbcluster".sl_event     (ev_origin, ev_seqno, ev_timestamp,      ev_minxid, ev_maxxid, ev_xip, ev_type , ev_data1, ev_data2, ev_data3    ) values ('1', '16', '2008-03-03 18:56:14.173481', '798269', '798271', '''798270''', 'FAILOVER_SET', '1', '2', '1'); insert into "_testdbcluster".sl_confirm   (con_origin, con_received, con_seqno, con_timestamp)    values (1, 3, '16', now()); commit transaction;
> -------------------------------
> 
> But, if it is executed in order of remoteWorkerThread_2(SELECT) and remoteWorkerThread_2(INSERT),
                                                                                         ~
                                                                           sorry typo 2->1

> we have  'ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep' loops.
> 
> -- this is postgresql SQL-log FAILED CASE: attention pid = 15771 ---
> 2008-03-03 19:13:51 JST[15771]LOG:  statement: begin transaction; set transaction isolation level serializable; /* ACCEPT_SET */ lock table "_testdbcluster".sl_config_lock;
> 2008-03-03 19:13:51 JST[15770]LOG:  statement: begin transaction; set transaction isolation level serializable; /* FAILOVER_SET */ lock table "_testdbcluster".sl_config_lock;
> 2008-03-03 19:13:51 JST[15771]LOG:  statement: select 1 from "_testdbcluster".sl_event where      (ev_origin = 1 and       ev_seqno = 22 and       ev_type = 'MOVE_SET' and       ev_data1 = '1' and      ev_data2 = '1' and       ev_data3 = '2') or      (ev_origin = 1 and       ev_seqno = 22 and       ev_type = 'FAILOVER_SET' and       ev_data1 = '1' and       ev_data2 = '2' and       ev_data3 = '1');
> ----------------------------------------------
> 
> Because of "lock table sl_config_lock", remoteWorkerThread_1 cannot insert "FAILOVER/MOVE_SET" event into sl_event!!




From cbbrowne at ca.afilias.info  Tue Mar  4 07:48:14 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Mar  4 07:48:20 2008
Subject: [Slony1-general] mkslonconf.sh and pg service
In-Reply-To: <d4f444290803040713p4ac08f99ufbcc771ccca69e79@mail.gmail.com>
References: <d4f444290803040713p4ac08f99ufbcc771ccca69e79@mail.gmail.com>
Message-ID: <47CD6F3E.1020909@ca.afilias.info>

lio bod wrote:
> hello world,
>  =

> i'm playin with mkslonconf.sh and i'm referring to =

> http://slony.info/documentation/adminscripts.html
> it's said set several variables and i'm lookin for what value i should =

> use for PGSERVICE
> i tried some but the scripts says :
>  =

> psql: ERROR: Service file '/etc/sysconfig/pgsql/pg_service.conf'
>  =

> of course i don't have this file on my system (RHEL4 / postgres 7.4.19).
> should i?
>  =

> i just understand it's something relative to run postgres as a service.
>  =

> any clue?
>  =

> thx
You can find more documentation here:
http://www.postgresql.org/docs/8.2/static/libpq-pgservice.html

I don't think its behaviour has changed much between 7.4 and 8.2...

The idea is that you can identify a whole set of conninfo parameters via =

referencing a named entry in pg_service.conf.

Thus, supposing pg_service.conf contains the following:

[loganalysis]
dbname=3Dlog_analysis
port=3D5777
host=3Dlocalhost
user=3Dlog_analysis

## Log analysis database
[s1-log-analysis]
dbname=3Dlog_analysis
port=3D5777
host=3Dlocalhost
user=3Dlog_analysis

[s2-log-analysis]
dbname=3Dlog_analysis
port=3D6436
host=3Drg550-da-njmisc
user=3Dlog_analysis

[s3-log-analysis]
dbname=3Dlog_analysis
port=3D5757
host=3Dyrg-misc
user=3Dlog_analysis

You can use "PGSERVICE=3Ds3-log-analysis" in lieu of the (obviously rather =

longer)...  "PGDATABASE=3Dlog_analysis PGPORT=3D5757 PGHOST=3Dyrg-misc =

PGUSER=3Dlog_analysis"

If you're not using PGSERVICE, already, nothing necessitates that you =

start using it.  But if you are using it to organize connection info, =

mkslonconf.sh does support it...

-- =

output =3D reverse("ofni.sailifa.ac" "@" "enworbbc")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)

-------------- next part --------------
A non-text attachment was scrubbed...
Name: cbbrowne.vcf
Type: text/x-vcard
Size: 286 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080304=
/3ac348d8/cbbrowne.vcf
From liobod.slony at gmail.com  Tue Mar  4 08:00:17 2008
From: liobod.slony at gmail.com (lio bod)
Date: Tue Mar  4 08:00:25 2008
Subject: [Slony1-general] Re: mkslonconf.sh and pg service
In-Reply-To: <d4f444290803040713p4ac08f99ufbcc771ccca69e79@mail.gmail.com>
References: <d4f444290803040713p4ac08f99ufbcc771ccca69e79@mail.gmail.com>
Message-ID: <d4f444290803040800s18e68e5axca143b95e01d7ee7@mail.gmail.com>

ok
i just found some piece of info here :
http://docs.postgresqlfr.org/7.4/libpq.html

the sample was here :

/usr/share/pgsql/pg_service.conf.sample

i don't know if the final file shoud be there :

/etc/sysconfig/pgsql/pg_service.conf

or there :

/opt/[...]/pgsql/etc (my default postgres directory)

Anyway i defined my service for slony and i'm facing a refused connection
for sl_path.

i go on searching.

rgds





i tried

2008/3/4, lio bod <liobod.slony@gmail.com>:
>
> hello world,
>
> i'm playin with mkslonconf.sh and i'm referring to
> http://slony.info/documentation/adminscripts.html
> it's said set several variables and i'm lookin for what value i should use
> for PGSERVICE
> i tried some but the scripts says :
>
> psql: ERROR: Service file '/etc/sysconfig/pgsql/pg_service.conf'
>
> of course i don't have this file on my system (RHEL4 / postgres 7.4.19).
> should i?
>
> i just understand it's something relative to run postgres as a service.
>
> any clue?
>
> thx
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080304/=
b350ed3b/attachment-0001.htm
From liobod.slony at gmail.com  Tue Mar  4 08:30:31 2008
From: liobod.slony at gmail.com (lio bod)
Date: Tue Mar  4 08:30:40 2008
Subject: [Slony1-general] Re: mkslonconf.sh and pg service
In-Reply-To: <d4f444290803040800s18e68e5axca143b95e01d7ee7@mail.gmail.com>
References: <d4f444290803040713p4ac08f99ufbcc771ccca69e79@mail.gmail.com>
	<d4f444290803040800s18e68e5axca143b95e01d7ee7@mail.gmail.com>
Message-ID: <d4f444290803040830s7def8ee0v9b39b24c16b623b5@mail.gmail.com>

bad copy/paste in pg_service.conf.
mkslonconf.sh seems to be working fine

cheers


2008/3/4, lio bod <liobod.slony@gmail.com>:
>
> ok
> i just found some piece of info here :
> http://docs.postgresqlfr.org/7.4/libpq.html
>
> the sample was here :
>
> /usr/share/pgsql/pg_service.conf.sample
>
> i don't know if the final file shoud be there :
>
> /etc/sysconfig/pgsql/pg_service.conf
>
> or there :
>
> /opt/[...]/pgsql/etc (my default postgres directory)
>
> Anyway i defined my service for slony and i'm facing a refused connection
> for sl_path.
>
> i go on searching.
>
> rgds
>
>
>
>
>
> i tried
>
> 2008/3/4, lio bod <liobod.slony@gmail.com>:
> >
> > hello world,
> >
> > i'm playin with mkslonconf.sh and i'm referring to
> > http://slony.info/documentation/adminscripts.html
> > it's said set several variables and i'm lookin for what value i should
> > use for PGSERVICE
> > i tried some but the scripts says :
> >
> > psql: ERROR: Service file '/etc/sysconfig/pgsql/pg_service.conf'
> >
> > of course i don't have this file on my system (RHEL4 / postgres 7.4.19).
> >
> > should i?
> >
> > i just understand it's something relative to run postgres as a service.
> >
> > any clue?
> >
> > thx
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080304/=
a3bcc5c1/attachment.htm
From cbbrowne at ca.afilias.info  Tue Mar  4 09:28:34 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Mar  4 09:28:42 2008
Subject: [Slony1-general] Re: mkslonconf.sh and pg service
In-Reply-To: <d4f444290803040800s18e68e5axca143b95e01d7ee7@mail.gmail.com>
	(lio bod's message of "Tue, 4 Mar 2008 17:00:17 +0100")
References: <d4f444290803040713p4ac08f99ufbcc771ccca69e79@mail.gmail.com>
	<d4f444290803040800s18e68e5axca143b95e01d7ee7@mail.gmail.com>
Message-ID: <60ejaqo0u5.fsf@dba2.int.libertyrms.com>

"lio bod" <liobod.slony@gmail.com> writes:
> ok
>
> i just found some piece of info here :
> http://docs.postgresqlfr.org/7.4/libpq.html
>
> ?
>
> the sample was here :
> /usr/share/pgsql/pg_service.conf.sample
>
> ?
>
> i don't know if?the final file?shoud be there :
>
> ?
>
> /etc/sysconfig/pgsql/pg_service.conf
>
> ?
>
> or there :
>
> ?
>
> /opt/[...]/pgsql/etc (my default postgres directory)

That may get customized by your favorite Linux distribution, so
there's nothing forcibly wrang about either...

> Anyway i defined my service for slony and i'm facing a refused connection for sl_path.
>
> i go on searching.

Well, once you're there, the issue is one you should be able to debug
by trying to connect using psql, so it's no longer directly
Slony-I-relevant.  That's a good thing...
-- 
(format nil "~S@~S" "cbbrowne" "acm.org")
http://www3.sympatico.ca/cbbrowne/linuxxian.html
Last night  I played a  blank tape at  full blast. The mime  next door
went nuts.
From satya461 at gmail.com  Wed Mar  5 00:38:23 2008
From: satya461 at gmail.com (Satya)
Date: Wed Mar  5 00:38:46 2008
Subject: [Slony1-general] Slony 1.2.13 tests failing?
Message-ID: <6ccff2720803050038g5d625aa3hf55e3e697ea18325@mail.gmail.com>

Hi ,

   I tried running the Slony 1.2.13 tests and found that two tests
testlogship and testmultiplemoves are failing.

   Please let me know the status of these two tests. Are these tests
supposed to pass?

   I am attaching the log file(stdout of the two tests). If you need further
info, please do let me know.

Thanks,
Satya
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080305/=
834b73d6/attachment.htm
From satya461 at gmail.com  Wed Mar  5 00:42:29 2008
From: satya461 at gmail.com (Satya)
Date: Wed Mar  5 00:42:53 2008
Subject: [Slony1-general] Re: Slony 1.2.13 tests failing?
In-Reply-To: <6ccff2720803050038g5d625aa3hf55e3e697ea18325@mail.gmail.com>
References: <6ccff2720803050038g5d625aa3hf55e3e697ea18325@mail.gmail.com>
Message-ID: <6ccff2720803050042n1dad3a94w276fb32169eb6d9e@mail.gmail.com>

Skipped content of type multipart/alternative-------------- next part -----=
---------
testmultiplemovies
PGBINDIR=3D/export/home/tmp/satya/postgres-bin/bin/  PGPORT=3D5432 PGUSER=
=3Dnb199489  ./run_test.sh testmultiplemoves/
test: testmultiplemoves/
----------------------------------------------------
$Id: README,v 1.2.2.1 2007-06-12 18:14:32 cbbrowne Exp $

testmultiplemoves is a test that exercises MOVE SET on a 3 node
cluster with 2 replication sets.
  =

The interesting bit is that it requests MOVE SET on all the sets...

This may be expected to "stress" things such as...

- Auto generation of partial indexes on sl_log_? tables
- Locking and unlocking sets

----------------------------------------------------
creating origin DB: nb199489 -h localhost -U nb199489 -p 5432 slonyregress1
add plpgsql to Origin
loading origin DB with testmultiplemoves//init_schema.sql
done
creating subscriber 2 DB: nb199489 -h localhost -U nb199489 -p 5432 slonyre=
gress2
add plpgsql to subscriber
loading subscriber 2 DB from slonyregress1
done
creating subscriber 3 DB: nb199489 -h localhost -U nb199489 -p 5432 slonyre=
gress3
add plpgsql to subscriber
loading subscriber 3 DB from slonyregress1
done
creating cluster
done
storing nodes
done
storing paths
done
launching originnode : /export/home/tmp/satya/postgres-bin/bin//slon -s500 =
-g10 -d2 slony_regress1 "dbname=3Dslonyregress1 host=3Dlocalhost user=3Dnb1=
99489 port=3D5432"
Creating log shipping directory - /tmp/slony-regress.vbdksudmh/archive_logs=
_2
launching: /export/home/tmp/satya/postgres-bin/bin//slon -s500 -g10 -d2 -a =
/tmp/slony-regress.vbdksudmh/archive_logs_2 slony_regress1 "dbname=3Dslonyr=
egress2 host=3Dlocalhost user=3Dnb199489 port=3D5432"
Creating log shipping directory - /tmp/slony-regress.vbdksudmh/archive_logs=
_3
launching: /export/home/tmp/satya/postgres-bin/bin//slon -s500 -g10 -d2 -a =
/tmp/slony-regress.vbdksudmh/archive_logs_3 slony_regress1 "dbname=3Dslonyr=
egress3 host=3Dlocalhost user=3Dnb199489 port=3D5432"
subscribing
./run_test.sh: ERROR: Slonik error see /tmp/slony-regress.vbdksudmh/slonik.=
log for details
------
less /tmp/slony-regress.vbdksudmh/slonik.log
stdin>:25: timeout exceeded while waiting for event confirmation

testlogship
[/export/home/tmp/satya/slony1-1.2.13/tests] 13:30:31 $ PGBINDIR=3D/export/=
home/tmp/satya/postgres-bin/bin/  PGPORT=3D5432 PGUSER=3Dnb199489  ./run_te=
st.sh testlogship/
test: testlogship/
----------------------------------------------------
$Id: README,v 1.1.2.3 2007-06-05 14:38:06 cbbrowne Exp $
  =

testlogship is a basic test that replication generally functions with
log shipping.  =

  =

It creates three simple tables as one replication set, and replicates
them from one database to another.
  =

The three tables are of the three interesting types:
  =

1.  table1 has a formal primary key

2.  table2 lacks a formal primary key, but has a candidate primary key

It tries replicating a third table, which has an invalid candidate
primary key (columns not defined NOT NULL), which should cause it to
be rejected.  That is done in a slonik TRY {} block.

It also creates...

3.  table4 which has columns of all sorts of vaguely esoteric types to
exercise that points, paths, bitmaps, mac addresses, and inet types
replicate properly.

It then loads data into these tables.

The test proceeds to run a DDL script which alters the schema for
table 4, adding two new columns, one to be populated via a default,
for new tuples; the other has no default, but we assign the value 42
to all tuples existing at the time that the DDL script runs.

Surrounding that DDL script are several STORE TRIGGER requests in
order to try to ensure that we have a series of non-SYNC events in a
row.
----------------------------------------------------
creating origin DB: nb199489 -h localhost -U nb199489 -p 5432 slonyregress1
add plpgsql to Origin
loading origin DB with testlogship//init_schema.sql
done
creating subscriber 2 DB: nb199489 -h localhost -U nb199489 -p 5432 slonyre=
gress2
add plpgsql to subscriber
loading subscriber 2 DB from slonyregress1
done
creating subscriber 3 DB: nb199489 -h localhost -U nb199489 -p 5432 slonyre=
gress3
add plpgsql to subscriber
loading subscriber 3 DB from slonyregress1
done
creating subscriber 4 DB: nb199489 -h localhost -U nb199489 -p 5432 slonyre=
gress4
add plpgsql to subscriber
loading subscriber 4 DB from slonyregress1
done
creating cluster
done
storing nodes
Node 3 is a log shipping node - no need for STORE NODE
done
storing paths
log shipping between nodes(1/3) - ls(/true) - omit STORE PATH
log shipping between nodes(2/3) - ls(/true) - omit STORE PATH
log shipping between nodes(3/1) - ls(true/) - omit STORE PATH
log shipping between nodes(3/2) - ls(true/) - omit STORE PATH
log shipping between nodes(3/4) - ls(true/) - omit STORE PATH
log shipping between nodes(4/3) - ls(/true) - omit STORE PATH
done
launching originnode : /export/home/tmp/satya/postgres-bin/bin//slon -s500 =
-g10 -d2 slony_regress1 "dbname=3Dslonyregress1 host=3Dlocalhost user=3Dnb1=
99489 port=3D5432"
Creating log shipping directory - /tmp/slony-regress.mqgltsdtx/archive_logs=
_2
launching: /export/home/tmp/satya/postgres-bin/bin//slon -s500 -g10 -d2 -a =
/tmp/slony-regress.mqgltsdtx/archive_logs_2 slony_regress1 "dbname=3Dslonyr=
egress2 host=3Dlocalhost user=3Dnb199489 port=3D5432"
Creating log shipping directory - /tmp/slony-regress.mqgltsdtx/archive_logs=
_3
do not launch slon for node 3 - it receives data via log shipping
Creating log shipping directory - /tmp/slony-regress.mqgltsdtx/archive_logs=
_4
launching: /export/home/tmp/satya/postgres-bin/bin//slon -s500 -g10 -d2 -a =
/tmp/slony-regress.mqgltsdtx/archive_logs_4 slony_regress1 "dbname=3Dslonyr=
egress4 host=3Dlocalhost user=3Dnb199489 port=3D5432"
subscribing
done
generating 146 transactions of random data
0 %
5 %
10 %
15 %
20 %
25 %
30 %
35 %
40 %
45 %
50 %
55 %
60 %
65 %
70 %
75 %
80 %
85 %
90 %
95 %
100 %
done
launching polling script
loading data
data load complete - nodes are seeded reasonably
purge archive log files up to present in order to eliminate those that cann=
ot be used
purge /tmp/slony-regress.mqgltsdtx/archive_logs_2/slony1_log_2_000000000000=
00000001.sql
purge /tmp/slony-regress.mqgltsdtx/archive_logs_2/slony1_log_2_000000000000=
00000002.sql
purge /tmp/slony-regress.mqgltsdtx/archive_logs_2/slony1_log_2_000000000000=
00000003.sql
purge /tmp/slony-regress.mqgltsdtx/archive_logs_2/slony1_log_2_000000000000=
00000004.sql
purge /tmp/slony-regress.mqgltsdtx/archive_logs_2/slony1_log_2_000000000000=
00000005.sql
purge /tmp/slony-regress.mqgltsdtx/archive_logs_2/slony1_log_2_000000000000=
00000006.sql
purge /tmp/slony-regress.mqgltsdtx/archive_logs_2/slony1_log_2_000000000000=
00000007.sql
purge /tmp/slony-regress.mqgltsdtx/archive_logs_2/slony1_log_2_000000000000=
00000008.sql
purge /tmp/slony-regress.mqgltsdtx/archive_logs_2/slony1_log_2_000000000000=
00000009.sql
purge /tmp/slony-regress.mqgltsdtx/archive_logs_2/slony1_log_2_000000000000=
00000010.sql
purge /tmp/slony-regress.mqgltsdtx/archive_logs_2/slony1_log_2_000000000000=
00000011.sql
purge /tmp/slony-regress.mqgltsdtx/archive_logs_2/slony1_log_2_000000000000=
00000012.sql
pull log shipping dump
WARNING:  nonstandard use of \\ in a string literal at character 8
HINT:  Use the escape string syntax for backslashes, e.g., E'\\'.
load schema for replicated tables into node #3
psql:testlogship/init_schema.sql:4: NOTICE:  CREATE TABLE will create impli=
cit sequence "table1_id_seq1" for serial column "table1.id"
psql:testlogship/init_schema.sql:4: ERROR:  relation "table1" already exists
psql:testlogship/init_schema.sql:11: NOTICE:  CREATE TABLE will create impl=
icit sequence "table2_id_seq1" for serial column "table2.id"
psql:testlogship/init_schema.sql:11: ERROR:  relation "table2" already exis=
ts
psql:testlogship/init_schema.sql:16: NOTICE:  CREATE TABLE will create impl=
icit sequence "table3_id_seq1" for serial column "table3.id"
psql:testlogship/init_schema.sql:16: ERROR:  relation "table3" already exis=
ts
psql:testlogship/init_schema.sql:18: ERROR:  relation "no_good_candidate_pk=
" already exists
psql:testlogship/init_schema.sql:31: NOTICE:  CREATE TABLE will create impl=
icit sequence "table4_id_seq1" for serial column "table4.id"
psql:testlogship/init_schema.sql:31: ERROR:  relation "table4" already exis=
ts
load log shipping dump into node #3
START TRANSACTION
CREATE SCHEMA
psql:/tmp/slony-regress.mqgltsdtx/logship_dump.sql:18: NOTICE:  CREATE TABL=
E / PRIMARY KEY will create implicit index "sl_sequence-pkey" for table "sl=
_sequence_offline"
CREATE TABLE
CREATE TABLE
CREATE FUNCTION
CREATE FUNCTION
CREATE FUNCTION
INSERT 0 1
COMMIT
generate more data to test log shipping
generating 187 transactions of random data
0 %
5 %
10 %
15 %
20 %
25 %
30 %
14 rows behind
35 %
40 %
45 %
slony is caught up
50 %
55 %
60 %
65 %
70 %
75 %
80 %
85 %
90 %
95 %
100 %
done
launching polling script
loading data
waiting for nodes to catchup
 st_origin | st_received | st_last_event |      st_last_event_ts      | st_=
last_
received |    st_last_received_ts     | st_last_received_event_ts  | st_lag=
_num_
events |   st_lag_time   =

-----------+-------------+---------------+----------------------------+----=
-----
---------+----------------------------+----------------------------+-------=
-----
-------+-----------------
         1 |           2 |            34 | 2008-03-05 13:33:34.290387 |    =
     =

      33 | 2008-03-05 13:33:33.984572 | 2008-03-05 13:33:33.770364 |       =
     =

     1 | 00:00:00.630055
         1 |           4 |            34 | 2008-03-05 13:33:34.290387 |    =
     =

      28 | 2008-03-05 13:33:26.620473 | 2008-03-05 13:33:26.590639 |       =
     =

     6 | 00:00:07.80978
(2 rows)

1 rows behind
 st_origin | st_received | st_last_event |     st_last_event_ts      | st_l=
ast_r
eceived |    st_last_received_ts     | st_last_received_event_ts | st_lag_n=
um_ev
ents |   st_lag_time   =

-----------+-------------+---------------+---------------------------+-----=
-----
--------+----------------------------+---------------------------+---------=
-----
-----+-----------------
         1 |           2 |            36 | 2008-03-05 13:33:45.00072 |     =
     =

     36 | 2008-03-05 13:33:46.052075 | 2008-03-05 13:33:45.00072 |         =
     =

   0 | 00:00:09.479582
         1 |           4 |            36 | 2008-03-05 13:33:45.00072 |     =
     =

     36 | 2008-03-05 13:33:53.212123 | 2008-03-05 13:33:45.00072 |         =
     =

   0 | 00:00:09.479582
(2 rows)

done
execute DDL script
completed DDL script
Generate some more data
generating 74 transactions of random data
0 %
5 %
10 %
8 rows behind
15 %
20 %
25 %
30 %
35 %
40 %
45 %
50 %
55 %
slony is caught up
60 %
65 %
70 %
75 %
80 %
85 %
90 %
95 %
100 %
105 %
110 %
115 %
120 %
done
loading extra data to node slonyregress1
waiting for nodes to catchup
 st_origin | st_received | st_last_event |      st_last_event_ts      | st_=
last_
received |    st_last_received_ts     | st_last_received_event_ts  | st_lag=
_num_
events |   st_lag_time   =

-----------+-------------+---------------+----------------------------+----=
-----
---------+----------------------------+----------------------------+-------=
-----
-------+-----------------
         1 |           4 |            47 | 2008-03-05 13:34:12.990829 |    =
     =

      45 | 2008-03-05 13:34:06.303909 | 2008-03-05 13:34:05.360853 |       =
     =

     2 | 00:00:07.966857
         1 |           2 |            47 | 2008-03-05 13:34:12.990829 |    =
     =

      46 | 2008-03-05 13:34:12.663501 | 2008-03-05 13:34:12.480887 |       =
     =

     1 | 00:00:00.846823
(2 rows)

 st_origin | st_received | st_last_event |      st_last_event_ts      | st_=
last_
received |    st_last_received_ts     | st_last_received_event_ts  | st_lag=
_num_
events |  st_lag_time   =

-----------+-------------+---------------+----------------------------+----=
-----
---------+----------------------------+----------------------------+-------=
-----
-------+----------------
         1 |           4 |            49 | 2008-03-05 13:34:23.701003 |    =
     =

      49 | 2008-03-05 13:34:26.202799 | 2008-03-05 13:34:23.701003 |       =
     =

     0 | 00:00:09.70844
         1 |           2 |            49 | 2008-03-05 13:34:23.701003 |    =
     =

      49 | 2008-03-05 13:34:30.722688 | 2008-03-05 13:34:23.701003 |       =
     =

     0 | 00:00:09.70844
(2 rows)

done
move set to node 4
origin moved
generating 601 transactions of random data
0 %
5 %
10 %
15 %
20 %
25 %
30 %
35 %
40 %
45 %
50 %
55 %
60 %
65 %
70 %
75 %
80 %
85 %
90 %
95 %
100 %
done
loading extra data to node slonyregress4
waiting for nodes to catchup
 st_origin | st_received | st_last_event |      st_last_event_ts      | st_=
last_
received |    st_last_received_ts     | st_last_received_event_ts  | st_lag=
_num_
events |   st_lag_time   =

-----------+-------------+---------------+----------------------------+----=
-----
---------+----------------------------+----------------------------+-------=
-----
-------+-----------------
         1 |           4 |            64 | 2008-03-05 13:36:36.062688 |    =
     =

      64 | 2008-03-05 13:36:36.664834 | 2008-03-05 13:36:36.062688 |       =
     =

     0 | 00:00:06.800465
         1 |           2 |            64 | 2008-03-05 13:36:36.062688 |    =
     =

      64 | 2008-03-05 13:36:36.165865 | 2008-03-05 13:36:36.062688 |       =
     =

     0 | 00:00:06.800465
(2 rows)

 st_origin | st_received | st_last_event |      st_last_event_ts      | st_=
last_
received |    st_last_received_ts     | st_last_received_event_ts  | st_lag=
_num_
events |   st_lag_time   =

-----------+-------------+---------------+----------------------------+----=
-----
---------+----------------------------+----------------------------+-------=
-----
-------+-----------------
         1 |           2 |            66 | 2008-03-05 13:36:56.392763 |    =
     =

      66 | 2008-03-05 13:36:56.424345 | 2008-03-05 13:36:56.392763 |       =
     =

     0 | 00:00:06.548586
         1 |           4 |            66 | 2008-03-05 13:36:56.392763 |    =
     =

      66 | 2008-03-05 13:36:57.084712 | 2008-03-05 13:36:56.392763 |       =
     =

     0 | 00:00:06.548586
(2 rows)

done
final data load complete - now load files into log shipped node
/usr/bin/find: illegal option -- n
/usr/bin/find: [-H | -L] path-list predicate-list
/usr/bin/find: illegal option -- n
/usr/bin/find: [-H | -L] path-list predicate-list
Logs numbered from  to =

current sequence value: 00000000000000000000
done
getting data from origin DB for diffing
done
getting data from node 2 for diffing against origin
comparing
subscriber node 2 is the same as origin node 1
done
getting data from node 3 for diffing against origin
ERROR:  column "newcol" does not exist
LINE 1: ...col,pathcol,polycol,circcol,ipcol,maccol, bitcol, newcol, ne...
                                                             ^
comparing
./run_test.sh: WARNING: /tmp/slony-regress.mqgltsdtx/db_1.dmp /tmp/slony-re=
gress.mqgltsdtx/db_3.dmp differ, see /tmp/slony-regress.mqgltsdtx/db_diff.3=
 for details
done
getting data from node 4 for diffing against origin
comparing
subscriber node 4 is the same as origin node 1
done
**** killing slon node 1
**** killing slon node 2
**** killing slon node 4
waiting for slons to die
done
dropping database
slonyregress1
slonyregress2
slonyregress3
slonyregress4
done
there were 1 warnings during the run of testlogship/, check the files in /t=
mp/slony-regress.mqgltsdtx for more details
From liobod.slony at gmail.com  Wed Mar  5 04:14:24 2008
From: liobod.slony at gmail.com (lio bod)
Date: Wed Mar  5 04:14:54 2008
Subject: [Slony1-general] configure-replication.sh and SEQUENCES
Message-ID: <d4f444290803050414s351caef4o5243fa84084c89f3@mail.gmail.com>

Hello world,

Still having fun with supplied shell scripts :

After setting env variales, i made work configure-replication.sh

>/tmp/slonytest-temp.Q17311/create_set.slonik
>/tmp/slonytest-temp.Q17311/create_set.slonik:29: ERROR: syntax error at or
near snum

The file create_set.slonik looks like that :

[.../...]
set add table (id=3D2, set id=3D1, origin=3D1, fully qualified name=3D'
public.mytable1', comment=3D'mycluster table public.mytable1');
set add sequence (id=3D1, set id=3D1, origin=3D1, fully qualified name=3D'
public.mysequence1', comment=3D'mycluster sequence public.mysequence1');
snum=3D2
[.../...]

Where does the 'snum=3D2' comes from?
Where is mystake?

Thx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080305/=
9914cf9b/attachment-0001.htm
From stephane.schildknecht at postgresqlfr.org  Wed Mar  5 07:20:28 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Wed Mar  5 07:20:37 2008
Subject: [Slony1-general] configure-replication.sh and SEQUENCES
In-Reply-To: <d4f444290803050414s351caef4o5243fa84084c89f3@mail.gmail.com>
References: <d4f444290803050414s351caef4o5243fa84084c89f3@mail.gmail.com>
Message-ID: <47CEBA3C.4040909@postgresqlfr.org>

lio bod a ?crit :
> Hello world,
>  
> Still having fun with supplied shell scripts :
>  
> After setting env variales, i made work configure-replication.sh
>  
>>/tmp/slonytest-temp.Q17311/create_set.slonik
>>/tmp/slonytest-temp.Q17311/create_set.slonik:29: ERROR: syntax error at
> or near snum
>  
> The file create_set.slonik looks like that :
>  
> [.../...]
> set add table (id=2, set id=1, origin=1, fully qualified
> name='public.mytable1', comment='mycluster table public.mytable1');
> set add sequence (id=1, set id=1, origin=1, fully qualified
> name='public.mysequence1', comment='mycluster sequence
> public.mysequence1'); snum=2
> [.../...]
>  
> Where does the 'snum=2' comes from?

It's a bug in the script. A carriage return has disappeared on line 150.

You should have

150c150,151
<     echo "set add sequence (id=${snum}, set id=1, origin=1, fully qualified
name='${seq}', comment='${CLUSTER} sequence ${seq}');" >> $SETUPSET
snum=`expr ${snum} + 1`
---
>     echo "set add sequence (id=${snum}, set id=1, origin=1, fully qualified
name='${seq}', comment='${CLUSTER} sequence ${seq}');" >> $SETUPSET
>     snum=`expr ${snum} + 1`

Best regards,
-- 
St?phane SCHILDKNECHT
Pr?sident de PostgreSQLFr
T?l. 09 53 69 97 12
http://www.postgresqlfr.org
From cbbrowne at ca.afilias.info  Wed Mar  5 08:16:19 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Mar  5 08:16:24 2008
Subject: [Slony1-general] configure-replication.sh and SEQUENCES
In-Reply-To: <47CEBA3C.4040909@postgresqlfr.org> (=?iso-8859-1?Q?St=E9phan?=
	=?iso-8859-1?Q?e?= A. Schildknecht's message of "Wed,
	05 Mar 2008 16:20:28 +0100")
References: <d4f444290803050414s351caef4o5243fa84084c89f3@mail.gmail.com>
	<47CEBA3C.4040909@postgresqlfr.org>
Message-ID: <60k5khm9ik.fsf@dba2.int.libertyrms.com>

"St?phane A. Schildknecht" <stephane.schildknecht@postgresqlfr.org> writes:
> lio bod a ?crit :
>> Hello world,
>>  
>> Still having fun with supplied shell scripts :
>>  
>> After setting env variales, i made work configure-replication.sh
>>  
>>>/tmp/slonytest-temp.Q17311/create_set.slonik
>>>/tmp/slonytest-temp.Q17311/create_set.slonik:29: ERROR: syntax error at
>> or near snum
>>  
>> The file create_set.slonik looks like that :
>>  
>> [.../...]
>> set add table (id=2, set id=1, origin=1, fully qualified
>> name='public.mytable1', comment='mycluster table public.mytable1');
>> set add sequence (id=1, set id=1, origin=1, fully qualified
>> name='public.mysequence1', comment='mycluster sequence
>> public.mysequence1'); snum=2
>> [.../...]
>>  
>> Where does the 'snum=2' comes from?
>
> It's a bug in the script. A carriage return has disappeared on line 150.
>
> You should have
>
> 150c150,151
> <     echo "set add sequence (id=${snum}, set id=1, origin=1, fully qualified
> name='${seq}', comment='${CLUSTER} sequence ${seq}');" >> $SETUPSET
> snum=`expr ${snum} + 1`
> ---
>>     echo "set add sequence (id=${snum}, set id=1, origin=1, fully qualified
> name='${seq}', comment='${CLUSTER} sequence ${seq}');" >> $SETUPSET
>>     snum=`expr ${snum} + 1`

Fixed in REL_1_2_STABLE; it's evidently fine in CVS HEAD...
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From satya461 at gmail.com  Wed Mar  5 22:39:03 2008
From: satya461 at gmail.com (Satya)
Date: Wed Mar  5 22:39:23 2008
Subject: [Slony1-general] Slony backward compatibility with Postgres?
Message-ID: <6ccff2720803052239w6e8bc3ddvc46071eba4531325@mail.gmail.com>

Hi,

 I have compiled Slony 1.2.13 with Postgres 8.3. Can I use this slony to run
with Postgres 8.2 or 8.1 ??

 Also I have seen when doing a configure with --libdir=3D'/some/other/path'=
 ,
slony still copies its libraries (xxid.so ,slony1_funcs.so)  to the
PostgreSQL  lib directory only and same is the case with  sql files . They
always go in Postgres Share directory.

 So I wasn't able make Slony 1.2.13 which is compiled against 8.3 to work
with Postgres 8.2. I  get this error

 <stdin>:22: PGRES_FATAL_ERROR load '$libdir/xxid';  - ERROR:  could not
access file "$libdir/xxid": No such file or directory

  This is because i didn't compile slony against postgres 8.2 and couldn't
xxid.so in PG 8.2 lib directory.


  Is there a way I can install Slony which is compiled using Postgres
8.3installed *
*completely** in a different location (including lib) and use the same Slony
which I compiled using postgres 8.3 to  make it work with postgres 8.2 or
8.1 ??

  I am expecting some directory layout like this

  [/export/home/tmp/satya/slony-bin] 12:01:29 $ ls -RCF
.:
bin/            etc/            slon-tools.pm  lib/

./bin:
show_configuration*        slonik*
slonik_execute_script*     slonik_restart_node*       slony_logshipper*
slon*                      slonik_build_env*
slonik_failover*           slonik_store_node*
slony_show_configuration*
slon_kill*                 slonik_create_set*
slonik_init_cluster*       slonik_subscribe_set*
slon_start*                slonik_drop_node*
slonik_merge_sets*         slonik_uninstall_nodes*
slon_watchdog*             slonik_drop_set*
slonik_move_set*           slonik_unsubscribe_set*
slon_watchdog2*            slonik_drop_table*
slonik_print_preamble*     slonik_update_nodes*

./etc:
slon_tools.conf-sample

./lib:
slony1_funcs.so*  xxid.so*


Hope you understood the problem. Please do let me know if you need more
information.

Thanks
Satya.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080306/=
112c022d/attachment.htm
From liobod.slony at gmail.com  Wed Mar  5 23:50:08 2008
From: liobod.slony at gmail.com (lio bod)
Date: Wed Mar  5 23:50:30 2008
Subject: [Slony1-general] configure-replication.sh and SEQUENCES
In-Reply-To: <60k5khm9ik.fsf@dba2.int.libertyrms.com>
References: <d4f444290803050414s351caef4o5243fa84084c89f3@mail.gmail.com>
	<47CEBA3C.4040909@postgresqlfr.org>
	<60k5khm9ik.fsf@dba2.int.libertyrms.com>
Message-ID: <d4f444290803052350h2b882cfcvb39efe4f866756ca@mail.gmail.com>

Thx for the fix.

I havn't test the last HEAD release yet. I 'm afraid i won' t have any time
for this by now.
The generated *.slonik files helps me as sample that i modify with my little
hands...

Btw, the store_paths.slonik file looks weird :

STORE PATH (SERVER=3D1, CLIENT=3D2, CONNINFO=3D'dbname=3Dmydb host=3Dmyhost1
user=3Dpgslony port=3D5432'); else
STORE PATH (SERVER=3D1, CLIENT=3D2, CONNINFO=3D'dbname=3Dmydb host=3Dmyhost2
user=3Dpgslony port=3D5432');
STORE PATH (SERVER=3D2, CLIENT=3D1, CONNINFO=3D'dbname=3Dmydb host=3Dmyhost2
user=3Dpgslony port=3D5432'); else
STORE PATH (SERVER=3D2, CLIENT=3D1, CONNINFO=3D'dbname=3Dmydb host=3Dmyhost2
user=3Dpgslony port=3D5432');

and i'm not really estonished by error produced :

$> slonik store_paths.slonik
store_paths.slonik:2: ERROR: syntax error at or near else

another bug or wrong use?


I go on on with with some idea of improvement even if i guess they are so
obvious that they might already be in your roadmap :
- why not separate files among thiner functionnality : 1 file *.slonik file
for creating cluster, another for nodes, another for set, another for
tables, another for sequences , ..., what else?, and subscription
- why not produces undo *.slonik files (with same granularity) : remove
subscription, ..., untill remove cluster (even i still not sure of my self
how to make so with bare metal).

With full respect due slony people work (I do know time is nerve war),

rgds


2008/3/5, Christopher Browne <cbbrowne@ca.afilias.info>:
>
> "St=E9phane A. Schildknecht" <stephane.schildknecht@postgresqlfr.org>
> writes:
> > lio bod a =E9crit :
> >> Hello world,
> >>
> >> Still having fun with supplied shell scripts :
> >>
> >> After setting env variales, i made work configure-replication.sh
> >>
> >>>/tmp/slonytest-temp.Q17311/create_set.slonik
> >>>/tmp/slonytest-temp.Q17311/create_set.slonik:29: ERROR: syntax error at
> >> or near snum
> >>
> >> The file create_set.slonik looks like that :
> >>
> >> [.../...]
> >> set add table (id=3D2, set id=3D1, origin=3D1, fully qualified
> >> name=3D'public.mytable1', comment=3D'mycluster table public.mytable1');
> >> set add sequence (id=3D1, set id=3D1, origin=3D1, fully qualified
> >> name=3D'public.mysequence1', comment=3D'mycluster sequence
> >> public.mysequence1'); snum=3D2
> >> [.../...]
> >>
> >> Where does the 'snum=3D2' comes from?
> >
> > It's a bug in the script. A carriage return has disappeared on line 150.
> >
> > You should have
> >
> > 150c150,151
> > <     echo "set add sequence (id=3D${snum}, set id=3D1, origin=3D1, ful=
ly
> qualified
> > name=3D'${seq}', comment=3D'${CLUSTER} sequence ${seq}');" >> $SETUPSET
> > snum=3D`expr ${snum} + 1`
> > ---
> >>     echo "set add sequence (id=3D${snum}, set id=3D1, origin=3D1, fully
> qualified
> > name=3D'${seq}', comment=3D'${CLUSTER} sequence ${seq}');" >> $SETUPSET
> >>     snum=3D`expr ${snum} + 1`
>
> Fixed in REL_1_2_STABLE; it's evidently fine in CVS HEAD...
> --
> select 'cbbrowne' || '@' || 'ca.afilias.info';
> <http://dba2.int.libertyrms.com/>
> Christopher Browne
> (416) 673-4124 (land)
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080306/=
d19a637d/attachment.htm
From liobod.slony at gmail.com  Thu Mar  6 02:55:12 2008
From: liobod.slony at gmail.com (lio bod)
Date: Thu Mar  6 02:55:37 2008
Subject: [Slony1-general] configure-replication.sh and SEQUENCES
In-Reply-To: <d4f444290803052350h2b882cfcvb39efe4f866756ca@mail.gmail.com>
References: <d4f444290803050414s351caef4o5243fa84084c89f3@mail.gmail.com>
	<47CEBA3C.4040909@postgresqlfr.org>
	<60k5khm9ik.fsf@dba2.int.libertyrms.com>
	<d4f444290803052350h2b882cfcvb39efe4f866756ca@mail.gmail.com>
Message-ID: <d4f444290803060255g507cd02av6c90e526b0a28a59@mail.gmail.com>

I go on : my generated create_set.slonik has always the same id for
sequences :

[.../...]
set add sequence (id=3D1, set id=3D1, origin=3D1, fully qualified name=3D'
public.mysequence1', comment=3D'mycluster sequence public.mysequence1');
set add sequence (id=3D1, set id=3D1, origin=3D1, fully qualified name=3D'
public.mysequence2', comment=3D'mycluster sequence public.mysequence2');
[.../...]

and it's badly appreciate by slonik :

$> slonik create_set.slonik
slonik create_set.slonik :3: PGRES_FATAL_ERROR select
"_mycluster".setAddSequence(1, 1, 'public.mysequence2', 'mycluster sequence
public.mysequence2');  - ERREUR:  Slony-I: setAddSequence_int(): sequence ID
1 has already been assigned
CONTEXT:  PL/pgSQL function "setaddsequence" line 36 at perform


For info, i run slony from a rpm
http://yum.pgsqlrpms.org/7.4/redhat/rhel-4-i386/slony1-1.2.12-2.rhel4.i686.=
rpm
And i can't really afford to be update from slony CVS, so may be things are
already fixed.

May the Force be with you, **


2008/3/6, lio bod <liobod.slony@gmail.com>:
>
> Thx for the fix.
>
> I havn't test the last HEAD release yet. I 'm afraid i won' t have any
> time for this by now.
> The generated *.slonik files helps me as sample that i modify with my
> little hands...
>
> Btw, the store_paths.slonik file looks weird :
>
> STORE PATH (SERVER=3D1, CLIENT=3D2, CONNINFO=3D'dbname=3Dmydb host=3Dmyho=
st1
> user=3Dpgslony port=3D5432'); else
> STORE PATH (SERVER=3D1, CLIENT=3D2, CONNINFO=3D'dbname=3Dmydb host=3Dmyho=
st2
> user=3Dpgslony port=3D5432');
> STORE PATH (SERVER=3D2, CLIENT=3D1, CONNINFO=3D'dbname=3Dmydb host=3Dmyho=
st2
> user=3Dpgslony port=3D5432'); else
> STORE PATH (SERVER=3D2, CLIENT=3D1, CONNINFO=3D'dbname=3Dmydb host=3Dmyho=
st2
> user=3Dpgslony port=3D5432');
>
> and i'm not really estonished by error produced :
>
> $> slonik store_paths.slonik
> store_paths.slonik:2: ERROR: syntax error at or near else
>
> another bug or wrong use?
>
>
> I go on on with with some idea of improvement even if i guess they are so
> obvious that they might already be in your roadmap :
> - why not separate files among thiner functionnality : 1 file *.slonik
> file for creating cluster, another for nodes, another for set, another for
> tables, another for sequences , ..., what else?, and subscription
> - why not produces undo *.slonik files (with same granularity) : remove
> subscription, ..., untill remove cluster (even i still not sure of my self
> how to make so with bare metal).
>
> With full respect due slony people work (I do know time is nerve war),
>
> rgds
>
>
> 2008/3/5, Christopher Browne <cbbrowne@ca.afilias.info>:
> >
> > "St=E9phane A. Schildknecht" <stephane.schildknecht@postgresqlfr.org>
> > writes:
> > > lio bod a =E9crit :
> > >> Hello world,
> > >>
> > >> Still having fun with supplied shell scripts :
> > >>
> > >> After setting env variales, i made work configure-replication.sh
> > >>
> > >>>/tmp/slonytest-temp.Q17311/create_set.slonik
> > >>>/tmp/slonytest-temp.Q17311/create_set.slonik:29: ERROR: syntax error
> > at
> > >> or near snum
> > >>
> > >> The file create_set.slonik looks like that :
> > >>
> > >> [.../...]
> > >> set add table (id=3D2, set id=3D1, origin=3D1, fully qualified
> > >> name=3D'public.mytable1', comment=3D'mycluster table public.mytable1=
');
> > >> set add sequence (id=3D1, set id=3D1, origin=3D1, fully qualified
> > >> name=3D'public.mysequence1', comment=3D'mycluster sequence
> > >> public.mysequence1'); snum=3D2
> > >> [.../...]
> > >>
> > >> Where does the 'snum=3D2' comes from?
> > >
> > > It's a bug in the script. A carriage return has disappeared on line
> > 150.
> > >
> > > You should have
> > >
> > > 150c150,151
> > > <     echo "set add sequence (id=3D${snum}, set id=3D1, origin=3D1, f=
ully
> > qualified
> > > name=3D'${seq}', comment=3D'${CLUSTER} sequence ${seq}');" >> $SETUPS=
ET
> > > snum=3D`expr ${snum} + 1`
> > > ---
> > >>     echo "set add sequence (id=3D${snum}, set id=3D1, origin=3D1, fu=
lly
> > qualified
> > > name=3D'${seq}', comment=3D'${CLUSTER} sequence ${seq}');" >> $SETUPS=
ET
> > >>     snum=3D`expr ${snum} + 1`
> >
> > Fixed in REL_1_2_STABLE; it's evidently fine in CVS HEAD...
> > --
> > select 'cbbrowne' || '@' || 'ca.afilias.info';
> > <http://dba2.int.libertyrms.com/>
> > Christopher Browne
> > (416) 673-4124 (land)
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080306/=
74c8a7d5/attachment-0001.htm
From vivek at khera.org  Thu Mar  6 07:38:31 2008
From: vivek at khera.org (Vivek Khera)
Date: Thu Mar  6 07:38:36 2008
Subject: [Slony1-general] Slony backward compatibility with Postgres?
In-Reply-To: <6ccff2720803052239w6e8bc3ddvc46071eba4531325@mail.gmail.com>
References: <6ccff2720803052239w6e8bc3ddvc46071eba4531325@mail.gmail.com>
Message-ID: <5CD0BE1E-5CD4-426D-BE58-D28284C57611@khera.org>


On Mar 6, 2008, at 1:39 AM, Satya wrote:

>  So I wasn't able make Slony 1.2.13 which is compiled against 8.3 to  
> work with Postgres 8.2. I  get this error
>
>  <stdin>:22: PGRES_FATAL_ERROR load '$libdir/xxid';  - ERROR:  could  
> not access file "$libdir/xxid": No such file or directory
>

Slony has two parts: the server component and the controlling daemon.   
You didn't build the server component for your 8.1 or 8.2 database  
server.

From liobod.slony at gmail.com  Thu Mar  6 08:28:39 2008
From: liobod.slony at gmail.com (lio bod)
Date: Thu Mar  6 08:28:47 2008
Subject: [Slony1-general] configure-replication.sh and SEQUENCES
In-Reply-To: <d4f444290803060255g507cd02av6c90e526b0a28a59@mail.gmail.com>
References: <d4f444290803050414s351caef4o5243fa84084c89f3@mail.gmail.com>
	<47CEBA3C.4040909@postgresqlfr.org>
	<60k5khm9ik.fsf@dba2.int.libertyrms.com>
	<d4f444290803052350h2b882cfcvb39efe4f866756ca@mail.gmail.com>
	<d4f444290803060255g507cd02av6c90e526b0a28a59@mail.gmail.com>
Message-ID: <d4f444290803060828sebb22bem86f48fccd92a862a@mail.gmail.com>

On the doc on line (http://slony.info/documentation/adminscripts.html 19.6.=
3.
Resulting slonik scripts)

it's said the frest step is create_set.slonik (would it mean
create_nodes.slonik?)
then store_paths.slonik
then create_set.slonik (once more?)

"D'oh!"
2008/3/6, lio bod liobod.slony@gmail.com:



> I go on : my generated create_set.slonik has always the same id for
> sequences :
>
> [.../...]
> set add sequence (id=3D1, set id=3D1, origin=3D1, fully qualified name=3D'
> public.mysequence1', comment=3D'mycluster sequence public.mysequence1');
> set add sequence (id=3D1, set id=3D1, origin=3D1, fully qualified name=3D'
> public.mysequence2', comment=3D'mycluster sequence public.mysequence2');
> [.../...]
>
> and it's badly appreciate by slonik :
>
> $> slonik create_set.slonik
> slonik create_set.slonik :3: PGRES_FATAL_ERROR select
> "_mycluster".setAddSequence(1, 1, 'public.mysequence2', 'mycluster
> sequence public.mysequence2');  - ERREUR:  Slony-I: setAddSequence_int():
> sequence ID 1 has already been assigned
> CONTEXT:  PL/pgSQL function "setaddsequence" line 36 at perform
>
>
> For info, i run slony from a rpm
> http://yum.pgsqlrpms.org/7.4/redhat/rhel-4-i386/slony1-1.2.12-2.rhel4.i68=
6.rpm
> And i can't really afford to be update from slony CVS, so may be things
> are already fixed.
>
> May the Force be with you, **
>
>
> 2008/3/6, lio bod <liobod.slony@gmail.com>:
> >
> > Thx for the fix.
> >
> > I havn't test the last HEAD release yet. I 'm afraid i won' t have any
> > time for this by now.
> > The generated *.slonik files helps me as sample that i modify with my
> > little hands...
> >
> > Btw, the store_paths.slonik file looks weird :
> >
> > STORE PATH (SERVER=3D1, CLIENT=3D2, CONNINFO=3D'dbname=3Dmydb host=3Dmy=
host1
> > user=3Dpgslony port=3D5432'); else
> > STORE PATH (SERVER=3D1, CLIENT=3D2, CONNINFO=3D'dbname=3Dmydb host=3Dmy=
host2
> > user=3Dpgslony port=3D5432');
> > STORE PATH (SERVER=3D2, CLIENT=3D1, CONNINFO=3D'dbname=3Dmydb host=3Dmy=
host2
> > user=3Dpgslony port=3D5432'); else
> > STORE PATH (SERVER=3D2, CLIENT=3D1, CONNINFO=3D'dbname=3Dmydb host=3Dmy=
host2
> > user=3Dpgslony port=3D5432');
> >
> > and i'm not really estonished by error produced :
> >
> > $> slonik store_paths.slonik
> > store_paths.slonik:2: ERROR: syntax error at or near else
> >
> > another bug or wrong use?
> >
> >
> > I go on on with with some idea of improvement even if i guess they are
> > so obvious that they might already be in your roadmap :
> > - why not separate files among thiner functionnality : 1 file *.slonik
> > file for creating cluster, another for nodes, another for set, another =
for
> > tables, another for sequences , ..., what else?, and subscription
> > - why not produces undo *.slonik files (with same granularity) : remove
> > subscription, ..., untill remove cluster (even i still not sure of my s=
elf
> > how to make so with bare metal).
> >
> > With full respect due slony people work (I do know time is nerve war),
> >
> > rgds
> >
> >
> > 2008/3/5, Christopher Browne <cbbrowne@ca.afilias.info>:
> > >
> > > "St=E9phane A. Schildknecht" <stephane.schildknecht@postgresqlfr.org>
> > > writes:
> > > > lio bod a =E9crit :
> > > >> Hello world,
> > > >>
> > > >> Still having fun with supplied shell scripts :
> > > >>
> > > >> After setting env variales, i made work configure-replication.sh
> > > >>
> > > >>>/tmp/slonytest-temp.Q17311/create_set.slonik
> > > >>>/tmp/slonytest-temp.Q17311/create_set.slonik:29: ERROR: syntax
> > > error at
> > > >> or near snum
> > > >>
> > > >> The file create_set.slonik looks like that :
> > > >>
> > > >> [.../...]
> > > >> set add table (id=3D2, set id=3D1, origin=3D1, fully qualified
> > > >> name=3D'public.mytable1', comment=3D'mycluster table public.mytabl=
e1');
> > > >> set add sequence (id=3D1, set id=3D1, origin=3D1, fully qualified
> > > >> name=3D'public.mysequence1', comment=3D'mycluster sequence
> > > >> public.mysequence1'); snum=3D2
> > > >> [.../...]
> > > >>
> > > >> Where does the 'snum=3D2' comes from?
> > > >
> > > > It's a bug in the script. A carriage return has disappeared on line
> > > 150.
> > > >
> > > > You should have
> > > >
> > > > 150c150,151
> > > > <     echo "set add sequence (id=3D${snum}, set id=3D1, origin=3D1,=
 fully
> > > qualified
> > > > name=3D'${seq}', comment=3D'${CLUSTER} sequence ${seq}');" >> $SETU=
PSET
> > > > snum=3D`expr ${snum} + 1`
> > > > ---
> > > >>     echo "set add sequence (id=3D${snum}, set id=3D1, origin=3D1, =
fully
> > > qualified
> > > > name=3D'${seq}', comment=3D'${CLUSTER} sequence ${seq}');" >> $SETU=
PSET
> > > >>     snum=3D`expr ${snum} + 1`
> > >
> > > Fixed in REL_1_2_STABLE; it's evidently fine in CVS HEAD...
> > > --
> > > select 'cbbrowne' || '@' || 'ca.afilias.info';
> > > <http://dba2.int.libertyrms.com/>
> > > Christopher Browne
> > > (416) 673-4124 (land)
> > >
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080306/=
85210ad0/attachment.htm
From cbbrowne at ca.afilias.info  Thu Mar  6 10:51:38 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Mar  6 10:51:48 2008
Subject: [Slony1-general] configure-replication.sh and SEQUENCES
In-Reply-To: <d4f444290803052350h2b882cfcvb39efe4f866756ca@mail.gmail.com>
	(lio bod's message of "Thu, 6 Mar 2008 08:50:08 +0100")
References: <d4f444290803050414s351caef4o5243fa84084c89f3@mail.gmail.com>
	<47CEBA3C.4040909@postgresqlfr.org>
	<60k5khm9ik.fsf@dba2.int.libertyrms.com>
	<d4f444290803052350h2b882cfcvb39efe4f866756ca@mail.gmail.com>
Message-ID: <60ejanlm85.fsf@dba2.int.libertyrms.com>

"lio bod" <liobod.slony@gmail.com> writes:
> Thx for the fix.

> I havn't test the last HEAD release yet. I 'm afraid?i won' t have any time for this by now.
>
> The generated *.slonik files helps me as sample that i modify with my little hands...
>
> ?
>
> Btw, the store_paths.slonik file looks weird :
>
> ?
>
> STORE PATH (SERVER=1, CLIENT=2, CONNINFO='dbname=mydb host=myhost1 user=pgslony port=5432'); else
> STORE PATH (SERVER=1, CLIENT=2, CONNINFO='dbname=mydb host=myhost2 user=pgslony port=5432');
> STORE PATH (SERVER=2, CLIENT=1, CONNINFO='dbname=mydb host=myhost2 user=pgslony port=5432'); else
> STORE PATH (SERVER=2, CLIENT=1, CONNINFO='dbname=mydb host=myhost2 user=pgslony port=5432');
>
> ?
>
> and i'm not really estonished by error produced :
>
> ?
>
> $> slonik store_paths.slonik
> store_paths.slonik:2: ERROR: syntax error at or near else
>
> another bug or wrong use?

Hmm.  That's the same sort of bug that you pointed out before, in
another spot.  I'm not sure how that broke in version 1.2.
Conceivably a cut'n'paste problem?

> I go on on with with some idea of improvement even if i guess they
> are so obvious that they might already be in your roadmap :
>
> - why not separate files among thiner functionnality : 1 file
> *.slonik file for creating cluster, another for nodes, another for
> set, another for tables, another for sequences , ..., what else?,
> and subscription

The purpose of the script was to configure replication fairly simply.
I'm not sure I want to add every conceivable bit of extra
functionality.

> - why not produces undo *.slonik files (with same?granularity) :
> remove subscription, ..., untill remove cluster (even i still not
> sure of my self how to make so with bare metal).

Why not?  Because that doesn't fit with the name:
configure_replication.sh.

I don't want to complicate it for the sake of complicating it.

We already have the whole set of "altperl" scripts that have grown
challenging to support because:

 1.  A lot of the "mature users" find it simpler to go straight to
     writing slonik scripts.

 2.  The "altperl" scripts have grown to require a pretty
     sophisticated set of Perl data structures to describe clusters, 
     which require that the users do some sophisticated reading,
     and nonetheless, the scripts make a whole lot of simplifying
     assumptions about the "shape" of the cluster, and therefore
     aren't anywhere near "fully general" in being able to express
     all the sorts of things that someone might want to do to a
     cluster.

     (Notably: if you have a pretty complex network structure, Slony-I
     can cope with this, but "altperl" has no way to represent that.)

I'm not sure what to do with/about the "altperl" tools...  They get
just enough "patch" attention from users that it would seem unjust to
deprecate them, but they aren't being really properly maintained, all
the same.  I decline to be the "proper maintainer;" if there isn't
enough community interest, I think they *should* get deprecated.

I think I'd prefer to keep "configure_replication.sh" on the simple
side, so that it's clear that it's a "crutch" to help, as opposed to
trying to make it the "comprehensive solution" that it really can't
be.
-- 
(format nil "~S@~S" "cbbrowne" "cbbrowne.com")
http://linuxdatabases.info/info/linux.html
WARNING - the content of this message may be erroneous, misspelled and
perhaps even flammable.  It also contains small parts that could cause
asphyxiation.  NOT RECOMMENDED FOR CHILDREN UNDER 3 YEARS OF AGE
From cbbrowne at ca.afilias.info  Thu Mar  6 10:57:13 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Mar  6 10:57:20 2008
Subject: [Slony1-general] Slony backward compatibility with Postgres?
In-Reply-To: <6ccff2720803052239w6e8bc3ddvc46071eba4531325@mail.gmail.com>
	(Satya's message of "Thu, 6 Mar 2008 12:09:03 +0530")
References: <6ccff2720803052239w6e8bc3ddvc46071eba4531325@mail.gmail.com>
Message-ID: <60ablbllyu.fsf@dba2.int.libertyrms.com>

Satya <satya461@gmail.com> writes:
> ?I have compiled Slony 1.2.13 with Postgres 8.3. Can I use this slony to run with Postgres 8.2 or 8.1 ??
> ?Also I have seen when doing a configure with --libdir='/some/other/path' , slony still copies its libraries (xxid.so ,slony1_funcs.so)? to the PostgreSQL? lib directory
> only and same is the case with? sql files . They always go in Postgres Share directory.
> ?So I wasn't able make Slony 1.2.13 which is compiled against 8.3 to work with Postgres 8.2. I? get this error
> ?<stdin>:22: PGRES_FATAL_ERROR load '$libdir/xxid';? - ERROR:? could not access file "$libdir/xxid": No such file or directory
> ? This is because i didn't compile slony against postgres 8.2 and couldn't xxid.so in PG 8.2 lib directory.
> ? Is there a way I can install Slony which is compiled using Postgres 8.3 installed *completely* in a different location (including lib) and use the same Slony which I
> compiled using postgres 8.3 to? make it work with postgres 8.2 or 8.1 ??
> ? I am expecting some directory layout like this
> ? [/export/home/tmp/satya/slony-bin] 12:01:29 $ ls -RCF
> .:
> bin/??????????? etc/??????????? slon-tools.pm? lib/
> ./bin:
> show_configuration*??????? slonik*??????????????????? slonik_execute_script*???? slonik_restart_node*?????? slony_logshipper*
> slon*????????????????????? slonik_build_env*????????? slonik_failover*?????????? slonik_store_node*???????? slony_show_configuration*
> slon_kill*???????????????? slonik_create_set*???????? slonik_init_cluster*?????? slonik_subscribe_set*
> slon_start*??????????????? slonik_drop_node*????????? slonik_merge_sets*???????? slonik_uninstall_nodes*
> slon_watchdog*???????????? slonik_drop_set*?????????? slonik_move_set*?????????? slonik_unsubscribe_set*
> slon_watchdog2*??????????? slonik_drop_table*???????? slonik_print_preamble*???? slonik_update_nodes*
> ./etc:
> slon_tools.conf-sample
> ./lib:
> slony1_funcs.so*? xxid.so*?
> Hope you understood the problem. Please do let me know if you need more information.

You will need to recompile Slony-I against 8.1 or 8.2 if you want to
use it with them, notably so that you get .so files that are
compatible with those versions.

There is no problem with running the pl/pgsql code against varying
PostgreSQL releases, and it should be fine to use the executable
binaries (e.g. - slon, slonik) against a different version of
PostgreSQL, but the .so files likely need to be compiled "custom" for
each release...
-- 
(format nil "~S@~S" "cbbrowne" "cbbrowne.com")
http://linuxdatabases.info/info/emacs.html
"Not  me, guy. I  read the  Bash man  page each  day like  a Jehovah's
Witness reads  the Bible.  No  wait, the Bash  man page IS  the bible.
Excuse    me..."    (More   on    confusing   aliases,    taken   from
comp.os.linux.misc)
From satya461 at gmail.com  Thu Mar  6 22:16:11 2008
From: satya461 at gmail.com (Satya)
Date: Thu Mar  6 22:16:33 2008
Subject: [Slony1-general] Slony backward compatibility with Postgres?
In-Reply-To: <60ablbllyu.fsf@dba2.int.libertyrms.com>
References: <6ccff2720803052239w6e8bc3ddvc46071eba4531325@mail.gmail.com>
	<60ablbllyu.fsf@dba2.int.libertyrms.com>
Message-ID: <6ccff2720803062216t576dc089ob01070ea8882484a@mail.gmail.com>

On Fri, Mar 7, 2008 at 12:27 AM, Christopher Browne <
cbbrowne@ca.afilias.info> wrote:

> Satya <satya461@gmail.com> writes:
> >  I have compiled Slony 1.2.13 with Postgres 8.3. Can I use this slony to
> run with Postgres 8.2 or 8.1 ??
> >  Also I have seen when doing a configure with
> --libdir=3D'/some/other/path' , slony still copies its libraries (xxid.so=
,slony1_funcs.so)  to the PostgreSQL  lib directory
> > only and same is the case with  sql files . They always go in Postgres
> Share directory.
> >  So I wasn't able make Slony 1.2.13 which is compiled against 8.3 to
> work with Postgres 8.2. I  get this error
> >  <stdin>:22: PGRES_FATAL_ERROR load '$libdir/xxid';  - ERROR:  could not
> access file "$libdir/xxid": No such file or directory
> >   This is because i didn't compile slony against postgres 8.2 and
> couldn't xxid.so in PG 8.2 lib directory.
> >   Is there a way I can install Slony which is compiled using Postgres
> 8.3 installed *completely* in a different location (including lib) and use
> the same Slony which I
> > compiled using postgres 8.3 to  make it work with postgres 8.2 or 8.1 ??
> >   I am expecting some directory layout like this
> >   [/export/home/tmp/satya/slony-bin] 12:01:29 $ ls -RCF
> > .:
> > bin/            etc/            slon-tools.pm  lib/
> > ./bin:
> > show_configuration*        slonik*
> slonik_execute_script*     slonik_restart_node*       slony_logshipper*
> > slon*                      slonik_build_env*
> slonik_failover*           slonik_store_node*
> slony_show_configuration*
> > slon_kill*                 slonik_create_set*
> slonik_init_cluster*       slonik_subscribe_set*
> > slon_start*                slonik_drop_node*
> slonik_merge_sets*         slonik_uninstall_nodes*
> > slon_watchdog*             slonik_drop_set*
> slonik_move_set*           slonik_unsubscribe_set*
> > slon_watchdog2*            slonik_drop_table*
> slonik_print_preamble*     slonik_update_nodes*
> > ./etc:
> > slon_tools.conf-sample
> > ./lib:
> > slony1_funcs.so*  xxid.so*
> > Hope you understood the problem. Please do let me know if you need more
> information.
>
> You will need to recompile Slony-I against 8.1 or 8.2 if you want to
> use it with them, notably so that you get .so files that are
> compatible with those versions.
>
> There is no problem with running the pl/pgsql code against varying
> PostgreSQL releases, and it should be fine to use the executable
> binaries (e.g. - slon, slonik) against a different version of
> PostgreSQL, but the .so files likely need to be compiled "custom" for
> each release...


 Ok what I understood is  that  I need to build the shared libraries (
xxid.so nad slon1_funcs.so) for each version of Postgres 8.1 or 8.3
 and i can replace them so that I can use with other versions.

 I have a problem here. The shared libraries always reside in postgres lib
directory lets say here /usr/postgres/8.3/lib/ and when I wanted to use the
same slon binaries to use with /usr/postgres/8.2/bin, I have to copy
xxid.soand slony1_funcs.so to /usr/postgres/8.2/lib  to make it work
with Postgres
8.2

  Instead it would be nice to have  all the slony  binaries and libraries
under  a  single directory so that I can easily keep overwriting the .so
files.

  In a simple way I want to know 'Is it possible to install  xxid.so ,
slony1_funcs.so and .sql files in some other location of my choice?

 Thanks for the inputs given until now.

-Satya

>
> --
> (format nil "~S@~S" "cbbrowne" "cbbrowne.com")
> http://linuxdatabases.info/info/emacs.html
> "Not  me, guy. I  read the  Bash man  page each  day like  a Jehovah's
> Witness reads  the Bible.  No  wait, the Bash  man page IS  the bible.
> Excuse    me..."    (More   on    confusing   aliases,    taken   from
> comp.os.linux.misc)
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080307/=
6ce9cfce/attachment.htm
From cedric.villemain at dalibo.com  Fri Mar  7 00:57:46 2008
From: cedric.villemain at dalibo.com (=?iso-8859-1?q?C=E9dric_Villemain?=)
Date: Fri Mar  7 00:58:31 2008
Subject: [Slony1-general] configure-replication.sh and SEQUENCES
In-Reply-To: <60ejanlm85.fsf@dba2.int.libertyrms.com>
References: <d4f444290803050414s351caef4o5243fa84084c89f3@mail.gmail.com>
	<d4f444290803052350h2b882cfcvb39efe4f866756ca@mail.gmail.com>
	<60ejanlm85.fsf@dba2.int.libertyrms.com>
Message-ID: <200803070957.57589.cedric.villemain@dalibo.com>

Le Thursday 06 March 2008, Christopher Browne a ?crit?:
> "lio bod" <liobod.slony@gmail.com> writes:
> > Thx for the fix.
> >
> > I havn't test the last HEAD release yet. I 'm afraid?i won' t have any
> > time for this by now.
> >
> > The generated *.slonik files helps me as sample that i modify with my
> > little hands...
> >
> > ?
> >
> > Btw, the store_paths.slonik file looks weird :
> >
> > ?
> >
> > STORE PATH (SERVER=1, CLIENT=2, CONNINFO='dbname=mydb host=myhost1
> > user=pgslony port=5432'); else STORE PATH (SERVER=1, CLIENT=2,
> > CONNINFO='dbname=mydb host=myhost2 user=pgslony port=5432'); STORE PATH
> > (SERVER=2, CLIENT=1, CONNINFO='dbname=mydb host=myhost2 user=pgslony
> > port=5432'); else STORE PATH (SERVER=2, CLIENT=1, CONNINFO='dbname=mydb
> > host=myhost2 user=pgslony port=5432');
> >
> > ?
> >
> > and i'm not really estonished by error produced :
> >
> > ?
> >
> > $> slonik store_paths.slonik
> > store_paths.slonik:2: ERROR: syntax error at or near else
> >
> > another bug or wrong use?
>
> Hmm.  That's the same sort of bug that you pointed out before, in
> another spot.  I'm not sure how that broke in version 1.2.
> Conceivably a cut'n'paste problem?
>
> > I go on on with with some idea of improvement even if i guess they
> > are so obvious that they might already be in your roadmap :
> >
> > - why not separate files among thiner functionnality : 1 file
> > *.slonik file for creating cluster, another for nodes, another for
> > set, another for tables, another for sequences , ..., what else?,
> > and subscription
>
> The purpose of the script was to configure replication fairly simply.
> I'm not sure I want to add every conceivable bit of extra
> functionality.
>
> > - why not produces undo *.slonik files (with same?granularity) :
> > remove subscription, ..., untill remove cluster (even i still not
> > sure of my self how to make so with bare metal).
>
> Why not?  Because that doesn't fit with the name:
> configure_replication.sh.
>
> I don't want to complicate it for the sake of complicating it.
>
> We already have the whole set of "altperl" scripts that have grown
> challenging to support because:
>
>  1.  A lot of the "mature users" find it simpler to go straight to
>      writing slonik scripts.
>
>  2.  The "altperl" scripts have grown to require a pretty
>      sophisticated set of Perl data structures to describe clusters,
>      which require that the users do some sophisticated reading,
>      and nonetheless, the scripts make a whole lot of simplifying
>      assumptions about the "shape" of the cluster, and therefore
>      aren't anywhere near "fully general" in being able to express
>      all the sorts of things that someone might want to do to a
>      cluster.
>
>      (Notably: if you have a pretty complex network structure, Slony-I
>      can cope with this, but "altperl" has no way to represent that.)
>
> I'm not sure what to do with/about the "altperl" tools...  They get
> just enough "patch" attention from users that it would seem unjust to
> deprecate them, but they aren't being really properly maintained, all
> the same.  I decline to be the "proper maintainer;" if there isn't
> enough community interest, I think they *should* get deprecated.

Slony release when main devs think it is time to do, and as you said, 
perltools are not maintain by core-dev. 
I find them usefull for my first approach of slony and wish them to keep 
up-to-date, or *at least* to be sync with a specific release. So, looking at 
the slony 'roadmap', perhaps it can be better to push tools scripts to 
another place : a pgfoundry or a slony-tools-1.2.X.tar. 
Then, it let 'mature users' get the core, and other users get the 
contrib-tools wich are *sync* and released with slony versions.

I prefer have to wget slony-1.2.13, slony-doc-1.2.13, slony-tools-1.2.13 
archives, but be sure they work all together. Than getting one package with 
doc from 1.2.12, tools between 1.1 and 1.2.12 and core from 1.2.13.

It can consolidate them by releasing each part singly (even if at the same 
time).

Views ?


>
> I think I'd prefer to keep "configure_replication.sh" on the simple
> side, so that it's clear that it's a "crutch" to help, as opposed to
> trying to make it the "comprehensive solution" that it really can't
> be.



-- 
C?dric Villemain
Administrateur de Base de Donn?es
Cel: +33 (0)6 74 15 56 53
http://dalibo.com - http://dalibo.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: This is a digitally signed message part.
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080307/523598ff/attachment.pgp
From glynastill at yahoo.co.uk  Fri Mar  7 01:55:54 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Fri Mar  7 01:56:21 2008
Subject: [Slony1-general] configure-replication.sh and SEQUENCES
In-Reply-To: <200803070957.57589.cedric.villemain@dalibo.com>
Message-ID: <737401.25231.qm@web25802.mail.ukl.yahoo.com>

> Slony release when main devs think it is time to do, and as you
> said, 
> perltools are not maintain by core-dev. 
> I find them usefull for my first approach of slony and wish them to
> keep 
> up-to-date, or *at least* to be sync with a specific release. So,
> looking at 
> the slony 'roadmap', perhaps it can be better to push tools scripts
> to 
> another place : a pgfoundry or a slony-tools-1.2.X.tar. 
> Then, it let 'mature users' get the core, and other users get the 
> contrib-tools wich are *sync* and released with slony versions.
> 
> I prefer have to wget slony-1.2.13, slony-doc-1.2.13,
> slony-tools-1.2.13 
> archives, but be sure they work all together. Than getting one
> package with 
> doc from 1.2.12, tools between 1.1 and 1.2.12 and core from 1.2.13.
> 
> It can consolidate them by releasing each part singly (even if at
> the same 
> time).
> 
> Views ?

I've never used the perl tools, partially out of laziness and
partially out of not seeing the need. Instead I just write little
slonik scripts. Do they really save much time?


      ___________________________________________________________ 
Rise to the challenge for Sport Relief with Yahoo! For Good  

http://uk.promotions.yahoo.com/forgood/

From cbbrowne at ca.afilias.info  Fri Mar  7 07:44:25 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Mar  7 07:44:32 2008
Subject: [Slony1-general] configure-replication.sh and SEQUENCES
In-Reply-To: <737401.25231.qm@web25802.mail.ukl.yahoo.com> (Glyn Astill's
	message of "Fri, 7 Mar 2008 01:55:54 -0800 (PST)")
References: <737401.25231.qm@web25802.mail.ukl.yahoo.com>
Message-ID: <60tzjik086.fsf@dba2.int.libertyrms.com>

Glyn Astill <glynastill@yahoo.co.uk> writes:
>> Slony release when main devs think it is time to do, and as you
>> said, 
>> perltools are not maintain by core-dev. 
>> I find them usefull for my first approach of slony and wish them to
>> keep 
>> up-to-date, or *at least* to be sync with a specific release. So,
>> looking at 
>> the slony 'roadmap', perhaps it can be better to push tools scripts
>> to 
>> another place : a pgfoundry or a slony-tools-1.2.X.tar. 
>> Then, it let 'mature users' get the core, and other users get the 
>> contrib-tools wich are *sync* and released with slony versions.
>> 
>> I prefer have to wget slony-1.2.13, slony-doc-1.2.13,
>> slony-tools-1.2.13 
>> archives, but be sure they work all together. Than getting one
>> package with 
>> doc from 1.2.12, tools between 1.1 and 1.2.12 and core from 1.2.13.
>> 
>> It can consolidate them by releasing each part singly (even if at
>> the same 
>> time).
>> 
>> Views ?
>
> I've never used the perl tools, partially out of laziness and
> partially out of not seeing the need. Instead I just write little
> slonik scripts. Do they really save much time?

The original point of the "altperl" scripts was as something of a
crutch for people that were having a hard time wrapping their heads
around the slonik language.

Over time, some of Afilias' staff has progressed from valuing the
altperl scripts to thinking them pointless because "slonik isn't that
hard to write."  (I'm certainly in that camp, but I don't consider
myself terribly representative since I have written quite a lot of
code that generates slonik code! :-))
-- 
(reverse (concatenate 'string "ofni.sesabatadxunil" "@" "enworbbc"))
http://www3.sympatico.ca/cbbrowne/oses.html
As usual,  this being  a 1.3.x release,  I haven't even  compiled this
kernel yet.   So if it works,  you should be  doubly impressed. (Linus
Torvalds, announcing kernel 1.3.3 on the linux-kernel mailing list.)
From cedric.villemain at dalibo.com  Fri Mar  7 07:55:30 2008
From: cedric.villemain at dalibo.com (=?iso-8859-1?q?C=E9dric_Villemain?=)
Date: Fri Mar  7 07:55:50 2008
Subject: [Slony1-general] configure-replication.sh and SEQUENCES
In-Reply-To: <60tzjik086.fsf@dba2.int.libertyrms.com>
References: <737401.25231.qm@web25802.mail.ukl.yahoo.com>
	<60tzjik086.fsf@dba2.int.libertyrms.com>
Message-ID: <200803071655.35384.cedric.villemain@dalibo.com>

Le Friday 07 March 2008, Christopher Browne a ?crit?:
> Glyn Astill <glynastill@yahoo.co.uk> writes:
> >> Slony release when main devs think it is time to do, and as you
> >> said,
> >> perltools are not maintain by core-dev.
> >> I find them usefull for my first approach of slony and wish them to
> >> keep
> >> up-to-date, or *at least* to be sync with a specific release. So,
> >> looking at
> >> the slony 'roadmap', perhaps it can be better to push tools scripts
> >> to
> >> another place : a pgfoundry or a slony-tools-1.2.X.tar.
> >> Then, it let 'mature users' get the core, and other users get the
> >> contrib-tools wich are *sync* and released with slony versions.
> >>
> >> I prefer have to wget slony-1.2.13, slony-doc-1.2.13,
> >> slony-tools-1.2.13
> >> archives, but be sure they work all together. Than getting one
> >> package with
> >> doc from 1.2.12, tools between 1.1 and 1.2.12 and core from 1.2.13.
> >>
> >> It can consolidate them by releasing each part singly (even if at
> >> the same
> >> time).
> >>
> >> Views ?
> >
> > I've never used the perl tools, partially out of laziness and
> > partially out of not seeing the need. Instead I just write little
> > slonik scripts. Do they really save much time?
>
> The original point of the "altperl" scripts was as something of a
> crutch for people that were having a hard time wrapping their heads
> around the slonik language.
>
> Over time, some of Afilias' staff has progressed from valuing the
> altperl scripts to thinking them pointless because "slonik isn't that
> hard to write."  (I'm certainly in that camp, but I don't consider
> myself terribly representative since I have written quite a lot of
> code that generates slonik code! :-))


I agree with the altperl tools need. 

What about removing the tools from the main package (all tools, not only the 
perl) and putting them in a dedicated package ?

I explain: what about the slony1.2.14 , slony2.0 ? the docs are going to be 
*fully* updated ? the tools too ? 

-- 
C?dric Villemain
Administrateur de Base de Donn?es
Cel: +33 (0)6 74 15 56 53
http://dalibo.com - http://dalibo.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: This is a digitally signed message part.
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080307/5420e81a/attachment-0001.pgp
From cbbrowne at ca.afilias.info  Fri Mar  7 08:55:51 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Mar  7 08:55:58 2008
Subject: [Slony1-general] configure-replication.sh and SEQUENCES
In-Reply-To: <200803071655.35384.cedric.villemain@dalibo.com>
	(=?iso-8859-1?Q?C=E9dric?= Villemain's message of "Fri,
	7 Mar 2008 16:55:30 +0100")
References: <737401.25231.qm@web25802.mail.ukl.yahoo.com>
	<60tzjik086.fsf@dba2.int.libertyrms.com>
	<200803071655.35384.cedric.villemain@dalibo.com>
Message-ID: <60lk4ujwx4.fsf@dba2.int.libertyrms.com>

C?dric Villemain <cedric.villemain@dalibo.com> writes:
> Le Friday 07 March 2008, Christopher Browne a ?crit?:
>> Glyn Astill <glynastill@yahoo.co.uk> writes:
>> >> Slony release when main devs think it is time to do, and as you
>> >> said,
>> >> perltools are not maintain by core-dev.
>> >> I find them usefull for my first approach of slony and wish them to
>> >> keep
>> >> up-to-date, or *at least* to be sync with a specific release. So,
>> >> looking at
>> >> the slony 'roadmap', perhaps it can be better to push tools scripts
>> >> to
>> >> another place : a pgfoundry or a slony-tools-1.2.X.tar.
>> >> Then, it let 'mature users' get the core, and other users get the
>> >> contrib-tools wich are *sync* and released with slony versions.
>> >>
>> >> I prefer have to wget slony-1.2.13, slony-doc-1.2.13,
>> >> slony-tools-1.2.13
>> >> archives, but be sure they work all together. Than getting one
>> >> package with
>> >> doc from 1.2.12, tools between 1.1 and 1.2.12 and core from 1.2.13.
>> >>
>> >> It can consolidate them by releasing each part singly (even if at
>> >> the same
>> >> time).
>> >>
>> >> Views ?
>> >
>> > I've never used the perl tools, partially out of laziness and
>> > partially out of not seeing the need. Instead I just write little
>> > slonik scripts. Do they really save much time?
>>
>> The original point of the "altperl" scripts was as something of a
>> crutch for people that were having a hard time wrapping their heads
>> around the slonik language.
>>
>> Over time, some of Afilias' staff has progressed from valuing the
>> altperl scripts to thinking them pointless because "slonik isn't that
>> hard to write."  (I'm certainly in that camp, but I don't consider
>> myself terribly representative since I have written quite a lot of
>> code that generates slonik code! :-))
>
>
> I agree with the altperl tools need. 
>
> What about removing the tools from the main package (all tools, not only the 
> perl) and putting them in a dedicated package ?

Some of the tools are forcibly necessary, to the point that they are
included in regression tests.

THOSE tools would need to stay in with the engine.

If we took everything possible out, which seems a good idea, that
would require the tools left out to either find maintainers, or die
off.

> I explain: what about the slony1.2.14 , slony2.0 ? the docs are going to be 
> *fully* updated ? the tools too ? 

I have quite a lot of updates to docs between the branches; the things
that have changed in 2.0 are documented.
-- 
(format nil "~S@~S" "cbbrowne" "acm.org")
http://linuxfinances.info/info/sap.html
"The  problem might  possibly be  to do  with the  fact that  asm code
written for the x86 environment  is, on other platforms, about as much
use  as  a   pork  pie  at  a  Jewish   wedding."-  Andrew  Gierth  in
comp.unix.programmer
From cbbrowne at ca.afilias.info  Fri Mar  7 14:06:01 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Mar  7 14:06:12 2008
Subject: [Slony1-general] Provisional release candidate for 1.2.14
Message-ID: <60fxv2jik6.fsf@dba2.int.libertyrms.com>

A provisional candidate for 1.2.14 has been made available.  Changes include:

- Fix typo in configure-replication.sh (missing CR)

- Per <a href="http://www.slony.info/bugzilla/show_bug.cgi?id=35">bug #35,</a>
  search the Slony share dir for scripts before falling back to the PG
  share dir on 8.0+

- This has resulted in quite a lot of discussion on bug #35; we
 _need_ agreement that this change is an apropos way to go...
  
- Change test framework to write out the test name into
  $TEMPDIR/TestName

- Patch that seems to resolve a race condition with ACCEPT_SET

See: <a href=
"http://lists.slony.info/pipermail/slony1-general/2008-March/007655.html">
email by Yoshiharu Mori </a>

See also <a href=
"http://lists.slony.info/pipermail/slony1-commit/2008-March/002205.html">
CVS commit note </a>.

This particular change has not yet seen heavy verification/testing;
please examine it carefully.
-- 
output = ("cbbrowne" "@" "linuxfinances.info")
http://cbbrowne.com/info/emacs.html
Why are there 5 syllables in the word "monosyllabic"? 
From mailings at oopsware.de  Mon Mar 10 09:25:50 2008
From: mailings at oopsware.de (Bernd Helmle)
Date: Mon Mar 10 09:26:31 2008
Subject: [Slony1-general] failover problems with 3 nodes
In-Reply-To: <20080303220220.41450d92.y-mori@sraoss.co.jp>
References: <20080226230734.de0f09d5.y-mori@sraoss.co.jp>
	<20080227170349.c9c17a42.y-mori@sraoss.co.jp>
	<47C52C89.7080307@postgresqlfr.org>
	<20080227201129.27ae8860.y-mori@sraoss.co.jp>
	<47C546FE.5090706@postgresqlfr.org>	<20080227132917.4489327c@amilo.home>
	<60tzjuc7ss.fsf@dba2.int.libertyrms.com>
	<20080303220220.41450d92.y-mori@sraoss.co.jp>
Message-ID: <E48FF286DFC6E26628022FBB@imhotep.credativ.de>

--On Montag, M?rz 03, 2008 22:02:20 +0900 Yoshiharu Mori 
<y-mori@sraoss.co.jp> wrote:

>
> Because of "lock table sl_config_lock", remoteWorkerThread_1 cannot
> insert "FAILOVER/MOVE_SET" event into sl_event!!
>
> I think this is big bug.
>
> my env is Cent OS x86_64, DUAL-CORE cpu.
>

FYI, we stumpled across exactly the same bug: three node cluster 
installation, MOVE SET to a new origin. We'll see wether we can get 
1.2.14RC running there, but i think the only safe way to get rid of this 
problem is to drop and resubscribe the node.


-- 
  Thanks

                    Bernd
From stephane.schildknecht at postgresqlfr.org  Tue Mar 11 08:13:26 2008
From: stephane.schildknecht at postgresqlfr.org (=?UTF-8?B?IlN0w6lwaGFuZSBBLiBTY2hpbGRrbmVjaHQi?=)
Date: Tue Mar 11 08:13:36 2008
Subject: [Slony1-general] Documentation of slonik_ref.sgml
Message-ID: <47D6A196.6010400@postgresqlfr.org>

Hi,

It seems two ']' are missing in file slonik_ref.sgml, line 78 and 79.


@@ -78,6 +78,6 @@
        commands;
        }
+       [on error { commands; }]
+       [on success { commands; }]
-       [on error { commands; }
-       [on success { commands; }
       </programlisting></para>

Regards,
-- 
St?phane SCHILDKNECHT
Pr?sident de PostgreSQLFr
T?l. 09 53 69 97 12
http://www.postgresqlfr.org
From victor.aluko at gmail.com  Wed Mar 12 02:30:04 2008
From: victor.aluko at gmail.com (ajcity)
Date: Wed Mar 12 02:30:31 2008
Subject: [Slony1-general] UTF-8 encoding Error
Message-ID: <16000710.post@talk.nabble.com>


 Hi all,
  I'm replicating a database that has 8 tables. 7 of them replicate fine on
first replication but the last table (reports table) keeps giving this error
whenever I try to replicate it:
2008-03-07 08:59:31 EST ERROR  remoteWorkerThread_1: copy from stdin on
local node - PGRES_FATAL_ERROR ERROR:  invalid byte sequence for encoding
"UTF8": 0x80

HINT:  This error can also happen if the byte sequence does not match the
encoding expected by the server, which is controlled by "client_encoding".
 I have ran a SET CLIENT_ENCODING TO 'UTF-8' command on both the master and
slave databases and I keep getting the same error. I have also dropped the
table on the slave and recreated it with the exact schema of the master and
still the same error.
 I noticed that if I use pg_dump to extract the data on the reports table of
the master and I insert it into the table on the slave, I don't get that
error. Also, it seems to be giving the error on the last row of the table
and the table is dynamically updated (on the average 5 times within the
hour).
  Does anyone have an idea as to what the problem could be?

     Victor
-- 
View this message in context: http://www.nabble.com/UTF-8-encoding-Error-tp16000710p16000710.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From reynaud at elma.fr  Wed Mar 12 08:03:52 2008
From: reynaud at elma.fr (Jean-Samuel Reynaud)
Date: Wed Mar 12 08:03:59 2008
Subject: [Slony1-general] Slave upgrade without break replication
Message-ID: <20080312160352.528e2488@jsr.elma.loc>

Hi all,

I have a slony cluster of 4 nodes (3 slaves).
My master database is about 180Gb (df on  /data directory).
All server are on postgresql 8.1.xx. But I wanted to upgrade one of
those slave server to postgresql 8.3. 
So I had made a test upgrade of postgresql on this slave server
WITHOUT remove/adding node from replication.

I follow this procedure:

On master: 
=> I had dump entire schema
On slave
=> Stop slony on this slave
=> Modify access (pg_hab.conf) to be sure that I will be the only one who access...
=> I had dump all data without schema
=> I dump on a separate file data from slony schema
=> Stop postgresql 8.1 and install postgresql 8.3 (and slony)
=> initdb start (without slony) and create the database
=> Import schema (from master)
=> Import data from slony scheme (from my separate file...)
=> run those query:
-- update all oid table/sequences on sl_table/sl_sequence
UPDATE _cluster_xxx.sl_table set tab_reloid=c.oid from pg_class c join
pg_namespace n on (n.oid = c.relnamespace) where
nspname||'.'||relname=tab_nspname||'.'||tab_relname; 

UPDATE _cluster_xxx.sl_sequence set seq_reloid=c.oid from pg_class c
join pg_namespace n on (n.oid = c.relnamespace) where
nspname||'.'||relname=seq_nspname||'.'||seq_relname;

-- mark all table as non altered on slave:
UPDATE _cluster_xxx.sl_table set tab_altered = false;
-- Alter all table (adding slony's deny trigger, disable of all existing triggers/contraint)
select _cluster_mxm.altertableforreplication(tab_id) from _cluster_xxx.sl_table;

=> Remove all data on slony's table
truncate _cluster_xxx.sl_* ...
=> Then I import all data:
I start a new psql session
-- Setting session role as slon to allow write on protected tables 
select _cluster_mxm.setsessionrole('_cluster_mxm','slon');
-- Importing data
begin;
\i data.sql
commit;

=> Reupdate oid on slony tables/sequences
UPDATE _cluster_xxx.sl_table set tab_reloid=c.oid from pg_class c join
pg_namespace n on (n.oid = c.relnamespace) where
nspname||'.'||relname=tab_nspname||'.'||tab_relname; 

UPDATE _cluster_xxx.sl_sequence set seq_reloid=c.oid from pg_class c
join pg_namespace n on (n.oid = c.relnamespace) where
nspname||'.'||relname=seq_nspname||'.'||seq_relname;

Then analyze database, opening access and restart slony...

The replication look to restarting in good state. lag time decrease and there is no error message in slony/postgresql logs...
Overload on my (busy) master database is realy lower than adding a new node (after drop old one...)

Is anybody try to do somethink like this ?

Regards,


From cbbrowne at ca.afilias.info  Wed Mar 12 08:33:08 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Mar 12 08:33:17 2008
Subject: [Slony1-general] Slave upgrade without break replication
In-Reply-To: <20080312160352.528e2488@jsr.elma.loc> (Jean-Samuel Reynaud's
	message of "Wed, 12 Mar 2008 16:03:52 +0100")
References: <20080312160352.528e2488@jsr.elma.loc>
Message-ID: <60y78ohs97.fsf@dba2.int.libertyrms.com>

Jean-Samuel Reynaud <reynaud@elma.fr> writes:
> I have a slony cluster of 4 nodes (3 slaves).
> My master database is about 180Gb (df on  /data directory).
> All server are on postgresql 8.1.xx. But I wanted to upgrade one of
> those slave server to postgresql 8.3. 
> So I had made a test upgrade of postgresql on this slave server
> WITHOUT remove/adding node from replication.
>
> I follow this procedure:

This seems rather like "rocket surgery"[1] to me, and not particularly
valuable in providing a solution somehow preferable to the more
mundane approach, which would be to simply...

- Dump schema from the master (as you did)
- Add the 8.3 database as a new node, with new paths
  Slonik: STORE NODE, STORE PATH
- Subscribe the new node to the desired set(s)
  Slonik: SUBSCRIBE SET
- Drop out the 8.1 node that is now unnecessary [2]
  Slonik: DROP NODE
- _Possibly_ adjust the new 8.3 node's configuration so it is on the
  host formerly used by the 8.1 node [3]
  Slonik: STORE PATH + a bit of reconfig in slon.conf

The approach you have described sounds feasible, but I can't be
certain it covers _everything_.  I'd much rather use the above
approach.

Footnotes: 
[1]  rocket surgery
n. a task requiring intelligence or higher education; a difficult undertaking. Subjects: English
Editorial Note: Often used jocularly in negative constructions similar to "It's not rocket surgery."   
Etymological Note: A blend of rocket science and brain surgery.

[2]  Note that you could drop the old node earlier in the process

[3] If you want to have the new 8.3 node be at the same host+port as
the former 8.1 node, I'd suggest dropping the 8.1 node first, so that
you don't need to go through the slightly "rocket surgery-ish"[1]
process of adjusting configurations in various places so that you
shift the location of the 8.3 node after it has been installed.
-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@" [name;tld];;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From reynaud at elma.fr  Wed Mar 12 09:01:24 2008
From: reynaud at elma.fr (Jean-Samuel Reynaud)
Date: Wed Mar 12 09:01:33 2008
Subject: [Slony1-general] Slave upgrade without break replication
In-Reply-To: <60y78ohs97.fsf@dba2.int.libertyrms.com>
References: <20080312160352.528e2488@jsr.elma.loc>
	<60y78ohs97.fsf@dba2.int.libertyrms.com>
Message-ID: <20080312170124.225ccbf5@jsr.elma.loc>

Yes I known that this procedure sound like a big cheat.
But I my case, I don't want to overload my master by adding a new
node (because of copy). 
But usualy, I use standard drop/store node procedure like in
documentation ;o)


Le Wed, 12 Mar 2008 15:33:08 +0000,
Christopher Browne <cbbrowne@ca.afilias.info> a ?crit :

> Jean-Samuel Reynaud <reynaud@elma.fr> writes:
> > I have a slony cluster of 4 nodes (3 slaves).
> > My master database is about 180Gb (df on  /data directory).
> > All server are on postgresql 8.1.xx. But I wanted to upgrade one of
> > those slave server to postgresql 8.3. 
> > So I had made a test upgrade of postgresql on this slave server
> > WITHOUT remove/adding node from replication.
> >
> > I follow this procedure:
> 
> This seems rather like "rocket surgery"[1] to me, and not particularly
> valuable in providing a solution somehow preferable to the more
> mundane approach, which would be to simply...
> 
> - Dump schema from the master (as you did)
> - Add the 8.3 database as a new node, with new paths
>   Slonik: STORE NODE, STORE PATH
> - Subscribe the new node to the desired set(s)
>   Slonik: SUBSCRIBE SET
> - Drop out the 8.1 node that is now unnecessary [2]
>   Slonik: DROP NODE
> - _Possibly_ adjust the new 8.3 node's configuration so it is on the
>   host formerly used by the 8.1 node [3]
>   Slonik: STORE PATH + a bit of reconfig in slon.conf
> 
> The approach you have described sounds feasible, but I can't be
> certain it covers _everything_.  I'd much rather use the above
> approach.
> 
> Footnotes: 
> [1]  rocket surgery
> n. a task requiring intelligence or higher education; a difficult
> undertaking. Subjects: English Editorial Note: Often used jocularly
> in negative constructions similar to "It's not rocket surgery."
> Etymological Note: A blend of rocket science and brain surgery.
> 
> [2]  Note that you could drop the old node earlier in the process
> 
> [3] If you want to have the new 8.3 node be at the same host+port as
> the former 8.1 node, I'd suggest dropping the 8.1 node first, so that
> you don't need to go through the slightly "rocket surgery-ish"[1]
> process of adjusting configurations in various places so that you
> shift the location of the 8.3 node after it has been installed.
From cbbrowne at ca.afilias.info  Wed Mar 12 09:42:21 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Mar 12 09:42:30 2008
Subject: [Slony1-general] Slave upgrade without break replication
In-Reply-To: <20080312170124.225ccbf5@jsr.elma.loc> (Jean-Samuel Reynaud's
	message of "Wed, 12 Mar 2008 17:01:24 +0100")
References: <20080312160352.528e2488@jsr.elma.loc>
	<60y78ohs97.fsf@dba2.int.libertyrms.com>
	<20080312170124.225ccbf5@jsr.elma.loc>
Message-ID: <60k5k7j3ma.fsf@dba2.int.libertyrms.com>

Jean-Samuel Reynaud <reynaud@elma.fr> writes:
> Yes I known that this procedure sound like a big cheat.
> But I my case, I don't want to overload my master by adding a new
> node (because of copy). 
> But usualy, I use standard drop/store node procedure like in
> documentation ;o)

You could always take that load off by having the new node subscribe
to one of the 3 existing subscribers.
-- 
let name="cbbrowne" and tld="linuxdatabases.info" in name ^ "@" ^ tld;;
http://cbbrowne.com/info/oses.html
It is usually a   good idea to  put  a capacitor of a  few microfarads
across the output, as shown.
From andreas at kostyrka.org  Wed Mar 12 09:55:00 2008
From: andreas at kostyrka.org (Andreas Kostyrka)
Date: Wed Mar 12 09:55:16 2008
Subject: [Slony1-general] UTF-8 encoding Error
In-Reply-To: <16000710.post@talk.nabble.com>
References: <16000710.post@talk.nabble.com>
Message-ID: <1205340900.7216.2.camel@localhost>

Some questions/hints:

1.) What versions of PG are you running? Some version of PG (I think it
was 8.2, but I'd had to check that) has become much more exact in
processing UTF-8. If you are replicating from an PG version that is
older than the version on the slaves this could be it.

2.) the client encoding is not much of an issue here, what is your
server encoding?

Andreas

Am Mittwoch, den 12.03.2008, 02:30 -0700 schrieb ajcity:
> Hi all,
>   I'm replicating a database that has 8 tables. 7 of them replicate fine on
> first replication but the last table (reports table) keeps giving this error
> whenever I try to replicate it:
> 2008-03-07 08:59:31 EST ERROR  remoteWorkerThread_1: copy from stdin on
> local node - PGRES_FATAL_ERROR ERROR:  invalid byte sequence for encoding
> "UTF8": 0x80
> 
> HINT:  This error can also happen if the byte sequence does not match the
> encoding expected by the server, which is controlled by "client_encoding".
>  I have ran a SET CLIENT_ENCODING TO 'UTF-8' command on both the master and
> slave databases and I keep getting the same error. I have also dropped the
> table on the slave and recreated it with the exact schema of the master and
> still the same error.
>  I noticed that if I use pg_dump to extract the data on the reports table of
> the master and I insert it into the table on the slave, I don't get that
> error. Also, it seems to be giving the error on the last row of the table
> and the table is dynamically updated (on the average 5 times within the
> hour).
>   Does anyone have an idea as to what the problem could be?
> 
>      Victor
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: Dies ist ein digital signierter Nachrichtenteil
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080312/3be83f8a/attachment.pgp
From pgsql at j-davis.com  Wed Mar 12 10:03:28 2008
From: pgsql at j-davis.com (Jeff Davis)
Date: Wed Mar 12 10:03:40 2008
Subject: [Slony1-general] UTF-8 encoding Error
In-Reply-To: <16000710.post@talk.nabble.com>
References: <16000710.post@talk.nabble.com>
Message-ID: <1205341408.4589.40.camel@dogma.ljc.laika.com>

On Wed, 2008-03-12 at 02:30 -0700, ajcity wrote:
>  Hi all,
>   I'm replicating a database that has 8 tables. 7 of them replicate fine on
> first replication but the last table (reports table) keeps giving this error
> whenever I try to replicate it:
> 2008-03-07 08:59:31 EST ERROR  remoteWorkerThread_1: copy from stdin on
> local node - PGRES_FATAL_ERROR ERROR:  invalid byte sequence for encoding
> "UTF8": 0x80
> 
> HINT:  This error can also happen if the byte sequence does not match the
> encoding expected by the server, which is controlled by "client_encoding".
>  I have ran a SET CLIENT_ENCODING TO 'UTF-8' command on both the master and
> slave databases and I keep getting the same error. 

SET commands are only active for that session.

> I have also dropped the
> table on the slave and recreated it with the exact schema of the master and
> still the same error.
>  I noticed that if I use pg_dump to extract the data on the reports table of
> the master and I insert it into the table on the slave, I don't get that
> error. Also, it seems to be giving the error on the last row of the table
> and the table is dynamically updated (on the average 5 times within the
> hour).

Before I read this paragraph, I thought maybe you got some invalid byte
sequences in your data, which is possible before 8.3. If you can
dump/reload fine, that's probably not the case.

What versions are the two servers?

Do they both have the same encoding and locale settings?

Regards,
	Jeff Davis

From reynaud at elma.fr  Wed Mar 12 10:12:08 2008
From: reynaud at elma.fr (Jean-Samuel Reynaud)
Date: Wed Mar 12 10:12:18 2008
Subject: [Slony1-general] Slave upgrade without break replication
In-Reply-To: <60k5k7j3ma.fsf@dba2.int.libertyrms.com>
References: <20080312160352.528e2488@jsr.elma.loc>
	<60y78ohs97.fsf@dba2.int.libertyrms.com>
	<20080312170124.225ccbf5@jsr.elma.loc>
	<60k5k7j3ma.fsf@dba2.int.libertyrms.com>
Message-ID: <20080312181208.7df6c7ef@jsr.elma.loc>

Of course, but I think It's insteresting to share my work.

Le Wed, 12 Mar 2008 16:42:21 +0000,
Christopher Browne <cbbrowne@ca.afilias.info> a ?crit :

> Jean-Samuel Reynaud <reynaud@elma.fr> writes:
> > Yes I known that this procedure sound like a big cheat.
> > But I my case, I don't want to overload my master by adding a new
> > node (because of copy). 
> > But usualy, I use standard drop/store node procedure like in
> > documentation ;o)
> 
> You could always take that load off by having the new node subscribe
> to one of the 3 existing subscribers.
From pgsql at j-davis.com  Wed Mar 12 10:21:00 2008
From: pgsql at j-davis.com (Jeff Davis)
Date: Wed Mar 12 10:21:21 2008
Subject: [Slony1-general] UTF-8 encoding Error
In-Reply-To: <1205340900.7216.2.camel@localhost>
References: <16000710.post@talk.nabble.com> <1205340900.7216.2.camel@localhost>
Message-ID: <1205342460.4589.46.camel@dogma.ljc.laika.com>

On Wed, 2008-03-12 at 17:55 +0100, Andreas Kostyrka wrote:
> Some questions/hints:
> 
> 1.) What versions of PG are you running? Some version of PG (I think it
> was 8.2, but I'd had to check that) has become much more exact in
> processing UTF-8. If you are replicating from an PG version that is
> older than the version on the slaves this could be it.

Just to clarify, 8.2 was a little weird regarding encoding. It was more
strict in some contexts, but there were still ways to get invalid byte
sequences inserted, and after the bad data was there, you couldn't COPY
the data (dump/restore wouldn't work). That could also explain his
problems.

8.3 became more completely strict, so there is now no way to get the
data in there in the first place.

I'm not entirely sure this is his problem, however, because he said he
could dump/restore. Maybe he's just got different server encodings?

Regards,
	Jeff Davis


From y-mori at sraoss.co.jp  Wed Mar 12 23:30:50 2008
From: y-mori at sraoss.co.jp (Yoshiharu Mori)
Date: Wed Mar 12 23:31:15 2008
Subject: [Slony1-general] Provisional release candidate for 1.2.14
In-Reply-To: <60fxv2jik6.fsf@dba2.int.libertyrms.com>
References: <60fxv2jik6.fsf@dba2.int.libertyrms.com>
Message-ID: <20080313153050.95568785.y-mori@sraoss.co.jp>

Hello, Christopher

The patch that I sent before seems not to have adjusted correctly.
The difference is as follows.
#please delete three lines.

Index: remote_worker.c
===================================================================
RCS file: /slony1/slony1-engine/src/slon/remote_worker.c,v
retrieving revision 1.124.2.32
diff -c -r1.124.2.32 remote_worker.c
*** remote_worker.c     7 Mar 2008 21:47:04 -0000       1.124.2.32
--- remote_worker.c     13 Mar 2008 06:14:51 -0000
***************
*** 677,685 ****
                        slon_appendquery(&query1,
                                                         "lock table %s.sl_config_lock; ",
                                                         rtcfg_namespace);
-                       if (query_execute(node, local_dbconn, &query1) < 0)
-                               slon_retry();
-                       dstring_reset(&query1);

                        /* start by trying to apply the lock to sl_config_lock */
                        if (strcmp(event->ev_type, "ACCEPT_SET") != 0)
--- 677,682 ----


> A provisional candidate for 1.2.14 has been made available.  Changes include:
> 
> - Fix typo in configure-replication.sh (missing CR)
> 
> - Per <a href="http://www.slony.info/bugzilla/show_bug.cgi?id=35">bug #35,</a>
>   search the Slony share dir for scripts before falling back to the PG
>   share dir on 8.0+
> 
> - This has resulted in quite a lot of discussion on bug #35; we
>  _need_ agreement that this change is an apropos way to go...
>   
> - Change test framework to write out the test name into
>   $TEMPDIR/TestName
> 
> - Patch that seems to resolve a race condition with ACCEPT_SET
> 
> See: <a href=
> "http://lists.slony.info/pipermail/slony1-general/2008-March/007655.html">
> email by Yoshiharu Mori </a>
> 
> See also <a href=
> "http://lists.slony.info/pipermail/slony1-commit/2008-March/002205.html">
> CVS commit note </a>.
> 
> This particular change has not yet seen heavy verification/testing;
> please examine it carefully.
> -- 
> output = ("cbbrowne" "@" "linuxfinances.info")
> http://cbbrowne.com/info/emacs.html
> Why are there 5 syllables in the word "monosyllabic"? 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 


-- 
SRA OSS, Inc. Japan
Yoshiharu Mori <y-mori@sraoss.co.jp>
http://www.sraoss.co.jp/
From victor.aluko at gmail.com  Thu Mar 13 01:12:07 2008
From: victor.aluko at gmail.com (ajcity)
Date: Thu Mar 13 01:12:34 2008
Subject: [Slony1-general] UTF-8 encoding Error
In-Reply-To: <1205340900.7216.2.camel@localhost>
References: <16000710.post@talk.nabble.com> <1205340900.7216.2.camel@localhost>
Message-ID: <16023017.post@talk.nabble.com>




Andreas Kostyrka wrote:
> 
> Some questions/hints:
> 
> 1.) What versions of PG are you running? Some version of PG (I think it
> was 8.2, but I'd had to check that) has become much more exact in
> processing UTF-8. If you are replicating from an PG version that is
> older than the version on the slaves this could be it.
> 

 The Master is running PG version 8.1.5 as a server but 8.2 as the client
querying interface (the psql reports 8.2 as its version but 8.1.5 as the
server version). I dont understand why or how the DB admin managed to set it
up that way on the Redhat Enterprise (RHEL 4) but thats what I have to work
with
  The Slave is PG 8.2.6 on OpenSuSE 10.3 but both slony are the same
(1.2.12)



> 2.) the client encoding is not much of an issue here, what is your
> server encoding?
> 
> 
  The Master server encoding is also UTF-8 (same as the Slave) and I started
the replication right after using the SET client_encoding command.
  I considered replacing the version of the postgresql on the Slave with the
same version on the Master but the issue is "Which one do I install? 8.1.5
or 8.2?" and "Shouldn't there be another solution that would not affect the
data on the other DBs on the Slave?"

   Victor

-- 
View this message in context: http://www.nabble.com/UTF-8-encoding-Error-tp16000710p16023017.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From victor.aluko at gmail.com  Thu Mar 13 01:40:05 2008
From: victor.aluko at gmail.com (ajcity)
Date: Thu Mar 13 01:40:32 2008
Subject: [Slony1-general] UTF-8 encoding Error
In-Reply-To: <1205342460.4589.46.camel@dogma.ljc.laika.com>
References: <16000710.post@talk.nabble.com> <1205340900.7216.2.camel@localhost>
	<1205342460.4589.46.camel@dogma.ljc.laika.com>
Message-ID: <16023404.post@talk.nabble.com>




> Just to clarify, 8.2 was a little weird regarding encoding. It was more
> strict in some contexts, but there were still ways to get invalid byte
> sequences inserted, and after the bad data was there, you couldn't COPY
> the data (dump/restore wouldn't work). That could also explain his
> problems.
> 
  Does that mean there is a way I can force Slony to insert the data and
ignore the error?



> I'm not entirely sure this is his problem, however, because he said he
> could dump/restore. Maybe he's just got different server encodings?
> 
  Yeah I could dump the data on the Master then insert it into the Slave
without any issues. Initially, I considered the process by which data was
been inserted into the table but since I could dump & insert, I then
expected that the function/method inserting data was inserting a character
that was not in UTF-8 format but then how come it was not rectified by the
"SET client_encoding" command?
  Really I need help....cos I've been thinking about it throughout the night
and I don't want to reinstall the postgresql on the Slave again.....it was
not a very sweet experience when I first did it.

   Regards........Victor


-- 
View this message in context: http://www.nabble.com/UTF-8-encoding-Error-tp16000710p16023404.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From emanuel.petr at nic.cz  Thu Mar 13 06:59:21 2008
From: emanuel.petr at nic.cz (Emanuel Petr)
Date: Thu Mar 13 06:59:21 2008
Subject: [Slony1-general] Replication problem of "BIG" table - COPY event in
	a loop
Message-ID: <47D93339.4040606@nic.cz>

Hi all,
we have problem to replicate 12 GB table.

Note: Other smaller tables were replicated without problem.

Here is what I see on "slave" node.
$ grep 'action"' /var/log/slony1/slon-db.log
DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
DEBUG2 remoteWorkerThread_1: copy table "public"."action"
DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
NOTICE:  truncate of "public"."action" failed - doing delete

DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
DEBUG2 remoteWorkerThread_1: copy table "public"."action"
DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
NOTICE:  truncate of "public"."action" failed - doing delete
DEBUG2 remoteWorkerThread_1: 6584477358 bytes copied for table
"public"."action"

DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
DEBUG2 remoteWorkerThread_1: copy table "public"."action"
DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
NOTICE:  truncate of "public"."action" failed - doing delete
DEBUG2 remoteWorkerThread_1: 6587572735 bytes copied for table 
"public"."action"

DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
DEBUG2 remoteWorkerThread_1: copy table "public"."action"
DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
NOTICE:  truncate of "public"."action" failed - doing delete

I can't find any error message in the log.

COPY event for this "big" table is in a loop and table size on "slave" 
node is still growing.

On "Master" node, the table size is 12GB.
On "Slave" node the tables size was 68GB before I have stopped the 
replication.


-------------

OS: Ubuntu 6.06.2 LTS , 2.6.15-29-amd64-server, x86_64

DB: postgresql-8.1

SLONY: Version: 1.2.9-2ubuntu0~dapper0


Detail of our problematic "12GB" table
=# \d+ action;
                                             Table "public.action"
    Column   |            Type             | 
Modifiers                      | Description
------------+-----------------------------+-----------------------------------------------------+-------------
  id         | integer                     | not null default 
nextval('action_id_seq'::regclass) |
  clientid   | integer                     | 
                          |
  action     | integer                     | not null 
                          |
  response   | integer                     | 
                          |
  startdate  | timestamp without time zone | not null default now() 
                          |
  clienttrid | character varying(128)      | not null 
                          |
  enddate    | timestamp without time zone | 
                          |
  servertrid | character varying(128)      | 
                          |
Indexes:
     "action_pkey" PRIMARY KEY, btree (id)
     "action_servertrid_key" UNIQUE, btree (servertrid)
     "action_action_idx" btree ("action")
     "action_clientid_idx" btree (clientid)
     "action_response_idx" btree (response)
     "action_startdate_idx" btree (startdate)
Foreign-key constraints:
     "action_action_fkey" FOREIGN KEY ("action") REFERENCES enum_action(id)
     "action_clientid_fkey" FOREIGN KEY (clientid) REFERENCES "login"(id)
     "action_response_fkey" FOREIGN KEY (response) REFERENCES enum_error(id)


---------------

Does anyone have an idea what could be wrong?

Thanks,
Petr
From cedric.villemain at dalibo.com  Thu Mar 13 08:00:00 2008
From: cedric.villemain at dalibo.com (=?iso-8859-1?q?C=E9dric_Villemain?=)
Date: Thu Mar 13 08:00:34 2008
Subject: [Slony1-general] Replication problem of "BIG" table - COPY event
	in a loop
In-Reply-To: <47D93339.4040606@nic.cz>
References: <47D93339.4040606@nic.cz>
Message-ID: <200803131600.15706.cedric.villemain@dalibo.com>

Le Thursday 13 March 2008, Emanuel Petr a ?crit?:
> Hi all,
> we have problem to replicate 12 GB table.
>
> Note: Other smaller tables were replicated without problem.
>
> Here is what I see on "slave" node.
> $ grep 'action"' /var/log/slony1/slon-db.log
> DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
> DEBUG2 remoteWorkerThread_1: copy table "public"."action"
> DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
> NOTICE:  truncate of "public"."action" failed - doing delete
>
> DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
> DEBUG2 remoteWorkerThread_1: copy table "public"."action"
> DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
> NOTICE:  truncate of "public"."action" failed - doing delete
> DEBUG2 remoteWorkerThread_1: 6584477358 bytes copied for table
> "public"."action"
>
> DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
> DEBUG2 remoteWorkerThread_1: copy table "public"."action"
> DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
> NOTICE:  truncate of "public"."action" failed - doing delete
> DEBUG2 remoteWorkerThread_1: 6587572735 bytes copied for table
> "public"."action"
>
> DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
> DEBUG2 remoteWorkerThread_1: copy table "public"."action"
> DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
> NOTICE:  truncate of "public"."action" failed - doing delete
>
> I can't find any error message in the log.

I think you probably have made other try before.

If for any reason, slony can not truncate the table before replicating it, it 
delete every lines.

68GB/12GB give about 5 tests to replicate the table ?

I ommit the last versions of everything you are using.... (os, kernel, 
postgres and slony)

You must truncate manualy the tables on the 'slave', then restarting your 
replica from scratch.

>
> COPY event for this "big" table is in a loop and table size on "slave"
> node is still growing.
>
> On "Master" node, the table size is 12GB.
> On "Slave" node the tables size was 68GB before I have stopped the
> replication.
>
>
> -------------
>
> OS: Ubuntu 6.06.2 LTS , 2.6.15-29-amd64-server, x86_64
>
> DB: postgresql-8.1
>
> SLONY: Version: 1.2.9-2ubuntu0~dapper0
>
>
> Detail of our problematic "12GB" table
> =# \d+ action;
>                                              Table "public.action"
>     Column   |            Type             |
> Modifiers                      | Description
> ------------+-----------------------------+--------------------------------
>---------------------+------------- id         | integer                    
> | not null default
> nextval('action_id_seq'::regclass) |
>   clientid   | integer                     |
>
>   action     | integer                     | not null
>
>   response   | integer                     |
>
>   startdate  | timestamp without time zone | not null default now()
>
>   clienttrid | character varying(128)      | not null
>
>   enddate    | timestamp without time zone |
>
>   servertrid | character varying(128)      |
>
> Indexes:
>      "action_pkey" PRIMARY KEY, btree (id)
>      "action_servertrid_key" UNIQUE, btree (servertrid)
>      "action_action_idx" btree ("action")
>      "action_clientid_idx" btree (clientid)
>      "action_response_idx" btree (response)
>      "action_startdate_idx" btree (startdate)
> Foreign-key constraints:
>      "action_action_fkey" FOREIGN KEY ("action") REFERENCES enum_action(id)
>      "action_clientid_fkey" FOREIGN KEY (clientid) REFERENCES "login"(id)
>      "action_response_fkey" FOREIGN KEY (response) REFERENCES
> enum_error(id)
>
>
> ---------------
>
> Does anyone have an idea what could be wrong?
>
> Thanks,
> Petr
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general



-- 
C?dric Villemain
Administrateur de Base de Donn?es
Cel: +33 (0)6 74 15 56 53
http://dalibo.com - http://dalibo.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: This is a digitally signed message part.
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080313/e9d78055/attachment.pgp
From emanuel.petr at nic.cz  Thu Mar 13 09:25:52 2008
From: emanuel.petr at nic.cz (Emanuel Petr)
Date: Thu Mar 13 09:25:53 2008
Subject: [Slony1-general] Replication problem of "BIG" table - COPY event
	in a loop
In-Reply-To: <200803131600.15706.cedric.villemain@dalibo.com>
References: <47D93339.4040606@nic.cz>
	<200803131600.15706.cedric.villemain@dalibo.com>
Message-ID: <47D95590.6060608@nic.cz>


C?dric Villemain wrote:
> Le Thursday 13 March 2008, Emanuel Petr a ?crit :
>> Hi all,
>> we have problem to replicate 12 GB table.
>>
>> Note: Other smaller tables were replicated without problem.
>>
>> Here is what I see on "slave" node.
>> $ grep 'action"' /var/log/slony1/slon-db.log
>> DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
>> DEBUG2 remoteWorkerThread_1: copy table "public"."action"
>> DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
>> NOTICE:  truncate of "public"."action" failed - doing delete
>>
>> DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
>> DEBUG2 remoteWorkerThread_1: copy table "public"."action"
>> DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
>> NOTICE:  truncate of "public"."action" failed - doing delete
>> DEBUG2 remoteWorkerThread_1: 6584477358 bytes copied for table
>> "public"."action"
>>
>> DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
>> DEBUG2 remoteWorkerThread_1: copy table "public"."action"
>> DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
>> NOTICE:  truncate of "public"."action" failed - doing delete
>> DEBUG2 remoteWorkerThread_1: 6587572735 bytes copied for table
>> "public"."action"
>>
>> DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
>> DEBUG2 remoteWorkerThread_1: copy table "public"."action"
>> DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
>> NOTICE:  truncate of "public"."action" failed - doing delete
>>
>> I can't find any error message in the log.
> 
> I think you probably have made other try before.

No, there was only one try. It was aim to show you, that COPY action for 
this table is in a loop.

> 
> If for any reason, slony can not truncate the table before replicating it, it 
> delete every lines.

The table on "slave" node was blank before replication start.
And the message "truncate of ... failed - doing delete" appears for 
another tables, which were replicated without problem. So I don't think 
it's the problem.

There is problem only with bigger (over 10GB) tables.

> 
> 68GB/12GB give about 5 tests to replicate the table ?
> 
> I ommit the last versions of everything you are using.... (os, kernel, 
> postgres and slony)
> 
> You must truncate manualy the tables on the 'slave', then restarting your 
> replica from scratch.

And now I'm not able to work with this "badly" replicated table. Each 
command on this table hang up. Note: Slony is stopped.


> 
>> COPY event for this "big" table is in a loop and table size on "slave"
>> node is still growing.
>>
>> On "Master" node, the table size is 12GB.
>> On "Slave" node the tables size was 68GB before I have stopped the
>> replication.
>>
>>
>> -------------
>>
>> OS: Ubuntu 6.06.2 LTS , 2.6.15-29-amd64-server, x86_64
>>
>> DB: postgresql-8.1
>>
>> SLONY: Version: 1.2.9-2ubuntu0~dapper0
>>
>>
>> Detail of our problematic "12GB" table
>> =# \d+ action;
>>                                              Table "public.action"
>>     Column   |            Type             |
>> Modifiers                      | Description
>> ------------+-----------------------------+--------------------------------
>> ---------------------+------------- id         | integer                    
>> | not null default
>> nextval('action_id_seq'::regclass) |
>>   clientid   | integer                     |
>>
>>   action     | integer                     | not null
>>
>>   response   | integer                     |
>>
>>   startdate  | timestamp without time zone | not null default now()
>>
>>   clienttrid | character varying(128)      | not null
>>
>>   enddate    | timestamp without time zone |
>>
>>   servertrid | character varying(128)      |
>>
>> Indexes:
>>      "action_pkey" PRIMARY KEY, btree (id)
>>      "action_servertrid_key" UNIQUE, btree (servertrid)
>>      "action_action_idx" btree ("action")
>>      "action_clientid_idx" btree (clientid)
>>      "action_response_idx" btree (response)
>>      "action_startdate_idx" btree (startdate)
>> Foreign-key constraints:
>>      "action_action_fkey" FOREIGN KEY ("action") REFERENCES enum_action(id)
>>      "action_clientid_fkey" FOREIGN KEY (clientid) REFERENCES "login"(id)
>>      "action_response_fkey" FOREIGN KEY (response) REFERENCES
>> enum_error(id)
>>
>>
>> ---------------
>>
>> Does anyone have an idea what could be wrong?
>>
>> Thanks,
>> Petr
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 
> 
From liobod.slony at gmail.com  Thu Mar 13 09:37:03 2008
From: liobod.slony at gmail.com (lio bod)
Date: Thu Mar 13 09:37:13 2008
Subject: [Slony1-general] slony1_extract_schema : request for information
Message-ID: <d4f444290803130937pe35cf19s4ecdf5e3627ee0d7@mail.gmail.com>

Hello world,

May i have more info about 'slony1_extract_schema.sh ' command.
I'm not sure to understand what it it it supposed to do.

For instance, looking at the doc (
http://slony.info/documentation/adminscripts.html 19.4.
slony1_extract_schema.sh) :


   - "It dumps the origin node's schema, including the data in the
   Slony-I cluster schema."
   What is called 'origin node's schema' : My schema database before
   intrusive slony hacks?
   - "A slonik script is run to perform UNINSTALL
NODE<http://slony.info/documentation/stmtuninstallnode.html>on the
temporary database. This eliminates all the special
   Slony-I tables, schema, and removes Slony-I triggers from replicated
   tables."
   At this step, does it mean i got my schema and data before any
   intrusive slony hacks?
   - " Finally, pg_dump is run against the temporary database, delivering
   a copy of the cleaned up schema to standard output"
   I expected a dumped file or new 'mytmpbase' in postgres.
   I understand this a design choice.
   Why not making a real dump on disk?
   Why removing the mytmpbase after the treatment?


Btw i would like to submit here a weird test : i perform the command
wheras the 'mytmpbase' already exits with a user is connected :

I run the command with the following usage :

$> slony1_extract_schema.sh mybase mycluster mytmpbase

   - At the beginning, it complains  :

   createdb: =E9chec lors de la cr=E9ation de la base de donn=E9es : ERREUR:
   La base de donn=E9es =AB mytmpbase=BB existe d=E9j=E0
   Sorry but my terminal are french. But i guess the translanlation woulk
   look like :
   createdb: error on creating database : ERROR:  database =AB mytmpbase=BB
   already exists

   But it goes on.
   - Btw, it always says at the begining of its process :
   Warning: Set 1 does not origin on node 1 - original triggers and
   constraints will not be included in the dump
   What are the potential impacts on my dump and on my 'mytmpbase'?
   Is that in contradiction with the sentence 'delivering copy of the
   cleaned up schema' ?
   - At the end of this treatment:

   dropdb: =E9chec de la suppression de la base de donn=E9es: ERREUR:  La
   base de donn=E9es =AB mytmpbase=BB est actuellement acc=E9d=E9e par d'au=
tres
   utilisateurs
   As my terminal are still french go on approximatively translating :
   dropdb: error on deleting database : ERROR: database  =AB mytmpbase=BB is
   beeing accessed by other users

   Indeed, 'mytmpbase' is not removed. May i trust it just like the dump
   into standard output? Will the restore of the dump be an image
   of 'mytmpbase'?

Forgive me if this too dense, but i think any piece of answer will be usful
not only for me but for the whole community,

cheers,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080313/=
94c6b23c/attachment-0001.htm
From cbbrowne at ca.afilias.info  Thu Mar 13 09:52:01 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Mar 13 09:52:10 2008
Subject: [Slony1-general] slony1_extract_schema : request for information
In-Reply-To: <d4f444290803130937pe35cf19s4ecdf5e3627ee0d7@mail.gmail.com> (lio
	bod's message of "Thu, 13 Mar 2008 17:37:03 +0100")
References: <d4f444290803130937pe35cf19s4ecdf5e3627ee0d7@mail.gmail.com>
Message-ID: <601w6ein2m.fsf@dba2.int.libertyrms.com>

"lio bod" <liobod.slony@gmail.com> writes:
> Hello world,
>
> ?
>
> May i have more info about 'slony1_extract_schema.sh ' command.
>
> I'm not sure to understand what it it it supposed to do.
>
> ?
>
> For instance, looking at the doc (http://slony.info/documentation/adminscripts.html?19.4. slony1_extract_schema.sh) :
>
> ?
>
>    o "It dumps the origin node's schema, including the data in the Slony-I cluster schema."
>      What is called 'origin node's schema' : My schema database before intrusive slony hacks?
>    o "A slonik script is run to perform UNINSTALL NODE on the temporary database. This eliminates all the special Slony-I tables, schema, and removes Slony-I triggers
>      from replicated tables."
>      At this step, does it mean i got my schema and data before any intrusive slony hacks?
>    o "?Finally, pg_dump is run against the temporary database, delivering a copy of the cleaned up schema to standard output"
>      
>
>
>
> I expected a dumped file or new 'mytmpbase' in postgres.
> I understand this a design choice.
> Why not making a real dump on disk?
> Why removing the mytmpbase after the treatment?

It's working "the UNIX way;"

  - By delivering output to STDOUT, *you* get to choose what you do with
    the output

  - It removes the temporary database in order to attempt to be an
    idempotent operation, so that you could run this a second time
    without needing some special cleanup procedure.


> Btw i would like to submit here a weird test : i perform the command
> wheras?the 'mytmpbase' already exits with a user is connected :

That obviously won't work, as creation of the database will fail, and
we really can't predict what that user might do, while connected.

That will injure the process, so don't do it.

>    o Btw, it always says at the begining of its process :
>      Warning: Set 1 does not origin on node 1 - original triggers and constraints will not be included in the dump
>      What are the potential impacts on my dump and on my 'mytmpbase'?
>      Is that in contradiction with the sentence 'delivering copy of the cleaned up schema' ?

It is warning you that it can't keep the promise it wanted to keep.

If you run it against a non-origin node, you will not get a totally
"cleaned up" schema.

>    o
>      
>
>    o At the end of this treatment:
>      
>      dropdb: ?chec de la suppression de la base de donn?es: ERREUR:? La base de donn?es ? mytmpbase? est actuellement acc?d?e par d'autres utilisateurs
>      As my terminal are still french go on?approximatively translating :
>      dropdb: error on deleting database : ERROR: database??? mytmpbase? is beeing accessed by other users
>      
>      Indeed, 'mytmpbase' is not removed. May i trust it just like the dump into standard output? Will the restore of the dump be an image of?'mytmpbase'?
>
> Forgive me if this too dense, but i think any piece of answer will be usful not only for me but for the whole community,

Apparently you ran it on other than the origin node, so the result
will be a broken schema.

Evidently I need to add a warning to the documentation to indicate
that you shouldn't run this against a slave.  Done...

<http://lists.slony.info/pipermail/slony1-commit/2008-March/002213.html>
-- 
"cbbrowne","@","linuxdatabases.info"
http://linuxdatabases.info/info/spreadsheets.html
DO IT -- it's easier to get forgiveness than permission.
From JanWieck at Yahoo.com  Thu Mar 13 09:53:16 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu Mar 13 09:53:26 2008
Subject: [Slony1-general] Replication problem of "BIG" table - COPY event
	in a loop
In-Reply-To: <47D95590.6060608@nic.cz>
References: <47D93339.4040606@nic.cz>	<200803131600.15706.cedric.villemain@dalibo.com>
	<47D95590.6060608@nic.cz>
Message-ID: <47D95BFC.3030600@Yahoo.com>

On 3/13/2008 12:25 PM, Emanuel Petr wrote:
> C?dric Villemain wrote:
>> Le Thursday 13 March 2008, Emanuel Petr a ?crit :
>>> Hi all,
>>> we have problem to replicate 12 GB table.
>>>
>>> Note: Other smaller tables were replicated without problem.
>>>
>>> Here is what I see on "slave" node.
>>> $ grep 'action"' /var/log/slony1/slon-db.log
>>> DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
>>> DEBUG2 remoteWorkerThread_1: copy table "public"."action"
>>> DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
>>> NOTICE:  truncate of "public"."action" failed - doing delete
>>>
>>> DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
>>> DEBUG2 remoteWorkerThread_1: copy table "public"."action"
>>> DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
>>> NOTICE:  truncate of "public"."action" failed - doing delete
>>> DEBUG2 remoteWorkerThread_1: 6584477358 bytes copied for table
>>> "public"."action"
>>>
>>> DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
>>> DEBUG2 remoteWorkerThread_1: copy table "public"."action"
>>> DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
>>> NOTICE:  truncate of "public"."action" failed - doing delete
>>> DEBUG2 remoteWorkerThread_1: 6587572735 bytes copied for table
>>> "public"."action"
>>>
>>> DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
>>> DEBUG2 remoteWorkerThread_1: copy table "public"."action"
>>> DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
>>> NOTICE:  truncate of "public"."action" failed - doing delete
>>>
>>> I can't find any error message in the log.
>> 
>> I think you probably have made other try before.
> 
> No, there was only one try. It was aim to show you, that COPY action for 
> this table is in a loop.

The question is what aborts the subscription process. Since the copy 
operation in the second and third attempt above did succeed, it might 
not be related to that table itself, but something that happens later. 
Can you provide the entire log from one "prepare to copy ..." to the next?


Jan

> 
>> 
>> If for any reason, slony can not truncate the table before replicating it, it 
>> delete every lines.
> 
> The table on "slave" node was blank before replication start.
> And the message "truncate of ... failed - doing delete" appears for 
> another tables, which were replicated without problem. So I don't think 
> it's the problem.
> 
> There is problem only with bigger (over 10GB) tables.
> 
>> 
>> 68GB/12GB give about 5 tests to replicate the table ?
>> 
>> I ommit the last versions of everything you are using.... (os, kernel, 
>> postgres and slony)
>> 
>> You must truncate manualy the tables on the 'slave', then restarting your 
>> replica from scratch.
> 
> And now I'm not able to work with this "badly" replicated table. Each 
> command on this table hang up. Note: Slony is stopped.
> 
> 
>> 
>>> COPY event for this "big" table is in a loop and table size on "slave"
>>> node is still growing.
>>>
>>> On "Master" node, the table size is 12GB.
>>> On "Slave" node the tables size was 68GB before I have stopped the
>>> replication.
>>>
>>>
>>> -------------
>>>
>>> OS: Ubuntu 6.06.2 LTS , 2.6.15-29-amd64-server, x86_64
>>>
>>> DB: postgresql-8.1
>>>
>>> SLONY: Version: 1.2.9-2ubuntu0~dapper0
>>>
>>>
>>> Detail of our problematic "12GB" table
>>> =# \d+ action;
>>>                                              Table "public.action"
>>>     Column   |            Type             |
>>> Modifiers                      | Description
>>> ------------+-----------------------------+--------------------------------
>>> ---------------------+------------- id         | integer                    
>>> | not null default
>>> nextval('action_id_seq'::regclass) |
>>>   clientid   | integer                     |
>>>
>>>   action     | integer                     | not null
>>>
>>>   response   | integer                     |
>>>
>>>   startdate  | timestamp without time zone | not null default now()
>>>
>>>   clienttrid | character varying(128)      | not null
>>>
>>>   enddate    | timestamp without time zone |
>>>
>>>   servertrid | character varying(128)      |
>>>
>>> Indexes:
>>>      "action_pkey" PRIMARY KEY, btree (id)
>>>      "action_servertrid_key" UNIQUE, btree (servertrid)
>>>      "action_action_idx" btree ("action")
>>>      "action_clientid_idx" btree (clientid)
>>>      "action_response_idx" btree (response)
>>>      "action_startdate_idx" btree (startdate)
>>> Foreign-key constraints:
>>>      "action_action_fkey" FOREIGN KEY ("action") REFERENCES enum_action(id)
>>>      "action_clientid_fkey" FOREIGN KEY (clientid) REFERENCES "login"(id)
>>>      "action_response_fkey" FOREIGN KEY (response) REFERENCES
>>> enum_error(id)
>>>
>>>
>>> ---------------
>>>
>>> Does anyone have an idea what could be wrong?
>>>
>>> Thanks,
>>> Petr
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general@lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>> 
>> 
>> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #

From liobod.slony at gmail.com  Thu Mar 13 10:44:02 2008
From: liobod.slony at gmail.com (lio bod)
Date: Thu Mar 13 10:44:16 2008
Subject: [Slony1-general] slony1_extract_schema : request for information
In-Reply-To: <601w6ein2m.fsf@dba2.int.libertyrms.com>
References: <d4f444290803130937pe35cf19s4ecdf5e3627ee0d7@mail.gmail.com>
	<601w6ein2m.fsf@dba2.int.libertyrms.com>
Message-ID: <d4f444290803131044o70a848bby5272810cc63fa56a@mail.gmail.com>

Is the option used for pg_dump visible in slony1_extract_schema.sh ?
Are they any constraints on how to use pg_restore with the dump?

Are there any restrictions using this scripts while slon process are
running?

You said : "It is warning you that it can't keep the promise it wanted to
keep"
Could you precise?.

For my weird test I did run it on the origin node (i.e my master).  the
'mytmpbase' already exits with a user is connected. I confirm the remaining
(impossible to delete) 'mytmpbase'  is corrupted.

With a proper use of slony1_extract_schema.sh, I confirm also my restore
seems ok.
I'll check that in deep tomorrow...

Thx for your very quick answers,


2008/3/13, Christopher Browne <cbbrowne@ca.afilias.info>:
>
> "lio bod" <liobod.slony@gmail.com> writes:
> > Hello world,
> >
> >
> >
> > May i have more info about 'slony1_extract_schema.sh ' command.
> >
> > I'm not sure to understand what it it it supposed to do.
> >
> >
> >
> > For instance, looking at the doc (
> http://slony.info/documentation/adminscripts.html 19.4.
> slony1_extract_schema.sh) :
> >
> >
> >
> >    o "It dumps the origin node's schema, including the data in the
> Slony-I cluster schema."
> >      What is called 'origin node's schema' : My schema database before
> intrusive slony hacks?
> >    o "A slonik script is run to perform UNINSTALL NODE on the temporary
> database. This eliminates all the special Slony-I tables, schema, and
> removes Slony-I triggers
> >      from replicated tables."
> >      At this step, does it mean i got my schema and data before any
> intrusive slony hacks?
> >    o " Finally, pg_dump is run against the temporary database,
> delivering a copy of the cleaned up schema to standard output"
> >
> >
> >
> >
> > I expected a dumped file or new 'mytmpbase' in postgres.
> > I understand this a design choice.
> > Why not making a real dump on disk?
> > Why removing the mytmpbase after the treatment?
>
> It's working "the UNIX way;"
>
> - By delivering output to STDOUT, *you* get to choose what you do with
>    the output
>
> - It removes the temporary database in order to attempt to be an
>    idempotent operation, so that you could run this a second time
>    without needing some special cleanup procedure.
>
>
> > Btw i would like to submit here a weird test : i perform the command
> > wheras the 'mytmpbase' already exits with a user is connected :
>
> That obviously won't work, as creation of the database will fail, and
> we really can't predict what that user might do, while connected.
>
> That will injure the process, so don't do it.
>
> >    o Btw, it always says at the begining of its process :
> >      Warning: Set 1 does not origin on node 1 - original triggers and
> constraints will not be included in the dump
> >      What are the potential impacts on my dump and on my 'mytmpbase'?
> >      Is that in contradiction with the sentence 'delivering copy of the
> cleaned up schema' ?
>
> It is warning you that it can't keep the promise it wanted to keep.
>
> If you run it against a non-origin node, you will not get a totally
> "cleaned up" schema.
>
> >    o
> >
> >
> >    o At the end of this treatment:
> >
> >      dropdb: =E9chec de la suppression de la base de donn=E9es: ERREUR:=
  La
> base de donn=E9es =AB mytmpbase=BB est actuellement acc=E9d=E9e par d'aut=
res
> utilisateurs
> >      As my terminal are still french go on approximatively translating :
> >      dropdb: error on deleting database : ERROR: database  =AB mytmpbas=
e=BB
> is beeing accessed by other users
> >
> >      Indeed, 'mytmpbase' is not removed. May i trust it just like the
> dump into standard output? Will the restore of the dump be an image of
> 'mytmpbase'?
> >
> > Forgive me if this too dense, but i think any piece of answer will be
> usful not only for me but for the whole community,
>
> Apparently you ran it on other than the origin node, so the result
> will be a broken schema.
>
> Evidently I need to add a warning to the documentation to indicate
> that you shouldn't run this against a slave.  Done...
>
> <http://lists.slony.info/pipermail/slony1-commit/2008-March/002213.html>
> --
> "cbbrowne","@","linuxdatabases.info"
> http://linuxdatabases.info/info/spreadsheets.html
> DO IT -- it's easier to get forgiveness than permission.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080313/=
f238c447/attachment.htm
From liobod.slony at gmail.com  Fri Mar 14 00:59:43 2008
From: liobod.slony at gmail.com (lio bod)
Date: Fri Mar 14 01:00:10 2008
Subject: [Slony1-general] out-of-sync status
Message-ID: <d4f444290803140059q26cdb03en404d7611263f11a2@mail.gmail.com>

Hello world,

I'm searching a way to get in real-time (while slon processes are running)
the out-of-sync status of Slony one.
I want to to know at any moment, which and how much data are in my master
and not yet replicated in my slave.

The idea is to be able to achivied a 'graceful' stop of SlonyI after a
disconnection of all clients on master.


rgds
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080314/=
d977ce75/attachment.htm
From liobod.slony at gmail.com  Fri Mar 14 01:06:24 2008
From: liobod.slony at gmail.com (lio bod)
Date: Fri Mar 14 01:06:49 2008
Subject: [Slony1-general] slony1_extract_schema : request for information
In-Reply-To: <d4f444290803131044o70a848bby5272810cc63fa56a@mail.gmail.com>
References: <d4f444290803130937pe35cf19s4ecdf5e3627ee0d7@mail.gmail.com>
	<601w6ein2m.fsf@dba2.int.libertyrms.com>
	<d4f444290803131044o70a848bby5272810cc63fa56a@mail.gmail.com>
Message-ID: <d4f444290803140106y55a4697dje516f9f2f09c47df@mail.gmail.com>

ok. saw them : pg_dump -s.
It make me realize i havn't read carefullly it delivers 'a copy of the
cleaned up schema to standard output'
My mistake. I thought it contains the data also
How could i add data in the dump?


2008/3/13, lio bod <liobod.slony@gmail.com>:
>
> Is the option used for pg_dump visible in slony1_extract_schema.sh ?
>
>
 Are they any constraints on how to use pg_restore with the dump?
>
> Are there any restrictions using this scripts while slon process are
> running?
>
> You said : "It is warning you that it can't keep the promise it wanted to
> keep"
> Could you precise?.
>
> For my weird test I did run it on the origin node (i.e my master).  the
> 'mytmpbase' already exits with a user is connected. I confirm the remaini=
ng
> (impossible to delete) 'mytmpbase'  is corrupted.
>
> With a proper use of slony1_extract_schema.sh, I confirm also my restore
> seems ok.
> I'll check that in deep tomorrow...
>
> Thx for your very quick answers,
>
>
> 2008/3/13, Christopher Browne <cbbrowne@ca.afilias.info>:
> >
> > "lio bod" <liobod.slony@gmail.com> writes:
> > > Hello world,
> > >
> > >
> > >
> > > May i have more info about 'slony1_extract_schema.sh ' command.
> > >
> > > I'm not sure to understand what it it it supposed to do.
> > >
> > >
> > >
> > > For instance, looking at the doc (
> > http://slony.info/documentation/adminscripts.html 19.4.
> > slony1_extract_schema.sh) :
> > >
> > >
> > >
> > >    o "It dumps the origin node's schema, including the data in the
> > Slony-I cluster schema."
> > >      What is called 'origin node's schema' : My schema database before
> > intrusive slony hacks?
> > >    o "A slonik script is run to perform UNINSTALL NODE on the
> > temporary database. This eliminates all the special Slony-I tables, sch=
ema,
> > and removes Slony-I triggers
> > >      from replicated tables."
> > >      At this step, does it mean i got my schema and data before any
> > intrusive slony hacks?
> > >    o " Finally, pg_dump is run against the temporary database,
> > delivering a copy of the cleaned up schema to standard output"
> > >
> > >
> > >
> > >
> > > I expected a dumped file or new 'mytmpbase' in postgres.
> > > I understand this a design choice.
> > > Why not making a real dump on disk?
> > > Why removing the mytmpbase after the treatment?
> >
> > It's working "the UNIX way;"
> >
> > - By delivering output to STDOUT, *you* get to choose what you do with
> >    the output
> >
> > - It removes the temporary database in order to attempt to be an
> >    idempotent operation, so that you could run this a second time
> >    without needing some special cleanup procedure.
> >
> >
> > > Btw i would like to submit here a weird test : i perform the command
> > > wheras the 'mytmpbase' already exits with a user is connected :
> >
> > That obviously won't work, as creation of the database will fail, and
> > we really can't predict what that user might do, while connected.
> >
> > That will injure the process, so don't do it.
> >
> > >    o Btw, it always says at the begining of its process :
> > >      Warning: Set 1 does not origin on node 1 - original triggers and
> > constraints will not be included in the dump
> > >      What are the potential impacts on my dump and on my 'mytmpbase'?
> > >      Is that in contradiction with the sentence 'delivering copy of
> > the cleaned up schema' ?
> >
> > It is warning you that it can't keep the promise it wanted to keep.
> >
> > If you run it against a non-origin node, you will not get a totally
> > "cleaned up" schema.
> >
> > >    o
> > >
> > >
> > >    o At the end of this treatment:
> > >
> > >      dropdb: =E9chec de la suppression de la base de donn=E9es:
> > ERREUR:  La base de donn=E9es =AB mytmpbase=BB est actuellement acc=E9d=
=E9e par
> > d'autres utilisateurs
> > >      As my terminal are still french go on approximatively translating
> > :
> > >      dropdb: error on deleting database : ERROR: database  =AB
> > mytmpbase=BB is beeing accessed by other users
> > >
> > >      Indeed, 'mytmpbase' is not removed. May i trust it just like the
> > dump into standard output? Will the restore of the dump be an image of
> > 'mytmpbase'?
> > >
> > > Forgive me if this too dense, but i think any piece of answer will be
> > usful not only for me but for the whole community,
> >
> > Apparently you ran it on other than the origin node, so the result
> > will be a broken schema.
> >
> > Evidently I need to add a warning to the documentation to indicate
> > that you shouldn't run this against a slave.  Done...
> >
> > <http://lists.slony.info/pipermail/slony1-commit/2008-March/002213.html>
> > --
> > "cbbrowne","@","linuxdatabases.info"
> > http://linuxdatabases.info/info/spreadsheets.html
> > DO IT -- it's easier to get forgiveness than permission.
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080314/=
3d3ba543/attachment.htm
From emanuel.petr at nic.cz  Fri Mar 14 06:24:31 2008
From: emanuel.petr at nic.cz (Emanuel Petr)
Date: Fri Mar 14 06:24:26 2008
Subject: [Slony1-general] Replication problem of "BIG" table - COPY event
	in a loop - SOLVED
In-Reply-To: <47D95BFC.3030600@Yahoo.com>
References: <47D93339.4040606@nic.cz>	<200803131600.15706.cedric.villemain@dalibo.com>
	<47D95590.6060608@nic.cz> <47D95BFC.3030600@Yahoo.com>
Message-ID: <47DA7C8F.3020201@nic.cz>

Jan Wieck wrote:
> On 3/13/2008 12:25 PM, Emanuel Petr wrote:
>> C?dric Villemain wrote:
>>> Le Thursday 13 March 2008, Emanuel Petr a ?crit :
>>>> Hi all,
>>>> we have problem to replicate 12 GB table.
>>>>
>>>> Note: Other smaller tables were replicated without problem.
>>>>
>>>> Here is what I see on "slave" node.
>>>> $ grep 'action"' /var/log/slony1/slon-db.log
>>>> DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
>>>> DEBUG2 remoteWorkerThread_1: copy table "public"."action"
>>>> DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
>>>> NOTICE:  truncate of "public"."action" failed - doing delete
>>>>
>>>> DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
>>>> DEBUG2 remoteWorkerThread_1: copy table "public"."action"
>>>> DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
>>>> NOTICE:  truncate of "public"."action" failed - doing delete
>>>> DEBUG2 remoteWorkerThread_1: 6584477358 bytes copied for table
>>>> "public"."action"
>>>>
>>>> DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
>>>> DEBUG2 remoteWorkerThread_1: copy table "public"."action"
>>>> DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
>>>> NOTICE:  truncate of "public"."action" failed - doing delete
>>>> DEBUG2 remoteWorkerThread_1: 6587572735 bytes copied for table
>>>> "public"."action"
>>>>
>>>> DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
>>>> DEBUG2 remoteWorkerThread_1: copy table "public"."action"
>>>> DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
>>>> NOTICE:  truncate of "public"."action" failed - doing delete
>>>>
>>>> I can't find any error message in the log.
>>>
>>> I think you probably have made other try before.
>>
>> No, there was only one try. It was aim to show you, that COPY action 
>> for this table is in a loop.
> 
> The question is what aborts the subscription process. Since the copy 
> operation in the second and third attempt above did succeed, it might 
> not be related to that table itself, but something that happens later. 
> Can you provide the entire log from one "prepare to copy ..." to the next?
> 
> 
> Jan
> 

Yes, you have true. I found out what aborts the subscription process - 
look at 'cleanupThread' and further.

DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"
DEBUG2 remoteWorkerThread_1: all tables for set 2 found on subscriber
DEBUG2 remoteWorkerThread_1: copy table "public"."action"
DEBUG2 remoteWorkerThread_1: Begin COPY of table "public"."action"
DEBUG2 remoteWorkerThread_1:  nodeon73 is 0
NOTICE:  truncate of "public"."action" failed - doing delete
DEBUG2 syncThread: new sl_action_seq 1 - SYNC 3
DEBUG2 localListenThread: Received event 2,3 SYNC
DEBUG2 remoteListenThread_1: queue event 1,1380846 SYNC
DEBUG2 syncThread: new sl_action_seq 1 - SYNC 119
....
DEBUG2 remoteListenThread_1: queue event 1,1381413 SYNC
DEBUG2 remoteListenThread_1: queue event 1,1381414 SYNC
DEBUG2 remoteWorkerThread_1: 6614093174 bytes copied for table 
"public"."action"
DEBUG2 remoteListenThread_1: queue event 1,1381415 SYNC
DEBUG2 remoteListenThread_1: queue event 1,1382888 SYNC
...
DEBUG2 localListenThread: Received event 2,419 SYNC
DEBUG2 remoteListenThread_1: queue event 1,1382889 SYNC
DEBUG2 remoteListenThread_1: queue event 1,1382890 SYNC
FATAL  cleanupThread: "delete from "_test_cluster".sl_log_1 where 
log_origin = '2' and log_xid < '562381542'; delete from 
"_test_cluster".sl_log_2 where log_origin = '2' and log_xid < 
'562381542'; delete from "_test_cluster".sl_seqlog where seql_origin = 
'2' and seql_ev_seqno < '3'; select "_test_cluster".logswitch_finish(); 
" - ERROR:  cancelling statement due to statement timeout
CONTEXT:  SQL statement "lock table "_test_cluster".sl_config_lock"
PL/pgSQL function "logswitch_finish" line 9 at SQL statement
DEBUG2 slon_retry() from pid=20337
DEBUG1 slon: retry requested
DEBUG2 slon: notify worker process to shutdown
INFO   remoteListenThread_1: disconnecting from 'dbname=test host=test 
user=slony password=pass'
DEBUG1 syncThread: thread done
DEBUG1 remoteListenThread_1: thread done
DEBUG1 localListenThread: thread done
DEBUG1 main: scheduler mainloop returned
DEBUG2 main: wait for remote threads
DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + worker signaled)
DEBUG1 slon: child termination timeout - kill child
DEBUG2 slon: child terminated status: 9; pid: 20337, current worker pid: 
20337
DEBUG1 slon: restart of worker in 10 seconds
CONFIG main: slon version 1.2.9 starting up
DEBUG2 slon: watchdog process started
DEBUG2 slon: watchdog ready - pid = 20336
DEBUG2 slon: worker process created - pid = 11621
CONFIG main: local node id = 2
DEBUG2 main: main process started
CONFIG main: launching sched_start_mainloop
CONFIG main: loading current cluster configuration
CONFIG storeNode: no_id=1 no_comment='Primary Node 1'
DEBUG2 setNodeLastEvent: no_id=1 event_seq=1380820
CONFIG storePath: pa_server=1 pa_client=2 pa_conninfo="dbname=test 
host=test user=slony password=pass" pa_connretry=10
CONFIG storeListen: li_origin=1 li_receiver=2 li_provider=1
CONFIG storeSet: set_id=1 set_origin=1 set_comment='test tables'
WARN   remoteWorker_wakeup: node 1 - no worker thread
DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + worker signaled)
CONFIG storeSet: set_id=2 set_origin=1 set_comment='test action table'
WARN   remoteWorker_wakeup: node 1 - no worker thread
DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + worker signaled)
CONFIG storeSubscribe: sub_set=2 sub_provider=1 sub_forward='f'
WARN   remoteWorker_wakeup: node 1 - no worker thread
DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + worker signaled)
DEBUG2 main: last local event sequence = 419
CONFIG main: configuration complete - starting threads
DEBUG1 localListenThread: thread starts
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=20342
CONFIG enableNode: no_id=1
DEBUG1 main: running scheduler mainloop
DEBUG1 cleanupThread: thread starts
DEBUG1 remoteListenThread_1: thread starts
DEBUG2 remoteListenThread_1: start listening for event origin 1
DEBUG1 remoteWorkerThread_1: thread starts
DEBUG1 syncThread: thread starts
DEBUG1 remoteListenThread_1: connected to 'dbname=test host=test 
user=slony password=pass'
DEBUG2 remoteListenThread_1: queue event 1,1380821 ENABLE_SUBSCRIPTION
DEBUG2 remoteListenThread_1: queue event 1,1380822 SYNC
DEBUG2 remoteListenThread_1: queue event 1,1380823 SYNC
...
DEBUG2 remoteListenThread_1: queue event 1,1381003 SYNC
DEBUG2 remoteListenThread_1: queue event 1,1381004 SYNC
DEBUG2 remoteListenThread_1: queue event 1,1381005 SYNC
DEBUG2 remoteWorkerThread_1: Received event 1,1380821 ENABLE_SUBSCRIPTION
DEBUG2 remoteListenThread_1: queue event 1,1381006 SYNC
DEBUG2 remoteListenThread_1: queue event 1,1381007 SYNC
...
DEBUG2 remoteListenThread_1: queue event 1,1383140 SYNC
DEBUG2 remoteListenThread_1: queue event 1,1383141 SYNC
DEBUG2 syncThread: new sl_action_seq 1 - SYNC 468
DEBUG2 localListenThread: Received event 2,468 SYNC
DEBUG1 copy_set 2
DEBUG1 remoteWorkerThread_1: connected to provider DB
DEBUG2 remoteWorkerThread_1: prepare to copy table "public"."action"


Connection was restarted  duo to "CleanupThread: statement timeout" and 
subscription process started from the scratch.


WHAT HAPPENED BEHIND:
After the table was copied. Slony has started this process (procpid:21563)
select "_test_cluster".finishTableAfterCopy(1); analyze
"public"."action";

which was running for longtime (in my case cca 70min)

and process (procpid:21562)
delete from "_test_cluster".sl_log_1 where log_origin = '2' and log_xid 
< '562440300'; delete from "....
was waiting for procpid:21563.


Postgres was configured with "statement_timeout = 3600000" (60min)

http://www.postgresql.org/docs/8.1/static/runtime-config-client.html#GUC-STATEMENT-TIMEOUT

Now, you can see what was the problem :) The Postgres abort this process 
and replication started again.


SOLUTION:
Check your 'statement_timeout' and ensure that "analyze" has enough time 
to finish.

e.g.
# postgresql.conf

statement_timeout = 10800000 # (3 hours)

or disabled it
statement_timeout = 0  #0 is disabled, in milliseconds


Thanks for help ;)


>>
>>>
>>> If for any reason, slony can not truncate the table before 
>>> replicating it, it delete every lines.
>>
>> The table on "slave" node was blank before replication start.
>> And the message "truncate of ... failed - doing delete" appears for 
>> another tables, which were replicated without problem. So I don't 
>> think it's the problem.
>>
>> There is problem only with bigger (over 10GB) tables.
>>
>>>
>>> 68GB/12GB give about 5 tests to replicate the table ?
>>>
>>> I ommit the last versions of everything you are using.... (os, 
>>> kernel, postgres and slony)
>>>
>>> You must truncate manualy the tables on the 'slave', then restarting 
>>> your replica from scratch.
>>
>> And now I'm not able to work with this "badly" replicated table. Each 
>> command on this table hang up. Note: Slony is stopped.
>>
>>
>>>
>>>> COPY event for this "big" table is in a loop and table size on "slave"
>>>> node is still growing.
>>>>
>>>> On "Master" node, the table size is 12GB.
>>>> On "Slave" node the tables size was 68GB before I have stopped the
>>>> replication.
>>>>
>>>>
>>>> -------------
>>>>
>>>> OS: Ubuntu 6.06.2 LTS , 2.6.15-29-amd64-server, x86_64
>>>>
>>>> DB: postgresql-8.1
>>>>
>>>> SLONY: Version: 1.2.9-2ubuntu0~dapper0
>>>>
>>>>
>>>> Detail of our problematic "12GB" table
>>>> =# \d+ action;
>>>>                                              Table "public.action"
>>>>     Column   |            Type             |
>>>> Modifiers                      | Description
>>>> ------------+-----------------------------+-------------------------------- 
>>>>
>>>> ---------------------+------------- id         | 
>>>> integer                    | not null default
>>>> nextval('action_id_seq'::regclass) |
>>>>   clientid   | integer                     |
>>>>
>>>>   action     | integer                     | not null
>>>>
>>>>   response   | integer                     |
>>>>
>>>>   startdate  | timestamp without time zone | not null default now()
>>>>
>>>>   clienttrid | character varying(128)      | not null
>>>>
>>>>   enddate    | timestamp without time zone |
>>>>
>>>>   servertrid | character varying(128)      |
>>>>
>>>> Indexes:
>>>>      "action_pkey" PRIMARY KEY, btree (id)
>>>>      "action_servertrid_key" UNIQUE, btree (servertrid)
>>>>      "action_action_idx" btree ("action")
>>>>      "action_clientid_idx" btree (clientid)
>>>>      "action_response_idx" btree (response)
>>>>      "action_startdate_idx" btree (startdate)
>>>> Foreign-key constraints:
>>>>      "action_action_fkey" FOREIGN KEY ("action") REFERENCES 
>>>> enum_action(id)
>>>>      "action_clientid_fkey" FOREIGN KEY (clientid) REFERENCES 
>>>> "login"(id)
>>>>      "action_response_fkey" FOREIGN KEY (response) REFERENCES
>>>> enum_error(id)
>>>>
>>>>
>>>> ---------------
>>>>
>>>> Does anyone have an idea what could be wrong?
>>>>
>>>> Thanks,
>>>> Petr
>>>> _______________________________________________
>>>> Slony1-general mailing list
>>>> Slony1-general@lists.slony.info
>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>
>>>
>>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

From JanWieck at Yahoo.com  Fri Mar 14 07:17:30 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Fri Mar 14 07:17:42 2008
Subject: [Slony1-general] Replication problem of "BIG" table - COPY event
	in a loop - SOLVED
In-Reply-To: <47DA7C8F.3020201@nic.cz>
References: <47D93339.4040606@nic.cz>	<200803131600.15706.cedric.villemain@dalibo.com>
	<47D95590.6060608@nic.cz> <47D95BFC.3030600@Yahoo.com>
	<47DA7C8F.3020201@nic.cz>
Message-ID: <47DA88FA.7070105@Yahoo.com>

On 3/14/2008 9:24 AM, Emanuel Petr wrote:

> Postgres was configured with "statement_timeout = 3600000" (60min)

That explains

> Thanks for help ;)

Glad to help.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From JanWieck at Yahoo.com  Fri Mar 14 07:21:13 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Fri Mar 14 07:21:26 2008
Subject: [Slony1-general] out-of-sync status
In-Reply-To: <d4f444290803140059q26cdb03en404d7611263f11a2@mail.gmail.com>
References: <d4f444290803140059q26cdb03en404d7611263f11a2@mail.gmail.com>
Message-ID: <47DA89D9.2090701@Yahoo.com>

On 3/14/2008 3:59 AM, lio bod wrote:
> Hello world,
>  
> I'm searching a way to get in real-time (while slon processes are 
> running) the out-of-sync status of Slony one.
> I want to to know at any moment, which and how much data are in my 
> master and not yet replicated in my slave.
>  
> The idea is to be able to achivied a 'graceful' stop of SlonyI after a 
> disconnection of all clients on master.

Have you taken a look at the sl_status view on the origin?


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From cbbrowne at ca.afilias.info  Fri Mar 14 07:43:11 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Mar 14 07:43:17 2008
Subject: [Slony1-general] slony1_extract_schema : request for information
In-Reply-To: <d4f444290803140106y55a4697dje516f9f2f09c47df@mail.gmail.com>
	(lio bod's message of "Fri, 14 Mar 2008 09:06:24 +0100")
References: <d4f444290803130937pe35cf19s4ecdf5e3627ee0d7@mail.gmail.com>
	<601w6ein2m.fsf@dba2.int.libertyrms.com>
	<d4f444290803131044o70a848bby5272810cc63fa56a@mail.gmail.com>
	<d4f444290803140106y55a4697dje516f9f2f09c47df@mail.gmail.com>
Message-ID: <60skytgydc.fsf@dba2.int.libertyrms.com>

"lio bod" <liobod.slony@gmail.com> writes:
> ok. saw them : pg_dump -s.
>
> It make me realize i havn't read carefullly it delivers 'a copy of the cleaned up schema to standard output'
>
> My mistake. I thought it contains the data also
>
> How could i add data in the dump?

You might modify or rewrite the script to accomplish whatever
additional tasks you wish it to perform; that is one of the merits of
"open source" software.

But that would break the stated purpose of it, which is to deliver "a
copy of the cleaned up schema."

I'd want to rename it before customizing it...
-- 
(reverse (concatenate 'string "moc.enworbbc" "@" "enworbbc"))
http://linuxfinances.info/info/
"On the Internet, no one knows you're using Windows NT"
-- Ramiro Estrugo, restrugo@fateware.com
From liobod.slony at gmail.com  Fri Mar 14 08:10:17 2008
From: liobod.slony at gmail.com (lio bod)
Date: Fri Mar 14 08:10:24 2008
Subject: [Slony1-general] slony1_extract_schema : request for information
In-Reply-To: <60skytgydc.fsf@dba2.int.libertyrms.com>
References: <d4f444290803130937pe35cf19s4ecdf5e3627ee0d7@mail.gmail.com>
	<601w6ein2m.fsf@dba2.int.libertyrms.com>
	<d4f444290803131044o70a848bby5272810cc63fa56a@mail.gmail.com>
	<d4f444290803140106y55a4697dje516f9f2f09c47df@mail.gmail.com>
	<60skytgydc.fsf@dba2.int.libertyrms.com>
Message-ID: <d4f444290803140810p5bd26a3di50bce733e10c77cb@mail.gmail.com>

Agree : that is one of the merits of "open source" software.
Even if that's not the only one...


I had a look on the script. Impacts are :

 # Step 1. :

pg_dump -s $dbname >$TMP.sql
remove the -s option

  # Step 4. :
pg_dump -s $tmpdb
remove the -s option

Is that correct? Am i foregtting something?




2008/3/14, Christopher Browne <cbbrowne@ca.afilias.info>:
>
> "lio bod" <liobod.slony@gmail.com> writes:
> > ok. saw them : pg_dump -s.
> >
> > It make me realize i havn't read carefullly it delivers 'a copy of the
> cleaned up schema to standard output'
> >
> > My mistake. I thought it contains the data also
> >
> > How could i add data in the dump?
>
> You might modify or rewrite the script to accomplish whatever
> additional tasks you wish it to perform; that is one of the merits of
> "open source" software.
>
> But that would break the stated purpose of it, which is to deliver "a
> copy of the cleaned up schema."
>
> I'd want to rename it before customizing it...
> --
> (reverse (concatenate 'string "moc.enworbbc" "@" "enworbbc"))
> http://linuxfinances.info/info/
> "On the Internet, no one knows you're using Windows NT"
> -- Ramiro Estrugo, restrugo@fateware.com
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080314/=
28b461aa/attachment.htm
From haugg at comdasys.com  Tue Mar 18 02:09:34 2008
From: haugg at comdasys.com (Alex Haugg)
Date: Tue Mar 18 02:09:35 2008
Subject: [Slony1-general] slony on two several hosts
Message-ID: <1205831374.16035.61.camel@alex>

hi, i have a question:

in every examples you have the master and the slave on the same host
(local host). 
but i would like to know the configuration from slony master and slony
slave on two several hosts.

were will defined the cluster and the tables( i think on the master )?

were will initialized the connection of slony master and the slony
slave?

were will started the slon process ( i think on the master for the
master and on the slave for the slave )?

please help me.

Thank you very much indeed.


From glynastill at yahoo.co.uk  Tue Mar 18 03:32:56 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue Mar 18 03:33:27 2008
Subject: [Slony1-general] Create user trigger?
Message-ID: <501266.73767.qm@web25812.mail.ukl.yahoo.com>

Hi Chaps,

We're setting up 3 servers replicating using slony. I was wondering if it'd be possible for me to create a set of triggers that fire whenever a user is created/dropped/modified on one of the servers that goes and performs the same action the other two servers.

Does that sound doable?

I'm not sure what the implications of doing this would be, and I'mm not sure the best way to do it.... But this is what I was thinking of doing:

Have a user table containing all the usernames, and every time a user is inserted/deleted from the table create/drop the user using a trigger. Then put the table in replication using slony.

Obviously I'd have to drop and then recreate the users when I wanted to alter them, and I'd have to meke sure permissions were set properly to the table.

Are there any better alternatives?

Thanks




      ___________________________________________________________ 
Rise to the challenge for Sport Relief with Yahoo! For Good  

http://uk.promotions.yahoo.com/forgood/

From nkiraly at collaborativefusion.com  Tue Mar 18 06:13:25 2008
From: nkiraly at collaborativefusion.com (Nicholas Kiraly)
Date: Tue Mar 18 06:13:33 2008
Subject: [Slony1-general] slony on two several hosts
In-Reply-To: <1205831374.16035.61.camel@alex>
References: <1205831374.16035.61.camel@alex>
Message-ID: <47DFBFF5.7080806@collaborativefusion.com>

When you use slonik scripts to create your replicated tables, you can 
run them through slonik on any machine that can login as the slony users 
to all of the database servers. Here's a breakdown of how I usually do it:


Setup your slony users on each database server.

Setup your slon-dbname.conf on each database server.

If your slonik scripts don't, create the matching schema for your 
replicated tables on each database.

Run your slonik scripts to initialize the slony installation for the 
databases on the master. When you do this, slonik will connect to the 
replicas as necessary to create the slony schema in those databases.

With the slony schema installed, then you can start the slon process for 
each database, on each machine. If you use SYNC( ID = source); and then 
a WAIT FOR EVENT to confirm in your slonik to confirm config and 
replication, you will need to start the slon on each machine before your 
script will exit cleanly.


If you watch the logs, you'll see a bunch of activity until all the 
replicas are up to date.



Alex Haugg wrote:
> hi, i have a question:
>
> in every examples you have the master and the slave on the same host
> (local host). 
> but i would like to know the configuration from slony master and slony
> slave on two several hosts.
>
> were will defined the cluster and the tables( i think on the master )?
>
> were will initialized the connection of slony master and the slony
> slave?
>
> were will started the slon process ( i think on the master for the
> master and on the slave for the slave )?
>
> please help me.
>
> Thank you very much indeed.
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>   

-- 



Nicholas J. Kiraly
Collaborative Fusion, Inc.
nkiraly@collaborativefusion.com
412-422-3463 x4024
5849 Forbes Avenue
Pittsburgh, PA 15217 

****************************************************************
IMPORTANT: This message contains confidential information
and is intended only for the individual named. If the reader of
this message is not an intended recipient (or the individual
responsible for the delivery of this message to an intended
recipient), please be advised that any re-use, dissemination,
distribution or copying of this message is prohibited. Please
notify the sender immediately by e-mail if you have received
this e-mail by mistake and delete this e-mail from your system.
E-mail transmission cannot be guaranteed to be secure or
error-free as information could be intercepted, corrupted, lost,
destroyed, arrive late or incomplete, or contain viruses. The
sender therefore does not accept liability for any errors or
omissions in the contents of this message, which arise as a
result of e-mail transmission.
****************************************************************


From darcyb at commandprompt.com  Tue Mar 18 08:52:34 2008
From: darcyb at commandprompt.com (Darcy Buskermolen)
Date: Tue Mar 18 08:53:36 2008
Subject: [Slony1-general] Create user trigger?
In-Reply-To: <501266.73767.qm@web25812.mail.ukl.yahoo.com>
References: <501266.73767.qm@web25812.mail.ukl.yahoo.com>
Message-ID: <200803180852.34708.darcyb@commandprompt.com>

On Tuesday 18 March 2008 03:32:56 Glyn Astill wrote:
> Hi Chaps,
>
> We're setting up 3 servers replicating using slony. I was wondering if it'd
> be possible for me to create a set of triggers that fire whenever a user is
> created/dropped/modified on one of the servers that goes and performs the
> same action the other two servers.
>
> Does that sound doable?
>
> I'm not sure what the implications of doing this would be, and I'mm not
> sure the best way to do it.... But this is what I was thinking of doing:
>
> Have a user table containing all the usernames, and every time a user is
> inserted/deleted from the table create/drop the user using a trigger. Then
> put the table in replication using slony.
>
> Obviously I'd have to drop and then recreate the users when I wanted to
> alter them, and I'd have to meke sure permissions were set properly to the
> table.


While I have not tested what I'm about to perpose, it should work fine.

CREATE TABLE replicated_users(
  usename TEXT PRIMARY KEY,
  password TEXT,
  options TEXT);

CREATE OR REPLACE FUNCTION create_user_func() RETURNS TRIGGER AS $$
DECLARE
  v_query TEXT;
BEGIN

if TG_OP = 'INSERT' THEN
  v_query := 'CREATE USER ' || NEW.usename || ' PASSWORD  ''' || 
NEW.password || ''' ' || NEW.options;
ELSIF TG_OP = 'UPDATE' THEN
  v_query := 'ALTER USER ' || NEW.usename || ' PASSWORD ' || 
NEW.password || ' ' || NEW.options;
ELSE 
  v_query := 'DROP USER ' || NEW.usename;
END IF;

PERFORM v_query;
RETURN NEW;
END;
$$
LANGUAGE 'plpgsql';

CREATE TRIGGER do_user_stuff AFTER INSERT OR UPDATE OR DELETE ON 
replicated_users FOR EACH ROW EXECUTE PROCEDURE create_user_func() ;

make sure you sue slonik's "STORE TRIGGER" command to enable this trigger to 
fire on the replicas as well. so in theory you will now have it all working.

Note I have not tested the above, and it does not contain any error handleing 
etc, but it should be enough to get you going down the right path.


>
> Are there any better alternatives?
>
> Thanks
>
>
>
>
>       ___________________________________________________________
> Rise to the challenge for Sport Relief with Yahoo! For Good
>
> http://uk.promotions.yahoo.com/forgood/
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general



-- 
Darcy Buskermolen
Command Prompt, Inc.
+1.503.667.4564 X 102
http://www.commandprompt.com/
PostgreSQL solutions since 1997
From nkiraly at collaborativefusion.com  Tue Mar 18 09:37:30 2008
From: nkiraly at collaborativefusion.com (Nicholas Kiraly)
Date: Tue Mar 18 09:37:42 2008
Subject: [Slony1-general] slony on two several hosts
In-Reply-To: <1205856654.16035.65.camel@alex>
References: <1205831374.16035.61.camel@alex>	
	<47DFBFF5.7080806@collaborativefusion.com>
	<1205856654.16035.65.camel@alex>
Message-ID: <47DFEFCA.4000602@collaborativefusion.com>

You can technically have all of the slons on another server, or all on 
the master, by specifying the conn_info for the node as a remote server.
However slony documentation, as well as myself recommend running a slon 
for each cluster node on the server that hosts the database. The slon 
communicates with the database often.

So, best practice, you need to setup a slon-dbname.conf on each 
postgresql server, that connects to the database on that server, and run 
the slon for the cluster on the master, and then also on each replica.


Alex Haugg wrote:
> hi,
>
> i have a question:
> you starts the slon processes for the master and the slaves on one
> machine (on the master) for all machines?
>
> thank you for the answer.
>
> Am Dienstag, den 18.03.2008, 09:13 -0400 schrieb Nicholas Kiraly:
>   
>> When you use slonik scripts to create your replicated tables, you can 
>> run them through slonik on any machine that can login as the slony users 
>> to all of the database servers. Here's a breakdown of how I usually do it:
>>
>>
>> Setup your slony users on each database server.
>>
>> Setup your slon-dbname.conf on each database server.
>>
>> If your slonik scripts don't, create the matching schema for your 
>> replicated tables on each database.
>>
>> Run your slonik scripts to initialize the slony installation for the 
>> databases on the master. When you do this, slonik will connect to the 
>> replicas as necessary to create the slony schema in those databases.
>>
>> With the slony schema installed, then you can start the slon process for 
>> each database, on each machine. If you use SYNC( ID = source); and then 
>> a WAIT FOR EVENT to confirm in your slonik to confirm config and 
>> replication, you will need to start the slon on each machine before your 
>> script will exit cleanly.
>>
>>
>> If you watch the logs, you'll see a bunch of activity until all the 
>> replicas are up to date.
>>
>>
>>
>> Alex Haugg wrote:
>>     
>>> hi, i have a question:
>>>
>>> in every examples you have the master and the slave on the same host
>>> (local host). 
>>> but i would like to know the configuration from slony master and slony
>>> slave on two several hosts.
>>>
>>> were will defined the cluster and the tables( i think on the master )?
>>>
>>> were will initialized the connection of slony master and the slony
>>> slave?
>>>
>>> were will started the slon process ( i think on the master for the
>>> master and on the slave for the slave )?
>>>
>>> please help me.
>>>
>>> Thank you very much indeed.
>>>
>>>
>>>
>>>       

From jennifer.spencer at stanford.edu  Tue Mar 18 13:28:40 2008
From: jennifer.spencer at stanford.edu (Jennifer Spencer)
Date: Tue Mar 18 13:28:52 2008
Subject: [Slony1-general] Log shipping, plain text only? If so,
	what numerical accuracy?
Message-ID: <47E025F8.1080000@stanford.edu>

Hi All -
I am new to Slony-1.  I have searched the archives for the last six months and haven't seen anything
about this topic, so I am going to ask.

First question: from what I can tell so far, Slony-1 log shipping only uses the Plain Text format for
its shipped logs.  True or false?  If false, how do I specify binary or any other format?  Or is plain
text my only option?  My psql on ingest (at the other node) really doesn't like the "/"s it encounters
in the plain text log.  Must I parse my log first to remove those, or what?

Second, we are setting up Slony-1 log shipping from our shop to a few others.  Application is a
science mission in the Physics Lab.  A lot of our data, essential for science, is stored as "double".
  We need to be able to log ship that science data.  If we must use plain text to log ship, what kind
of accuracy can we expect for data type 'double', or 64 bit floating point numbers contained in that log?

Thank you for any answers,
Jennifer Spencer

From mdavidson at trenstar.com  Wed Mar 19 08:02:25 2008
From: mdavidson at trenstar.com (Melvin Davidson)
Date: Wed Mar 19 08:01:44 2008
Subject: [Slony1-general] Slony with Fedora & Unbuntu
Message-ID: <5CA4D07E8D52ED4AB5499CA4E089858110A0A0EA6D@star2.corp.trenstar.net>

My organization is planning to replace our Fedora servers with Unbuntu. That means that for a short while, we may need to replicate with Slony
from Fedora / PostgreSQL 8.2.5 to Unbuntu / PostgreSQL 8.2.5. I am hoping that as long as the PostgreSQL version is the same, it will work.
Can anyone confirm this?

Thanks in advance,
Melvin Davidson
mdavidson!@trenstar.com<mailto:mdavidson!@trenstar.com>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080319/aa86dc3a/attachment.htm
From jeff at frostconsultingllc.com  Wed Mar 19 08:05:11 2008
From: jeff at frostconsultingllc.com (Jeff Frost)
Date: Wed Mar 19 08:05:16 2008
Subject: [Slony1-general] Slony with Fedora & Unbuntu
In-Reply-To: <5CA4D07E8D52ED4AB5499CA4E089858110A0A0EA6D@star2.corp.trenstar.net>
References: <5CA4D07E8D52ED4AB5499CA4E089858110A0A0EA6D@star2.corp.trenstar.net>
Message-ID: <Pine.LNX.4.64.0803190804430.6069@discord.home.frostconsultingllc.com>

On Wed, 19 Mar 2008, Melvin Davidson wrote:

> My organization is planning to replace our Fedora servers with Unbuntu. That means that for a short while, we may need to replicate with Slony
> from Fedora / PostgreSQL 8.2.5 to Unbuntu / PostgreSQL 8.2.5. I am hoping that as long as the PostgreSQL version is the same, it will work.
> Can anyone confirm this?

It will work.  The PostgreSQL versions do not need to be the same.  You could 
couple this with an upgrade to 8.2.7 or even 8.3.1 if you like.

-- 
Jeff Frost, Owner 	<jeff@frostconsultingllc.com>
Frost Consulting, LLC 	http://www.frostconsultingllc.com/
Phone: 650-780-7908	FAX: 650-649-1954
From darcyb at commandprompt.com  Wed Mar 19 08:07:20 2008
From: darcyb at commandprompt.com (Darcy Buskermolen)
Date: Wed Mar 19 08:08:22 2008
Subject: [Slony1-general] Slony with Fedora & Unbuntu
In-Reply-To: <5CA4D07E8D52ED4AB5499CA4E089858110A0A0EA6D@star2.corp.trenstar.net>
References: <5CA4D07E8D52ED4AB5499CA4E089858110A0A0EA6D@star2.corp.trenstar.net>
Message-ID: <200803190807.20959.darcyb@commandprompt.com>

On Wednesday 19 March 2008 08:02:25 Melvin Davidson wrote:
> My organization is planning to replace our Fedora servers with Unbuntu.
> That means that for a short while, we may need to replicate with Slony from
> Fedora / PostgreSQL 8.2.5 to Unbuntu / PostgreSQL 8.2.5. I am hoping that
> as long as the PostgreSQL version is the same, it will work. Can anyone
> confirm this?


That's one of the large strengths of Slony, it will allow you do to migrations 
across architectures and versions. 

So yes replicating from Fedora to Ubuntu is fine, even if the versions of 
PostgreSQL are not the same
 

>
> Thanks in advance,
> Melvin Davidson
> mdavidson!@trenstar.com<mailto:mdavidson!@trenstar.com>



-- 
Darcy Buskermolen
Command Prompt, Inc.
+1.503.667.4564 X 102
http://www.commandprompt.com/
PostgreSQL solutions since 1997
From liobod.slony at gmail.com  Fri Mar 21 03:10:26 2008
From: liobod.slony at gmail.com (lio bod)
Date: Fri Mar 21 03:10:56 2008
Subject: [Slony1-general] hot /etc/slon.conf
Message-ID: <d4f444290803210310y4097228bwfc3cfa345a9011f0@mail.gmail.com>

Hello world,

Is there a way to change values in /etc/slon.conf without stop/restarting
slon processes?

cheers
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080321/=
b5d709b6/attachment.htm
From liobod.slony at gmail.com  Fri Mar 21 06:21:04 2008
From: liobod.slony at gmail.com (lio bod)
Date: Fri Mar 21 06:21:09 2008
Subject: [Slony1-general] slon_start & config file slon_tools.conf
Message-ID: <d4f444290803210621p40b452fet464074e65b58912@mail.gmail.com>

Hello world,

I wonder about two things using perl sscripts slon_start.
I use it this way (with a conf file not located in default place
/usr/local/etc/slon_tools.conf)

$> slon_start =96config /myPath/slon_tools.conf 1

In this case, the first issue is to see some output going over log
configured in conf.
Some std err not properly defined? for Slon? for perl? Any hint?

As example :


Slony-I: cleanup stale sl_nodelock entry for pid=3D10838

CONTEXT:  PL/pgSQL function "cleanupevent" line 77 at perform

Can't locate /usr/local/etc/slon_tools.conf in @INC (@INC contains:
/usr/lib/perl5/5.8.5/i386-linux-thread-multi /usr/lib/perl5/5.8.5
/usr/lib/perl5/site_perl/5.8.5/i386-linux-thread-multi
/usr/lib/perl5/site_perl/5.8.4/i386-linux-thread-multi
/usr/lib/perl5/site_perl/5.8.3/i386-linux-thread-multi
/usr/lib/perl5/site_perl/5.8.2/i386-linux-thread-multi
/usr/lib/perl5/site_perl/5.8.1/i386-linux-thread-multi
/usr/lib/perl5/site_perl/5.8.0/i386-linux-thread-multi
/usr/lib/perl5/site_perl/5.8.5 /usr/lib/perl5/site_perl/5.8.4
/usr/lib/perl5/site_perl/5.8.3 /usr/lib/perl5/site_perl/5.8.2
/usr/lib/perl5/site_perl/5.8.1 /usr/lib/perl5/site_perl/5.8.0
/usr/lib/perl5/site_perl
/usr/lib/perl5/vendor_perl/5.8.5/i386-linux-thread-multi
/usr/lib/perl5/vendor_perl/5.8.4/i386-linux-thread-multi
/usr/lib/perl5/vendor_perl/5.8.3/i386-linux-thread-multi
/usr/lib/perl5/vendor_perl/5.8.2/i386-linux-thread-multi
/usr/lib/perl5/vendor_perl/5.8.1/i386-linux-thread-multi
/usr/lib/perl5/vendor_perl/5.8.0/i386-linux-thread-multi
/usr/lib/perl5/vendor_perl/5.8.5 /usr/lib/perl5/vendor_perl/5.8.4
/usr/lib/perl5/vendor_perl/5.8.3 /usr/lib/perl5/vendor_perl/5.8.2
/usr/lib/perl5/vendor_perl/5.8.1 /usr/lib/perl5/vendor_perl/5.8.0
/usr/lib/perl5/vendor_perl .) at /usr/local/bin/slonik_restart _node line
31.

Second issue is to see such a warning about perl searching
/usr/local/etc/slon_tools.conf
whereas the command tell clearly defined to use it in may path.

I know a workaround would be to put my config file precisly in replacement
of /usr/local/etc/slon_tools.conf but it breaks my deploiement procedure.

Comments are welcome,

rgds
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080321/=
61c150c9/attachment.htm
From liobod.slony at gmail.com  Fri Mar 21 06:58:00 2008
From: liobod.slony at gmail.com (lio bod)
Date: Fri Mar 21 06:58:06 2008
Subject: [Slony1-general] redhat service and slons process
Message-ID: <d4f444290803210658x782dbfc9iad4beb076b8591b5@mail.gmail.com>

(re)hello world,

I installed slony with a rpm and i notice it has installed a redhat service.

Here is what i see :
 $> chkconfig --list | grep slony
postgresql-slony1       0:off 1:off 2:off 3:off 4:off 5:off 6:off

Does it make sense to use that feature to make my slons process restart at
boot time?
If so, how can i make sure which config is going to take?
At this moment, i start slon with perl and i do not use default
/usr/local/etc/slon_tools.conf.
Would it be compliant?

thx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080321/=
69f25885/attachment.htm
From dba at richyen.com  Fri Mar 21 12:23:23 2008
From: dba at richyen.com (Richard Yen)
Date: Fri Mar 21 12:23:37 2008
Subject: [Slony1-general] hot /etc/slon.conf
In-Reply-To: <d4f444290803210310y4097228bwfc3cfa345a9011f0@mail.gmail.com>
References: <d4f444290803210310y4097228bwfc3cfa345a9011f0@mail.gmail.com>
Message-ID: <05BB622B-70E8-4999-9619-15E8D63311F6@richyen.com>

In my experience, I just do "kill -HUP <processID>" on the slon  
process, and it reloads the conf, as with any other app...

--Richard



On Mar 21, 2008, at 3:10 AM, lio bod wrote:

> Hello world,
>
> Is there a way to change values in /etc/slon.conf without stop/ 
> restarting slon processes?
>
> cheers
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

From henry at zen.co.za  Mon Mar 24 02:02:18 2008
From: henry at zen.co.za (Henry)
Date: Mon Mar 24 02:02:51 2008
Subject: [Slony1-general] Initial replication analyze
Message-ID: <63157.196.23.181.69.1206349338.squirrel@zenmail.co.za>

Hello all,

During an initial replication from a master to many slaves, why does slony
perform an ANALYZE after the COPY?  For paltry tables this is OK, but for
massive (multiple GB) tables, this is something which takes frigging
*days* per table.

Besides, what's the point of analyzing after a fresh COPY?

Even if there is a valid reason to analyze after a copy, is there a way to
disable this behavior (short of hacking the src)?  The docs don't tell me
much.

Thanks
Henry

From henry at zen.co.za  Mon Mar 24 02:10:40 2008
From: henry at zen.co.za (Henry)
Date: Mon Mar 24 02:11:11 2008
Subject: [Slony1-general] Where to look if a node falls behind
Message-ID: <63160.196.23.181.69.1206349840.squirrel@zenmail.co.za>

Hello once again,

Let's say you have many slaves being rep'd from a master.  Sometimes, one
of these slaves will fall behind in a big way.  Even stopping all activity
on all systems to allow it to catch up doesn't resolve the problem.

My question is the following:  from an admin point of view in trying to
resolve this kind of issue, what slony tables should I poke around in (and
what flag/s should I take note of), and what errors/footprints should I
look for in the slony logs which might be contributing to the node in
question never catching up?

My (horribly noob) solution so far has been to stop everything, drop
replication systems from all nodes, and start again (a process which can
throw a week in the drain).

Pointers and/or suggestions would be welcomed.  I've plodded through the
docs, but the obvious isn't jumping out at me, and my stupid approach to
solving the problem is wasting eons of time each time this occurs.

Thanks
Henry

From ajs at crankycanuck.ca  Mon Mar 24 07:13:25 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon Mar 24 07:13:39 2008
Subject: [Slony1-general] Initial replication analyze
In-Reply-To: <63157.196.23.181.69.1206349338.squirrel@zenmail.co.za>
References: <63157.196.23.181.69.1206349338.squirrel@zenmail.co.za>
Message-ID: <20080324141325.GA22774@crankycanuck.ca>

On Mon, Mar 24, 2008 at 11:02:18AM +0200, Henry wrote:
> Hello all,
> 
> During an initial replication from a master to many slaves, why does slony
> perform an ANALYZE after the COPY?  For paltry tables this is OK, but for

For the same reason you need to ANALYSE after a restore from backup.

> massive (multiple GB) tables, this is something which takes frigging
> *days* per table.

?!  If so, you have something wrong.  ANALYSE shouldn't take that long.  It
samples the table.

> Even if there is a valid reason to analyze after a copy, is there a way to
> disable this behavior (short of hacking the src)?  The docs don't tell me
> much.

Nope.  Your performance on replication will be totally miserable without it. 
You MUST do it.

A
From ajs at crankycanuck.ca  Mon Mar 24 07:18:40 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon Mar 24 07:18:50 2008
Subject: [Slony1-general] Where to look if a node falls behind
In-Reply-To: <63160.196.23.181.69.1206349840.squirrel@zenmail.co.za>
References: <63160.196.23.181.69.1206349840.squirrel@zenmail.co.za>
Message-ID: <20080324141840.GB22774@crankycanuck.ca>

On Mon, Mar 24, 2008 at 11:10:40AM +0200, Henry wrote:

> Let's say you have many slaves being rep'd from a master.  Sometimes, one
> of these slaves will fall behind in a big way.  Even stopping all activity
> on all systems to allow it to catch up doesn't resolve the problem.

If you restart the slons, does it help?
 
> My question is the following:  from an admin point of view in trying to
> resolve this kind of issue, what slony tables should I poke around in (and
> what flag/s should I take note of), and what errors/footprints should I
> look for in the slony logs which might be contributing to the node in
> question never catching up?

It's sort of impossible to say in your case, because you've given us so
little to work with.  But I'd start looking at _slony_schema.sl_status.  I'd
also have a look at the syncs in the logs from the slons for the origin and
that replica, and compare with the slon logs from a working replica.  I'd
also look at the pg_locks view on the affected node.

> My (horribly noob) solution so far has been to stop everything, drop
> replication systems from all nodes, and start again (a process which can
> throw a week in the drain).

That does not seem to be a great idea, I agree.  You could improve this
global thermonuclear war option to be merely a neutron bomb by performing a
DROP NODE for just the bad node.  But it'd be better to figure out what's
wrong.

A
From cbbrowne at ca.afilias.info  Mon Mar 24 07:55:23 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Mar 24 07:55:31 2008
Subject: [Slony1-general] Initial replication analyze
In-Reply-To: <63157.196.23.181.69.1206349338.squirrel@zenmail.co.za>
	(henry@zen.co.za's message of "Mon,
	24 Mar 2008 11:02:18 +0200 (SAST)")
References: <63157.196.23.181.69.1206349338.squirrel@zenmail.co.za>
Message-ID: <60lk488944.fsf@dba2.int.libertyrms.com>

"Henry" <henry@zen.co.za> writes:
> Hello all,
>
> During an initial replication from a master to many slaves, why does slony
> perform an ANALYZE after the COPY?  For paltry tables this is OK, but for
> massive (multiple GB) tables, this is something which takes frigging
> *days* per table.
>
> Besides, what's the point of analyzing after a fresh COPY?
>
> Even if there is a valid reason to analyze after a copy, is there a way to
> disable this behavior (short of hacking the src)?  The docs don't tell me
> much.

COPY does not populate statistics in pg_statistic, so it is
*necessary* to ANALYZE the table so that you don't get pathological
awful behaviour like a default of assuming the table contains 1000
tuples so that updates may be reasonably performed via Seq Scan.

The ANALYZE is there because it is necessary.  And unless you have
been heavily hacking with "SET STATISTICS," an analyze on even a large
table shouldn't be taking painfully long - it normally scans 3000
pages, many of which ought to be _in memory_, and even if they're not,
that's still merely 24MB worth of reads.
-- 
(format nil "~S@~S" "cbbrowne" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From cbbrowne at ca.afilias.info  Mon Mar 24 08:01:47 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Mar 24 08:01:53 2008
Subject: [Slony1-general] Where to look if a node falls behind
In-Reply-To: <63160.196.23.181.69.1206349840.squirrel@zenmail.co.za>
	(henry@zen.co.za's message of "Mon,
	24 Mar 2008 11:10:40 +0200 (SAST)")
References: <63160.196.23.181.69.1206349840.squirrel@zenmail.co.za>
Message-ID: <60hcew88tg.fsf@dba2.int.libertyrms.com>

"Henry" <henry@zen.co.za> writes:
> Hello once again,
>
> Let's say you have many slaves being rep'd from a master.  Sometimes, one
> of these slaves will fall behind in a big way.  Even stopping all activity
> on all systems to allow it to catch up doesn't resolve the problem.
>
> My question is the following:  from an admin point of view in trying to
> resolve this kind of issue, what slony tables should I poke around in (and
> what flag/s should I take note of), and what errors/footprints should I
> look for in the slony logs which might be contributing to the node in
> question never catching up?
>
> My (horribly noob) solution so far has been to stop everything, drop
> replication systems from all nodes, and start again (a process which can
> throw a week in the drain).
>
> Pointers and/or suggestions would be welcomed.  I've plodded through the
> docs, but the obvious isn't jumping out at me, and my stupid approach to
> solving the problem is wasting eons of time each time this occurs.

Step 0.

Run test_slony_state.pl / test_slony_state-dbi.pl (depending on
whether you prefer Perl Pg or Perl DBI::Pg).

That does quite a bit of analysis that should be helpful in figuring
out where problems may lie.

[This is step 0, not step 1, because Best Practices indicate that you
should set up every replication cluster to run
test_slony_state-dbi.pl/test_slony_state.pl frequently, likely
hourly.]

If you look at that script, and look at the tests that it runs to
check the health of the cluster, that should be at least somewhat
helpful in figuring out some of the things that can go wrong.
-- 
let name="cbbrowne" and tld="acm.org" in String.concat "@" [name;tld];;
http://cbbrowne.com/info/finances.html
Would-be National Mottos:
Switzerland: "You wouldn't hit a country that's neutral, would you?"
From henry at zen.co.za  Mon Mar 24 10:48:04 2008
From: henry at zen.co.za (Henry)
Date: Mon Mar 24 10:48:24 2008
Subject: [Slony1-general] Initial replication analyze
In-Reply-To: <20080324141325.GA22774@crankycanuck.ca>
References: <63157.196.23.181.69.1206349338.squirrel@zenmail.co.za>
	<20080324141325.GA22774@crankycanuck.ca>
Message-ID: <63794.196.23.181.69.1206380884.squirrel@zenmail.co.za>

On Mon, March 24, 2008 4:13 pm, Andrew Sullivan wrote:
> On Mon, Mar 24, 2008 at 11:02:18AM +0200, Henry wrote:
>> Hello all,
>>
>> During an initial replication from a master to many slaves, why does
>> slony
>> perform an ANALYZE after the COPY?  For paltry tables this is OK, but
>> for
>
> For the same reason you need to ANALYSE after a restore from backup.


Hmm, ok.  I was under the mistaken impression that ANALYZE after COPY was
like fsck after mkfs ... ie, unnecessary.  I re-read the PG docs, and yup,
you're right.  ANALYZE after bulk loading is always a good idea for
performance.

>> massive (multiple GB) tables, this is something which takes frigging
>> *days* per table.
>
> ?!  If so, you have something wrong.  ANALYSE shouldn't take that long.
> It samples the table.

OK, maybe not days per table, but what feels like hundreds of hours per
bloody table... ;-)

>
>> Even if there is a valid reason to analyze after a copy, is there a way
>> to
>> disable this behavior (short of hacking the src)?  The docs don't tell
>> me
>> much.
>
> Nope.  Your performance on replication will be totally miserable without
> it.
> You MUST do it.

Right you are.

Thanks for the comments.

From henry at zen.co.za  Mon Mar 24 10:54:39 2008
From: henry at zen.co.za (Henry)
Date: Mon Mar 24 10:54:55 2008
Subject: [Slony1-general] Where to look if a node falls behind
In-Reply-To: <20080324141840.GB22774@crankycanuck.ca>
References: <63160.196.23.181.69.1206349840.squirrel@zenmail.co.za>
	<20080324141840.GB22774@crankycanuck.ca>
Message-ID: <63802.196.23.181.69.1206381279.squirrel@zenmail.co.za>



On Mon, March 24, 2008 4:18 pm, Andrew Sullivan wrote:
> On Mon, Mar 24, 2008 at 11:10:40AM +0200, Henry wrote:
>
>> Let's say you have many slaves being rep'd from a master.  Sometimes,
>> one
>> of these slaves will fall behind in a big way.  Even stopping all
>> activity
>> on all systems to allow it to catch up doesn't resolve the problem.
>
> If you restart the slons, does it help?

Yes, that's always been my first step.  Problem is, there's still a very
slow trickle of mods coming in externally from the cluster (but with major
volume mods stopped), and even this slow trickle seems to prevent the
offending node from catching up.

>
>> My question is the following:  from an admin point of view in trying to
>> resolve this kind of issue, what slony tables should I poke around in
>> (and
>> what flag/s should I take note of), and what errors/footprints should I
>> look for in the slony logs which might be contributing to the node in
>> question never catching up?
>
> It's sort of impossible to say in your case, because you've given us so
> little to work with.  But I'd start looking at _slony_schema.sl_status.
> I'd
> also have a look at the syncs in the logs from the slons for the origin
> and
> that replica, and compare with the slon logs from a working replica.  I'd
> also look at the pg_locks view on the affected node.
>
>> My (horribly noob) solution so far has been to stop everything, drop
>> replication systems from all nodes, and start again (a process which can
>> throw a week in the drain).
>
> That does not seem to be a great idea, I agree.  You could improve this
> global thermonuclear war option to be merely a neutron bomb by performing
> a DROP NODE for just the bad node.  But it'd be better to figure out
> what's wrong.

Now that sounds like a killer idea (at least until I figure out what the
hell's causing the problem).  I'll poke around and see if I can figure out
how to drop a node and re-subscribe it (not sure if that's idiomatically
correct).

h

From henry at zen.co.za  Mon Mar 24 11:21:20 2008
From: henry at zen.co.za (Henry)
Date: Mon Mar 24 11:21:37 2008
Subject: [Slony1-general] Initial replication analyze
In-Reply-To: <60lk488944.fsf@dba2.int.libertyrms.com>
References: <63157.196.23.181.69.1206349338.squirrel@zenmail.co.za>
	<60lk488944.fsf@dba2.int.libertyrms.com>
Message-ID: <63820.196.23.181.69.1206382880.squirrel@zenmail.co.za>

On Mon, March 24, 2008 4:55 pm, Christopher Browne wrote:
> COPY does not populate statistics in pg_statistic, so it is
> *necessary* to ANALYZE the table so that you don't get pathological
> awful behaviour like a default of assuming the table contains 1000
> tuples so that updates may be reasonably performed via Seq Scan.
>
> The ANALYZE is there because it is necessary.  And unless you have
> been heavily hacking with "SET STATISTICS," an analyze on even a large
> table shouldn't be taking painfully long - it normally scans 3000
> pages, many of which ought to be _in memory_, and even if they're not,
> that's still merely 24MB worth of reads.

Thanks for the comments.  You learn something new every day.  OK, after
what you've said I've been comparing ANALYZE performance on different
nodes to figure out what's going on.

Here's the strange thing: if I manually ANALYZE a comparably sized table
on the same node as the one below, then it completes in under a minute. 
However, as you can see below, the age of the finishTableAfterCopy/analyze
has been sawing away for over 7 hours...

current_query | select "_xxx_cluster".finishTableAfterCopy(19); analyze
"public"."indexing_page";
age           | 07:10:51.116112

Am I miss-reading the info above?  Is it busy with an analyze, or is it
busy chugging away on something else prior to the analyze (the delimiter ;
seems to imply that)?

Regards
h

From ajs at crankycanuck.ca  Mon Mar 24 11:35:05 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon Mar 24 11:35:34 2008
Subject: [Slony1-general] Initial replication analyze
In-Reply-To: <63820.196.23.181.69.1206382880.squirrel@zenmail.co.za>
References: <63157.196.23.181.69.1206349338.squirrel@zenmail.co.za>
	<60lk488944.fsf@dba2.int.libertyrms.com>
	<63820.196.23.181.69.1206382880.squirrel@zenmail.co.za>
Message-ID: <20080324183505.GF23447@crankycanuck.ca>

On Mon, Mar 24, 2008 at 08:21:20PM +0200, Henry wrote:
> current_query | select "_xxx_cluster".finishTableAfterCopy(19); analyze
> "public"."indexing_page";
> age           | 07:10:51.116112
> 
> Am I miss-reading the info above?  Is it busy with an analyze, or is it
> busy chugging away on something else prior to the analyze (the delimiter ;
> seems to imply that)?

Yes, busy with SELECT "_xxx_cluster".finishTableAfterCopy(19);

I don't know why that'd take a long time, though, unless there's a lock
somewhere.  Is there?  What does pg_locks say?

A

From henry at zen.co.za  Mon Mar 24 11:36:21 2008
From: henry at zen.co.za (Henry)
Date: Mon Mar 24 11:36:35 2008
Subject: [Slony1-general] Where to look if a node falls behind
In-Reply-To: <5CA4D07E8D52ED4AB5499CA4E089858110A0A0EBAC@star2.corp.trenstar.net>
References: <63160.196.23.181.69.1206349840.squirrel@zenmail.co.za>
	<20080324141840.GB22774@crankycanuck.ca>
	<63802.196.23.181.69.1206381279.squirrel@zenmail.co.za>
	<5CA4D07E8D52ED4AB5499CA4E089858110A0A0EBAC@star2.corp.trenstar.net>
Message-ID: <63835.196.23.181.69.1206383781.squirrel@zenmail.co.za>



On Mon, March 24, 2008 8:12 pm, Melvin Davidson wrote:
>>Now that sounds like a killer idea (at least until I figure out what the
>>hell's causing the problem).  I'll poke around and see if I can figure
>> out
>>how to drop a node and re-subscribe it (not sure if that's idiomatically
>>correct).
>
> Does this help?
>
> eg: DROP NODE ( ID = 2 );
>     STORE NODE (id=2, comment='Slave for whatever');
>     SUBSCRIBE SET (ID=1, PROVIDER=1, RECEIVER=2, FORWARD=YES);


Looks good, thanks.  Will give it a try the next time things go awry.

Regards
henry

From ajs at crankycanuck.ca  Mon Mar 24 11:36:53 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon Mar 24 11:37:14 2008
Subject: [Slony1-general] Where to look if a node falls behind
In-Reply-To: <63802.196.23.181.69.1206381279.squirrel@zenmail.co.za>
References: <63160.196.23.181.69.1206349840.squirrel@zenmail.co.za>
	<20080324141840.GB22774@crankycanuck.ca>
	<63802.196.23.181.69.1206381279.squirrel@zenmail.co.za>
Message-ID: <20080324183653.GG23447@crankycanuck.ca>

On Mon, Mar 24, 2008 at 07:54:39PM +0200, Henry wrote:
> Yes, that's always been my first step.  Problem is, there's still a very
> slow trickle of mods coming in externally from the cluster (but with major
> volume mods stopped), and even this slow trickle seems to prevent the
> offending node from catching up.

Well, define "catching up".  Is it getting _anything_ in?  Is this the same
one you're having the other analyse problem with?
> 
> Now that sounds like a killer idea (at least until I figure out what the

No, it's a miserable hack and nothing more.  

More information on your cluster layout &c. would probably help.

A

From henry at zen.co.za  Mon Mar 24 11:50:45 2008
From: henry at zen.co.za (Henry)
Date: Mon Mar 24 11:51:02 2008
Subject: [Slony1-general] Initial replication analyze
In-Reply-To: <20080324183505.GF23447@crankycanuck.ca>
References: <63157.196.23.181.69.1206349338.squirrel@zenmail.co.za>
	<60lk488944.fsf@dba2.int.libertyrms.com>
	<63820.196.23.181.69.1206382880.squirrel@zenmail.co.za>
	<20080324183505.GF23447@crankycanuck.ca>
Message-ID: <63902.196.23.181.69.1206384645.squirrel@zenmail.co.za>

On Mon, March 24, 2008 8:35 pm, Andrew Sullivan wrote:
> Yes, busy with SELECT "_xxx_cluster".finishTableAfterCopy(19);
>
> I don't know why that'd take a long time, though, unless there's a lock
> somewhere.  Is there?  What does pg_locks say?

I'm not sure what I'm looking at in pg_locks.  If I exec:

select  p.*,t.relname from pg_locks p, pg_stat_all_tables t where
p.relation=t.relid;

I get lots of info.  Any idea what relation/lock should I look out/refine
for?

Thanks
h


From ajs at crankycanuck.ca  Mon Mar 24 12:02:19 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon Mar 24 12:02:34 2008
Subject: [Slony1-general] Initial replication analyze
In-Reply-To: <63902.196.23.181.69.1206384645.squirrel@zenmail.co.za>
References: <63157.196.23.181.69.1206349338.squirrel@zenmail.co.za>
	<60lk488944.fsf@dba2.int.libertyrms.com>
	<63820.196.23.181.69.1206382880.squirrel@zenmail.co.za>
	<20080324183505.GF23447@crankycanuck.ca>
	<63902.196.23.181.69.1206384645.squirrel@zenmail.co.za>
Message-ID: <20080324190218.GH23447@crankycanuck.ca>

On Mon, Mar 24, 2008 at 08:50:45PM +0200, Henry wrote:
> 
> I'm not sure what I'm looking at in pg_locks.  If I exec:

You need to look for a lock that isn't granted.

A

From henry at zen.co.za  Mon Mar 24 12:24:22 2008
From: henry at zen.co.za (Henry)
Date: Mon Mar 24 12:24:40 2008
Subject: [Slony1-general] Initial replication analyze
In-Reply-To: <20080324190218.GH23447@crankycanuck.ca>
References: <63157.196.23.181.69.1206349338.squirrel@zenmail.co.za>
	<60lk488944.fsf@dba2.int.libertyrms.com>
	<63820.196.23.181.69.1206382880.squirrel@zenmail.co.za>
	<20080324183505.GF23447@crankycanuck.ca>
	<63902.196.23.181.69.1206384645.squirrel@zenmail.co.za>
	<20080324190218.GH23447@crankycanuck.ca>
Message-ID: <63931.196.23.181.69.1206386662.squirrel@zenmail.co.za>



On Mon, March 24, 2008 9:02 pm, Andrew Sullivan wrote:
> On Mon, Mar 24, 2008 at 08:50:45PM +0200, Henry wrote:
>>
>> I'm not sure what I'm looking at in pg_locks.  If I exec:
>
> You need to look for a lock that isn't granted.

Ah, there are a few, all on sl_config_lock.

These seem to be related to:

begin transaction; set transaction isolation level serializable; lock
table "_zen_cluster".sl_config_lock;

I've always noticed several of these hanging around on all the nodes -
always for extended periods of time.

Is this normal, or is something not happy?

h

From cbbrowne at ca.afilias.info  Mon Mar 24 12:40:17 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Mar 24 12:40:30 2008
Subject: [Slony1-general] Initial replication analyze
In-Reply-To: <63820.196.23.181.69.1206382880.squirrel@zenmail.co.za>
	(henry@zen.co.za's message of "Mon,
	24 Mar 2008 20:21:20 +0200 (SAST)")
References: <63157.196.23.181.69.1206349338.squirrel@zenmail.co.za>
	<60lk488944.fsf@dba2.int.libertyrms.com>
	<63820.196.23.181.69.1206382880.squirrel@zenmail.co.za>
Message-ID: <60bq539ahq.fsf@dba2.int.libertyrms.com>

"Henry" <henry@zen.co.za> writes:

> On Mon, March 24, 2008 4:55 pm, Christopher Browne wrote:
>> COPY does not populate statistics in pg_statistic, so it is
>> *necessary* to ANALYZE the table so that you don't get pathological
>> awful behaviour like a default of assuming the table contains 1000
>> tuples so that updates may be reasonably performed via Seq Scan.
>>
>> The ANALYZE is there because it is necessary.  And unless you have
>> been heavily hacking with "SET STATISTICS," an analyze on even a large
>> table shouldn't be taking painfully long - it normally scans 3000
>> pages, many of which ought to be _in memory_, and even if they're not,
>> that's still merely 24MB worth of reads.
>
> Thanks for the comments.  You learn something new every day.  OK, after
> what you've said I've been comparing ANALYZE performance on different
> nodes to figure out what's going on.
>
> Here's the strange thing: if I manually ANALYZE a comparably sized table
> on the same node as the one below, then it completes in under a minute. 
> However, as you can see below, the age of the finishTableAfterCopy/analyze
> has been sawing away for over 7 hours...
>
> current_query | select "_xxx_cluster".finishTableAfterCopy(19); analyze
> "public"."indexing_page";
> age           | 07:10:51.116112
>
> Am I miss-reading the info above?  Is it busy with an analyze, or is it
> busy chugging away on something else prior to the analyze (the delimiter ;
> seems to imply that)?

Yes, it's busy chugging away reindexing the tables.  That's the main
thing that finishTableAfterCopy() does.

We drop the indexes before loading data in, then reindex, as that is
*WAY* faster than loading the data into a table with indexes on it.

If the table is really big, it is not remarkable for it to take 7h to
regenerate indexes, and the approach we took saved you way more than
7h worth of subscription time...
-- 
"cbbrowne","@","linuxdatabases.info"
http://linuxdatabases.info/info/sap.html
Is A.I. Possible?
Some ask "Can humans create intelligent machines?" In fact, humans do
it all the time. The question needs to be "Since it's possible in the
bedroom, why shouldn't it be possible in the laboratory?"
-- Mark Miller
From henry at zen.co.za  Mon Mar 24 13:39:53 2008
From: henry at zen.co.za (Henry)
Date: Mon Mar 24 13:40:13 2008
Subject: [Slony1-general] Initial replication analyze
In-Reply-To: <60bq539ahq.fsf@dba2.int.libertyrms.com>
References: <63157.196.23.181.69.1206349338.squirrel@zenmail.co.za>
	<60lk488944.fsf@dba2.int.libertyrms.com>
	<63820.196.23.181.69.1206382880.squirrel@zenmail.co.za>
	<60bq539ahq.fsf@dba2.int.libertyrms.com>
Message-ID: <64016.196.23.181.69.1206391193.squirrel@zenmail.co.za>


On Mon, March 24, 2008 9:40 pm, Christopher Browne wrote:
> Yes, it's busy chugging away reindexing the tables.  That's the main
> thing that finishTableAfterCopy() does.

ding-ding.

> We drop the indexes before loading data in, then reindex, as that is
> *WAY* faster than loading the data into a table with indexes on it.

agreed.

> If the table is really big, it is not remarkable for it to take 7h to
> regenerate indexes, and the approach we took saved you way more than
> 7h worth of subscription time...

ok - looks like it's about _reindexing_ and not the subsequent ANALYZE.

Thanks to all for the enlightening comments.

Regards
Henry

From kgorman at hi5.com  Mon Mar 24 13:44:18 2008
From: kgorman at hi5.com (Kenny Gorman)
Date: Mon Mar 24 13:44:34 2008
Subject: [Slony1-general] disable initial COPY
Message-ID: <5D028942-9C35-48A3-973B-43F0A5960B86@hi5.com>

I poked through the archives but didn't see any answers to this  
question.


We have many databases that we replicate via slony (200+ GB each).   
The problem is that the initial COPY is slow, and a single process.   
We have a faster, lower level mechanism to clone an entire database  
much faster, thus I would like to disable the initial truncate/COPY of  
slony during initialization.  This approach would require a small  
downtime to perform the last sync/quiesce, but our application can  
tolerate that.  Essentially I would guarantee that the databases are  
in sync, exactly, using a different mechanism (in this case disk level  
snapshots).  We use this mechanism currently to clone databases, but  
it would be nice to extend it for slony setup.  I looked through the  
source, and it seems possible to change things to support this type of  
initialization.

The process I am thinking of is something like:

start disk sync on A to B
shutdown A
sync last bits from A to B
startup A on different port ( no traffic except me )
startup B on different port ( no traffic except me )
subscribe set in slony w/o TRUNCATE/COPY
restart A on prod port
restart B on prod port
start slon processes


Thanks
Kenny Gorman
DBA
www.hi5.com
From cbbrowne at ca.afilias.info  Mon Mar 24 14:39:43 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Mar 24 14:40:00 2008
Subject: [Slony1-general] disable initial COPY
In-Reply-To: <5D028942-9C35-48A3-973B-43F0A5960B86@hi5.com> (Kenny Gorman's
	message of "Mon, 24 Mar 2008 13:44:18 -0700")
References: <5D028942-9C35-48A3-973B-43F0A5960B86@hi5.com>
Message-ID: <607ifr94yo.fsf@dba2.int.libertyrms.com>

Kenny Gorman <kgorman@hi5.com> writes:
> I poked through the archives but didn't see any answers to this
> question.
>
>
> We have many databases that we replicate via slony (200+ GB each).
> The problem is that the initial COPY is slow, and a single process.
> We have a faster, lower level mechanism to clone an entire database
> much faster, thus I would like to disable the initial truncate/COPY of
> slony during initialization.  This approach would require a small
> downtime to perform the last sync/quiesce, but our application can
> tolerate that.  Essentially I would guarantee that the databases are
> in sync, exactly, using a different mechanism (in this case disk level
> snapshots).  We use this mechanism currently to clone databases, but
> it would be nice to extend it for slony setup.  I looked through the
> source, and it seems possible to change things to support this type of
> initialization.
>
> The process I am thinking of is something like:
>
> start disk sync on A to B
> shutdown A
> sync last bits from A to B
> startup A on different port ( no traffic except me )
> startup B on different port ( no traffic except me )
> subscribe set in slony w/o TRUNCATE/COPY
> restart A on prod port
> restart B on prod port
> start slon processes

Patches are in place to support this process in CVS HEAD, via a new
pair of commands:

 CLONE PREPARE - which you would run somewhere near the beginning of
                 the process; this indicates the plan for a new node 
                 to be created;

 CLONE FINISH - runs later, in effect, once the DB for B becomes
                available.  It then does the "identity change"
                to tell database "B" (which still thinks it's A)
                that it is now "B".

http://lists.slony.info/pipermail/slony1-commit/2008-January/002145.html
http://lists.slony.info/pipermail/slony1-commit/2008-January/002146.html
http://lists.slony.info/pipermail/slony1-commit/2008-January/002147.html
http://lists.slony.info/pipermail/slony1-commit/2008-January/002148.html
http://lists.slony.info/pipermail/slony1-commit/2008-January/002149.html
http://lists.slony.info/pipermail/slony1-commit/2008-January/002150.html
http://lists.slony.info/pipermail/slony1-commit/2008-January/002151.html
http://lists.slony.info/pipermail/slony1-commit/2008-January/002152.html
-- 
output = ("cbbrowne" "@" "linuxdatabases.info")
http://cbbrowne.com/info/lsf.html
Rules of the  Evil Overlord #22. "No matter how tempted  I am with the
prospect  of unlimited  power, I  will  not consume  any energy  field
bigger than my head. <http://www.eviloverlord.com/>
From mailings at oopsware.de  Tue Mar 25 05:10:23 2008
From: mailings at oopsware.de (Bernd Helmle)
Date: Tue Mar 25 05:14:39 2008
Subject: [Slony1-general] Timeframe for 1.2.14 release
Message-ID: <DCD0187753E422095F9E216A@imhotep.credativ.de>

Hi folks,

do we have any timeline when 1.2.14 is going to be released?

-- 
  Thanks

                    Bernd
From kgorman at hi5.com  Tue Mar 25 11:20:47 2008
From: kgorman at hi5.com (Kenny Gorman)
Date: Tue Mar 25 11:21:06 2008
Subject: [Slony1-general] disable initial COPY
In-Reply-To: <607ifr94yo.fsf@dba2.int.libertyrms.com>
References: <5D028942-9C35-48A3-973B-43F0A5960B86@hi5.com>
	<607ifr94yo.fsf@dba2.int.libertyrms.com>
Message-ID: <04D34E8A-3649-418A-84C4-3B40603E2EFD@hi5.com>


On Mar 24, 2008, at 2:39 PM, Christopher Browne wrote:
> Kenny Gorman <kgorman@hi5.com> writes:
>> I poked through the archives but didn't see any answers to this
>> question.
>>
>>
>> We have many databases that we replicate via slony (200+ GB each).
>> The problem is that the initial COPY is slow, and a single process.
>> We have a faster, lower level mechanism to clone an entire database
>> much faster, thus I would like to disable the initial truncate/COPY  
>> of
>> slony during initialization.  This approach would require a small
>> downtime to perform the last sync/quiesce, but our application can
>> tolerate that.  Essentially I would guarantee that the databases are
>> in sync, exactly, using a different mechanism (in this case disk  
>> level
>> snapshots).  We use this mechanism currently to clone databases, but
>> it would be nice to extend it for slony setup.  I looked through the
>> source, and it seems possible to change things to support this type  
>> of
>> initialization.
>>
>> The process I am thinking of is something like:
>>
>> start disk sync on A to B
>> shutdown A
>> sync last bits from A to B
>> startup A on different port ( no traffic except me )
>> startup B on different port ( no traffic except me )
>> subscribe set in slony w/o TRUNCATE/COPY
>> restart A on prod port
>> restart B on prod port
>> start slon processes
>
> Patches are in place to support this process in CVS HEAD, via a new
> pair of commands:
>
> CLONE PREPARE - which you would run somewhere near the beginning of
>                 the process; this indicates the plan for a new node
>                 to be created;
>
> CLONE FINISH - runs later, in effect, once the DB for B becomes
>                available.  It then does the "identity change"
>                to tell database "B" (which still thinks it's A)
>                that it is now "B".
>
> http://lists.slony.info/pipermail/slony1-commit/2008-January/002145.html
> http://lists.slony.info/pipermail/slony1-commit/2008-January/002146.html
> http://lists.slony.info/pipermail/slony1-commit/2008-January/002147.html
> http://lists.slony.info/pipermail/slony1-commit/2008-January/002148.html
> http://lists.slony.info/pipermail/slony1-commit/2008-January/002149.html
> http://lists.slony.info/pipermail/slony1-commit/2008-January/002150.html
> http://lists.slony.info/pipermail/slony1-commit/2008-January/002151.html
> http://lists.slony.info/pipermail/slony1-commit/2008-January/002152.html
> -- 
>

Christopher,

Thanks for pointing me in the right direction!  This should suffice  
for what we need, I will check it out.  Anyone using this option with  
success?

Thanks,
Kenny Gorman
DBA
www.hi5.com

From mdavidson at trenstar.com  Wed Mar 26 09:45:28 2008
From: mdavidson at trenstar.com (Melvin Davidson)
Date: Wed Mar 26 09:44:41 2008
Subject: [Slony1-general] Extra processes?
Message-ID: <5CA4D07E8D52ED4AB5499CA4E089858110A0A0EC98@star2.corp.trenstar.net>

When I start slony, I see two processes which I expect.
ie;
postgres 17090     1  0 10:30 pts/0    00:00:00 slon -d1 -pslon_fsasset_1.pid fsasset_rep dbname=FSAssetMgmt20080310 user=slony host=tsdbdev01.corp.trenstar.net port=5432
postgres 17091 17090  0 10:30 pts/0    00:00:00 slon -d1 -pslon_fsasset_1.pid fsasset_rep dbname=FSAssetMgmt20080310 user=slony host=tsdbdev01.corp.trenstar.net port=5432

However, what I don't expect, is 5 extra processes as below.
postgres 17095 25256  0 10:30 ?        00:00:00 postgres: slony FSAssetMgmt20080310 10.2.0.36(60353) idle
postgres 17097 25256  0 10:30 ?        00:00:00 postgres: slony FSAssetMgmt20080310 10.2.0.36(60354) idle
postgres 17100 25256  0 10:30 ?        00:00:00 postgres: slony FSAssetMgmt20080310 10.2.0.36(60355) idle
postgres 17102 25256  0 10:30 ?        00:00:00 postgres: slony FSAssetMgmt20080310 10.2.0.36(60356) idle
postgres 17106 25256  0 10:30 ?        00:00:00 postgres: slony FSAssetMgmt20080310 10.2.0.12(44119) idle
postgres 17167 25256  0 10:37 ?        00:00:00 postgres: slony FSAssetMgmt20080310 10.2.0.12(60806) idle

Previously I was using version 1.1.2

Is this something new for version 1.2.13 ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080326/d8998515/attachment.htm
From jc at oxado.com  Wed Mar 26 10:00:55 2008
From: jc at oxado.com (Jacques Caron)
Date: Wed Mar 26 10:01:30 2008
Subject: [Slony1-general] Extra processes?
In-Reply-To: <5CA4D07E8D52ED4AB5499CA4E089858110A0A0EC98@star2.corp.tren
	star.net>
References: <5CA4D07E8D52ED4AB5499CA4E089858110A0A0EC98@star2.corp.trenstar.net>
Message-ID: <20080326170143.C314F11B3000@zeus.directinfos.com>

At 17:45 26/03/2008, Melvin Davidson wrote:
>When I start slony, I see two processes which I expect.
>ie;
>postgres 17090     1  0 10:30 pts/0    00:00:00 slon -d1 
>-pslon_fsasset_1.pid fsasset_rep dbname=FSAssetMgmt20080310 
>user=slony host=tsdbdev01.corp.trenstar.net port=5432
>postgres 17091 17090  0 10:30 pts/0    00:00:00 slon -d1 
>-pslon_fsasset_1.pid fsasset_rep dbname=FSAssetMgmt20080310 
>user=slony host=tsdbdev01.corp.trenstar.net port=5432

Those are the slony processes.

>However, what I don't expect, is 5 extra processes as below.
>postgres 17095 25256  0 10:30 ?        00:00:00 postgres: slony 
>FSAssetMgmt20080310 10.2.0.36(60353) idle
>postgres 17097 25256  0 10:30 ?        00:00:00 postgres: slony 
>FSAssetMgmt20080310 10.2.0.36(60354) idle
>postgres 17100 25256  0 10:30 ?        00:00:00 postgres: slony 
>FSAssetMgmt20080310 10.2.0.36(60355) idle
>postgres 17102 25256  0 10:30 ?        00:00:00 postgres: slony 
>FSAssetMgmt20080310 10.2.0.36(60356) idle
>postgres 17106 25256  0 10:30 ?        00:00:00 postgres: slony 
>FSAssetMgmt20080310 10.2.0.12(44119) idle
>postgres 17167 25256  0 10:37 ?        00:00:00 postgres: slony 
>FSAssetMgmt20080310 10.2.0.12(60806) idle

Those are the postgres back-ends slony is connect to (some of them 
are local and some remote). Slony can't replicate much without being 
connected to the databases...

>Previously I was using version 1.1.2
>
>Is this something new for version 1.2.13 ?

Nope.

Jacques.

From jc at oxado.com  Wed Mar 26 10:32:46 2008
From: jc at oxado.com (Jacques Caron)
Date: Wed Mar 26 10:33:11 2008
Subject: [Slony1-general] Extra processes?
In-Reply-To: <5CA4D07E8D52ED4AB5499CA4E089858110A0A0ECA1@star2.corp.tren
	star.net>
References: <5CA4D07E8D52ED4AB5499CA4E089858110A0A0EC98@star2.corp.trenstar.net>
	<20080326170143.C314F11B3000@zeus.directinfos.com>
	<5CA4D07E8D52ED4AB5499CA4E089858110A0A0ECA1@star2.corp.trenstar.net>
Message-ID: <20080326173333.D8F7511B3008@zeus.directinfos.com>

At 18:14 26/03/2008, Melvin Davidson wrote:
>I must disagree with that answer. I can understand 1 backend, but 5 ?
>I only have 1 slave, so 5 processes is excessive.

The slon process (the one that is really active) actually has several 
threads performing various tasks. Each of these threads connects to a 
local or remote postgres backend to do its work. There are worker, 
listener, cleanups threads and more that I forget, so you have many 
threads (check out with ps -axlH or the equivalent for your OS), and 
hence many backends.

>Finally, as stated before, this did not occur when I was using Slony 1.1.2
>There were no extra processes at all then.

You may not have seen them (because you weren't using user "slony" 
and it didn't show up in "ps -axl | grep slon" maybe?), but they were 
most definitely there. It is totally impossible slony could be 
performing any kind of replication without any connections to postgres!

Jacques.

From cbbrowne at ca.afilias.info  Wed Mar 26 11:23:18 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Mar 26 11:23:29 2008
Subject: [Slony1-general] Extra processes?
In-Reply-To: <5CA4D07E8D52ED4AB5499CA4E089858110A0A0EC98@star2.corp.trenstar.net>
	(Melvin Davidson's message of "Wed, 26 Mar 2008 10:45:28 -0600")
References: <5CA4D07E8D52ED4AB5499CA4E089858110A0A0EC98@star2.corp.trenstar.net>
Message-ID: <60od9173ah.fsf@dba2.int.libertyrms.com>

Melvin Davidson <mdavidson@trenstar.com> writes:
> 		   :v="urn:schemas-microsoft-com:vml" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:w="urn:schemas-microsoft-com:office:word"
> 				   xmlns:m="http://schemas.microsoft.com/office/2004/12/omml" xmlns="http://www.w3.org/TR/REC-html40">
>
> 							When I start slony, I see two processes which I expect.:p>
>
> 										  ie;:p>
>
>      postgres 17090???? 1? 0 10:30 pts/0??? 00:00:00 slon -d1 -pslon_fsasset_1.pid fsasset_rep dbname=FSAssetMgmt20080310 user=slony host=tsdbdev01.corp.trenstar.net
> 									       port=5432:p>
>
>      postgres 17091 17090? 0 10:30 pts/0??? 00:00:00 slon -d1 -pslon_fsasset_1.pid fsasset_rep dbname=FSAssetMgmt20080310 user=slony host=tsdbdev01.corp.trenstar.net
> 									       port=5432:p>
>
> 										   :p>?
>
> 						     However, what I don't expect, is 5 extra processes as below.:p>
>
> 			       postgres 17095 25256? 0 10:30 ???????? 00:00:00 postgres: slony FSAssetMgmt20080310 10.2.0.36(60353) idle:p>
>
> 			       postgres 17097 25256? 0 10:30 ???????? 00:00:00 postgres: slony FSAssetMgmt20080310 10.2.0.36(60354) idle:p>
>
> 			       postgres 17100 25256? 0 10:30 ???????? 00:00:00 postgres: slony FSAssetMgmt20080310 10.2.0.36(60355) idle:p>
>
> 			       postgres 17102 25256? 0 10:30 ???????? 00:00:00 postgres: slony FSAssetMgmt20080310 10.2.0.36(60356) idle:p>
>
> 			       postgres 17106 25256? 0 10:30 ???????? 00:00:00 postgres: slony FSAssetMgmt20080310 10.2.0.12(44119) idle:p>
>
> 			       postgres 17167 25256? 0 10:37 ???????? 00:00:00 postgres: slony FSAssetMgmt20080310 10.2.0.12(60806) idle:p>
>
> 										   :p>?
>
> 								 Previously I was using version 1.1.2:p>
>
> 										   :p>?
>
> 							      Is this something new for version 1.2.13 ?:p>

No, that shouldn't be new...

For a given slon (and you have 2 of them), there will be:

a) A connection for the worker thread for the node being managed;

b) A connection apiece for each listener thread needed by this node,
   which will, in a 2 node cluster, where there are necessarily 2
   nodes to listen for, means 2 such connections.

That adds up to 3 connections, for each slon, and if you multiply by 2
slons, you get 6 connections, which is what we see in that process
list.

This is therefore perfectly normal.
-- 
let name="cbbrowne" and tld="linuxfinances.info" in String.concat "@" [name;tld];;
http://www3.sympatico.ca/cbbrowne/rdbms.html
"It   can be   shown   that for any  nutty  theory,  beyond-the-fringe
political view or  strange religion there  exists  a proponent  on the
Net. The proof is left as an exercise for your kill-file."
-- Bertil Jonell
From cbbrowne at ca.afilias.info  Wed Mar 26 11:43:27 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Mar 26 11:43:39 2008
Subject: [Slony1-general] Timeframe for 1.2.14 release
In-Reply-To: <DCD0187753E422095F9E216A@imhotep.credativ.de> (Bernd Helmle's
	message of "Tue, 25 Mar 2008 13:10:23 +0100")
References: <DCD0187753E422095F9E216A@imhotep.credativ.de>
Message-ID: <60k5jp72cw.fsf@dba2.int.libertyrms.com>

Bernd Helmle <mailings@oopsware.de> writes:
> do we have any timeline when 1.2.14 is going to be released?

Jan Wieck is looking at the race condition that Yoshiharu Mori
submitted patches for; we'll have conversation about this ("this"
being both "the race condition" as well as "preparations for release")
while at the conference in Maryland this weekend, and hopefully can do
a release next week.
-- 
let name="cbbrowne" and tld="linuxfinances.info" in name ^ "@" ^ tld;;
http://cbbrowne.com/info/oses.html
Self Reference is its Own Reward
"If tautologies do not convey information, mathematicians would not be
surprised by them."
-- Mark Miller
From haugg at comdasys.com  Thu Mar 27 00:30:10 2008
From: haugg at comdasys.com (Alex Haugg)
Date: Thu Mar 27 00:29:39 2008
Subject: [Slony1-general] database dump
Message-ID: <1206603010.16035.86.camel@alex>

hi,

i have a tiny problem,

i need a db dump command without slony configurations (without:
slony_cluster, slony functions and without slony trigger).

thanks for your answer,

alexander haugg

From stephane.schildknecht at postgresqlfr.org  Thu Mar 27 01:05:59 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-15?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Thu Mar 27 01:06:30 2008
Subject: [Slony1-general] database dump
In-Reply-To: <1206603010.16035.86.camel@alex>
References: <1206603010.16035.86.camel@alex>
Message-ID: <47EB5567.1060306@postgresqlfr.org>

Alex Haugg a ?crit :
> hi,
> 
> i have a tiny problem,
> 
> i need a db dump command without slony configurations (without:
> slony_cluster, slony functions and without slony trigger).
> 
> thanks for your answer,
> 
> alexander haugg

Hi,

It's not a problem, indeed :-)

You may copy the whole master database to a new temporary database (pg_dump).
Then, you issue the SQL command "select _REPLICATION.uninstallnode();" on the
temp database.
Finally you can drop the slony schema on the temp database : "drop schema
_REPLICATION cascade;"

You can then dump the temporary database. It should be free of all slony stuff.

Be aware that you have to dump the master database to create the temporary one,
otherwise you won't get the whole schema of your database (you may lack some
triggers, for instance).

Regards,
-- 
St?phane SCHILDKNECHT
Pr?sident de PostgreSQLFr
T?l. 09 53 69 97 12
http://www.postgresqlfr.org
From cbbrowne at ca.afilias.info  Thu Mar 27 07:24:41 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Mar 27 07:24:47 2008
Subject: [Slony1-general] database dump
In-Reply-To: <1206603010.16035.86.camel@alex> (Alex Haugg's message of "Thu,
	27 Mar 2008 08:30:10 +0100")
References: <1206603010.16035.86.camel@alex>
Message-ID: <603aqc6y8m.fsf@dba2.int.libertyrms.com>

Alex Haugg <haugg@comdasys.com> writes:
> i have a tiny problem,
>
> i need a db dump command without slony configurations (without:
> slony_cluster, slony functions and without slony trigger).
>
> thanks for your answer,

If it is acceptable to do this in two parts, there are two tools that
can be used:

1.  tools/slony1_extract_schema.sh is a script that extracts the user
schema for a Slony-I node in the original state with all the
Slony-I-related "cruft" removed.

You could run that against the "master" node, and that will give you a
suitable schema.

2.  You could subsequently run "pg_dump --data-only" against a node
(could be any node) and that will give you a dump of all the data.
The dump will include Slony-I-internal "cruft", but since the schema
doesn't have those tables, the data won't load :-)

Ideally, you would split apart the schema from #1 into two pieces:
  - Firstly, all the CREATE TABLE DDL;

  - Then, you would load the data from #2...

  - Finally, you would run the remainder of the schema from #1, to
    create indexes, triggers, and such like.
-- 
let name="cbbrowne" and tld="acm.org" in name ^ "@" ^ tld;;
http://linuxfinances.info/info/oses.html
Why is  it that when  you're driving and  looking for an  address, you
turn down the volume on the radio?
From mdavidson at trenstar.com  Thu Mar 27 08:07:52 2008
From: mdavidson at trenstar.com (Melvin Davidson)
Date: Thu Mar 27 08:07:01 2008
Subject: [Slony1-general] database dump
In-Reply-To: <603aqc6y8m.fsf@dba2.int.libertyrms.com>
References: <1206603010.16035.86.camel@alex>
	<603aqc6y8m.fsf@dba2.int.libertyrms.com>
Message-ID: <5CA4D07E8D52ED4AB5499CA4E089858110A0A0ECD1@star2.corp.trenstar.net>

-----Original Message-----
From: slony1-general-bounces@lists.slony.info [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Christopher Browne
Sent: Thursday, March 27, 2008 8:25 AM
To: Alex Haugg
Cc: Slony1-general@lists.slony.info
Subject: Re: [Slony1-general] database dump

Alex Haugg <haugg@comdasys.com> writes:
> i have a tiny problem,
>
> i need a db dump command without slony configurations (without:
> slony_cluster, slony functions and without slony trigger).
>
> thanks for your answer,

Another method to do this would be to use SQL to generate scripts to dump only the non slony schemas.
EG:
SELECT 'pg_dump -c -f ' || nspname || '_file_name.backup -i -n ' || nspname
  FROM pg_namespace nsp
  WHERE nsp.oid NOT IN
  (SELECT pronamespace FROM pg_proc WHERE proname = 'slonyversion')
 ORDER BY nspname;

Sample Output:
"pg_dump -c -f information_schema_file_name.backup -i -n information_schema"
"pg_dump -c -f pg_catalog_file_name.backup -i -n pg_catalog"
"pg_dump -c -f pg_temp_1_file_name.backup -i -n pg_temp_1"
"pg_dump -c -f pg_toast_file_name.backup -i -n pg_toast"
"pg_dump -c -f public_file_name.backup -i -n public"

Remove the quotes and you are set.

Melvin Davidson
From rod at iol.ie  Thu Mar 27 09:46:14 2008
From: rod at iol.ie (Raymond O'Donnell)
Date: Thu Mar 27 09:46:23 2008
Subject: [Slony1-general] database dump
In-Reply-To: <1206603010.16035.86.camel@alex>
References: <1206603010.16035.86.camel@alex>
Message-ID: <47EBCF56.1040004@iol.ie>

On 27/03/2008 07:30, Alex Haugg wrote:
> i need a db dump command without slony configurations (without:
> slony_cluster, slony functions and without slony trigger).

This is a FAQ item if ever there was one. :-)

I had a dig around for a FAQ on the Slony website, and couldn't find 
one....did I miss it?

Ray.

---------------------------------------------------------------
Raymond O'Donnell, Director of Music, Galway Cathedral, Ireland
rod@iol.ie
---------------------------------------------------------------
From cbbrowne at ca.afilias.info  Thu Mar 27 11:23:50 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Mar 27 11:24:02 2008
Subject: [Slony1-general] database dump
In-Reply-To: <47EBCF56.1040004@iol.ie> (Raymond O'Donnell's message of "Thu,
	27 Mar 2008 16:46:14 +0000")
References: <1206603010.16035.86.camel@alex> <47EBCF56.1040004@iol.ie>
Message-ID: <60prtg58ll.fsf@dba2.int.libertyrms.com>

Raymond O'Donnell <rod@iol.ie> writes:
> On 27/03/2008 07:30, Alex Haugg wrote:
>> i need a db dump command without slony configurations (without:
>> slony_cluster, slony functions and without slony trigger).
>
> This is a FAQ item if ever there was one. :-)
>
> I had a dig around for a FAQ on the Slony website, and couldn't find
> one....did I miss it?

I have added this to CVS HEAD...
http://lists.slony.info/pipermail/slony1-commit/2008-March/002218.html

At some point, that will be found at the URL below.
-- 
(reverse (concatenate 'string "ofni.secnanifxunil" "@" "enworbbc"))
http://slony.info/documentation/faq.html
Love the   scientific  sampling  language,  when any  sample   that is
selected from  Usenet readers and  additionally self-selected is about
as representative as a wombat is of European wildlife.
-- Madeleine Page
From wmoran at collaborativefusion.com  Thu Mar 27 11:43:26 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Thu Mar 27 11:43:55 2008
Subject: [Slony1-general] Questions on EXECUTE SCRIPT() for dynamically
	generated SQL
Message-ID: <20080327144326.67caa699.wmoran@collaborativefusion.com>


Has anyone figured out a way to feed dynamically generated SQL to
EXECUTE SCRIPT() without creating a temporary file?

Our upgrade process could be a good bit simpler if we could figure
out how to do this.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From wmoran at collaborativefusion.com  Thu Mar 27 11:46:09 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Thu Mar 27 11:46:39 2008
Subject: [Slony1-general] EXECUTE SCRIPT() documentation confusion
Message-ID: <20080327144609.a24802fd.wmoran@collaborativefusion.com>


Looking at the docs for EXECUTE SCRIPT():
http://slony.info/documentation/stmtddlscript.html

I see the following:
"If a table's columns are modified, it is very important that the triggers
be regenerated, otherwise they may be inappropriate for the new form of
the table schema."

I find this statement somewhat ambiguous.  Is it saying that I should do
something in addition to EXECUTE SCRIPT() when altering table columns,
or is it simply pointing out that this is one of the functions that is
performed by EXECUTE SCRIPT()?

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From cbbrowne at ca.afilias.info  Thu Mar 27 12:54:12 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Mar 27 12:54:25 2008
Subject: [Slony1-general] EXECUTE SCRIPT() documentation confusion
In-Reply-To: <20080327144609.a24802fd.wmoran@collaborativefusion.com> (Bill
	Moran's message of "Thu, 27 Mar 2008 14:46:09 -0400")
References: <20080327144609.a24802fd.wmoran@collaborativefusion.com>
Message-ID: <60lk4454ez.fsf@dba2.int.libertyrms.com>

Bill Moran <wmoran@collaborativefusion.com> writes:
> Looking at the docs for EXECUTE SCRIPT():
> http://slony.info/documentation/stmtddlscript.html
>
> I see the following:
> "If a table's columns are modified, it is very important that the triggers
> be regenerated, otherwise they may be inappropriate for the new form of
> the table schema."
>
> I find this statement somewhat ambiguous.  Is it saying that I should do
> something in addition to EXECUTE SCRIPT() when altering table columns,
> or is it simply pointing out that this is one of the functions that is
> performed by EXECUTE SCRIPT()?

It's really just pointing out that it is essential that
alterTableForReplication(tab_id) be run any time a table's schema is
altered.  That is indeed performed automatically.

I'm patching the docs a little bit.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/sap.html
The quickest way to a man's heart is through his chest, with an axe. 
From Nirbhay.Choubey at Sun.COM  Mon Mar 31 01:51:42 2008
From: Nirbhay.Choubey at Sun.COM (Nirbhay Choubey)
Date: Mon Mar 31 01:52:44 2008
Subject: [Slony1-general] Slony-I test status
Message-ID: <47F0A61E.7080202@Sun.COM>


Hi

When I am running the slony-I tests with Postgres 8.3 on Solaris 10 the 
following tests are failing:

1. Testmultiplemoves (hanged at subscribing stage)
2. Testlogship (1 warning)


Apart from this Testpartition is completed successfully with the 
following error :  relation "sales_txns_2006_4" does not exist.

Should the tests mentioned above  supposed to fail?

regards
Nirbhay
 

From jc at oxado.com  Mon Mar 31 07:09:53 2008
From: jc at oxado.com (Jacques Caron)
Date: Mon Mar 31 07:10:28 2008
Subject: [Slony1-general] sl_log indexes
Message-ID: <20080331141043.5626D11B4FEA@zeus.directinfos.com>

Hi all,

I'm trying to figure out the need for all the indexes that are 
created on the sl_log_* tables... They seem redudant to me:

     "PartInd_ad_sl_log_1-node-1" btree (log_xid _ad.xxid_ops) WHERE 
log_origin = 1
     "PartInd_ad_sl_log_1-node-3" btree (log_xid _ad.xxid_ops) WHERE 
log_origin = 3
     "PartInd_ad_sl_log_1-node-8" btree (log_xid _ad.xxid_ops) WHERE 
log_origin = 8
     "sl_log_1_idx1" btree (log_origin, log_xid _ad.xxid_ops, log_actionseq)

Obviously, to make an index lookup based on log_origin and log_xid, 
one can either use the appropriate partial index or the main index, 
and they would seem to me to do exactly the same thing (there's just 
the little difference with the log_actionseq column). So either the 
partial indexes are really not that useful (my opinion), or the full 
index is a leftover from previous versions, is now obsolete and 
should be removed?

Did I miss something?

Thanks,

Jacques.

From cbbrowne at ca.afilias.info  Mon Mar 31 08:37:01 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Mar 31 08:37:09 2008
Subject: [Slony1-general] sl_log indexes
In-Reply-To: <20080331141043.5626D11B4FEA@zeus.directinfos.com> (Jacques
	Caron's message of "Mon, 31 Mar 2008 16:09:53 +0200")
References: <20080331141043.5626D11B4FEA@zeus.directinfos.com>
Message-ID: <60od8u52hu.fsf@dba2.int.libertyrms.com>

Jacques Caron <jc@oxado.com> writes:

> Hi all,
>
> I'm trying to figure out the need for all the indexes that are created
> on the sl_log_* tables... They seem redudant to me:
>
>     "PartInd_ad_sl_log_1-node-1" btree (log_xid _ad.xxid_ops) WHERE
> log_origin = 1
>     "PartInd_ad_sl_log_1-node-3" btree (log_xid _ad.xxid_ops) WHERE
> log_origin = 3
>     "PartInd_ad_sl_log_1-node-8" btree (log_xid _ad.xxid_ops) WHERE
> log_origin = 8
>     "sl_log_1_idx1" btree (log_origin, log_xid _ad.xxid_ops, log_actionseq)
>
> Obviously, to make an index lookup based on log_origin and log_xid,
> one can either use the appropriate partial index or the main index,
> and they would seem to me to do exactly the same thing (there's just
> the little difference with the log_actionseq column). So either the
> partial indexes are really not that useful (my opinion), or the full
> index is a leftover from previous versions, is now obsolete and should
> be removed?
>
> Did I miss something?

The introduction of these partial indices dates back to the following
discussion thread on pgsql-hackers:
http://archives.postgresql.org/pgsql-hackers/2006-06/msg01516.php

At one point, we had this as a second index:
 create index sl_log_1_idx2 on @NAMESPACE@.sl_log_1
	(log_xid @NAMESPACE@.xxid_ops);

Unfortunately, it was apparently leading to problems in that data
sourced from different origins might have xxid values of varying sign.

So, in lieu of that, I introduced code that would generate a
per-origin partial index, which would necessarily not suffer from the
rollover problem that sl_log_1_idx2 would run into.

It is quite likely that the partial indices will be preferred, as the
first column in sl_log_1_idx1 doesn't discriminate much.
-- 
let name="cbbrowne" and tld="linuxdatabases.info" in name ^ "@" ^ tld;;
http://cbbrowne.com/info/linuxxian.html
"Those who doubt the importance  of  a convenient notation should  try
writing a LISP interpreter in COBOL  or doing long division with Roman
numerals." -- Hal Fulton
From ahodgson at simkin.ca  Mon Mar 31 14:37:46 2008
From: ahodgson at simkin.ca (Alan Hodgson)
Date: Mon Mar 31 14:36:26 2008
Subject: [Slony1-general] add_empty_table_to_replication()
Message-ID: <200803311437.46611@hal.medialogik.com>

I would like to start making use of this function to add new tables, but I'm 
having trouble making sense of it.

How would one actually use this function? Run it through a DDL script? The 
docs seem to imply that.

Why does the function call alterTableRestore at the end? That seems wrong; 
calling the function seems to add the table to a set on the local node but 
it leaves it in a non-replicated state.

Also, what happens if that script executes on a node that is neither an 
origin or a subscriber to the table? It would appear that it doesn't 
actually check if the table should be touched on the local node.

Thanks in advance.

-- 
Alan
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: This is a digitally signed message part.
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080331/22be12df/attachment.pgp
From henry at zen.co.za  Mon Mar 31 23:33:25 2008
From: henry at zen.co.za (Henry)
Date: Mon Mar 31 23:33:58 2008
Subject: [Slony1-general] Feature request for next slony version: initial
 replication of large DBs
Message-ID: <64706.196.23.181.69.1207031605.squirrel@zenmail.co.za>

Good morning all,

I seem to recall (I think it was touched on by someone else) that this
aspect *might* be addressed in a near-future version of slony:

Given that some users of slony have large (your definition of large will
vary, but let's say DBs in excess of 100GB) databases, and that the
initial replication of a DB can take weeks, would it not be a great idea
to have a more efficient initial 'copy' mode?  ie, either allow a user to
dump/restore to all slaves, then start replication from that point
(without truncating/copying/etc), or, build this functionality into slony
so it does the dump/restore, but does not then truncate all tables and
start from scratch?

For smaller DBs this doesn't present a problem, but for larger DDs coupled
with LARGE clusters, this is a major problem.

Regards
Henry

From abrown at bzzagent.com  Thu Mar 20 10:19:39 2008
From: abrown at bzzagent.com (Aaron Brown)
Date: Thu Sep 25 08:26:56 2008
Subject: [Slony1-general] Proper backup routine
Message-ID: <C40814DF.5BB9%abrown@bzzagent.com>

What is the proper backup routine to create a slony-free dump of a database
so that I can restore a complete copy of the database and the schema on a
separate machine with no hint of slony ever having been there?  Ideally, in
a single file, just like you would have when running pg_dump without the =
=ADa
option.

I have used slony1_clean_schema.sh to create a separate schema file, then
created a data only dump, but in order to load my database, I need to then
drop all the indices and triggers, load the data, and then re-add them.
This is incredibly inconvenient and not at all ideal for any sort of
real-world environment.

This seems like it should be a trivially simple thing to do, but I can=B9t
find information about it anywhere.

Thanks for any help you can provide,
Aaron
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080320/=
13f07024/attachment-0001.htm
From jennifer at sun.Stanford.EDU  Thu Mar 13 11:08:56 2008
From: jennifer at sun.Stanford.EDU (Jennifer Spencer)
Date: Thu Sep 25 08:27:12 2008
Subject: [Slony1-general] Log shipping, plain text only? If so,
	what numerical accuracy?
Message-ID: <Pine.LNX.4.64.0803131103210.10071@tucano.Stanford.EDU>

Hi All -
I am new to Slony-1.  I have searched the archives for the last six 
months and haven't seen anything about this topic, so I am going to ask.

We are hoping to set up Slony-1 log shipping from our shop to a few 
others.  My shop is part of a science mission in the Physics Lab.  A lot 
of our data, essential for science, is stored as "double".  We need to be 
able to log ship that science data.

But, from what I can tell so far, Slony-1 log shipping only uses the Plain 
Text format for its shipped logs.  True or false?  If false, how do I 
specify binary or any other format?

If we must use plain text to log ship, what kind of accuracy can we 
expect for data type 'double', or 64 bit floating point numbers contained 
in that log?

Thank you,
Jennifer Spencer
From mdavidson at trenstar.com  Mon Mar 17 13:26:46 2008
From: mdavidson at trenstar.com (Melvin Davidson)
Date: Thu Sep 25 08:27:13 2008
Subject: [Slony1-general] Slony with Fedora & Unbuntu
Message-ID: <5CA4D07E8D52ED4AB5499CA4E089858110A0A0E991@star2.corp.trenstar.net>

My organization is planning to replace our Fedora servers with Unbuntu. That means that for a short while, we may need to replicate with Slony
from Fedora / PostgreSQL 8.2.5 to Unbuntu / PostgreSQL 8.2.5. I am hoping that as long as the PostgreSQL version is the same, it will work.
Can anyone confirm this?

Thanks in advance,
Melvin Davidson
mdavidson!@trenstar.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080317/da95883b/attachment-0001.htm
From haugg at comdasys.com  Tue Mar 18 01:53:35 2008
From: haugg at comdasys.com (Alex Haugg)
Date: Thu Sep 25 08:27:14 2008
Subject: [Slony1-general] slony on two several hosts
Message-ID: <1205830415.16035.59.camel@alex>

hi, i have a question:

in every examples you have the master and the slave on the same host
(local host). 
but i would like to know the configuration from slony master and slony
slave on two several hosts.

were will defined the cluster and the tables( i think on the master )?

were will initialized the connection of slony master and the slony
slave?

were will started the slon process ( i think on the master for the
master and on the slave for the slave )?

please help me.

Thank you very much indeed.

From m_faisal at erp-bd.com  Mon Mar 24 02:35:30 2008
From: m_faisal at erp-bd.com (Mustafa Amir Faisal)
Date: Thu Sep 25 08:27:14 2008
Subject: [Slony1-general] Replication problem in windows
Message-ID: <000601c88d92$514754b0$6e00a8c0@wahid>

Skipped content of type multipart/alternative-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/gif
Size: 145 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080324/dbb3edfe/attachment-0001.gif
