From msteben at autorevenue.com  Fri Aug  1 13:11:41 2008
From: msteben at autorevenue.com (Mark Steben)
Date: Fri Aug  1 13:11:25 2008
Subject: [Slony1-general] provider I/P address has been changed 
In-Reply-To: <20080801190004.77A2E290E29@main.slony.info>
Message-ID: <003001c8f412$cfdb3ee0$14050a0a@dei26g028575>

Hi,
Due to an I/P addressing conflict with our parent company our systems
administrator had to change the IP addresses of each of our desktop
machines.  So, the provider of our working slonik configuration has been
Changed to 10.10.5.25, from 10.10.1.25.  Do I have to run another 'store
path' and 'store listen' to point to the new I/P?
Currently we have running:
  
# Create the second node
store node (id =2, comment = 'subscriber node - hummer');
# echo "successful store node"
store path (server=1, client=2, conninfo='dbname=slony_practice
host=10.10.1.25 user=postgres');
store path (server=2, client=1, conninfo='dbname=slony_practice
host=192.168.1.29 user=postgres');
# echo "successful store paths"
store listen (origin=1, provider=1, receiver=2);
store listen (origin=2, provider=2, receiver=1);
# echo "successful store listens"

Thanks for your help,
________________________________

Mark Steben?Database Administrator? @utoRevenueT
480 Pleasant Street, Suite B200, Lee, MA 01238 
413-243-4800 x1512 (Phone) ? 413-243-4809 (Fax)
A Division of Dominion Enterprises

 
 
 
************************

From kevin at kevinkempterllc.com  Sat Aug  2 16:33:18 2008
From: kevin at kevinkempterllc.com (kevin kempter)
Date: Sat Aug  2 16:33:37 2008
Subject: [Slony1-general] How to add more slaves to an existing replication
	set
Message-ID: <5C793C67-AE8E-41B6-BF43-97F50A414A1F@kevinkempterllc.com>

Hi All;

I setup a single master & slave like this:

- I did a pg_dump from the master to the slave

- set the cluster name

- init cluster ( id=1, comment = 'Master Node');

- create set ( id=1, origin=1, comment = 'PA REPLICATION SET' );

- ran all the 'set add table' commands

- store node (id=2, comment = 'Slave Node 1');

- store path (server = 1, client = 2, conninfo='dbname=$MASTERDBNAME  
host=$MASTERHOST port=$MASTERPORT user=$REPUSER');

- store path (server = 2, client = 1, conninfo='dbname=$SLAVEDBNAME  
host=$SLAVEHOST port=$SLAVEPORT user=$REPUSER');

- store listen (origin=1, provider = 1, receiver =2);

- store listen (origin=2, provider = 2, receiver =1);

- started the slon daemon on the master:
slon $CLUSTERNAME "dbname=$MASTERDBNAME port=$MASTERPORT host= 
$MASTERHOST user=$REPUSER"  > ./slon.master.log 2>&1 &

- started the slon daemon on the slave

- subscribe set ( id = 1, provider = 1, receiver = 2, forward = no);


Now I want to add 2 more slaves, I plan to run this:

- do a pg_dump from the master to slaves 3 & 4

- store node (id=3, comment = 'Slave Node 2');

- store node (id=4, comment = 'Slave Node 3');

- store path (server = 1, client = 3, conninfo='dbname=$MASTERDBNAME  
host=$MASTERHOST port=$MASTERPORT user=$REPUSER');

- store path (server = 3, client = 1, conninfo='dbname=$SLAVEDBNAME  
host=$SLAVEHOST port=$SLAVEPORT user=$REPUSER');

- store path (server = 1, client = 4, conninfo='dbname=$MASTERDBNAME  
host=$MASTERHOST port=$MASTERPORT user=$REPUSER');

- store path (server = 4, client = 1, conninfo='dbname=$SLAVEDBNAME  
host=$SLAVEHOST port=$SLAVEPORT user=$REPUSER');

- store listen (origin=1, provider = 1, receiver =3);

- store listen (origin=3, provider = 3, receiver =1);

- store listen (origin=1, provider = 1, receiver =4);

- store listen (origin=4, provider = 4, receiver =1);

- Start the slon daemon on slaves 3 & 4

- subscribe set ( id = 1, provider = 1, receiver = 3, forward = no);

- subscribe set ( id = 1, provider = 1, receiver = 4, forward = no);



Is this correct, or am I missing something here?


Thanks in advance

/Kevin



From kevin at kevinkempterllc.com  Sun Aug  3 09:35:12 2008
From: kevin at kevinkempterllc.com (kevin kempter)
Date: Sun Aug  3 09:35:29 2008
Subject: [Slony1-general] how to determine if the slave is caught up/in sync
	with the master
Message-ID: <F1DC4D50-5B7A-45FA-B191-F1114734EBFC@kevinkempterllc.com>

Hi List;

How do I know if a slave is up to date with the master ?

From vivek at khera.org  Sun Aug  3 13:18:09 2008
From: vivek at khera.org (Vivek Khera)
Date: Sun Aug  3 13:18:22 2008
Subject: [Slony1-general] how to determine if the slave is caught up/in
	sync with the master
In-Reply-To: <F1DC4D50-5B7A-45FA-B191-F1114734EBFC@kevinkempterllc.com>
References: <F1DC4D50-5B7A-45FA-B191-F1114734EBFC@kevinkempterllc.com>
Message-ID: <2968dfd60808031318w3b02fe0flec083bd95e088258@mail.gmail.com>

On Sun, Aug 3, 2008 at 12:35 PM, kevin kempter
<kevin@kevinkempterllc.com> wrote:
> Hi List;
>
> How do I know if a slave is up to date with the master ?
>

On the master, run this SQL query, replacing _CLUSTERNAME with the
name you selected for your cluster.

select * from _CLUSTERNAME.sl_status;
From stephane.schildknecht at postgresqlfr.org  Mon Aug  4 01:27:28 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Mon Aug  4 01:28:00 2008
Subject: [Slony1-general] How to add more slaves to an existing replication
	set
In-Reply-To: <5C793C67-AE8E-41B6-BF43-97F50A414A1F@kevinkempterllc.com>
References: <5C793C67-AE8E-41B6-BF43-97F50A414A1F@kevinkempterllc.com>
Message-ID: <4896BD70.8030208@postgresqlfr.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

kevin kempter a ?crit :
> Hi All;
> 
> I setup a single master & slave like this:
> 
> - I did a pg_dump from the master to the slave
> 
> - set the cluster name
> 
> - init cluster ( id=1, comment = 'Master Node');
> 
> - create set ( id=1, origin=1, comment = 'PA REPLICATION SET' );
> 
> - ran all the 'set add table' commands
> 
> - store node (id=2, comment = 'Slave Node 1');
> 
> - store path (server = 1, client = 2, conninfo='dbname=$MASTERDBNAME
> host=$MASTERHOST port=$MASTERPORT user=$REPUSER');
> 
> - store path (server = 2, client = 1, conninfo='dbname=$SLAVEDBNAME
> host=$SLAVEHOST port=$SLAVEPORT user=$REPUSER');
> 
> - store listen (origin=1, provider = 1, receiver =2);
> 
> - store listen (origin=2, provider = 2, receiver =1);
> 
> - started the slon daemon on the master:
> slon $CLUSTERNAME "dbname=$MASTERDBNAME port=$MASTERPORT
> host=$MASTERHOST user=$REPUSER"  > ./slon.master.log 2>&1 &
> 
> - started the slon daemon on the slave
> 
> - subscribe set ( id = 1, provider = 1, receiver = 2, forward = no);
> 
> 
> Now I want to add 2 more slaves, I plan to run this:
> 
> - do a pg_dump from the master to slaves 3 & 4
> 
> - store node (id=3, comment = 'Slave Node 2');
> 
> - store node (id=4, comment = 'Slave Node 3');
> 
> - store path (server = 1, client = 3, conninfo='dbname=$MASTERDBNAME
> host=$MASTERHOST port=$MASTERPORT user=$REPUSER');
> 
> - store path (server = 3, client = 1, conninfo='dbname=$SLAVEDBNAME
> host=$SLAVEHOST port=$SLAVEPORT user=$REPUSER');
> 
> - store path (server = 1, client = 4, conninfo='dbname=$MASTERDBNAME
> host=$MASTERHOST port=$MASTERPORT user=$REPUSER');
> 
> - store path (server = 4, client = 1, conninfo='dbname=$SLAVEDBNAME
> host=$SLAVEHOST port=$SLAVEPORT user=$REPUSER');
> 
> - store listen (origin=1, provider = 1, receiver =3);
> 
> - store listen (origin=3, provider = 3, receiver =1);
> 
> - store listen (origin=1, provider = 1, receiver =4);
> 
> - store listen (origin=4, provider = 4, receiver =1);
> 
> - Start the slon daemon on slaves 3 & 4
> 
> - subscribe set ( id = 1, provider = 1, receiver = 3, forward = no);
> 
> - subscribe set ( id = 1, provider = 1, receiver = 4, forward = no);
> 
> 
> 
> Is this correct, or am I missing something here?
> 
> 
> Thanks in advance
> 
> /Kevin

Hi,

When pg_dumping master to create the 2 new slaves, be aware to get rid of slony
information on the dump. Otherwise, make sure you drop every slony specific
thing on new slaves before trying to execute the store node.

You may, for instance do something like "select
_/replication_cluster_name/.uninstallnode();" which will drop slony triggers
and restore normal tables constraints and triggers, if appliable.

You can then do a "drop schema _/replication_cluster_name/ cascade ;".

What's more, you don't need store listen anymore. It is done by store path.

Be aware that with the so defined paths, node 2, 3 and 4 won't see each other.


Regards,
- --
St?phane Schildknecht
PostgreSQLFr : http://www.postgresql.fr

Venez nous rencontrer le 4 octobre lors du plus important ?v?nement
PostgreSQL francophone : http://www.pgday.fr

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFIlr1wA+REPKWGI0ERAvPhAKC9JCB0xZBuYbDt8Q11dUOJunckeACeKR5k
plnRV5lNUJu8Y0yNaAZZQVY=
=E4JM
-----END PGP SIGNATURE-----
From stephane.schildknecht at postgresqlfr.org  Mon Aug  4 01:34:30 2008
From: stephane.schildknecht at postgresqlfr.org (=?UTF-8?B?IlN0w6lwaGFuZSBBLiBTY2hpbGRrbmVjaHQi?=)
Date: Mon Aug  4 01:35:03 2008
Subject: [Slony1-general] Bug ? (was: subscribe_set falling in error)
In-Reply-To: <489050D2.3050908@postgresqlfr.org>
References: <489050D2.3050908@postgresqlfr.org>
Message-ID: <4896BF16.20107@postgresqlfr.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

St?phane A. Schildknecht a ?crit :
> Hi,
> 
> Slony 1.2.13, PG 8.2.{7|9}
> 
> I'm facing a really strange and annoying situation.
> 
> After having unsubscribed a node (72) from replication of set 1 (issuing the
> command "select _slonrep.unsubscribeset(1,72);" on node 72), I am trying to
> resubscribe it to the set by issuing the command "select
> _slonrep.subscribeset(1,71,72,'t');" on node 71.
> 
> Trouble is averything seems to go well to a certain extent... Indeed,
> subscription is done for tables, data are copied, but then I get the following
> message :
> 
> 2008-07-30 11:05:13 CEST ERROR  remoteWorkerThread_1: "select
> "_slonturf".setAddSequence_int(1, 2, '"public"."some_seq"', 'sequence
> public.some_seq')" PGRES_FATAL_ERROR ERREUR: Slony-I: setAddSequence_int():
> sequence ID 2 has already been assigned
> 2008-07-30 11:05:13 CEST WARN   remoteWorkerThread_1: data copy for set 1
> failed - sleep 15 seconds
> 
> Or, this sequence was not present in table _slonrep.sl_sequence before I issued
> the subscribe_set() command.
> Why does slon consider it to be already present now ?
> What can I do now ?
> 
> My replication scheme is as follows :
> 
> In cascade, let's assume 1 -> 71 -> 72, to summary.
> 72 is master for set 2 and last subscriber for set 1 (provider is 71, master is
> node 1).

Am I facing a bug, which could have been solved in 1.2 14 ? Or am I just doing
something wrong ?

Is there any piece of information I could give ?

Regards,
- --
St?phane Schildknecht
PostgreSQLFr : http://www.postgresql.fr

Venez nous rencontrer le 4 octobre lors du plus important ?v?nement
PostgreSQL francophone : http://www.pgday.fr

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFIlr8WA+REPKWGI0ERAmBAAJ9w1ytul0dYSP9wna28QcRwqbF3BQCfSdf2
djci8uR1jw4Iq8M1ekMdZLE=
=ISTO
-----END PGP SIGNATURE-----
From glynastill at yahoo.co.uk  Mon Aug  4 01:53:44 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Mon Aug  4 01:54:13 2008
Subject: [Slony1-general] REPAIR CONFIG function
Message-ID: <687189.97483.qm@web25806.mail.ukl.yahoo.com>

Is there a baremetal function, or collection of functions that I can call to do the equivalent?


      __________________________________________________________
Not happy with your email address?.
Get the one you really want - millions of new email addresses available now at Yahoo! http://uk.docs.yahoo.com/ymail/new.html
From stephane.schildknecht at postgresqlfr.org  Mon Aug  4 02:01:29 2008
From: stephane.schildknecht at postgresqlfr.org (=?UTF-8?B?IlN0w6lwaGFuZSBBLiBTY2hpbGRrbmVjaHQi?=)
Date: Mon Aug  4 02:02:06 2008
Subject: [Slony1-general] REPAIR CONFIG function
In-Reply-To: <687189.97483.qm@web25806.mail.ukl.yahoo.com>
References: <687189.97483.qm@web25806.mail.ukl.yahoo.com>
Message-ID: <4896C569.1000809@postgresqlfr.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Glyn Astill a ?crit :
> Is there a baremetal function, or collection of functions that I can call to do the equivalent?
> 
> 
>       __________________________________________________________
> Not happy with your email address?.
> Get the one you really want - millions of new email addresses available now at Yahoo! http://uk.docs.yahoo.com/ymail/new.html
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

Hi,

I think you are looking for : updatereloid( integer, integer ) (see
http://slony.info/documentation/function.updatereloid-integer-integer.html ).

I found that in slony sources for repair_config :

       slon_mkquery(&query,
                                 "select \"_%s\".updateReloid(%d, %d); ",
                                 stmt->hdr.script->clustername,
                                 stmt->set_id, stmt->only_on_node);


Regards,
- --
St?phane Schildknecht
PostgreSQLFr : http://www.postgresql.fr

Venez nous rencontrer le 4 octobre lors du plus important ?v?nement
PostgreSQL francophone : http://www.pgday.fr

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFIlsVpA+REPKWGI0ERAk5kAJ0emCKVkpu3c+qzb3Bz+l9O/nTWYACeIqq3
QZilZGP16/Ia6AfuMDSN/mQ=
=uGCJ
-----END PGP SIGNATURE-----
From wmoran at collaborativefusion.com  Mon Aug  4 04:43:09 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Mon Aug  4 04:43:40 2008
Subject: [Slony1-general] provider I/P address has been changed
In-Reply-To: <003001c8f412$cfdb3ee0$14050a0a@dei26g028575>
References: <20080801190004.77A2E290E29@main.slony.info>
	<003001c8f412$cfdb3ee0$14050a0a@dei26g028575>
Message-ID: <20080804074309.190b28aa.wmoran@collaborativefusion.com>

In response to "Mark Steben" <msteben@autorevenue.com>:

> Hi,
> Due to an I/P addressing conflict with our parent company our systems
> administrator had to change the IP addresses of each of our desktop
> machines.  So, the provider of our working slonik configuration has been
> Changed to 10.10.5.25, from 10.10.1.25.  Do I have to run another 'store
> path' and 'store listen' to point to the new I/P?

You don't say what version of Slony you're running, but newer versions
don't need the "store listen" at all.

However, a new "store path" should take care of things.  You may need
to restart the slons to get them to notice.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023

****************************************************************
IMPORTANT: This message contains confidential information and is
intended only for the individual named. If the reader of this
message is not an intended recipient (or the individual
responsible for the delivery of this message to an intended
recipient), please be advised that any re-use, dissemination,
distribution or copying of this message is prohibited. Please
notify the sender immediately by e-mail if you have received
this e-mail by mistake and delete this e-mail from your system.
E-mail transmission cannot be guaranteed to be secure or
error-free as information could be intercepted, corrupted, lost,
destroyed, arrive late or incomplete, or contain viruses. The
sender therefore does not accept liability for any errors or
omissions in the contents of this message, which arise as a
result of e-mail transmission.
****************************************************************
From glynastill at yahoo.co.uk  Mon Aug  4 04:45:58 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Mon Aug  4 04:46:30 2008
Subject: [Slony1-general] REPAIR CONFIG function
In-Reply-To: <4896C569.1000809@postgresqlfr.org>
Message-ID: <794465.15915.qm@web25806.mail.ukl.yahoo.com>

Seems to do the trick. Rather than just drop my replication schema, I now do the following.

select (_main_replication.updatereloid(set_id,1)) from _main_replication.sl_set;

select _main_replication.uninstallnode();

drop schema _main_replication cascade;



--- On Mon, 4/8/08, "St?phane A. Schildknecht" <stephane.schildknecht@postgresqlfr.org> wrote:

> From: "St?phane A. Schildknecht" <stephane.schildknecht@postgresqlfr.org>
> Subject: Re: [Slony1-general] REPAIR CONFIG function
> To: glynastill@yahoo.co.uk
> Cc: slony1-general@lists.slony.info
> Date: Monday, 4 August, 2008, 10:01 AM
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> Glyn Astill a ?crit :
> > Is there a baremetal function, or collection of
> functions that I can call to do the equivalent?
> > 
> > 
> >      
> __________________________________________________________
> > Not happy with your email address?.
> > Get the one you really want - millions of new email
> addresses available now at Yahoo!
> http://uk.docs.yahoo.com/ymail/new.html
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general@lists.slony.info
> >
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> Hi,
> 
> I think you are looking for : updatereloid( integer,
> integer ) (see
> http://slony.info/documentation/function.updatereloid-integer-integer.html
> ).
> 
> I found that in slony sources for repair_config :
> 
>        slon_mkquery(&query,
>                                  "select
> \"_%s\".updateReloid(%d, %d); ",
>                                 
> stmt->hdr.script->clustername,
>                                  stmt->set_id,
> stmt->only_on_node);
> 
> 
> Regards,
> - --
> St?phane Schildknecht
> PostgreSQLFr : http://www.postgresql.fr
> 
> Venez nous rencontrer le 4 octobre lors du plus important
> ?v?nement
> PostgreSQL francophone : http://www.pgday.fr
> 
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.6 (GNU/Linux)
> Comment: Using GnuPG with Mozilla -
> http://enigmail.mozdev.org
> 
> iD8DBQFIlsVpA+REPKWGI0ERAk5kAJ0emCKVkpu3c+qzb3Bz+l9O/nTWYACeIqq3
> QZilZGP16/Ia6AfuMDSN/mQ=
> =uGCJ
> -----END PGP SIGNATURE-----


      __________________________________________________________
Not happy with your email address?.
Get the one you really want - millions of new email addresses available now at Yahoo! http://uk.docs.yahoo.com/ymail/new.html
From pelin.01 at gmail.com  Mon Aug  4 05:43:26 2008
From: pelin.01 at gmail.com (pelin)
Date: Mon Aug  4 05:43:33 2008
Subject: [Slony1-general] consulta slony a la inversa
Message-ID: <6a8366e40808040543i2f3e3e38i206bd94b108c169e@mail.gmail.com>

gentes,como estan.
les hago la siguiente consulta:
se que slony es un sistema master-multislave de replicacion.
es posible hacer una estructura de arbol invertida como la que planteo
abajo?

master1 master2     master3     master4
   |            |                |                |
   --------------                 -----------------
         |                                |
slave1/2(master5)       slave3/4 (master6)
             |                          |
              --------------------------
                            |
                        slave5/6

yo lo habia pensado con varios clusters como ser:

  cluster name =3D rama1 que incluya a
     master1 - slave 1/2 - slave 5/6

y
    cluster name =3D rama2 que incluya a
     master2 - slave 1/2
y
  cluster name =3D rama3 que incluya a
     master3 - slave 3/4 - slave 5/6
y
  cluster name =3D rama4 que incluya a
     master4 - slave 3/4

puede ser esa una solucion o hay otra manera?

Saludos y espero su respuesta!!

Atentamente
Rolando Sergio Rodriguez
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080804/=
64a8cdf2/attachment-0001.htm
From jcasanov at systemguards.com.ec  Mon Aug  4 11:34:16 2008
From: jcasanov at systemguards.com.ec (Jaime Casanova)
Date: Mon Aug  4 11:34:28 2008
Subject: [Slony1-general] consulta slony a la inversa
In-Reply-To: <6a8366e40808040543i2f3e3e38i206bd94b108c169e@mail.gmail.com>
References: <6a8366e40808040543i2f3e3e38i206bd94b108c169e@mail.gmail.com>
Message-ID: <3073cc9b0808041134u1ff46f4dsca693f2881da1450@mail.gmail.com>

On 8/4/08, pelin <pelin.01@gmail.com> wrote:
> gentes,como estan.
> les hago la siguiente consulta:
> se que slony es un sistema master-multislave de replicacion.
> es posible hacer una estructura de arbol invertida como la que planteo
> abajo?
>
> master1 master2     master3     master4
>    |            |                |                |
>    --------------                 -----------------
>          |                                |
> slave1/2(master5)       slave3/4 (master6)
>              |                          |
>               --------------------------
>                             |
>                         slave5/6
>

Esta es una lista en ingles...
Mientras en master1, master2, master3, master4 estes pasando tablas
diferentes, si

This is an english list...
If master1, master2, master3, master4 have all different tables, then yes

-- 
Atentamente,
Jaime Casanova
Soporte y capacitaci?n de PostgreSQL
Guayaquil - Ecuador
Cel. (593) 87171157
From msteben at autorevenue.com  Mon Aug  4 12:39:55 2008
From: msteben at autorevenue.com (Mark Steben)
Date: Mon Aug  4 12:40:26 2008
Subject: [Slony1-general] provider I/P address has been changed
In-Reply-To: <20080804074309.190b28aa.wmoran@collaborativefusion.com>
Message-ID: <002a01c8f669$df1edd40$14050a0a@dei26g028575>

Thanks Bill for your reply about store_path.
With a little massaging it finally worked. 

Now I am going to show my total inexperience with this product.

We are running slony 1.2.14 by the way.  I am running it in development
where I have the luxury of totally shutting down the provider machine and
restarting if need be.  This is what I had to do to restart the slons
pointing to the new I/P address and now we are up again. 
  Before I performed this 'last resort' I attempted to 
Restart the slons by simply reissuing the slon command:

  Slon slonypractice "dbname=whatever user=postgres host=newipaddress"

And of course I got a 'slon already running - duplicate pk' type of error
There must be a better way of restarting the slons short of killing and
Restarting the machine.  I tried 'DROP NODE' couldn't get that to work.
I'd appreciate some insight on this.

Is there a document within or without www.slony.info that has basic 
'how to' administrative tasks that I can refer to? 
Thanks for your time.  
   
______________________________

Mark Steben?Database Administrator? @utoRevenueT
480 Pleasant Street, Suite B200, Lee, MA 01238 
413-243-4800 x1512 (Phone) ? 413-243-4809 (Fax)
A Division of Dominion Enterprises

 
 
 
 
 
-----Original Message-----
From: Bill Moran [mailto:wmoran@collaborativefusion.com] 
Sent: Monday, August 04, 2008 7:43 AM
To: Mark Steben
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] provider I/P address has been changed

In response to "Mark Steben" <msteben@autorevenue.com>:

> Hi,
> Due to an I/P addressing conflict with our parent company our systems
> administrator had to change the IP addresses of each of our desktop
> machines.  So, the provider of our working slonik configuration has been
> Changed to 10.10.5.25, from 10.10.1.25.  Do I have to run another 'store
> path' and 'store listen' to point to the new I/P?

You don't say what version of Slony you're running, but newer versions
don't need the "store listen" at all.

However, a new "store path" should take care of things.  You may need
to restart the slons to get them to notice.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023

****************************************************************
IMPORTANT: This message contains confidential information and is
intended only for the individual named. If the reader of this
message is not an intended recipient (or the individual
responsible for the delivery of this message to an intended
recipient), please be advised that any re-use, dissemination,
distribution or copying of this message is prohibited. Please
notify the sender immediately by e-mail if you have received
this e-mail by mistake and delete this e-mail from your system.
E-mail transmission cannot be guaranteed to be secure or
error-free as information could be intercepted, corrupted, lost,
destroyed, arrive late or incomplete, or contain viruses. The
sender therefore does not accept liability for any errors or
omissions in the contents of this message, which arise as a
result of e-mail transmission.
****************************************************************

From ckrueger at avaya.com  Mon Aug  4 15:05:21 2008
From: ckrueger at avaya.com (CarlaMcNearny)
Date: Mon Aug  4 15:05:36 2008
Subject: [Slony1-general] Drop Node fails after Failover
Message-ID: <18820805.post@talk.nabble.com>


 Hi folks,

Looking for some help here.

I am using Slony-I 1.2.12 and have a simple 2 node Master/Slave environment.
Node 1 is the Master/provider and node 2 is the slave/receiver.
Node 1 goes down, ie database not accessible, and I failover to node 2:

failover ( id = 1, backup node = 2);
drop node ( id = 1);

I receive the following error from the drop node command:
Error: Node ID and event node cannot be identical

Of note, when node 2 is the master/provider and node 1 is the
slave/receiver, and I take node 2 database out of service, I am able to
successfully failover to node 1 (using failover command and dropping the
node). And subsequently reconfigure node 2 (store node, store path and
subscribe set) to the replication set.

I suspect the drop node is failing because when I configured Slony-I
cluster, node 1 was established as the "origin" i.e.:  
init cluster (id = 1, comment = 'Node 1');


Yes I'm new to all of this, so any help/suggestions is appreciated!!
Thank you!
Carla McNearny 
-- 
View this message in context: http://www.nabble.com/Drop-Node-fails-after-Failover-tp18820805p18820805.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From charles.duffy at gmail.com  Mon Aug  4 17:07:01 2008
From: charles.duffy at gmail.com (Charles Duffy)
Date: Mon Aug  4 17:07:19 2008
Subject: [Slony1-general] Drop Node fails after Failover
In-Reply-To: <18820805.post@talk.nabble.com>
References: <18820805.post@talk.nabble.com>
Message-ID: <dfdaea8f0808041707r4903bf47g3aae29c749490e79@mail.gmail.com>

>
> I am using Slony-I 1.2.12 and have a simple 2 node Master/Slave environment.
> Node 1 is the Master/provider and node 2 is the slave/receiver.
> Node 1 goes down, ie database not accessible, and I failover to node 2:
>
> failover ( id = 1, backup node = 2);
> drop node ( id = 1);
>
> I receive the following error from the drop node command:
> Error: Node ID and event node cannot be identical
>

You should specify an event node in the 'drop node' slonik command.
Something like:

try {
      drop node (id = 1, event node = 2);
  } on error {
      echo 'Failed to drop node 1 from cluster';
      exit 1;
  }

In your case, node 2 should be the node that the event is generated
on. Without an explicit 'event node' parameter, it'll default to 1.


Thanks,

Charles Duffy
From m.eriksson at albourne.com  Tue Aug  5 00:18:29 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Tue Aug  5 00:19:03 2008
Subject: [Slony1-general] 
	Strange behavior adding a new node, very, VERY slow
Message-ID: <4897FEC5.1080905@albourne.com>

Hi everyone.

I've been using slonly for a while now and feel pretty confident with 
what im doing but I can not understand what is going now!

current setup:
1 Master
2 slave1 (provider = 1)
3 slave2 (provider = 1)

adding a new node 4  (provider = 1)

machines on same hardware, all machines are pretty nice machines, 8 gigs 
of ram in each machine
master got 6 gigs allocated to postgres, slave machines got 3.2 gigs 
allocated. all running ubuntu 64 bit

database is a total of 7.9 gigs (including the slony schema, total data 
that need to be replicated around 3.5 gigs)

master and slave 1 are sitting next to each other connected with a 1 
GB/s line on a separate interface.

now node 4, I created a new postgres installation on slave 1 machine, 
running on different port same memory allocation (3.2 gigs) so total 
usage of memory on that machine by the two postgres servers is 6.4 gig 
(still 1.4 gig free)

On saturday I did sync up node 2 from scratch and it toke a total of 20 
minutes.

Sunday afternoon database was put in production and being used, its not 
a overly used database around 18000, slony event per 24h with a total of 
2000-3000 db commits on Master per 24h

So yesterday morning I started to sync node 4, and now 22h later it is 
still running!!! and its only 1/3rd done!!!

does anyone got a good explination for this?

I look on the slave 2 machine, 0.2-0.4 load, memory is available, only 
using a fraction of the bandwidth, io-stats are down. It is more or less 
the same for the Master as low cpu load and low io load, and low 
bandwidth usage.

looking on the db, it appear that its trying to do EVERYTHING in a 
single transaction as tables that have been copied are still showing up 
as count(*) = 0, is there a way to not do everything in a single 
transaction??

or anyone got some other idea??

From m.eriksson at albourne.com  Tue Aug  5 00:19:40 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Tue Aug  5 00:20:10 2008
Subject: [Slony1-general] 	Strange behavior adding a new node, very, VERY
	slow
In-Reply-To: <4897FEC5.1080905@albourne.com>
References: <4897FEC5.1080905@albourne.com>
Message-ID: <4897FF0C.503@albourne.com>

Sorry,

I should mention that this is Postgres 8.2.4, and Slony 1.2.14

Martin Eriksson wrote:
> Hi everyone.
>
> I've been using slonly for a while now and feel pretty confident with 
> what im doing but I can not understand what is going now!
>
> current setup:
> 1 Master
> 2 slave1 (provider = 1)
> 3 slave2 (provider = 1)
>
> adding a new node 4  (provider = 1)
>
> machines on same hardware, all machines are pretty nice machines, 8 
> gigs of ram in each machine
> master got 6 gigs allocated to postgres, slave machines got 3.2 gigs 
> allocated. all running ubuntu 64 bit
>
> database is a total of 7.9 gigs (including the slony schema, total 
> data that need to be replicated around 3.5 gigs)
>
> master and slave 1 are sitting next to each other connected with a 1 
> GB/s line on a separate interface.
>
> now node 4, I created a new postgres installation on slave 1 machine, 
> running on different port same memory allocation (3.2 gigs) so total 
> usage of memory on that machine by the two postgres servers is 6.4 gig 
> (still 1.4 gig free)
>
> On saturday I did sync up node 2 from scratch and it toke a total of 
> 20 minutes.
>
> Sunday afternoon database was put in production and being used, its 
> not a overly used database around 18000, slony event per 24h with a 
> total of 2000-3000 db commits on Master per 24h
>
> So yesterday morning I started to sync node 4, and now 22h later it is 
> still running!!! and its only 1/3rd done!!!
>
> does anyone got a good explination for this?
>
> I look on the slave 2 machine, 0.2-0.4 load, memory is available, only 
> using a fraction of the bandwidth, io-stats are down. It is more or 
> less the same for the Master as low cpu load and low io load, and low 
> bandwidth usage.
>
> looking on the db, it appear that its trying to do EVERYTHING in a 
> single transaction as tables that have been copied are still showing 
> up as count(*) = 0, is there a way to not do everything in a single 
> transaction??
>
> or anyone got some other idea??
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

From stephane.schildknecht at postgresqlfr.org  Tue Aug  5 00:50:50 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Tue Aug  5 00:51:21 2008
Subject: [Slony1-general] 	Strange behavior adding a new node, very, VERY
	slow
In-Reply-To: <4897FF0C.503@albourne.com>
References: <4897FEC5.1080905@albourne.com> <4897FF0C.503@albourne.com>
Message-ID: <4898065A.1030904@postgresqlfr.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Martin Eriksson a ?crit :
> Sorry,
> 
> I should mention that this is Postgres 8.2.4, and Slony 1.2.14
> 
> Martin Eriksson wrote:
>> Hi everyone.
>>
>> I've been using slonly for a while now and feel pretty confident with
>> what im doing but I can not understand what is going now!
>>
>> current setup:
>> 1 Master
>> 2 slave1 (provider = 1)
>> 3 slave2 (provider = 1)
>>
>> adding a new node 4  (provider = 1)
>>
>> machines on same hardware, all machines are pretty nice machines, 8
>> gigs of ram in each machine
>> master got 6 gigs allocated to postgres, slave machines got 3.2 gigs
>> allocated. all running ubuntu 64 bit
>>
>> database is a total of 7.9 gigs (including the slony schema, total
>> data that need to be replicated around 3.5 gigs)
>>
>> master and slave 1 are sitting next to each other connected with a 1
>> GB/s line on a separate interface.
>>
>> now node 4, I created a new postgres installation on slave 1 machine,
>> running on different port same memory allocation (3.2 gigs) so total
>> usage of memory on that machine by the two postgres servers is 6.4 gig
>> (still 1.4 gig free)
>>
>> On saturday I did sync up node 2 from scratch and it toke a total of
>> 20 minutes.
>>
>> Sunday afternoon database was put in production and being used, its
>> not a overly used database around 18000, slony event per 24h with a
>> total of 2000-3000 db commits on Master per 24h
>>
>> So yesterday morning I started to sync node 4, and now 22h later it is
>> still running!!! and its only 1/3rd done!!!
>>
>> does anyone got a good explination for this?
>>
>> I look on the slave 2 machine, 0.2-0.4 load, memory is available, only
>> using a fraction of the bandwidth, io-stats are down. It is more or
>> less the same for the Master as low cpu load and low io load, and low
>> bandwidth usage.
>>
>> looking on the db, it appear that its trying to do EVERYTHING in a
>> single transaction as tables that have been copied are still showing
>> up as count(*) = 0, is there a way to not do everything in a single
>> transaction??
>>
>> or anyone got some other idea??
>>

Do you have any error messages ?
As you noticed, the first synchronisation is done in a sigle transaction.
That's why any failure (network failure, schema not exactly the same on both
nodes...) will interrupt replication and make it begin from scratch again and
again.

Further reading let me think it can't be a network trouble.
How did you get the schema for that new slave ?

A quick look at pg_stat_activity may tell you which table is been synchronized.

Regards,
- --
St?phane Schildknecht
PostgreSQLFr : http://www.postgresql.fr

Venez nous rencontrer le 4 octobre lors du plus important ?v?nement
PostgreSQL francophone : http://www.pgday.fr

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFImAZaA+REPKWGI0ERAn1dAJ0VA5GY04W5Bl96pEk1GcuFHAkf2gCfQQdk
y12rN2fShxthch5cMtJn5Ek=
=qIRa
-----END PGP SIGNATURE-----
From m.eriksson at albourne.com  Tue Aug  5 01:31:57 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Tue Aug  5 01:32:30 2008
Subject: [Slony1-general] 	Strange behavior adding a new node, very, VERY
	slow
In-Reply-To: <4898065A.1030904@postgresqlfr.org>
References: <4897FEC5.1080905@albourne.com> <4897FF0C.503@albourne.com>
	<4898065A.1030904@postgresqlfr.org>
Message-ID: <48980FFD.5090408@albourne.com>

this is where it start getting weird,

No error anywhere in any log.

with

pg_stat_activity

I just got 3 transactions sitting in idle (to the three other nodes, 
Master, node2,3)

It is using exactly the same schema as i was using for node 2 (as its on 
the same machine so used the same one!)

and its not re-doing it, it is just being very very very slow, and about 
99% of the time its doing nothing...

its kind of freakish actually...

:/

St?phane A. Schildknecht wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Martin Eriksson a ?crit :
>   
>> Sorry,
>>
>> I should mention that this is Postgres 8.2.4, and Slony 1.2.14
>>
>> Martin Eriksson wrote:
>>     
>>> Hi everyone.
>>>
>>> I've been using slonly for a while now and feel pretty confident with
>>> what im doing but I can not understand what is going now!
>>>
>>> current setup:
>>> 1 Master
>>> 2 slave1 (provider = 1)
>>> 3 slave2 (provider = 1)
>>>
>>> adding a new node 4  (provider = 1)
>>>
>>> machines on same hardware, all machines are pretty nice machines, 8
>>> gigs of ram in each machine
>>> master got 6 gigs allocated to postgres, slave machines got 3.2 gigs
>>> allocated. all running ubuntu 64 bit
>>>
>>> database is a total of 7.9 gigs (including the slony schema, total
>>> data that need to be replicated around 3.5 gigs)
>>>
>>> master and slave 1 are sitting next to each other connected with a 1
>>> GB/s line on a separate interface.
>>>
>>> now node 4, I created a new postgres installation on slave 1 machine,
>>> running on different port same memory allocation (3.2 gigs) so total
>>> usage of memory on that machine by the two postgres servers is 6.4 gig
>>> (still 1.4 gig free)
>>>
>>> On saturday I did sync up node 2 from scratch and it toke a total of
>>> 20 minutes.
>>>
>>> Sunday afternoon database was put in production and being used, its
>>> not a overly used database around 18000, slony event per 24h with a
>>> total of 2000-3000 db commits on Master per 24h
>>>
>>> So yesterday morning I started to sync node 4, and now 22h later it is
>>> still running!!! and its only 1/3rd done!!!
>>>
>>> does anyone got a good explination for this?
>>>
>>> I look on the slave 2 machine, 0.2-0.4 load, memory is available, only
>>> using a fraction of the bandwidth, io-stats are down. It is more or
>>> less the same for the Master as low cpu load and low io load, and low
>>> bandwidth usage.
>>>
>>> looking on the db, it appear that its trying to do EVERYTHING in a
>>> single transaction as tables that have been copied are still showing
>>> up as count(*) = 0, is there a way to not do everything in a single
>>> transaction??
>>>
>>> or anyone got some other idea??
>>>
>>>       
>
> Do you have any error messages ?
> As you noticed, the first synchronisation is done in a sigle transaction.
> That's why any failure (network failure, schema not exactly the same on both
> nodes...) will interrupt replication and make it begin from scratch again and
> again.
>
> Further reading let me think it can't be a network trouble.
> How did you get the schema for that new slave ?
>
> A quick look at pg_stat_activity may tell you which table is been synchronized.
>
> Regards,
> - --
> St?phane Schildknecht
> PostgreSQLFr : http://www.postgresql.fr
>
> Venez nous rencontrer le 4 octobre lors du plus important ?v?nement
> PostgreSQL francophone : http://www.pgday.fr
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.6 (GNU/Linux)
> Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org
>
> iD8DBQFImAZaA+REPKWGI0ERAn1dAJ0VA5GY04W5Bl96pEk1GcuFHAkf2gCfQQdk
> y12rN2fShxthch5cMtJn5Ek=
> =qIRa
> -----END PGP SIGNATURE-----
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>   

From shahaf at redfin.com  Tue Aug  5 10:41:25 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Tue Aug  5 10:41:37 2008
Subject: [Slony1-general] how to determine if the slave is caught
	up/insync with the master
In-Reply-To: <2968dfd60808031318w3b02fe0flec083bd95e088258@mail.gmail.com>
References: <F1DC4D50-5B7A-45FA-B191-F1114734EBFC@kevinkempterllc.com>
	<2968dfd60808031318w3b02fe0flec083bd95e088258@mail.gmail.com>
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB22047C7206@mail-1.rf.lan>

I ended up using a different approach...

1. I created a table called replication_heartbeat that has two columns:
id, last_modified
2. I added this table to the replication set
3. I pre-populated a single row into this table
4. I set up a cron job on the master that executes the following query
once a minute: update replication_heartbeat set last_modified=now()
5. I created Nagios alerts on both the master and the slaves that checks
this date.  On the master if the date goes stale then I know that
something is wrong with the update command in cron.  On the slaves if
the date is stale then I know something is wrong with replication.

It's a poor man's approach and it's probably silly to do all this work
when there's a built-in mechanism.  The main advantage is that it's dead
simple to understand.

--S

-----Original Message-----
From: slony1-general-bounces@lists.slony.info
[mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Vivek
Khera
Sent: Sunday, August 03, 2008 1:18 PM
To: kevin kempter
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] how to determine if the slave is caught
up/insync with the master

On Sun, Aug 3, 2008 at 12:35 PM, kevin kempter
<kevin@kevinkempterllc.com> wrote:
> Hi List;
>
> How do I know if a slave is up to date with the master ?
>

On the master, run this SQL query, replacing _CLUSTERNAME with the
name you selected for your cluster.

select * from _CLUSTERNAME.sl_status;
_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


From kevin at kevinkempterllc.com  Tue Aug  5 10:53:54 2008
From: kevin at kevinkempterllc.com (kevin kempter)
Date: Tue Aug  5 10:54:05 2008
Subject: [Slony1-general] how to determine if the slave is caught
	up/insync with the master
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB22047C7206@mail-1.rf.lan>
References: <F1DC4D50-5B7A-45FA-B191-F1114734EBFC@kevinkempterllc.com>
	<2968dfd60808031318w3b02fe0flec083bd95e088258@mail.gmail.com>
	<082D8A131DF72A4D88C908A1AD3DEB22047C7206@mail-1.rf.lan>
Message-ID: <40B0DAF3-A02B-4B68-984F-8C608116C1BD@kevinkempterllc.com>


On Aug 5, 2008, at 11:41 AM, Shahaf Abileah wrote:

> I ended up using a different approach...
>
> 1. I created a table called replication_heartbeat that has two  
> columns:
> id, last_modified
> 2. I added this table to the replication set
> 3. I pre-populated a single row into this table
> 4. I set up a cron job on the master that executes the following query
> once a minute: update replication_heartbeat set last_modified=now()
> 5. I created Nagios alerts on both the master and the slaves that  
> checks
> this date.  On the master if the date goes stale then I know that
> something is wrong with the update command in cron.  On the slaves if
> the date is stale then I know something is wrong with replication.
>
> It's a poor man's approach and it's probably silly to do all this work
> when there's a built-in mechanism.  The main advantage is that it's  
> dead
> simple to understand.
>
> --S
>
> -----Original Message-----
> From: slony1-general-bounces@lists.slony.info
> [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Vivek
> Khera
> Sent: Sunday, August 03, 2008 1:18 PM
> To: kevin kempter
> Cc: slony1-general@lists.slony.info
> Subject: Re: [Slony1-general] how to determine if the slave is caught
> up/insync with the master
>
> On Sun, Aug 3, 2008 at 12:35 PM, kevin kempter
> <kevin@kevinkempterllc.com> wrote:
>> Hi List;
>>
>> How do I know if a slave is up to date with the master ?
>>
>
> On the master, run this SQL query, replacing _CLUSTERNAME with the
> name you selected for your cluster.
>
> select * from _CLUSTERNAME.sl_status;
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>


Is there  documentation someone can point me towards that defines with  
some detail all the columns of the sl_status table, or even better all  
the _CLUSTERNAME tables ?


From stephane.schildknecht at postgresqlfr.org  Tue Aug  5 11:17:34 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Tue Aug  5 11:17:47 2008
Subject: [Slony1-general] how to determine if the slave is caught	up/insync
	with the master
In-Reply-To: <40B0DAF3-A02B-4B68-984F-8C608116C1BD@kevinkempterllc.com>
References: <F1DC4D50-5B7A-45FA-B191-F1114734EBFC@kevinkempterllc.com>	<2968dfd60808031318w3b02fe0flec083bd95e088258@mail.gmail.com>	<082D8A131DF72A4D88C908A1AD3DEB22047C7206@mail-1.rf.lan>
	<40B0DAF3-A02B-4B68-984F-8C608116C1BD@kevinkempterllc.com>
Message-ID: <4898993E.8000609@postgresqlfr.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

kevin kempter a ?crit :
> 
> On Aug 5, 2008, at 11:41 AM, Shahaf Abileah wrote:
> 
>> I ended up using a different approach...
>>
>> 1. I created a table called replication_heartbeat that has two columns:
>> id, last_modified
>> 2. I added this table to the replication set
>> 3. I pre-populated a single row into this table
>> 4. I set up a cron job on the master that executes the following query
>> once a minute: update replication_heartbeat set last_modified=now()
>> 5. I created Nagios alerts on both the master and the slaves that checks
>> this date.  On the master if the date goes stale then I know that
>> something is wrong with the update command in cron.  On the slaves if
>> the date is stale then I know something is wrong with replication.
>>
>> It's a poor man's approach and it's probably silly to do all this work
>> when there's a built-in mechanism.  The main advantage is that it's dead
>> simple to understand.
>>
>> --S
>>
>> -----Original Message-----
>> From: slony1-general-bounces@lists.slony.info
>> [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Vivek
>> Khera
>> Sent: Sunday, August 03, 2008 1:18 PM
>> To: kevin kempter
>> Cc: slony1-general@lists.slony.info
>> Subject: Re: [Slony1-general] how to determine if the slave is caught
>> up/insync with the master
>>
>> On Sun, Aug 3, 2008 at 12:35 PM, kevin kempter
>> <kevin@kevinkempterllc.com> wrote:
>>> Hi List;
>>>
>>> How do I know if a slave is up to date with the master ?
>>>
>>
>> On the master, run this SQL query, replacing _CLUSTERNAME with the
>> name you selected for your cluster.
>>
>> select * from _CLUSTERNAME.sl_status;
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>>
> 
> 
> Is there  documentation someone can point me towards that defines with
> some detail all the columns of the sl_status table, or even better all
> the _CLUSTERNAME tables ?

You could have a look at the official documentation :

http://slony.info/documentation/schema.html

I'm afraid though that sl_status which is a view is not documented there...

A \d will tell you more :

cave2=# \d _test.sl_status
                       View "_test.sl_status"
          Column           |            Type             | Modifiers
- ---------------------------+-----------------------------+-----------
 st_origin                 | integer                     |
 st_received               | integer                     |
 st_last_event             | bigint                      |
 st_last_event_ts          | timestamp without time zone |
 st_last_received          | bigint                      |
 st_last_received_ts       | timestamp without time zone |
 st_last_received_event_ts | timestamp without time zone |
 st_lag_num_events         | bigint                      |
 st_lag_time               | interval                    |
View definition:
 SELECT e.ev_origin AS st_origin, c.con_received AS st_received, e.ev_seqno AS
st_last_event, e.ev_timestamp AS st_last_event_ts, c.con_seqno AS
st_last_received, c.con_timestamp AS st_last_received_ts, ce.ev_timestamp AS
st_last_received_event_ts, e.ev_seqno - c.con_seqno AS st_lag_num_events, now()
- - ce.ev_timestamp::timestamp with time zone AS st_lag_time
   FROM _test.sl_event e, _test.sl_confirm c, _test.sl_event ce
  WHERE e.ev_origin = c.con_origin AND ce.ev_origin = e.ev_origin AND
ce.ev_seqno = c.con_seqno AND ((e.ev_origin, e.ev_seqno) IN ( SELECT
sl_event.ev_origin, max(sl_event.ev_seqno) AS max
           FROM _test.sl_event
          WHERE sl_event.ev_origin = _test.getlocalnodeid('_test'::name)
          GROUP BY sl_event.ev_origin)) AND ((c.con_origin, c.con_received,
c.con_seqno) IN ( SELECT sl_confirm.con_origin, sl_confirm.con_received,
max(sl_confirm.con_seqno) AS max
           FROM _test.sl_confirm
          WHERE sl_confirm.con_origin = _test.getlocalnodeid('_test'::name)
          GROUP BY sl_confirm.con_origin, sl_confirm.con_received));

I think column name are self-explanatory.

Best regards,
- --
St?phane Schildknecht
PostgreSQLFr : http://www.postgresql.fr

Venez nous rencontrer le 4 octobre lors du plus important ?v?nement
PostgreSQL francophone : http://www.pgday.fr

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFImJk+A+REPKWGI0ERAnTXAJ9mJcdpC9ImkNVfHwdgw06jE7WAzACeLMKr
tBtK6m6YVvZ/rBzokrp0uq4=
=IxNQ
-----END PGP SIGNATURE-----
From hannu at krosing.net  Tue Aug  5 11:50:57 2008
From: hannu at krosing.net (Hannu Krosing)
Date: Tue Aug  5 11:51:30 2008
Subject: [Slony1-general] 	Strange behavior adding a new node, very,
	VERY slow
In-Reply-To: <48980FFD.5090408@albourne.com>
References: <4897FEC5.1080905@albourne.com> <4897FF0C.503@albourne.com>
	<4898065A.1030904@postgresqlfr.org>  <48980FFD.5090408@albourne.com>
Message-ID: <1217962257.6984.1.camel@huvostro>

On Tue, 2008-08-05 at 11:31 +0300, Martin Eriksson wrote:
> this is where it start getting weird,
> 
> No error anywhere in any log.
> 
> with
> 
> pg_stat_activity
> 
> I just got 3 transactions sitting in idle (to the three other nodes, 
> Master, node2,3)
> 
> It is using exactly the same schema as i was using for node 2 (as its on 
> the same machine so used the same one!)

Did you by accident include existing slony schema in your copy ?

-----------------
Hannu


From m.eriksson at albourne.com  Tue Aug  5 23:16:55 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Tue Aug  5 23:17:23 2008
Subject: [Slony1-general] 	Strange behavior adding a new node, very, VERY
	slow
In-Reply-To: <1217962257.6984.1.camel@huvostro>
References: <4897FEC5.1080905@albourne.com> <4897FF0C.503@albourne.com>	
	<4898065A.1030904@postgresqlfr.org> <48980FFD.5090408@albourne.com>
	<1217962257.6984.1.camel@huvostro>
Message-ID: <489941D7.10803@albourne.com>

Thanks to everyone that been helping.

Figured it out.

basically I screwed up.... there was a ip typo in my slon_tools.conf and 
it went to a completely different database server (where it just happen 
to be sitting a db with a correct schema, just that this database was 
far away and hence very very slow)

so from the master it looked like it went to the correct machine, but it 
just went to the daemon and then the daemon was passing it on to the 
other db.. so a bit of a screw up.. took a while to figure that one out, 
as often tend to overlook the most basic stuff!!

doh!

cheers
Martin

Hannu Krosing wrote:
> On Tue, 2008-08-05 at 11:31 +0300, Martin Eriksson wrote:
>   
>> this is where it start getting weird,
>>
>> No error anywhere in any log.
>>
>> with
>>
>> pg_stat_activity
>>
>> I just got 3 transactions sitting in idle (to the three other nodes, 
>> Master, node2,3)
>>
>> It is using exactly the same schema as i was using for node 2 (as its on 
>> the same machine so used the same one!)
>>     
>
> Did you by accident include existing slony schema in your copy ?
>
> -----------------
> Hannu
>
>
>   

From jeff at frostconsultingllc.com  Wed Aug  6 10:29:07 2008
From: jeff at frostconsultingllc.com (Jeff Frost)
Date: Wed Aug  6 10:29:36 2008
Subject: [Slony1-general] strange problem during initial sync
Message-ID: <Pine.LNX.4.64.0808061025540.4756@discord.home.frostconsultingllc.com>

I've got a strange problem with one of my slony slaves that I dropped from 
replication and resubscribed.  During initial sync it is complaining that 
sequence id 1 has already been assigned when it's trying to add the first 
sequence in the set (after syncing all the tables):

http://pgsql.privatepaste.com/a88xkleQyj

Aug 4 18:01:42 proddb1-slave2 postgres[26048]: [707-2] STATEMENT: SELECT
"_engage_cluster".setAddSequence_int(1, 1, 
'"public"."hibernate_sequence"','public.hibernate_sequence
Aug 4 18:01:42 proddb1-slave2 postgres[26048]: [707-3] table')
Aug 4 18:01:42 proddb1-slave2 slon[26034]: [250-1] 2008-08-04 18:01:42 PDT
ERROR remoteWorkerThread_1: "select "_engage_cluster".setAddSequence_int(1,1,
Aug 4 18:01:42 proddb1-slave2 slon[26034]: [250-2]
'"public"."hibernate_sequence"', 'public.hibernate_sequence table')"
PGRES_FATAL_ERROR ERROR: Slony-I: SET AddSequence_int():
Aug 4 18:01:42 proddb1-slave2 slon[26034]: [250-3] sequence ID 1 has
already been assigned
Aug 4 18:01:42 proddb1-slave2 slon[26034]: [251-1] 2008-08-04 18:01:42 PDT
WARN remoteWorkerThread_1: DATA copy FOR SET 1 failed - sleep 60 seconds

Any idea how to fix this without dropping it from subscription and subscribing 
it again?  I would expect that no sequences have this id yet since this is the 
initial sync.  Maybe it's possible I didn't successfully remove the entire 
slony schema before resubscribing?  That would have slonik throw an error 
correct?

-- 
Jeff Frost, Owner 	<jeff@frostconsultingllc.com>
Frost Consulting, LLC 	http://www.frostconsultingllc.com/
Phone: 916-647-6411	FAX: 916-405-4032
From stephane.schildknecht at postgresqlfr.org  Wed Aug  6 14:00:31 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?St=E9phane_Schildknecht?=)
Date: Wed Aug  6 14:00:53 2008
Subject: [Slony1-general] strange problem during initial sync
In-Reply-To: <Pine.LNX.4.64.0808061025540.4756@discord.home.frostconsultingllc.com>
References: <Pine.LNX.4.64.0808061025540.4756@discord.home.frostconsultingllc.com>
Message-ID: <489A10EF.9080000@postgresqlfr.org>

Jeff Frost a ?crit :
> I've got a strange problem with one of my slony slaves that I dropped 
> from replication and resubscribed.  During initial sync it is 
> complaining that sequence id 1 has already been assigned when it's 
> trying to add the first sequence in the set (after syncing all the 
> tables):
>
> http://pgsql.privatepaste.com/a88xkleQyj
>
> Aug 4 18:01:42 proddb1-slave2 postgres[26048]: [707-2] STATEMENT: SELECT
> "_engage_cluster".setAddSequence_int(1, 1, 
> '"public"."hibernate_sequence"','public.hibernate_sequence
> Aug 4 18:01:42 proddb1-slave2 postgres[26048]: [707-3] table')
> Aug 4 18:01:42 proddb1-slave2 slon[26034]: [250-1] 2008-08-04 18:01:42 
> PDT
> ERROR remoteWorkerThread_1: "select 
> "_engage_cluster".setAddSequence_int(1,1,
> Aug 4 18:01:42 proddb1-slave2 slon[26034]: [250-2]
> '"public"."hibernate_sequence"', 'public.hibernate_sequence table')"
> PGRES_FATAL_ERROR ERROR: Slony-I: SET AddSequence_int():
> Aug 4 18:01:42 proddb1-slave2 slon[26034]: [250-3] sequence ID 1 has
> already been assigned
> Aug 4 18:01:42 proddb1-slave2 slon[26034]: [251-1] 2008-08-04 18:01:42 
> PDT
> WARN remoteWorkerThread_1: DATA copy FOR SET 1 failed - sleep 60 seconds
>
> Any idea how to fix this without dropping it from subscription and 
> subscribing it again?  I would expect that no sequences have this id 
> yet since this is the initial sync.  Maybe it's possible I didn't 
> successfully remove the entire slony schema before resubscribing?  
> That would have slonik throw an error correct?
>

Well, something tells me you're facing what we could call a bug, no ?

I had the same problem last week. What version of slony are you runinng ?

Regards,
St?phane
From jeff at frostconsultingllc.com  Wed Aug  6 14:04:42 2008
From: jeff at frostconsultingllc.com (Jeff Frost)
Date: Wed Aug  6 14:05:19 2008
Subject: [Slony1-general] strange problem during initial sync
In-Reply-To: <489A10EF.9080000@postgresqlfr.org>
References: <Pine.LNX.4.64.0808061025540.4756@discord.home.frostconsultingllc.com>
	<489A10EF.9080000@postgresqlfr.org>
Message-ID: <Pine.LNX.4.64.0808061402260.4756@discord.home.frostconsultingllc.com>

On Wed, 6 Aug 2008, St=E9phane Schildknecht wrote:

> Jeff Frost a =E9crit :
>> I've got a strange problem with one of my slony slaves that I dropped fr=
om =

>> replication and resubscribed.  During initial sync it is complaining tha=
t =

>> sequence id 1 has already been assigned when it's trying to add the firs=
t =

>> sequence in the set (after syncing all the tables):
>> =

>> http://pgsql.privatepaste.com/a88xkleQyj
>> =

>> Aug 4 18:01:42 proddb1-slave2 postgres[26048]: [707-2] STATEMENT: SELECT
>> "_engage_cluster".setAddSequence_int(1, 1, =

>> '"public"."hibernate_sequence"','public.hibernate_sequence
>> Aug 4 18:01:42 proddb1-slave2 postgres[26048]: [707-3] table')
>> Aug 4 18:01:42 proddb1-slave2 slon[26034]: [250-1] 2008-08-04 18:01:42 P=
DT
>> ERROR remoteWorkerThread_1: "select =

>> "_engage_cluster".setAddSequence_int(1,1,
>> Aug 4 18:01:42 proddb1-slave2 slon[26034]: [250-2]
>> '"public"."hibernate_sequence"', 'public.hibernate_sequence table')"
>> PGRES_FATAL_ERROR ERROR: Slony-I: SET AddSequence_int():
>> Aug 4 18:01:42 proddb1-slave2 slon[26034]: [250-3] sequence ID 1 has
>> already been assigned
>> Aug 4 18:01:42 proddb1-slave2 slon[26034]: [251-1] 2008-08-04 18:01:42 P=
DT
>> WARN remoteWorkerThread_1: DATA copy FOR SET 1 failed - sleep 60 seconds
>> =

>> Any idea how to fix this without dropping it from subscription and =

>> subscribing it again?  I would expect that no sequences have this id yet =

>> since this is the initial sync.  Maybe it's possible I didn't successful=
ly =

>> remove the entire slony schema before resubscribing?  That would have =

>> slonik throw an error correct?
>> =

>
> Well, something tells me you're facing what we could call a bug, no ?
>
> I had the same problem last week. What version of slony are you runinng ?
>

This is slony version 1.2.14 on Postgresql 8.3.1.  I've torn it down and =

resubscribed it to see if it is happy this time.

-- =

Jeff Frost, Owner 	<jeff@frostconsultingllc.com>
Frost Consulting, LLC 	http://www.frostconsultingllc.com/
Phone: 916-647-6411	FAX: 916-405-4032
From jeff at frostconsultingllc.com  Wed Aug  6 21:55:25 2008
From: jeff at frostconsultingllc.com (Jeff Frost)
Date: Wed Aug  6 21:56:06 2008
Subject: [Slony1-general] strange problem during initial sync
In-Reply-To: <Pine.LNX.4.64.0808061402260.4756@discord.home.frostconsultingllc.com>
References: <Pine.LNX.4.64.0808061025540.4756@discord.home.frostconsultingllc.com>
	<489A10EF.9080000@postgresqlfr.org>
	<Pine.LNX.4.64.0808061402260.4756@discord.home.frostconsultingllc.com>
Message-ID: <Pine.LNX.4.64.0808062154500.5513@discord.home.frostconsultingllc.com>
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From glynastill at yahoo.co.uk  Thu Aug  7 09:22:07 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Thu Aug  7 09:22:15 2008
Subject: [Slony1-general] IP address change
Message-ID: <214281.54752.qm@web25807.mail.ukl.yahoo.com>

It looks like at some point in the future I'm going to have to shift my production servers off onto another subnet.  Is there a set way to go about changing the IP addresses of slony nodes?  Or do I just go in and piddle with sl_path manually?


      __________________________________________________________
Not happy with your email address?.
Get the one you really want - millions of new email addresses available now at Yahoo! http://uk.docs.yahoo.com/ymail/new.html
From kevin at kevinkempterllc.com  Thu Aug  7 09:46:06 2008
From: kevin at kevinkempterllc.com (kevin kempter)
Date: Thu Aug  7 09:46:21 2008
Subject: [Slony1-general] How to determine how long it took for a slave to
	'catch up'
Message-ID: <5EDD09AE-7E16-41C7-B4E3-2E372753B5C3@kevinkempterllc.com>

Hi All;

I setup a SLONY cluster yesterday and it worked without any issues.  
Can anyone point me to some documentation, or tell me how to determine  
how long it took for the slave to be completely caught up with the  
master?



From shahaf at redfin.com  Thu Aug  7 09:54:18 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Thu Aug  7 09:54:26 2008
Subject: [Slony1-general] How to determine how long it took for a slave
	to'catch up'
In-Reply-To: <5EDD09AE-7E16-41C7-B4E3-2E372753B5C3@kevinkempterllc.com>
References: <5EDD09AE-7E16-41C7-B4E3-2E372753B5C3@kevinkempterllc.com>
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB22048306A4@mail-1.rf.lan>

I generally do something like this:

grep "seconds to copy" <log file for slave>

--S

-----Original Message-----
From: slony1-general-bounces@lists.slony.info
[mailto:slony1-general-bounces@lists.slony.info] On Behalf Of kevin
kempter
Sent: Thursday, August 07, 2008 9:46 AM
To: slony1-general@lists.slony.info
Subject: [Slony1-general] How to determine how long it took for a slave
to'catch up'

Hi All;

I setup a SLONY cluster yesterday and it worked without any issues.  
Can anyone point me to some documentation, or tell me how to determine  
how long it took for the slave to be completely caught up with the  
master?



_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


From glynastill at yahoo.co.uk  Thu Aug  7 10:12:16 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Thu Aug  7 10:12:25 2008
Subject: [Slony1-general] IP address change
In-Reply-To: <873algu5sd.fsf@dba2.int.libertyrms.com>
Message-ID: <685534.11604.qm@web25808.mail.ukl.yahoo.com>

> There are two parts to this:
> 
> 1.  You need to change the configuration in sl_path on all
> nodes.
> 
> The *right* way to do this is by using the Slonik command
> STORE PATH
> to submit requests that indicate the revised addresses.
> 
> Don't change the IPs manually in sl_path; use STORE
> PATH.
> 
> 2.  You need to change the configuration used to get the
> slon
> processes to connect to the nodes that they manage.
> 
> Where to change that depends on how you launch slon
> processes.

Thanks Chris,

Which way around should I do this? I can only think as follows:

1) stop the slons
2) change servers ip addresses, addresses in slon.conf's and the preambles in my slonik scripts
3) run slonik STORE PATH for each node
4) restart slons
5a) Smile
5b) Cry


      __________________________________________________________
Not happy with your email address?.
Get the one you really want - millions of new email addresses available now at Yahoo! http://uk.docs.yahoo.com/ymail/new.html
From cbbrowne at ca.afilias.info  Thu Aug  7 12:03:24 2008
From: cbbrowne at ca.afilias.info (cbbrowne)
Date: Thu Aug  7 12:03:36 2008
Subject: [Slony1-general] IP address change
In-Reply-To: <685534.11604.qm@web25808.mail.ukl.yahoo.com>
References: <685534.11604.qm@web25808.mail.ukl.yahoo.com>
Message-ID: <489B46FC.60702@ca.afilias.info>

Glyn Astill wrote:
>> There are two parts to this:
>>
>> 1.  You need to change the configuration in sl_path on all
>> nodes.
>>
>> The *right* way to do this is by using the Slonik command
>> STORE PATH
>> to submit requests that indicate the revised addresses.
>>
>> Don't change the IPs manually in sl_path; use STORE
>> PATH.
>>
>> 2.  You need to change the configuration used to get the
>> slon
>> processes to connect to the nodes that they manage.
>>
>> Where to change that depends on how you launch slon
>> processes.
>>     
>
> Thanks Chris,
>
> Which way around should I do this? I can only think as follows:
>
> 1) stop the slons
> 2) change servers ip addresses, addresses in slon.conf's and the preambles in my slonik scripts
> 3) run slonik STORE PATH for each node
> 4) restart slons
> 5a) Smile
> 5b) Cry
>   
Yeah, that sounds about right.  Hopefully with more smiles than cries ;-).

-- 
let name="cbbrowne" and tld="ca.afilias.info" in name ^ "@" ^ tld;;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)

From glynastill at yahoo.co.uk  Thu Aug  7 13:44:15 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Thu Aug  7 13:44:28 2008
Subject: [Slony1-general] IP address change
In-Reply-To: <489B46FC.60702@ca.afilias.info>
Message-ID: <313812.54901.qm@web25808.mail.ukl.yahoo.com>

 
> Yeah, that sounds about right.  Hopefully with more smiles
> than cries ;-).
> 

Cool, Ta :)


      __________________________________________________________
Not happy with your email address?.
Get the one you really want - millions of new email addresses available now at Yahoo! http://uk.docs.yahoo.com/ymail/new.html
From msteben at autorevenue.com  Fri Aug  8 12:56:50 2008
From: msteben at autorevenue.com (Mark Steben)
Date: Fri Aug  8 12:58:08 2008
Subject: [Slony1-general] node -1 error after restarting servers
Message-ID: <002201c8f990$e59b0d90$14050a0a@dei26g028575>
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From msteben at autorevenue.com  Mon Aug 11 05:05:18 2008
From: msteben at autorevenue.com (Mark Steben)
Date: Mon Aug 11 05:05:37 2008
Subject: [Slony1-general] RE: node -1 error after restarting servers
Message-ID: <001601c8fbaa$85af4f10$14050a0a@dei26g028575>

I messed up and sent this originally as HTML.  Resending as plain text.

________________________________________
From: Mark Steben [mailto:msteben@autorevenue.com] 
Sent: Friday, August 08, 2008 3:57 PM
To: 'slony1-general@lists.slony.info'
Subject: node -1 error after restarting servers

Hi - hoping for some help.
I'm running Slony 1.2.14 on a simple 1 master 1 slave configuration.? Each
server is running Postgres 8.2.5.
I had to restart both master and slave servers.? When I tried to restart the
slons I got errors in the slave log that
Table ids were already assigned within the set.? So I dropped the set,
dropped the path and the listens,
And recreated all with CREATE SET,? STORE PATH, and STORE LISTENS on the
provider

Now when I restart the slons and subscribe the newly defined set I get the
following error in the logs
On the slave:
?? ?node -1 not found in runtime configuration

and the copy fails.

When I query SL_NODE on the master I get:

? ?no_id | no_active |?? no_comment??? | no_spool 
-------+-----------+-----------------+----------
???? 1 | t???????? | Master Node???? | f
???? 2 | f???????? | <event pending> | f

And the same query on the slave gives:

no_id | no_active |??????? no_comment??????? | no_spool 
-------+-----------+--------------------------+----------
???? 1 | t???????? | Master Node?????????? ?????????| f
???? 2 | t???????? | subscriber node?????????????? ?| f
(2 rows)

Do I have to recreate the node(s) as well?? ?

Any help would be appreciated.? thanks
. 
________________________________________
Mark Steben?Database Administrator? @utoRevenueT
480 Pleasant Street, Suite B200, Lee, MA 01238?
413-243-4800 x1512?(Phone)???413-243-4809 (Fax)
A Division of Dominion Enterprises
?
?
?


From msteben at autorevenue.com  Mon Aug 11 08:48:01 2008
From: msteben at autorevenue.com (Mark Steben)
Date: Mon Aug 11 08:48:06 2008
Subject: [Slony1-general] RE: node -1 error after restarting servers
In-Reply-To: <87r68vr476.fsf@dba2.int.libertyrms.com>
Message-ID: <002c01c8fbc9$a28bac40$14050a0a@dei26g028575>



Thanks Chris for the response,

I have tried to restart the slons several times with no success.

I should have added that, prior to this problem, I was succesfully
Replicating 7 tables.
Then I added an 8th table to the replication through the recommended
scenario of
 CREATE SET (Set add table)
 SUBSCRIBE SET
 MERGE SET

This worked.  Now, upon further investigation, I do a query
 On the slave:

select * from sl_set   
slony_practice-# ;
 set_id | set_origin | set_locked |          set_comment           
--------+------------+------------+--------------------------------
      1 |          1 |            | 7 mavmail tables
     99 |          1 |            | merged feedback response table

 
But On the master: 
 
 slony_practice=# select * from sl_set;
 set_id | set_origin | set_locked |     set_comment      
--------+------------+------------+----------------------
      1 |          1 |            | 7 + 1 mavmail tables


This condition occurs because, when I restarted the servers I did a
DROP SET / CREATE SET set of sloniks on the master where I recreated the
Set 1 with all 8 tables and dropped set 99. So now obviously I need to
Get them back in sync.  I tried a DROP SET on the slave to attempt to
Drop set 99 but it errors out saying that:
  
"set 99 does not originate on local node:

And I tried the same thing on the master node and got the same message.

Do I need to rerun the entire MERGE SET scenario where I create set 99 on
the master again?

Thanks for your time.

Mark

From: chris [mailto:chris@dba2.int.libertyrms.com] 
Sent: Monday, August 11, 2008 10:56 AM
To: Mark Steben
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] RE: node -1 error after restarting servers

"Mark Steben" <msteben@autorevenue.com> writes:
> I messed up and sent this originally as HTML.  Resending as plain text.

Thanks, that's helpful!

> Hi - hoping for some help.
> I'm running Slony 1.2.14 on a simple 1 master 1 slave configuration.? Each
> server is running Postgres 8.2.5.
> I had to restart both master and slave servers.? When I tried to restart
the
> slons I got errors in the slave log that
> Table ids were already assigned within the set.? So I dropped the set,
> dropped the path and the listens,
> And recreated all with CREATE SET,? STORE PATH, and STORE LISTENS on the
> provider
>
> Now when I restart the slons and subscribe the newly defined set I get the
> following error in the logs
> On the slave:
> ?? ?node -1 not found in runtime configuration
>
> and the copy fails.
>
> When I query SL_NODE on the master I get:
>
> ? ?no_id | no_active |?? no_comment??? | no_spool 
> -------+-----------+-----------------+----------
> ???? 1 | t???????? | Master Node???? | f
> ???? 2 | f???????? | <event pending> | f
>
> And the same query on the slave gives:
>
> no_id | no_active |??????? no_comment??????? | no_spool 
> -------+-----------+--------------------------+----------
> ???? 1 | t???????? | Master Node?????????? ?????????| f
> ???? 2 | t???????? | subscriber node?????????????? ?| f
> (2 rows)
>
> Do I have to recreate the node(s) as well?? ?
>
> Any help would be appreciated.? thanks

That error message takes place at the beginning of the function
"copy_set", and indicates that the slon couldn't find the node in the
in-memory configuration.

That can commonly be rectified by restarting the slon, which will
cause the slon to reread its configuration.

Except, we should take a step back.  The trouble is that copy_set()
was trying to access a node that it wasn't properly aware of.

The data that you have provided about sl_node on both nodes looks
useful, actually.  It indicates that the "master" node hasn't yet
figured out its configuration, which seems quite plausible to cause
further troubles.

If I had to guess, I'd imagine that perhaps you hadn't run a slon
against the "master" node and that it isn't properly aware, yet, that
node #2 has been set up.

Before you start trying to set up subscriptions, make sure you can run
things like "STORE PATH" and see, in the logs, that this has been
processed by both nodes.
-- 
let name="cbbrowne" and tld="acm.org" in String.concat "@" [name;tld];;
http://cbbrowne.com/info/x.html
"What  you end  up with,  after  running an  operating system  concept
through these  many marketing coffee filters, is  something not unlike
plain hot water."  -- Matt Welsh

From jennifer.spencer at stanford.edu  Mon Aug 11 13:51:49 2008
From: jennifer.spencer at stanford.edu (Jennifer Spencer)
Date: Mon Aug 11 13:52:12 2008
Subject: [Slony1-general] RE: node -1 error after restarting servers
In-Reply-To: <002c01c8fbc9$a28bac40$14050a0a@dei26g028575>
References: <002c01c8fbc9$a28bac40$14050a0a@dei26g028575>
Message-ID: <48A0A665.5070400@stanford.edu>

Hi Mark - I have had this problem myself.  The only way I solved it was to remove all reference to the 
set merge/creation (set 99 in your case) from my sl_events table.  It's not enough to remove 
references to the set from your sl_set and sl_table tables, unfortunately.

It's a little tricky to find the event number you need to remove once you've gotten rid of the set 
from sl_set, sl_table, etc.  Maybe someone else here will chime in on that?

-Jennifer

Mark Steben wrote:
> 
> Thanks Chris for the response,
> 
> I have tried to restart the slons several times with no success.
> 
> I should have added that, prior to this problem, I was succesfully
> Replicating 7 tables.
> Then I added an 8th table to the replication through the recommended
> scenario of
>  CREATE SET (Set add table)
>  SUBSCRIBE SET
>  MERGE SET
> 
> This worked.  Now, upon further investigation, I do a query
>  On the slave:
> 
> select * from sl_set   
> slony_practice-# ;
>  set_id | set_origin | set_locked |          set_comment           
> --------+------------+------------+--------------------------------
>       1 |          1 |            | 7 mavmail tables
>      99 |          1 |            | merged feedback response table
> 
>  
> But On the master: 
>  
>  slony_practice=# select * from sl_set;
>  set_id | set_origin | set_locked |     set_comment      
> --------+------------+------------+----------------------
>       1 |          1 |            | 7 + 1 mavmail tables
> 
> 
> This condition occurs because, when I restarted the servers I did a
> DROP SET / CREATE SET set of sloniks on the master where I recreated the
> Set 1 with all 8 tables and dropped set 99. So now obviously I need to
> Get them back in sync.  I tried a DROP SET on the slave to attempt to
> Drop set 99 but it errors out saying that:
>   
> "set 99 does not originate on local node:
> 
> And I tried the same thing on the master node and got the same message.
> 
> Do I need to rerun the entire MERGE SET scenario where I create set 99 on
> the master again?
> 
> Thanks for your time.
> 
> Mark
> 
> From: chris [mailto:chris@dba2.int.libertyrms.com] 
> Sent: Monday, August 11, 2008 10:56 AM
> To: Mark Steben
> Cc: slony1-general@lists.slony.info
> Subject: Re: [Slony1-general] RE: node -1 error after restarting servers
> 
> "Mark Steben" <msteben@autorevenue.com> writes:
>> I messed up and sent this originally as HTML.  Resending as plain text.
> 
> Thanks, that's helpful!
> 
>> Hi - hoping for some help.
>> I'm running Slony 1.2.14 on a simple 1 master 1 slave configuration.  Each
>> server is running Postgres 8.2.5.
>> I had to restart both master and slave servers.  When I tried to restart
> the
>> slons I got errors in the slave log that
>> Table ids were already assigned within the set.  So I dropped the set,
>> dropped the path and the listens,
>> And recreated all with CREATE SET,  STORE PATH, and STORE LISTENS on the
>> provider
>>
>> Now when I restart the slons and subscribe the newly defined set I get the
>> following error in the logs
>> On the slave:
>>     node -1 not found in runtime configuration
>>
>> and the copy fails.
>>
>> When I query SL_NODE on the master I get:
>>
>>    no_id | no_active |   no_comment    | no_spool 
>> -------+-----------+-----------------+----------
>>      1 | t         | Master Node     | f
>>      2 | f         | <event pending> | f
>>
>> And the same query on the slave gives:
>>
>> no_id | no_active |        no_comment        | no_spool 
>> -------+-----------+--------------------------+----------
>>      1 | t         | Master Node                    | f
>>      2 | t         | subscriber node                | f
>> (2 rows)
>>
>> Do I have to recreate the node(s) as well?   
>>
>> Any help would be appreciated.  thanks
> 
> That error message takes place at the beginning of the function
> "copy_set", and indicates that the slon couldn't find the node in the
> in-memory configuration.
> 
> That can commonly be rectified by restarting the slon, which will
> cause the slon to reread its configuration.
> 
> Except, we should take a step back.  The trouble is that copy_set()
> was trying to access a node that it wasn't properly aware of.
> 
> The data that you have provided about sl_node on both nodes looks
> useful, actually.  It indicates that the "master" node hasn't yet
> figured out its configuration, which seems quite plausible to cause
> further troubles.
> 
> If I had to guess, I'd imagine that perhaps you hadn't run a slon
> against the "master" node and that it isn't properly aware, yet, that
> node #2 has been set up.
> 
> Before you start trying to set up subscriptions, make sure you can run
> things like "STORE PATH" and see, in the logs, that this has been
> processed by both nodes.
From scetbon at echo.fr  Tue Aug 12 06:57:19 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Tue Aug 12 06:57:25 2008
Subject: [Slony1-general] logswitch_start does not occurs
Message-ID: <48A196BF.8040103@echo.fr>

Hi,

We're using slony 1.2.10 and documentation says :

/ As of version 1.2, "log switching" functionality is in place; every so 
often, it seeks to switch between storing data in sl_log_1 
<file:///tmp/.fr-HbzwQF/slony1-1.2.10/doc/adminguide/table.sl-log-1.html> 
and sl_log_2 
<file:///tmp/.fr-HbzwQF/slony1-1.2.10/doc/adminguide/table.sl-log-2.html> 
so that it may seek to TRUNCATE the "elder" data.
/
However, I cannot catch any flip/flop stage. I've searched in sources 
and I can't find logswitch_start function call except in code slonik.c
Do I have to use this command or is it really supposed to be done 
automatically ?

-- 
Cyril SCETBON
From ajs at crankycanuck.ca  Tue Aug 12 07:05:50 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue Aug 12 07:05:56 2008
Subject: [Slony1-general] logswitch_start does not occurs
In-Reply-To: <48A196BF.8040103@echo.fr>
References: <48A196BF.8040103@echo.fr>
Message-ID: <20080812140549.GA27174@crankycanuck.ca>

On Tue, Aug 12, 2008 at 03:57:19PM +0200, Cyril SCETBON wrote:

> However, I cannot catch any flip/flop stage. I've searched in sources and I 
> can't find logswitch_start function call except in code slonik.c
> Do I have to use this command or is it really supposed to be done 
> automatically ?

It happens automatically.  

A

-- 
Andrew Sullivan
ajs@commandprompt.com
+1 503 667 4564 x104
http://www.commandprompt.com/
From scetbon at echo.fr  Tue Aug 12 07:08:56 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Tue Aug 12 07:09:06 2008
Subject: [Slony1-general] logswitch_start does not occurs
In-Reply-To: <48A196BF.8040103@echo.fr>
References: <48A196BF.8040103@echo.fr>
Message-ID: <48A19978.9040702@echo.fr>



Cyril SCETBON wrote:
> Hi,
>
> We're using slony 1.2.10 and documentation says :
>
> / As of version 1.2, "log switching" functionality is in place; every 
> so often, it seeks to switch between storing data in sl_log_1 
> <file:///tmp/.fr-HbzwQF/slony1-1.2.10/doc/adminguide/table.sl-log-1.html> 
> and sl_log_2 
> <file:///tmp/.fr-HbzwQF/slony1-1.2.10/doc/adminguide/table.sl-log-2.html> 
> so that it may seek to TRUNCATE the "elder" data.
> /
> However, I cannot catch any flip/flop stage. I've searched in sources 
> and I can't find logswitch_start function call except in code slonik.c
> Do I have to use this command or is it really supposed to be done 
> automatically ?
>
The only calls I see are logswitch_finish and logswitch_weekly. If I 
understand correctly flip/flop occurs once a week. As we have a lot of 
statement, I should use logswitch_start manually if I want it to happen 
more often. But what means "every so often" in the documentation ? (see 
above)

-- 
Cyril SCETBON
From msteben at autorevenue.com  Tue Aug 12 08:03:01 2008
From: msteben at autorevenue.com (Mark Steben)
Date: Tue Aug 12 08:02:35 2008
Subject: [Slony1-general] RE: node -1 error after restarting servers
In-Reply-To: <48A0A665.5070400@stanford.edu>
Message-ID: <002301c8fc8c$83d69bc0$14050a0a@dei26g028575>

Thanks, Jennifer - I finally got this to work
Not sure exactly why - I'll describe the scenario and welcome comments:
   1. UNSUSCRIBE SET
   2. Drop SET 1 on master (containing the original 7 tables)
      a. Drop associated paths
      b. Drop associated listens
   3. Drop SET 99 on master (containing the 8th table)
   4. Drop slave node (node 2 - running from master)
   5. Login to slave and sql delete node 99
       (delete from sl_set where id = 99)
   

Did this all yesterday. The slony log was still producing the error
  (node -1 not found in runtime configuration)
The missing piece I believe was that the Postgres server got restarted
Last night WITH THE SLONS STILL RUNNING:
   This morning I looked at the slony logs running and didn't
   See the error after the restart.  Slony restarted, removed a stale   
    Nodelock and essentially cleaned itself up.

  6. Did another CREATE SET, including all 8 tables in the replication
     a. STORE PATH(s)
     b. STORE LISTEN(s)

  7. SUBSCRIBE SET 1

The slon took about 10 minutes to catch up replicating then we were good to
go. 
One thing I think I was doing wrong (again I welcome comments) was that
I manually killed the slons every time I received the node -1 error
I would restart them when I attempted another solution.  I guess Slony
Needs to be allowed to clean itself up.

Hopefully this will help someone avoid this pain in the future.
      (Add to Doc?)  I'll help any way I can.

________________________________

Mark Steben?Database Administrator? @utoRevenueT
480 Pleasant Street, Suite B200, Lee, MA 01238 
413-243-4800 x1512 (Phone) ? 413-243-4809 (Fax)
A Division of Dominion Enterprises

 
 
 
 
 
-----Original Message-----
From: Jennifer Spencer [mailto:jennifer.spencer@stanford.edu] 
Sent: Monday, August 11, 2008 4:52 PM
To: Mark Steben
Cc: 'chris'; slony1-general@lists.slony.info
Subject: Re: [Slony1-general] RE: node -1 error after restarting servers

Hi Mark - I have had this problem myself.  The only way I solved it was to
remove all reference to the 
set merge/creation (set 99 in your case) from my sl_events table.  It's not
enough to remove 
references to the set from your sl_set and sl_table tables, unfortunately.

It's a little tricky to find the event number you need to remove once you've
gotten rid of the set 
from sl_set, sl_table, etc.  Maybe someone else here will chime in on that?

-Jennifer

Mark Steben wrote:
> 
> Thanks Chris for the response,
> 
> I have tried to restart the slons several times with no success.
> 
> I should have added that, prior to this problem, I was succesfully
> Replicating 7 tables.
> Then I added an 8th table to the replication through the recommended
> scenario of
>  CREATE SET (Set add table)
>  SUBSCRIBE SET
>  MERGE SET
> 
> This worked.  Now, upon further investigation, I do a query
>  On the slave:
> 
> select * from sl_set   
> slony_practice-# ;
>  set_id | set_origin | set_locked |          set_comment           
> --------+------------+------------+--------------------------------
>       1 |          1 |            | 7 mavmail tables
>      99 |          1 |            | merged feedback response table
> 
>  
> But On the master: 
>  
>  slony_practice=# select * from sl_set;
>  set_id | set_origin | set_locked |     set_comment      
> --------+------------+------------+----------------------
>       1 |          1 |            | 7 + 1 mavmail tables
> 
> 
> This condition occurs because, when I restarted the servers I did a
> DROP SET / CREATE SET set of sloniks on the master where I recreated the
> Set 1 with all 8 tables and dropped set 99. So now obviously I need to
> Get them back in sync.  I tried a DROP SET on the slave to attempt to
> Drop set 99 but it errors out saying that:
>   
> "set 99 does not originate on local node:
> 
> And I tried the same thing on the master node and got the same message.
> 
> Do I need to rerun the entire MERGE SET scenario where I create set 99 on
> the master again?
> 
> Thanks for your time.
> 
> Mark
> 
> From: chris [mailto:chris@dba2.int.libertyrms.com] 
> Sent: Monday, August 11, 2008 10:56 AM
> To: Mark Steben
> Cc: slony1-general@lists.slony.info
> Subject: Re: [Slony1-general] RE: node -1 error after restarting servers
> 
> "Mark Steben" <msteben@autorevenue.com> writes:
>> I messed up and sent this originally as HTML.  Resending as plain text.
> 
> Thanks, that's helpful!
> 
>> Hi - hoping for some help.
>> I'm running Slony 1.2.14 on a simple 1 master 1 slave configuration.
Each
>> server is running Postgres 8.2.5.
>> I had to restart both master and slave servers.  When I tried to restart
> the
>> slons I got errors in the slave log that
>> Table ids were already assigned within the set.  So I dropped the set,
>> dropped the path and the listens,
>> And recreated all with CREATE SET,  STORE PATH, and STORE LISTENS on the
>> provider
>>
>> Now when I restart the slons and subscribe the newly defined set I get
the
>> following error in the logs
>> On the slave:
>>     node -1 not found in runtime configuration
>>
>> and the copy fails.
>>
>> When I query SL_NODE on the master I get:
>>
>>    no_id | no_active |   no_comment    | no_spool 
>> -------+-----------+-----------------+----------
>>      1 | t         | Master Node     | f
>>      2 | f         | <event pending> | f
>>
>> And the same query on the slave gives:
>>
>> no_id | no_active |        no_comment        | no_spool 
>> -------+-----------+--------------------------+----------
>>      1 | t         | Master Node                    | f
>>      2 | t         | subscriber node                | f
>> (2 rows)
>>
>> Do I have to recreate the node(s) as well?   
>>
>> Any help would be appreciated.  thanks
> 
> That error message takes place at the beginning of the function
> "copy_set", and indicates that the slon couldn't find the node in the
> in-memory configuration.
> 
> That can commonly be rectified by restarting the slon, which will
> cause the slon to reread its configuration.
> 
> Except, we should take a step back.  The trouble is that copy_set()
> was trying to access a node that it wasn't properly aware of.
> 
> The data that you have provided about sl_node on both nodes looks
> useful, actually.  It indicates that the "master" node hasn't yet
> figured out its configuration, which seems quite plausible to cause
> further troubles.
> 
> If I had to guess, I'd imagine that perhaps you hadn't run a slon
> against the "master" node and that it isn't properly aware, yet, that
> node #2 has been set up.
> 
> Before you start trying to set up subscriptions, make sure you can run
> things like "STORE PATH" and see, in the logs, that this has been
> processed by both nodes.

From yi.zhao at alibaba-inc.com  Tue Aug 12 20:49:40 2008
From: yi.zhao at alibaba-inc.com (Yi Zhao)
Date: Tue Aug 12 20:47:31 2008
Subject: [Slony1-general] how to add a table which have unique(too column)
	into slony?
Message-ID: <1218599380.3186.4.camel@localhost.localdomain>

I have a table which have unique like:
unique(foo_id, bar_id);

my slonik script is below:
set add table (set id = 1, origin = 1, id = 2, full qualified name =
'search.thread', key = 'foo_id, bar_id', comment = 'nodesc');

when I execute:
<stdin>:16: PGRES_FATAL_ERROR select
"_replication".determineIdxnameUnique('search.thread', 'foo_id,
bar_id');  - ERROR:  Slony-I: table "search"."thread" has no unique
index foo_id, bar_id

how to add a table which have unique with two column into slony???

thanks all;
regards,

From charles.duffy at gmail.com  Wed Aug 13 00:04:29 2008
From: charles.duffy at gmail.com (Charles Duffy)
Date: Wed Aug 13 00:04:59 2008
Subject: [Slony1-general] how to add a table which have unique(too column)
	into slony?
In-Reply-To: <1218599380.3186.4.camel@localhost.localdomain>
References: <1218599380.3186.4.camel@localhost.localdomain>
Message-ID: <dfdaea8f0808130004m58eb92b3v9afa00b82e4a49d8@mail.gmail.com>

On Wed, Aug 13, 2008 at 1:49 PM, Yi Zhao <yi.zhao@alibaba-inc.com> wrote:
> I have a table which have unique like:
> unique(foo_id, bar_id);
>
> my slonik script is below:
> set add table (set id = 1, origin = 1, id = 2, full qualified name =
> 'search.thread', key = 'foo_id, bar_id', comment = 'nodesc');
>
> when I execute:
> <stdin>:16: PGRES_FATAL_ERROR select
> "_replication".determineIdxnameUnique('search.thread', 'foo_id,
> bar_id');  - ERROR:  Slony-I: table "search"."thread" has no unique
> index foo_id, bar_id
>
> how to add a table which have unique with two column into slony???

The unique constraint on your table will make use of an index.
You should specify the name of this index in the 'key' parameter, not
the name of the columns in the constraint definition.
Like:

postgres=# create table table1 (foo_id int, bar_id int);
CREATE TABLE
postgres=# alter table table1 add constraint unique_idx unique (foo_id,bar_id);
NOTICE:  ALTER TABLE / ADD UNIQUE will create implicit index
"unique_idx" for table "table1"
ALTER TABLE
postgres=# \d table1
    Table "public.table1"
 Column |  Type   | Modifiers
--------+---------+-----------
 foo_id | integer |
 bar_id | integer |
Indexes:
    "unique_idx" UNIQUE, btree (foo_id, bar_id)

Then your slonik command should look like:

set add table (set id = 1, origin = 1, id = 2, full qualified name =
'search.thread', key = 'unique_idx' comment = 'nodesc');

Note that you also need to have NOT NULL on all the columns composing
the index. As the manual says, you might as well define a primary key
consisting of these columns.

Thanks,

Charles Duffy
From yi.zhao at alibaba-inc.com  Wed Aug 13 06:37:27 2008
From: yi.zhao at alibaba-inc.com (Yi Zhao)
Date: Wed Aug 13 06:34:58 2008
Subject: [Slony1-general] how to add a table which have unique(too
	column) into slony?
In-Reply-To: <dfdaea8f0808130004m58eb92b3v9afa00b82e4a49d8@mail.gmail.com>
References: <1218599380.3186.4.camel@localhost.localdomain>
	<dfdaea8f0808130004m58eb92b3v9afa00b82e4a49d8@mail.gmail.com>
Message-ID: <1218634647.3186.7.camel@localhost.localdomain>

yes, I got it!

thanks very much:D

regards,
Yi
On Wed, 2008-08-13 at 17:04 +1000, Charles Duffy wrote:
> On Wed, Aug 13, 2008 at 1:49 PM, Yi Zhao <yi.zhao@alibaba-inc.com> wrote:
> > I have a table which have unique like:
> > unique(foo_id, bar_id);
> >
> > my slonik script is below:
> > set add table (set id = 1, origin = 1, id = 2, full qualified name =
> > 'search.thread', key = 'foo_id, bar_id', comment = 'nodesc');
> >
> > when I execute:
> > <stdin>:16: PGRES_FATAL_ERROR select
> > "_replication".determineIdxnameUnique('search.thread', 'foo_id,
> > bar_id');  - ERROR:  Slony-I: table "search"."thread" has no unique
> > index foo_id, bar_id
> >
> > how to add a table which have unique with two column into slony???
> 
> The unique constraint on your table will make use of an index.
> You should specify the name of this index in the 'key' parameter, not
> the name of the columns in the constraint definition.
> Like:
> 
> postgres=# create table table1 (foo_id int, bar_id int);
> CREATE TABLE
> postgres=# alter table table1 add constraint unique_idx unique (foo_id,bar_id);
> NOTICE:  ALTER TABLE / ADD UNIQUE will create implicit index
> "unique_idx" for table "table1"
> ALTER TABLE
> postgres=# \d table1
>     Table "public.table1"
>  Column |  Type   | Modifiers
> --------+---------+-----------
>  foo_id | integer |
>  bar_id | integer |
> Indexes:
>     "unique_idx" UNIQUE, btree (foo_id, bar_id)
> 
> Then your slonik command should look like:
> 
> set add table (set id = 1, origin = 1, id = 2, full qualified name =
> 'search.thread', key = 'unique_idx' comment = 'nodesc');
> 
> Note that you also need to have NOT NULL on all the columns composing
> the index. As the manual says, you might as well define a primary key
> consisting of these columns.
> 
> Thanks,
> 
> Charles Duffy
> 

From yi.zhao at alibaba-inc.com  Wed Aug 13 06:48:32 2008
From: yi.zhao at alibaba-inc.com (Yi Zhao)
Date: Wed Aug 13 06:46:04 2008
Subject: [Slony1-general] why my slony became invalidation?
Message-ID: <1218635312.3186.18.camel@localhost.localdomain>

I have a replaction cluster which works well someday ago: when I insert
a record into master, the slave will get the record.

today, I want to add some other table into it, so I create a new set
like this:
#!/bin/bash

SLONIK=/usr/local/pgsql/bin/slonik

$SLONIK <<_SCRIPT_
cluster name = replication;
	node 1 admin conninfo='host=localhost dbname=bbs user=ya port=5432';
	node 2 admin conninfo='host=localhost dbname=bbs user=ya port=5433';
	node 3 admin conninfo='host=localhost dnname=bbs user=ya port=5434';

	try {
		create set (id = 2, origin = 1, comment = 'Set 2 for replication');
	} on error {
		echo 'Could not create subscription set 2 for replication';
		exit -1;
	}

	echo 'Subscription set 2 created';

	echo 'Adding tables to subscription set';
	set add table (set id = 2, origin = 1, id = 523, full qualified name =
'content.thread_preference', comment = 'nodesc');

	try {
		subscribe set (id=2, provider=1, receiver=2, forward=yes);
		subscribe set (id=2, provider=1, receiver=3, forward=yes);
	} on error {
		echo 'Could not create subscribe and merge for replication';
		exit -1;
	}
_SCRIPT_

but, when the set created, I found that slony became invalidation, for
example, when I delete/insert a record from master, there is no affect
to slave.
(all the trigger created by slony is exist)

I don't know what's wrong with it? I can't get any information from the
slony's log.

can someone tell me what I should do with it???

thanks very much.

regards.
Yi

From wmoran at collaborativefusion.com  Wed Aug 13 07:04:04 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Wed Aug 13 07:04:02 2008
Subject: [Slony1-general] why my slony became invalidation?
In-Reply-To: <1218635312.3186.18.camel@localhost.localdomain>
References: <1218635312.3186.18.camel@localhost.localdomain>
Message-ID: <20080813100404.c00cedf5.wmoran@collaborativefusion.com>

In response to Yi Zhao <yi.zhao@alibaba-inc.com>:

> I have a replaction cluster which works well someday ago: when I insert
> a record into master, the slave will get the record.
> 
> today, I want to add some other table into it, so I create a new set
> like this:
> #!/bin/bash
> 
> SLONIK=/usr/local/pgsql/bin/slonik
> 
> $SLONIK <<_SCRIPT_
> cluster name = replication;
> 	node 1 admin conninfo='host=localhost dbname=bbs user=ya port=5432';
> 	node 2 admin conninfo='host=localhost dbname=bbs user=ya port=5433';
> 	node 3 admin conninfo='host=localhost dnname=bbs user=ya port=5434';
> 
> 	try {
> 		create set (id = 2, origin = 1, comment = 'Set 2 for replication');
> 	} on error {
> 		echo 'Could not create subscription set 2 for replication';
> 		exit -1;
> 	}
> 
> 	echo 'Subscription set 2 created';
> 
> 	echo 'Adding tables to subscription set';
> 	set add table (set id = 2, origin = 1, id = 523, full qualified name =
> 'content.thread_preference', comment = 'nodesc');
> 
> 	try {
> 		subscribe set (id=2, provider=1, receiver=2, forward=yes);
> 		subscribe set (id=2, provider=1, receiver=3, forward=yes);
> 	} on error {
> 		echo 'Could not create subscribe and merge for replication';
> 		exit -1;
> 	}
> _SCRIPT_
> 
> but, when the set created, I found that slony became invalidation, for
> example, when I delete/insert a record from master, there is no affect
> to slave.
> (all the trigger created by slony is exist)

How long did you wait?  Is it possible that the initial sync of the
new table is taking long enough that your changes are simply backlogged
and haven't propagated yet?

> I don't know what's wrong with it? I can't get any information from the
> slony's log.

You're going to have to fix this problem to get more information before
anyone will be able to help you with the other problem.  Why aren't
the logs working?  Fix that first.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023

****************************************************************
IMPORTANT: This message contains confidential information and is
intended only for the individual named. If the reader of this
message is not an intended recipient (or the individual
responsible for the delivery of this message to an intended
recipient), please be advised that any re-use, dissemination,
distribution or copying of this message is prohibited. Please
notify the sender immediately by e-mail if you have received
this e-mail by mistake and delete this e-mail from your system.
E-mail transmission cannot be guaranteed to be secure or
error-free as information could be intercepted, corrupted, lost,
destroyed, arrive late or incomplete, or contain viruses. The
sender therefore does not accept liability for any errors or
omissions in the contents of this message, which arise as a
result of e-mail transmission.
****************************************************************
From shahaf at redfin.com  Fri Aug 15 14:34:30 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Fri Aug 15 14:34:59 2008
Subject: [Slony1-general] duplicate key value violates unique constraint
	"sl_nodelock-pkey"
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB22049C187D@mail-1.rf.lan>

Hello list,

 

I've been using Slony in production for months and things have worked
out pretty well so far.  But this past week I ran into problems when we
switched from Postgres 8.2 to Postgres 8.3 (still using Slony version
1.2.13).  I don't think the error results from the version upgrade, but
I could be wrong.

 

I set up the master database (query-1) and then proceeded to set up
replication to two slaves (query-2 and query-4).  I kicked off the slon
processes for all three simultaneously.  Replication to query-4 got
started right away and succeeded - it took 2 hours for the initial bulk
copy of data, and soon after that it was caught up with incremental
changes.  However, replication to query-2 died right away.  The slon
process for query-2 quit immediately and I saw the following message in
the log:

 

2008-08-15 13:57:01 PDT FATAL  localListenThread: "select
"_stingray_cluster".cleanupNodelock(); insert into
"_stingray_cluster".sl_nodelock values (    2, 0,
"pg_catalog".pg_backend_pid()); " - ERROR:  duplicate key value violates
unique constraint "sl_nodelock-pkey"

 

Now, I searched the interweb for solutions to this problem and I found
several suggestions, but none of them have helped so far...

 

1.	People suggest that this happens when you try to start multiple
slon processes for the same node at the same time.  I'm quite certain
that this is not the case.  I've checked and double-checked and
quadruple-checked.  For what it's worth, remember that I've successfully
run Slony replication to multiple slaves for months.

 

Here are the ones that are running:

 

25537 pts/3    S      0:00 slon -d 2 stingray_cluster
host=query-1.colo.redfin.com dbname=my_db user=my_user
password=my_password

25541 pts/3    Sl     0:00  \_ slon -d 2 stingray_cluster
host=query-1.colo.redfin.com dbname= my_db user= my_user password=
my_password

25544 pts/3    S      0:00 slon -d 2 stingray_cluster
host=query-4.colo.redfin.com dbname= my_db user= my_user password=
my_password

25545 pts/3    Sl     0:00  \_ slon -d 2 stingray_cluster
host=query-4.colo.redfin.com dbname= my_db user= my_user password=
my_password

 

            And the one that starts and then immediately dies has
query-2 as the host.

 

2.	Some people suggest that this happens when a slon process dies
and leaves a stale connection, a connection so deep in the network stack
that slony and postgres are not aware.  For this people suggest either
waiting about two hours for some timeout to kick in, or else having the
admin kill -2 the zombie process/connection.  Apparently this happens
more often if you try to run slony across a wan.

 

Well, our slon processes all run on the master node and the other nodes
are right there in the same LAN.  (we did once try to run Slony across a
WAN but that never worked)

 

This duplicate key issue first happened more than 12 hours ago, and it
continues to happen.  Each time I try to start a slon process for
query-2, it dies right away with the same error.

 

So, I don't think there's a zombie connection, but I also don't know
exactly how to check for one.  I can say that all the proc's in the
pg_stat_activity have a xact_start that is less than an hour old.

 

3.	One person mentioned having to go into the sl_nodelock table and
manually whack the bad row.  I'm a little concerned about doing this
kind of surgery because I don't know the sl_nodelock table and therefore
I don't know what would be the consequence of whacking a row (especially
if I accidentally whack the wrong row).

 

Here's what I see in that table:

 

stingray_prod=# select * from _stingray_cluster.sl_nodelock;

 nl_nodeid | nl_conncnt | nl_backendpid

-----------+------------+---------------

         2 |         11 |         14774

         2 |         15 |         20569

         1 |          0 |         25551

         4 |         16 |         25568

         4 |         17 |         25570

(5 rows)

 

What do these columns mean exactly?  Is there documentation on this
table somewhere?  Which one is likely to be the offending row?  Is it
crazy to whack the row manually?  Would I need to also whack
corresponding rows in other tables?

 

Any help with this would be much appreciated.

 

On a related note - is there a psql command (like \d) that I can use to
list all the objects in my _stingray_cluster schema?

 

Thanks,

 

--S

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080815/0b06263b/attachment.htm
From shahaf at redfin.com  Fri Aug 15 17:27:06 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Fri Aug 15 17:27:28 2008
Subject: [Slony1-general] RE: duplicate key value violates unique constraint
	"sl_nodelock-pkey"
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB22049C187D@mail-1.rf.lan>
References: <082D8A131DF72A4D88C908A1AD3DEB22049C187D@mail-1.rf.lan>
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB22049C1A34@mail-1.rf.lan>

OK, I was wrong.

 

It turns out that there *were* two slon processes for the same node.
How did I miss this fact?  Because they were running on two different
machines.  I assumed that all slon processes were running on the same
server (the master DB machine) so it would be sufficient to run a "ps
afx | grep slon" command on that server.  But of course this command
completely missed the slon process that was running on the other server.

 

How did I discover this?  The following query was helpful...

 

stingray_prod=# select nl.*, psa.client_addr from
_stingray_cluster.sl_nodelock nl join pg_stat_activity psa on
nl.nl_backendpid=psa.procpid;
 nl_nodeid | nl_conncnt | nl_backendpid | client_addr
-----------+------------+---------------+-------------
         2 |         11 |         14774 | (query-2)
         2 |         15 |         20569 | (query-2)
         1 |          0 |         30549 | (query-1)
         4 |         20 |         30566 | (query-1)
         4 |         21 |         30571 | (query-1)
(5 rows)

 

What are slon processes doing on query-2???  Ah-ha!

 

It goes to show that it's not sufficient to double check and quadruple
check, you have to go further.

 

--S

 

________________________________

From: Shahaf Abileah 
Sent: Friday, August 15, 2008 2:35 PM
To: slony1-general@lists.slony.info
Subject: duplicate key value violates unique constraint
"sl_nodelock-pkey"

 

Hello list,

 

I've been using Slony in production for months and things have worked
out pretty well so far.  But this past week I ran into problems when we
switched from Postgres 8.2 to Postgres 8.3 (still using Slony version
1.2.13).  I don't think the error results from the version upgrade, but
I could be wrong.

 

I set up the master database (query-1) and then proceeded to set up
replication to two slaves (query-2 and query-4).  I kicked off the slon
processes for all three simultaneously.  Replication to query-4 got
started right away and succeeded - it took 2 hours for the initial bulk
copy of data, and soon after that it was caught up with incremental
changes.  However, replication to query-2 died right away.  The slon
process for query-2 quit immediately and I saw the following message in
the log:

 

2008-08-15 13:57:01 PDT FATAL  localListenThread: "select
"_stingray_cluster".cleanupNodelock(); insert into
"_stingray_cluster".sl_nodelock values (    2, 0,
"pg_catalog".pg_backend_pid()); " - ERROR:  duplicate key value violates
unique constraint "sl_nodelock-pkey"

 

Now, I searched the interweb for solutions to this problem and I found
several suggestions, but none of them have helped so far...

 

1.	People suggest that this happens when you try to start multiple
slon processes for the same node at the same time.  I'm quite certain
that this is not the case.  I've checked and double-checked and
quadruple-checked.  For what it's worth, remember that I've successfully
run Slony replication to multiple slaves for months.

 

Here are the ones that are running:

 

25537 pts/3    S      0:00 slon -d 2 stingray_cluster
host=query-1.colo.redfin.com dbname=my_db user=my_user
password=my_password

25541 pts/3    Sl     0:00  \_ slon -d 2 stingray_cluster
host=query-1.colo.redfin.com dbname= my_db user= my_user password=
my_password

25544 pts/3    S      0:00 slon -d 2 stingray_cluster
host=query-4.colo.redfin.com dbname= my_db user= my_user password=
my_password

25545 pts/3    Sl     0:00  \_ slon -d 2 stingray_cluster
host=query-4.colo.redfin.com dbname= my_db user= my_user password=
my_password

 

            And the one that starts and then immediately dies has
query-2 as the host.

 

2.	Some people suggest that this happens when a slon process dies
and leaves a stale connection, a connection so deep in the network stack
that slony and postgres are not aware.  For this people suggest either
waiting about two hours for some timeout to kick in, or else having the
admin kill -2 the zombie process/connection.  Apparently this happens
more often if you try to run slony across a wan.

 

Well, our slon processes all run on the master node and the other nodes
are right there in the same LAN.  (we did once try to run Slony across a
WAN but that never worked)

 

This duplicate key issue first happened more than 12 hours ago, and it
continues to happen.  Each time I try to start a slon process for
query-2, it dies right away with the same error.

 

So, I don't think there's a zombie connection, but I also don't know
exactly how to check for one.  I can say that all the proc's in the
pg_stat_activity have a xact_start that is less than an hour old.

 

3.	One person mentioned having to go into the sl_nodelock table and
manually whack the bad row.  I'm a little concerned about doing this
kind of surgery because I don't know the sl_nodelock table and therefore
I don't know what would be the consequence of whacking a row (especially
if I accidentally whack the wrong row).

 

Here's what I see in that table:

 

stingray_prod=# select * from _stingray_cluster.sl_nodelock;

 nl_nodeid | nl_conncnt | nl_backendpid

-----------+------------+---------------

         2 |         11 |         14774

         2 |         15 |         20569

         1 |          0 |         25551

         4 |         16 |         25568

         4 |         17 |         25570

(5 rows)

 

What do these columns mean exactly?  Is there documentation on this
table somewhere?  Which one is likely to be the offending row?  Is it
crazy to whack the row manually?  Would I need to also whack
corresponding rows in other tables?

 

Any help with this would be much appreciated.

 

On a related note - is there a psql command (like \d) that I can use to
list all the objects in my _stingray_cluster schema?

 

Thanks,

 

--S

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080815/db11ad3c/attachment-0001.htm
From spreng at socket.ch  Tue Aug 19 02:41:45 2008
From: spreng at socket.ch (Thomas Spreng)
Date: Tue Aug 19 02:42:24 2008
Subject: [Slony1-general] Random Duplicates Error
Message-ID: <D9F0F15D-C55E-45FD-A325-47131023FB9F@socket.ch>

hi everyone,

I have a problem with one of our current slony clusters.
The cluster consists of one origin and 3 subscribers all running  
Slony1 v.1.2.14 and PostgreSQL v.8.3.3 on Debian Etch.
There are around 3 mil. rows in one big table that are deleted and  
newly inserted on a daily basis.

Every now and then (around once a week) one (any of them) of the  
subscribers fails to replicate the origin's data with the following  
message in the log:
ERROR:  duplicate key value violates unique constraint "xyz"

There is no further indication in the logs what could have been the  
cause for this problem.

1) is this a known problem that the replication may fail occasionally  
when there are lots of insert's going on? Any suggestions on what I  
could do to prevent this from happening?
2) what's the preferred workaround for such a situation? right, now  
I'm just dropping the corresponding node and re-create it from scratch.

cheers,

tom.


PS: other clusters on the same hosts don't never have such problems  
but they also don't have that many deletes/inserts going on.
From glynastill at yahoo.co.uk  Tue Aug 19 03:56:00 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue Aug 19 03:56:32 2008
Subject: [Slony1-general] Random Duplicates Error
In-Reply-To: <D9F0F15D-C55E-45FD-A325-47131023FB9F@socket.ch>
Message-ID: <888361.13432.qm@web25807.mail.ukl.yahoo.com>

> 
> Every now and then (around once a week) one (any of them)
> of the  
> subscribers fails to replicate the origin's data with
> the following  
> message in the log:
> ERROR:  duplicate key value violates unique constraint
> "xyz"
> 
> There is no further indication in the logs what could have
> been the  
> cause for this problem.
> 

Can you post the table definition?  Do you have any sequences on the table? And if so are they replicated?

Send instant messages to your online friends http://uk.messenger.yahoo.com 
From spreng at socket.ch  Tue Aug 19 05:26:16 2008
From: spreng at socket.ch (Thomas Spreng)
Date: Tue Aug 19 05:26:28 2008
Subject: [Slony1-general] Random Duplicates Error
In-Reply-To: <888361.13432.qm@web25807.mail.ukl.yahoo.com>
References: <888361.13432.qm@web25807.mail.ukl.yahoo.com>
Message-ID: <784FE2C8-A5D9-4453-9314-CD2E224C2539@socket.ch>


On 19. Aug, 2008, at 12:56, Glyn Astill wrote:
>> Every now and then (around once a week) one (any of them)
>> of the
>> subscribers fails to replicate the origin's data with
>> the following
>> message in the log:
>> ERROR:  duplicate key value violates unique constraint
>> "xyz"
>>
>> There is no further indication in the logs what could have
>> been the
>> cause for this problem.

> Do you have any sequences on the table?

Yes, the primary key field uses a sequence:

<sql>
CREATE SEQUENCE records_id_seq
   INCREMENT 1
   MINVALUE 1
   MAXVALUE 9223372036854775807
   START 624877670
   CACHE 1;
</sql>

> And if so are they replicated?

Yes, it's in the slony replication set.
But the sql insert error is always caused by the constraint
'records_vendor_id_key' AFAIK, whose fields don't use any sequence.

> Can you post the table definition?


<sql>
CREATE TABLE records
(
   id serial NOT NULL,
   vendor_id character varying NOT NULL,
   subvendor_id integer NOT NULL,
   destination character varying NOT NULL,
   departure_city character varying NOT NULL,
   departure_date date NOT NULL,
   duration integer NOT NULL,
   description text NOT NULL,
   price numeric(10,2) NOT NULL,
   price_inaccurate boolean NOT NULL DEFAULT false,
   adults character varying NOT NULL DEFAULT ''::character varying,
   children character varying NOT NULL DEFAULT ''::character varying,
   babies character varying NOT NULL DEFAULT ''::character varying,
   total_price numeric(10,2),
   url character varying NOT NULL,
   accomodation boolean NOT NULL,
   transport boolean NOT NULL,
   car boolean NOT NULL,
   offer_type integer,
   offer_id character varying,
   offer_name character varying,
   attributes character varying NOT NULL DEFAULT ''::character varying,
   location_id integer,
   vendor_rec_id character varying NOT NULL,
   vendor_group_id character varying NOT NULL,
   unlinked integer NOT NULL DEFAULT 0,
   created_on timestamp without time zone NOT NULL DEFAULT now(),
   modified_on timestamp without time zone NOT NULL DEFAULT now(),
   enabled boolean NOT NULL DEFAULT true,
   price_foreign numeric(10,2),
   currency_foreign character varying(3),
   CONSTRAINT records_pkey PRIMARY KEY (id),
   CONSTRAINT records_location_id_fkey FOREIGN KEY (location_id)
       REFERENCES locations (id) MATCH SIMPLE
       ON UPDATE CASCADE ON DELETE SET NULL,
   CONSTRAINT records_vendor_id_key UNIQUE (vendor_id, vendor_rec_id)
)
WITH (OIDS=FALSE);
ALTER TABLE records ALTER COLUMN id SET DEFAULT  
nextval('records_id_seq'::regclass);
</sql>

cheers,

tom.

From shahaf at redfin.com  Tue Aug 19 07:17:13 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Tue Aug 19 07:17:27 2008
Subject: [Slony1-general] Random Duplicates Error
In-Reply-To: <784FE2C8-A5D9-4453-9314-CD2E224C2539@socket.ch>
References: <888361.13432.qm@web25807.mail.ukl.yahoo.com>
	<784FE2C8-A5D9-4453-9314-CD2E224C2539@socket.ch>
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB22049C25E4@mail-1.rf.lan>

For what it's worth, we have several unique constraints in the tables we
replicate and I haven't seen this issue.

--S

-----Original Message-----
From: slony1-general-bounces@lists.slony.info
[mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Thomas
Spreng
Sent: Tuesday, August 19, 2008 5:26 AM
To: glynastill@yahoo.co.uk
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] Random Duplicates Error


On 19. Aug, 2008, at 12:56, Glyn Astill wrote:
>> Every now and then (around once a week) one (any of them)
>> of the
>> subscribers fails to replicate the origin's data with
>> the following
>> message in the log:
>> ERROR:  duplicate key value violates unique constraint
>> "xyz"
>>
>> There is no further indication in the logs what could have
>> been the
>> cause for this problem.

> Do you have any sequences on the table?

Yes, the primary key field uses a sequence:

<sql>
CREATE SEQUENCE records_id_seq
   INCREMENT 1
   MINVALUE 1
   MAXVALUE 9223372036854775807
   START 624877670
   CACHE 1;
</sql>

> And if so are they replicated?

Yes, it's in the slony replication set.
But the sql insert error is always caused by the constraint
'records_vendor_id_key' AFAIK, whose fields don't use any sequence.

> Can you post the table definition?


<sql>
CREATE TABLE records
(
   id serial NOT NULL,
   vendor_id character varying NOT NULL,
   subvendor_id integer NOT NULL,
   destination character varying NOT NULL,
   departure_city character varying NOT NULL,
   departure_date date NOT NULL,
   duration integer NOT NULL,
   description text NOT NULL,
   price numeric(10,2) NOT NULL,
   price_inaccurate boolean NOT NULL DEFAULT false,
   adults character varying NOT NULL DEFAULT ''::character varying,
   children character varying NOT NULL DEFAULT ''::character varying,
   babies character varying NOT NULL DEFAULT ''::character varying,
   total_price numeric(10,2),
   url character varying NOT NULL,
   accomodation boolean NOT NULL,
   transport boolean NOT NULL,
   car boolean NOT NULL,
   offer_type integer,
   offer_id character varying,
   offer_name character varying,
   attributes character varying NOT NULL DEFAULT ''::character varying,
   location_id integer,
   vendor_rec_id character varying NOT NULL,
   vendor_group_id character varying NOT NULL,
   unlinked integer NOT NULL DEFAULT 0,
   created_on timestamp without time zone NOT NULL DEFAULT now(),
   modified_on timestamp without time zone NOT NULL DEFAULT now(),
   enabled boolean NOT NULL DEFAULT true,
   price_foreign numeric(10,2),
   currency_foreign character varying(3),
   CONSTRAINT records_pkey PRIMARY KEY (id),
   CONSTRAINT records_location_id_fkey FOREIGN KEY (location_id)
       REFERENCES locations (id) MATCH SIMPLE
       ON UPDATE CASCADE ON DELETE SET NULL,
   CONSTRAINT records_vendor_id_key UNIQUE (vendor_id, vendor_rec_id)
)
WITH (OIDS=FALSE);
ALTER TABLE records ALTER COLUMN id SET DEFAULT  
nextval('records_id_seq'::regclass);
</sql>

cheers,

tom.

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


From glynastill at yahoo.co.uk  Tue Aug 19 09:31:55 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue Aug 19 09:32:04 2008
Subject: [Slony1-general] Random Duplicates Error
In-Reply-To: <784FE2C8-A5D9-4453-9314-CD2E224C2539@socket.ch>
Message-ID: <214171.46585.qm@web25802.mail.ukl.yahoo.com>

> > And if so are they replicated?
> 
> Yes, it's in the slony replication set.
> But the sql insert error is always caused by the constraint
> 'records_vendor_id_key' AFAIK, whose fields
> don't use any sequence.
> 

I'm afraid I can't see what the problem would be then, but I'm far from an expert here.

Send instant messages to your online friends http://uk.messenger.yahoo.com 
From arolsen at gmail.com  Tue Aug 19 09:32:14 2008
From: arolsen at gmail.com (Adam Olsen)
Date: Tue Aug 19 09:32:23 2008
Subject: [Slony1-general] Help with grouping sets
Message-ID: <2473b43f0808190932h35a4b472na6c042d3b07d20c4@mail.gmail.com>

Hello,

First off, I'm sorry if this seems like a stupid question.

I'm trying to group my tables into sets.  In the online documentation,
it says that "It will be vital to group tables together into a single
set if those tables are related via foreign key constraints".  I am
creating a Django application, and every table is related to some
other table via a foreign key.  Sometimes this relation is to the
"User" table, and sometimes this relation is to a table that has a
relation to the "User" table.  We don't have any tables that aren't
somehow associated to the "User" table.

We have quite a few tables, so this means that, to my understanding,
all of our tables need to be in a single set.  From what I understand,
this can cause deadlocking issues, and I'd obviously rather avoid
that.

What can I do to better group my sets?

-- 
Adam Olsen
SendOutCards.com
http://www.vimtips.org
http://last.fm/user/synic
From JanWieck at Yahoo.com  Tue Aug 19 08:02:40 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Tue Aug 19 09:49:26 2008
Subject: [Slony1-general] Random Duplicates Error
In-Reply-To: <D9F0F15D-C55E-45FD-A325-47131023FB9F@socket.ch>
References: <D9F0F15D-C55E-45FD-A325-47131023FB9F@socket.ch>
Message-ID: <48AAE090.5050405@Yahoo.com>

On 8/19/2008 5:41 AM, Thomas Spreng wrote:
> hi everyone,
> 
> I have a problem with one of our current slony clusters.
> The cluster consists of one origin and 3 subscribers all running  
> Slony1 v.1.2.14 and PostgreSQL v.8.3.3 on Debian Etch.
> There are around 3 mil. rows in one big table that are deleted and  
> newly inserted on a daily basis.

When that error occurs, slony would retry over and over again. In that 
case, is it failing on the exact same key value over and over, or is the 
key value changing?

Does reindexing the sl_log_1 and sl_log_2 tables on the data provider 
for the failed node eventually help? If so, something on that data 
provider causes index corruption. We know of one Postgres bug that was 
supposedly fixed some time ago that caused this symptom, so Postgres 
itself being at fault is not out of the question.


Jan


> 
> Every now and then (around once a week) one (any of them) of the  
> subscribers fails to replicate the origin's data with the following  
> message in the log:
> ERROR:  duplicate key value violates unique constraint "xyz"
> 
> There is no further indication in the logs what could have been the  
> cause for this problem.
> 
> 1) is this a known problem that the replication may fail occasionally  
> when there are lots of insert's going on? Any suggestions on what I  
> could do to prevent this from happening?
> 2) what's the preferred workaround for such a situation? right, now  
> I'm just dropping the corresponding node and re-create it from scratch.
> 
> cheers,
> 
> tom.
> 
> 
> PS: other clusters on the same hosts don't never have such problems  
> but they also don't have that many deletes/inserts going on.
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From shahaf at redfin.com  Tue Aug 19 09:56:51 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Tue Aug 19 09:57:06 2008
Subject: [Slony1-general] Help with grouping sets
In-Reply-To: <2473b43f0808190932h35a4b472na6c042d3b07d20c4@mail.gmail.com>
References: <2473b43f0808190932h35a4b472na6c042d3b07d20c4@mail.gmail.com>
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB22049C270D@mail-1.rf.lan>

What deadlocking issues?

We have about 100 tables in a single set and things are working out
pretty well.

Note: not all of our 100 tables are related to one another via foreign
keys; we chose to keep everything in a single set purely for simplicity.

--S

-----Original Message-----
From: slony1-general-bounces@lists.slony.info
[mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Adam Olsen
Sent: Tuesday, August 19, 2008 9:32 AM
To: slony1-general@lists.slony.info
Subject: [Slony1-general] Help with grouping sets

Hello,

First off, I'm sorry if this seems like a stupid question.

I'm trying to group my tables into sets.  In the online documentation,
it says that "It will be vital to group tables together into a single
set if those tables are related via foreign key constraints".  I am
creating a Django application, and every table is related to some
other table via a foreign key.  Sometimes this relation is to the
"User" table, and sometimes this relation is to a table that has a
relation to the "User" table.  We don't have any tables that aren't
somehow associated to the "User" table.

We have quite a few tables, so this means that, to my understanding,
all of our tables need to be in a single set.  From what I understand,
this can cause deadlocking issues, and I'd obviously rather avoid
that.

What can I do to better group my sets?

-- 
Adam Olsen
SendOutCards.com
http://www.vimtips.org
http://last.fm/user/synic
_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


From arolsen at gmail.com  Tue Aug 19 10:00:25 2008
From: arolsen at gmail.com (Adam Olsen)
Date: Tue Aug 19 10:00:33 2008
Subject: [Slony1-general] Help with grouping sets
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB22049C270D@mail-1.rf.lan>
References: <2473b43f0808190932h35a4b472na6c042d3b07d20c4@mail.gmail.com>
	<082D8A131DF72A4D88C908A1AD3DEB22049C270D@mail-1.rf.lan>
Message-ID: <2473b43f0808191000l73c24883v60f4788772b5d8ca@mail.gmail.com>

The issues described in section 7.2 here:
http://slony.info/documentation/definingsets.html

The most concerning one is the one regarding "EXECUTE SCRIPT", as, in
the beginning our schema might change somewhat frequently.

Adam

On Tue, Aug 19, 2008 at 10:56 AM, Shahaf Abileah <shahaf@redfin.com> wrote:
> What deadlocking issues?
>
> We have about 100 tables in a single set and things are working out
> pretty well.
>
> Note: not all of our 100 tables are related to one another via foreign
> keys; we chose to keep everything in a single set purely for simplicity.
>
> --S
>
> -----Original Message-----
> From: slony1-general-bounces@lists.slony.info
> [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Adam Olsen
> Sent: Tuesday, August 19, 2008 9:32 AM
> To: slony1-general@lists.slony.info
> Subject: [Slony1-general] Help with grouping sets
>
> Hello,
>
> First off, I'm sorry if this seems like a stupid question.
>
> I'm trying to group my tables into sets.  In the online documentation,
> it says that "It will be vital to group tables together into a single
> set if those tables are related via foreign key constraints".  I am
> creating a Django application, and every table is related to some
> other table via a foreign key.  Sometimes this relation is to the
> "User" table, and sometimes this relation is to a table that has a
> relation to the "User" table.  We don't have any tables that aren't
> somehow associated to the "User" table.
>
> We have quite a few tables, so this means that, to my understanding,
> all of our tables need to be in a single set.  From what I understand,
> this can cause deadlocking issues, and I'd obviously rather avoid
> that.
>
> What can I do to better group my sets?
>
> --
> Adam Olsen
> SendOutCards.com
> http://www.vimtips.org
> http://last.fm/user/synic
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
>



-- 
Adam Olsen
SendOutCards.com
http://www.vimtips.org
http://last.fm/user/synic
From shahaf at redfin.com  Tue Aug 19 10:00:47 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Tue Aug 19 10:00:58 2008
Subject: [Slony1-general] Random Duplicates Error
In-Reply-To: <48AAE090.5050405@Yahoo.com>
References: <D9F0F15D-C55E-45FD-A325-47131023FB9F@socket.ch>
	<48AAE090.5050405@Yahoo.com>
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB22049C2725@mail-1.rf.lan>

Is it possible someone made a manual change to the slave?  If so, that
could definitely lead to a duplicate key error.

I know that to do that you'd have to first manually remove the trigger
that's there to keep you from making manual changes.  Just throwing out
ideas...

--S


-----Original Message-----
From: slony1-general-bounces@lists.slony.info
[mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Jan Wieck
Sent: Tuesday, August 19, 2008 8:03 AM
To: Thomas Spreng
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] Random Duplicates Error

On 8/19/2008 5:41 AM, Thomas Spreng wrote:
> hi everyone,
> 
> I have a problem with one of our current slony clusters.
> The cluster consists of one origin and 3 subscribers all running  
> Slony1 v.1.2.14 and PostgreSQL v.8.3.3 on Debian Etch.
> There are around 3 mil. rows in one big table that are deleted and  
> newly inserted on a daily basis.

When that error occurs, slony would retry over and over again. In that 
case, is it failing on the exact same key value over and over, or is the

key value changing?

Does reindexing the sl_log_1 and sl_log_2 tables on the data provider 
for the failed node eventually help? If so, something on that data 
provider causes index corruption. We know of one Postgres bug that was 
supposedly fixed some time ago that caused this symptom, so Postgres 
itself being at fault is not out of the question.


Jan


> 
> Every now and then (around once a week) one (any of them) of the  
> subscribers fails to replicate the origin's data with the following  
> message in the log:
> ERROR:  duplicate key value violates unique constraint "xyz"
> 
> There is no further indication in the logs what could have been the  
> cause for this problem.
> 
> 1) is this a known problem that the replication may fail occasionally

> when there are lots of insert's going on? Any suggestions on what I  
> could do to prevent this from happening?
> 2) what's the preferred workaround for such a situation? right, now  
> I'm just dropping the corresponding node and re-create it from
scratch.
> 
> cheers,
> 
> tom.
> 
> 
> PS: other clusters on the same hosts don't never have such problems  
> but they also don't have that many deletes/inserts going on.
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


From shahaf at redfin.com  Tue Aug 19 10:12:01 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Tue Aug 19 10:12:21 2008
Subject: [Slony1-general] Help with grouping sets
In-Reply-To: <2473b43f0808191000l73c24883v60f4788772b5d8ca@mail.gmail.com>
References: <2473b43f0808190932h35a4b472na6c042d3b07d20c4@mail.gmail.com><082D8A131DF72A4D88C908A1AD3DEB22049C270D@mail-1.rf.lan>
	<2473b43f0808191000l73c24883v60f4788772b5d8ca@mail.gmail.com>
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB2204A2CF6C@mail-1.rf.lan>

Yup, that's true -- when you issue EXECUTE SCRIPT, it locks every table
in the set.

We've found that even if the locking imposed by Slony didn't exist,
there's still some other locking we can't avoid.  E.g. when you issue
any kind of alter table statement, it locks the table (we use Postgres;
your results may vary).  That alone is bad enough that we don't want to
do schema changes while the site is live, so we generally have short
site-down times to perform schema changes.  If your schema changes are
too frequent to deal with all these site down times, you might want to
consider a less strongly typed schema.

By the way, several times now we've taken the approach of switching
master DB's when we have significant schema changes to make.  The
general process is:
1. Get a slave up-to-date
2. Stop all access to the master DB so that the data there doesn't
change anymore
3. Remove the slave from replication
4. Run your schema changes on the slave
5. Reconfigure the apps to use this slave as the new master and start
them up again.
6. Re-initialize replication from the new master to the other nodes.

Note: this approach doesn't involve EXECUTE SCRIPT at all.

The nice thing about this approach is that there's a pretty simple
fallback mechanism -- the old master DB is still there, unchanged, in
case you need it.  The flip side is that it requires downtime and that
for the first while after the release you're running on a master with no
slaves.  That may or may not be acceptable in your situation.

--S


-----Original Message-----
From: slony1-general-bounces@lists.slony.info
[mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Adam Olsen
Sent: Tuesday, August 19, 2008 10:00 AM
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] Help with grouping sets

The issues described in section 7.2 here:
http://slony.info/documentation/definingsets.html

The most concerning one is the one regarding "EXECUTE SCRIPT", as, in
the beginning our schema might change somewhat frequently.

Adam

On Tue, Aug 19, 2008 at 10:56 AM, Shahaf Abileah <shahaf@redfin.com>
wrote:
> What deadlocking issues?
>
> We have about 100 tables in a single set and things are working out
> pretty well.
>
> Note: not all of our 100 tables are related to one another via foreign
> keys; we chose to keep everything in a single set purely for
simplicity.
>
> --S
>
> -----Original Message-----
> From: slony1-general-bounces@lists.slony.info
> [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Adam
Olsen
> Sent: Tuesday, August 19, 2008 9:32 AM
> To: slony1-general@lists.slony.info
> Subject: [Slony1-general] Help with grouping sets
>
> Hello,
>
> First off, I'm sorry if this seems like a stupid question.
>
> I'm trying to group my tables into sets.  In the online documentation,
> it says that "It will be vital to group tables together into a single
> set if those tables are related via foreign key constraints".  I am
> creating a Django application, and every table is related to some
> other table via a foreign key.  Sometimes this relation is to the
> "User" table, and sometimes this relation is to a table that has a
> relation to the "User" table.  We don't have any tables that aren't
> somehow associated to the "User" table.
>
> We have quite a few tables, so this means that, to my understanding,
> all of our tables need to be in a single set.  From what I understand,
> this can cause deadlocking issues, and I'd obviously rather avoid
> that.
>
> What can I do to better group my sets?
>
> --
> Adam Olsen
> SendOutCards.com
> http://www.vimtips.org
> http://last.fm/user/synic
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
>



-- 
Adam Olsen
SendOutCards.com
http://www.vimtips.org
http://last.fm/user/synic
_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


From remuscat at gmail.com  Tue Aug 19 10:24:22 2008
From: remuscat at gmail.com (=?ISO-8859-1?Q?Ren=E9-Etienne_Muscat?=)
Date: Tue Aug 19 10:24:30 2008
Subject: [Slony1-general] Dropped Master Cluster
Message-ID: <46d2bc8f0808191024y3c759ef3m7a57dac7eb765c1a@mail.gmail.com>

Hi everyone, I have dropped the master cluster from pgadmin and cannot do
the same to the slave cluster.
What can I do to remove this replication set from the slave and get the
tables back to normal?
P.S. I have other replication sets working fine, so I do not want to do any
changes that would affect the other replication sets.

Rene
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080819/=
008648d2/attachment.htm
From joshques at gmail.com  Tue Aug 19 12:10:58 2008
From: joshques at gmail.com (Josh Harrison)
Date: Tue Aug 19 12:11:11 2008
Subject: [Slony1-general] Partial replication
Message-ID: <8d89ea1d0808191210l5d943b0bi84a4e0989688adc4@mail.gmail.com>

Hi,
Can Slony used for partial replication of the database? ie., can I only
replicate parts of the tables(rows or columns) ?

Thanks
Josh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080819/=
5dc7edca/attachment.htm
From rod at iol.ie  Tue Aug 19 12:12:55 2008
From: rod at iol.ie (Raymond O'Donnell)
Date: Tue Aug 19 12:20:09 2008
Subject: [Slony1-general] Partial replication
In-Reply-To: <8d89ea1d0808191210l5d943b0bi84a4e0989688adc4@mail.gmail.com>
References: <8d89ea1d0808191210l5d943b0bi84a4e0989688adc4@mail.gmail.com>
Message-ID: <48AB1B37.9040700@iol.ie>

On 19/08/2008 20:10, Josh Harrison wrote:

> Can Slony used for partial replication of the database? ie., can I only
> replicate parts of the tables(rows or columns) ?

You can replicate certain tables in the database, but AFAIK not parts of
a table.

Ray.

------------------------------------------------------------------
Raymond O'Donnell, Director of Music, Galway Cathedral, Ireland
rod@iol.ie
Galway Cathedral Recitals: http://www.galwaycathedral.org/recitals
------------------------------------------------------------------
From cbbrowne at ca.afilias.info  Tue Aug 19 12:27:01 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Aug 19 12:27:14 2008
Subject: [Slony1-general] Partial replication
In-Reply-To: <8d89ea1d0808191210l5d943b0bi84a4e0989688adc4@mail.gmail.com>
	(Josh Harrison's message of "Tue, 19 Aug 2008 15:10:58 -0400")
References: <8d89ea1d0808191210l5d943b0bi84a4e0989688adc4@mail.gmail.com>
Message-ID: <87abf8olfe.fsf@dba2.int.libertyrms.com>

"Josh Harrison" <joshques@gmail.com> writes:
> Can Slony used for partial replication of the database? ie., can I
> only replicate parts of the tables(rows or columns) ?

There has been some talk of doing this; it would require making the
replication engine a fair bit more intelligent, which isn't a terrible
idea.

At present, the "level of granularity" that is available is to
replicate on a by-table basis.  You could, in principle, put triggers
on a subscriber to NULL out particular columns and/or add additional
columns.  But dropping out rows or columns altogether isn't possible
as things stand.
-- 
(format nil "~S@~S" "cbbrowne" "acm.org")
http://linuxfinances.info/info/finances.html
"My soul is more than matched; she's overmanned; and by a madman!
Insufferable sting, that sanity should ground arms on such a field!
But he drilled deep down, and blasted all my reason out of me! I think
I see his impious end; but feel that I must help him to it. Will I,
nill I, the ineffable thing has tied me to him; tows me with a cable I
have no knife to cut. Horrible old man!
[...] Oh, life! 'tis now that I do feel the latent horror in thee!"
--Moby Dick, Ch 38
From joshques at gmail.com  Tue Aug 19 12:34:22 2008
From: joshques at gmail.com (Josh Harrison)
Date: Tue Aug 19 12:34:35 2008
Subject: [Slony1-general] Partial replication
In-Reply-To: <87abf8olfe.fsf@dba2.int.libertyrms.com>
References: <8d89ea1d0808191210l5d943b0bi84a4e0989688adc4@mail.gmail.com>
	<87abf8olfe.fsf@dba2.int.libertyrms.com>
Message-ID: <8d89ea1d0808191234x199d10e8ifd6a2c7afbae6fcf@mail.gmail.com>

Thanks all
Josh

On Tue, Aug 19, 2008 at 3:27 PM, Christopher Browne <
cbbrowne@ca.afilias.info> wrote:

> "Josh Harrison" <joshques@gmail.com> writes:
> > Can Slony used for partial replication of the database? ie., can I
> > only replicate parts of the tables(rows or columns) ?
>
> There has been some talk of doing this; it would require making the
> replication engine a fair bit more intelligent, which isn't a terrible
> idea.
>
> At present, the "level of granularity" that is available is to
> replicate on a by-table basis.  You could, in principle, put triggers
> on a subscriber to NULL out particular columns and/or add additional
> columns.  But dropping out rows or columns altogether isn't possible
> as things stand.
> --
> (format nil "~S@~S" "cbbrowne" "acm.org")
> http://linuxfinances.info/info/finances.html
> "My soul is more than matched; she's overmanned; and by a madman!
> Insufferable sting, that sanity should ground arms on such a field!
> But he drilled deep down, and blasted all my reason out of me! I think
> I see his impious end; but feel that I must help him to it. Will I,
> nill I, the ineffable thing has tied me to him; tows me with a cable I
> have no knife to cut. Horrible old man!
> [...] Oh, life! 'tis now that I do feel the latent horror in thee!"
> --Moby Dick, Ch 38
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080819/=
ad531693/attachment.htm
From spreng at socket.ch  Tue Aug 19 23:55:40 2008
From: spreng at socket.ch (Thomas Spreng)
Date: Tue Aug 19 23:56:11 2008
Subject: [Slony1-general] Random Duplicates Error
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB22049C2725@mail-1.rf.lan>
References: <D9F0F15D-C55E-45FD-A325-47131023FB9F@socket.ch>
	<48AAE090.5050405@Yahoo.com>
	<082D8A131DF72A4D88C908A1AD3DEB22049C2725@mail-1.rf.lan>
Message-ID: <74A376D6-CD2F-49B5-A9EF-406127C63367@socket.ch>


On 19. Aug, 2008, at 19:00, Shahaf Abileah wrote:

> Is it possible someone made a manual change to the slave?  If so, that
> could definitely lead to a duplicate key error.
>
> I know that to do that you'd have to first manually remove the trigger
> that's there to keep you from making manual changes.  Just throwing  
> out
> ideas...

that's one of the first things I've checked as well but I can definitly
say that changes are only made on the origin node.
From m.eriksson at albourne.com  Wed Aug 20 01:49:26 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Wed Aug 20 01:50:02 2008
Subject: [Slony1-general] Help with grouping sets
In-Reply-To: <2473b43f0808191000l73c24883v60f4788772b5d8ca@mail.gmail.com>
References: <2473b43f0808190932h35a4b472na6c042d3b07d20c4@mail.gmail.com>	<082D8A131DF72A4D88C908A1AD3DEB22049C270D@mail-1.rf.lan>
	<2473b43f0808191000l73c24883v60f4788772b5d8ca@mail.gmail.com>
Message-ID: <48ABDA96.8020004@albourne.com>

hi,
we got about 440 tables currently in one set (with some 90 sequences)

and we are replicating over a WAN (across the world) and we do all 
schema changes with out-most care but we do modify the schema pretty 
heavily using EXECUTE_SCRIPT, but in a very organized manner (see my 
older comments). we got short release cycles around 3 months, and at 
that time we got about 200-300 DDL changes on average (new tables, 
dropping tables, column changes, renamings, etc)

But yes you need to do it with great care and extensive testing, and we 
do have a production outage of about 1h. but then again if it does fail 
it does take us a good 72h to fully replicate everything to all the 
nodes.. so not really an option.

normally we do full physical backup of all postgres data/ directories on 
all nodes before starting the update scripts, so if things do happen to 
go to crap due to some unforseen incident then we can just bring it back 
to the old state very quickly and try again.

cheers
Martin


Adam Olsen wrote:
> The issues described in section 7.2 here:
> http://slony.info/documentation/definingsets.html
>
> The most concerning one is the one regarding "EXECUTE SCRIPT", as, in
> the beginning our schema might change somewhat frequently.
>
> Adam
>
> On Tue, Aug 19, 2008 at 10:56 AM, Shahaf Abileah <shahaf@redfin.com> wrote:
>   
>> What deadlocking issues?
>>
>> We have about 100 tables in a single set and things are working out
>> pretty well.
>>
>> Note: not all of our 100 tables are related to one another via foreign
>> keys; we chose to keep everything in a single set purely for simplicity.
>>
>> --S
>>
>> -----Original Message-----
>> From: slony1-general-bounces@lists.slony.info
>> [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Adam Olsen
>> Sent: Tuesday, August 19, 2008 9:32 AM
>> To: slony1-general@lists.slony.info
>> Subject: [Slony1-general] Help with grouping sets
>>
>> Hello,
>>
>> First off, I'm sorry if this seems like a stupid question.
>>
>> I'm trying to group my tables into sets.  In the online documentation,
>> it says that "It will be vital to group tables together into a single
>> set if those tables are related via foreign key constraints".  I am
>> creating a Django application, and every table is related to some
>> other table via a foreign key.  Sometimes this relation is to the
>> "User" table, and sometimes this relation is to a table that has a
>> relation to the "User" table.  We don't have any tables that aren't
>> somehow associated to the "User" table.
>>
>> We have quite a few tables, so this means that, to my understanding,
>> all of our tables need to be in a single set.  From what I understand,
>> this can cause deadlocking issues, and I'd obviously rather avoid
>> that.
>>
>> What can I do to better group my sets?
>>
>> --
>> Adam Olsen
>> SendOutCards.com
>> http://www.vimtips.org
>> http://last.fm/user/synic
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>>
>>
>>     
>
>
>
>   

From m.eriksson at albourne.com  Wed Aug 20 01:51:29 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Wed Aug 20 01:51:52 2008
Subject: [Slony1-general] Dropped Master Cluster
In-Reply-To: <46d2bc8f0808191024y3c759ef3m7a57dac7eb765c1a@mail.gmail.com>
References: <46d2bc8f0808191024y3c759ef3m7a57dac7eb765c1a@mail.gmail.com>
Message-ID: <48ABDB11.9040608@albourne.com>

on the slaves nodes issue:

drop schema <cluster name> cascade;

this will remove ALL slony stuff from the database it is run on.

/martin


Ren?-Etienne Muscat wrote:
> Hi everyone, I have dropped the master cluster from pgadmin and cannot 
> do the same to the slave cluster.
> What can I do to remove this replication set from the slave and get 
> the tables back to normal?
> P.S. I have other replication sets working fine, so I do not want to 
> do any changes that would affect the other replication sets.
>  
> Rene
> ------------------------------------------------------------------------
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>   

From remuscat at gmail.com  Wed Aug 20 02:43:46 2008
From: remuscat at gmail.com (=?ISO-8859-1?Q?Ren=E9-Etienne_Muscat?=)
Date: Wed Aug 20 02:44:18 2008
Subject: [Slony1-general] Dropped Master Cluster
In-Reply-To: <48ABDB11.9040608@albourne.com>
References: <46d2bc8f0808191024y3c759ef3m7a57dac7eb765c1a@mail.gmail.com>
	<48ABDB11.9040608@albourne.com>
Message-ID: <46d2bc8f0808200243n40bc9b5aldc4a8d07a749e6a4@mail.gmail.com>

But this will not remove the triggers from the tables! Is there a clean way
of getting this done or is this the only method?

Thanks for your help,
Rene

On Wed, Aug 20, 2008 at 10:51 AM, Martin Eriksson
<m.eriksson@albourne.com>wrote:

> on the slaves nodes issue:
>
> drop schema <cluster name> cascade;
>
> this will remove ALL slony stuff from the database it is run on.
>
> /martin
>
>
> Ren=E9-Etienne Muscat wrote:
>
>> Hi everyone, I have dropped the master cluster from pgadmin and cannot do
>> the same to the slave cluster.
>> What can I do to remove this replication set from the slave and get the
>> tables back to normal?
>> P.S. I have other replication sets working fine, so I do not want to do
>> any changes that would affect the other replication sets.
>>  Rene
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080820/=
37631e97/attachment-0001.htm
From m.eriksson at albourne.com  Wed Aug 20 02:46:55 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Wed Aug 20 02:47:20 2008
Subject: [Slony1-general] Dropped Master Cluster
In-Reply-To: <46d2bc8f0808200243n40bc9b5aldc4a8d07a749e6a4@mail.gmail.com>
References: <46d2bc8f0808191024y3c759ef3m7a57dac7eb765c1a@mail.gmail.com>	
	<48ABDB11.9040608@albourne.com>
	<46d2bc8f0808200243n40bc9b5aldc4a8d07a749e6a4@mail.gmail.com>
Message-ID: <48ABE80F.5070805@albourne.com>

Yes it will,

the cascade will remove ALL slony triggers.

I do it all the time..


Ren?-Etienne Muscat wrote:
> But this will not remove the triggers from the tables! Is there a 
> clean way of getting this done or is this the only method?
>
> Thanks for your help,
> Rene
>
> On Wed, Aug 20, 2008 at 10:51 AM, Martin Eriksson 
> <m.eriksson@albourne.com <mailto:m.eriksson@albourne.com>> wrote:
>
>     on the slaves nodes issue:
>
>     drop schema <cluster name> cascade;
>
>     this will remove ALL slony stuff from the database it is run on.
>
>     /martin
>
>
>     Ren?-Etienne Muscat wrote:
>
>         Hi everyone, I have dropped the master cluster from pgadmin
>         and cannot do the same to the slave cluster.
>         What can I do to remove this replication set from the slave
>         and get the tables back to normal?
>         P.S. I have other replication sets working fine, so I do not
>         want to do any changes that would affect the other replication
>         sets.
>          Rene
>         ------------------------------------------------------------------------
>
>         _______________________________________________
>         Slony1-general mailing list
>         Slony1-general@lists.slony.info
>         <mailto:Slony1-general@lists.slony.info>
>         http://lists.slony.info/mailman/listinfo/slony1-general
>          
>
>
>     _______________________________________________
>     Slony1-general mailing list
>     Slony1-general@lists.slony.info
>     <mailto:Slony1-general@lists.slony.info>
>     http://lists.slony.info/mailman/listinfo/slony1-general
>
>

From bpineau at elma.fr  Wed Aug 20 03:15:20 2008
From: bpineau at elma.fr (Benjamin Pineau)
Date: Wed Aug 20 03:15:54 2008
Subject: [Slony1-general] Replication node suddenly lagging,
	CPU bound postmaster
Message-ID: <20080820101520.GA4523@mailer.elma.fr>

Hi everyone.

I have a replicating node that suddenly started to lag, on a 4 nodes Slony
cluster that worked well for months. This node is powerful enough (ie. older,
slower machines on the cluster achieve to keep up well). Network and block
devices are mostly idling (with regard to interrupts/second and throughput).
Strangely, the replication on this node seems CPU bound by the postmaster
process doing the actual inserts/updates for slon (this postmaster process
is stuck at 99% CPU usage since the beginning of the problem).
Neither Slony (at "slon -d2" level) nor PostgreSQL did log any warning or
error message, and the replication did not stopped on this node (it makes
progresses, but too slowly to keep up, so it's now 3 days behind master).

Any clue?

Where should I look to better diagnose this problem (I may miss the obvious,
I'm very new to Slony and PostgreSQL)?


More details:


* Slony 1.2.13 on PostgreSQL 8.1.10
* "psql_replication_check.pl" and "test_slony_state" do not find problem
* A few hours before I noticed the problem, I had an almost (98%) full
  tablespace on this node. Which I solved by moving a large index on an
  other tablespace (with "ALTER INDEX foo SET TABLESPACE bar;").
* Selects runs fast as usual on this node (ie. faster than some other nodes)
* According to pg_stat_activity , the only activity running now on the
  busy/working postmaster is the replication's inserts/updates. And there's
  no significant remaining activity running on this machine, beside replication
* No other processes than replication do writes on the node's database.
* The replicating/inserting postmaster process eats 99% of one CPU. Slon
  uses between 4% to 20% of an other CPU (this is a 8 cpu machine).
* The block devices aren't busy at all (iostat show a very low block I/O
  activity, ie. about 100KB to 500KB read&writes/second on an hardware
  raid10 array of 15Krpm scsi drives that can handle much more activity). 
  Actually, block devices are mostly idling on this node.
* The replication is not totally halted, it just became very slow (ie. the
  node has an increasing lag behind the master node, but makes progresses).
  So I guess it's not locked rebuilding a large index or halted on some
  error condition. And I can see the running replication insert/update
  requests evolve over time (according to pg_stat_activity).
* This node is one of the fastest machines of the cluster (wrt both cpu
  and blocks devices) and the slower nodes has no problem keeping up with
  replication.
* This cluster was working for months, and there was no activity peack the
  day it failed.
* We didn't change the replication set, and didn't alter any replicated
  table.
* Stracing the working postmaster process, I didn't see any suspect/long 
  syscalls that could explain the cpu usage or lag (just a few lseek, read, 
  send, recv, but since block devices and network link are mostly idle, that
  shouldn't cause slowdowns). I guess this process consumes cpu by doing
  some internal heavy calculations (?).
* I restarted the whole slon cluster twice, restarded this node's postgresql,
  and even (in despair) rebooted the node.

logs etc :


* examples logs :
Aug 18 17:39:22 mynode cluster_myclust: 2008-08-18 17:39:22 CEST DEBUG2 remoteListenThread_2: LISTEN
Aug 18 17:39:22 mynode cluster_myclust: 2008-08-18 17:39:22 CEST DEBUG2 remoteWorkerThread_2: forward confirm 1,66023883 received by 2
Aug 18 17:39:23 mynode cluster_myclust: 2008-08-18 17:39:23 CEST DEBUG2 remoteWorkerThread_2: forward confirm 1,66023884 received by 2
Aug 18 17:39:23 mynode cluster_myclust: 2008-08-18 17:39:23 CEST DEBUG2 remoteWorkerThread_2: forward confirm 2,7658887 received by 1
Aug 18 17:39:23 mynode cluster_myclust: 2008-08-18 17:39:23 CEST DEBUG2 remoteWorkerThread_2: forward confirm 3,7751837 received by 1
Aug 18 17:39:23 mynode cluster_myclust: 2008-08-18 17:39:23 CEST DEBUG2 remoteWorkerThread_2: forward confirm 4,1550604 received by 1
Aug 18 17:39:23 mynode cluster_myclust: 2008-08-18 17:39:23 CEST DEBUG2 remoteListenThread_1: queue event 1,66023883 SYNC
Aug 18 17:39:23 mynode cluster_myclust: 2008-08-18 17:39:23 CEST DEBUG2 remoteListenThread_1: queue event 1,66023884 SYNC
Aug 18 17:39:23 mynode cluster_myclust: 2008-08-18 17:39:23 CEST DEBUG2 remoteListenThread_1: queue event 1,66023885 SYNC
Aug 18 17:39:23 mynode cluster_myclust: 2008-08-18 17:39:23 CEST DEBUG2 remoteListenThread_1: UNLISTEN
[...]
Aug 19 13:57:51 mynode cluster_myclust: 2008-08-19 13:57:51 CEST DEBUG2 remoteListenThread_1: queue event 1,65832742 SYNC
Aug 19 13:57:51 mynode cluster_myclust: 2008-08-19 13:57:51 CEST DEBUG2 remoteListenThread_1: queue event 1,65832743 SYNC
Aug 19 13:57:51 mynode cluster_myclust: 2008-08-19 13:57:51 CEST DEBUG2 remoteWorkerThread_1: Received event 1,65832742 SYNC
Aug 19 13:57:51 mynode cluster_myclust: 2008-08-19 13:57:51 CEST DEBUG2 remoteListenThread_1: queue event 1,65832744 SYNC
Aug 19 13:57:51 mynode cluster_myclust: 2008-08-19 13:57:51 CEST DEBUG2 calc sync size - last time: 1 last length: 1660 ideal: 36 proposed size: 3
Aug 19 13:57:51 mynode cluster_myclust: 2008-08-19 13:57:51 CEST DEBUG2 remoteListenThread_1: queue event 1,65832745 SYNC
Aug 19 13:57:51 mynode cluster_myclust: 2008-08-19 13:57:51 CEST DEBUG2 remoteListenThread_1: queue event 1,65832746 SYNC
[...]
Aug 19 13:57:51 mynode cluster_myclust: 2008-08-19 13:57:51 CEST DEBUG2 remoteListenThread_1: queue event 1,65832758 SYNC
Aug 19 13:57:51 mynode cluster_myclust: 2008-08-19 13:57:51 CEST DEBUG2 remoteListenThread_1: queue event 1,65832759 SYNC
Aug 19 13:57:51 mynode cluster_myclust: 2008-08-19 13:57:51 CEST DEBUG2 remoteWorkerThread_1: SYNC 65832744 processing



* strace  -ttt -p 17450 :
1219073660.529469 _llseek(196, 17924096, [17924096], SEEK_SET) = 0
1219073660.529530 read(196, "p\v\0\0\0vke\1\0\0\0\344\0\20\33\360\37\3
\330\2370\0\270"..., 8192) = 8192
1219073660.531583 _llseek(214, 6086656, [6086656], SEEK_SET) = 0
1219073660.531680 read(214, "p\v\0\0\350oke\1\0\0\0<\2\340\v\0 \3
\260\233h\0|\233h"..., 8192) = 8192
1219073660.536787 send(6, "\3\0\0\0&\0\0\0\3\0\0\0*D\0\0<IDLE> in transa"...,
38, 0) = 38

From ajs at crankycanuck.ca  Wed Aug 20 05:46:28 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Aug 20 05:46:35 2008
Subject: [Slony1-general] Replication node suddenly lagging, CPU bound
	postmaster
In-Reply-To: <20080820101520.GA4523@mailer.elma.fr>
References: <20080820101520.GA4523@mailer.elma.fr>
Message-ID: <20080820124628.GA72091@crankycanuck.ca>

On Wed, Aug 20, 2008 at 12:15:20PM +0200, Benjamin Pineau wrote:

> Strangely, the replication on this node seems CPU bound by the postmaster
> process doing the actual inserts/updates for slon (this postmaster process
> is stuck at 99% CPU usage since the beginning of the problem).

Is your vacuum regimen correct?  I've seen that cause this sort of
problem.

A


-- 
Andrew Sullivan
ajs@commandprompt.com
+1 503 667 4564 x104
http://www.commandprompt.com/
From bpineau at elma.fr  Wed Aug 20 06:41:40 2008
From: bpineau at elma.fr (Benjamin Pineau)
Date: Wed Aug 20 06:41:48 2008
Subject: [Slony1-general] Replication node suddenly lagging,
	CPU bound postmaster
In-Reply-To: <20080820124628.GA72091@crankycanuck.ca>
References: <20080820101520.GA4523@mailer.elma.fr>
	<20080820124628.GA72091@crankycanuck.ca>
Message-ID: <20080820134140.GA29856@mailer.elma.fr>

Hi Andrew,

On Wed, Aug 20, 2008 at 08:46:28AM -0400, Andrew Sullivan wrote:
> On Wed, Aug 20, 2008 at 12:15:20PM +0200, Benjamin Pineau wrote:
> 
> > Strangely, the replication on this node seems CPU bound by the postmaster
> > process doing the actual inserts/updates for slon (this postmaster process
> > is stuck at 99% CPU usage since the beginning of the problem).
> 
> Is your vacuum regimen correct?  I've seen that cause this sort of
> problem.

Cron do run an "ANALYZE verbose" at midnight every day but sunday. 
On sunday it launch "vacuumdb -a -z -e -v" (so a "simple" vacuum). 
I think this maintainance vacuum should have completed by the time 
problem appeared (more than 12 hours laters), but I don't know for
sure. And I'm not very sure wether my setup is correct or not (being 
a pg newbie ;).

And this node's postgresql.conf setup is as follow :
vacuum_cost_delay = 0			# 0-1000 milliseconds
vacuum_cost_page_hit = 1		# 0-10000 credits
vacuum_cost_page_miss = 10		# 0-10000 credits
vacuum_cost_page_dirty = 20		# 0-10000 credits
vacuum_cost_limit = 200			# 0-10000 credits
autovacuum = on				# enable autovacuum subprocess
autovacuum_naptime = 600		# time between autovacuum runs, in secs
autovacuum_vacuum_threshold = 10000	# min # of tuple updates before
autovacuum_analyze_threshold = 5000	# min # of tuple updates before 

I don't see any pending locks (appart from the currently running
replication's inserts). Wouldn't an intrusive, full vacuum leave such 
locks visible in pg_locks+pg_stat_activity? 
Or do you rather mean that launching a full vacuum may help?

Thank you for the tip, it does rings a bell (ie. since I had an "almost
disk full" situation just before the replication problem, maybe pg may
have launched an emergency autovacuum of some sort? I need to explore).

From ajs at crankycanuck.ca  Wed Aug 20 07:04:09 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Aug 20 07:04:16 2008
Subject: [Slony1-general] Replication node suddenly lagging, CPU bound
	postmaster
In-Reply-To: <20080820134140.GA29856@mailer.elma.fr>
References: <20080820101520.GA4523@mailer.elma.fr>
	<20080820124628.GA72091@crankycanuck.ca>
	<20080820134140.GA29856@mailer.elma.fr>
Message-ID: <20080820140409.GB72091@crankycanuck.ca>

On Wed, Aug 20, 2008 at 03:41:40PM +0200, Benjamin Pineau wrote:

> Cron do run an "ANALYZE verbose" at midnight every day but sunday. 
> On sunday it launch "vacuumdb -a -z -e -v" (so a "simple" vacuum). 

[. . .]
> 
> And this node's postgresql.conf setup is as follow :
> vacuum_cost_delay = 0			# 0-1000 milliseconds
> vacuum_cost_page_hit = 1		# 0-10000 credits
> vacuum_cost_page_miss = 10		# 0-10000 credits
> vacuum_cost_page_dirty = 20		# 0-10000 credits
> vacuum_cost_limit = 200			# 0-10000 credits
> autovacuum = on				# enable autovacuum subprocess
> autovacuum_naptime = 600		# time between autovacuum runs, in secs
> autovacuum_vacuum_threshold = 10000	# min # of tuple updates before
> autovacuum_analyze_threshold = 5000	# min # of tuple updates before 

Wju are you running manual vacuums and autovacuum too?  You shouldn't
need the full vacuum.  Anyway, assuming this is a release after 8.1 (I
don't actually recommend autovac on 8.1 in most cases), have a look at
the pg_stat_*_tables tables and see whether things are being vacuumed
adequately.  You might also want to run a VACUUM VERBOSE on the
database and see if your FSM is correct.

> Thank you for the tip, it does rings a bell (ie. since I had an "almost
> disk full" situation just before the replication problem, maybe pg may
> have launched an emergency autovacuum of some sort? I need to explore).

No, PG won't do that (the emergency autovac happens in 8.3 if you're
about to roll over your xid space); but it does suggest the table
bloat I'm supposing, from inadequate vacuuming.

To solve it, you could do VACUUM FULL and REINDEX.  Just be prepared
to wait a long while. 

A

-- 
Andrew Sullivan
ajs@commandprompt.com
+1 503 667 4564 x104
http://www.commandprompt.com/
From ahodgson at simkin.ca  Wed Aug 20 08:24:09 2008
From: ahodgson at simkin.ca (Alan Hodgson)
Date: Wed Aug 20 08:24:19 2008
Subject: [Slony1-general] Replication node suddenly lagging,
	CPU bound postmaster
In-Reply-To: <20080820101520.GA4523@mailer.elma.fr>
References: <20080820101520.GA4523@mailer.elma.fr>
Message-ID: <200808200824.09185@hal.medialogik.com>

On Wednesday 20 August 2008, Benjamin Pineau <bpineau@elma.fr> wrote:
> Hi everyone.
>
> I have a replicating node that suddenly started to lag, on a 4 nodes
> Slony cluster that worked well for months. This node is powerful enough
> (ie. older, slower machines on the cluster achieve to keep up well).
> Network and block devices are mostly idling (with regard to
> interrupts/second and throughput). Strangely, the replication on this
> node seems CPU bound by the postmaster process doing the actual
> inserts/updates for slon (this postmaster process is stuck at 99% CPU
> usage since the beginning of the problem).
> Neither Slony (at "slon -d2" level) nor PostgreSQL did log any warning or
> error message, and the replication did not stopped on this node (it makes
> progresses, but too slowly to keep up, so it's now 3 days behind master).
>
> Any clue?

Look at pg_stat_activity for the slon process on the slave - you'll probably 
see a bunch of updates or deletes that look like they should be finishing 
fast, but aren't. When this happens here it's usually because the target 
table that's causing problems needs an ANALYZE (for us it usually happens 
on the first of the month when new month's data starts showing up and the 
planner loses its mind and stops using the primary key to find 
update/delete rows).

-- 
Alan
From shahaf at redfin.com  Wed Aug 20 08:35:11 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Wed Aug 20 08:35:25 2008
Subject: [Slony1-general] Dropped Master Cluster
In-Reply-To: <48ABE80F.5070805@albourne.com>
References: <46d2bc8f0808191024y3c759ef3m7a57dac7eb765c1a@mail.gmail.com>	<48ABDB11.9040608@albourne.com><46d2bc8f0808200243n40bc9b5aldc4a8d07a749e6a4@mail.gmail.com>
	<48ABE80F.5070805@albourne.com>
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB2204A2D6DF@mail-1.rf.lan>

I agree.  It does remove all the triggers.  I also do it regularly.  More specifically, I generally use slonik commands to unsubscribe and remove slave nodes, and then I use the "drop schema" command to clean up the master node.

--S

-----Original Message-----
From: slony1-general-bounces@lists.slony.info [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Martin Eriksson
Sent: Wednesday, August 20, 2008 2:47 AM
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] Dropped Master Cluster

Yes it will,

the cascade will remove ALL slony triggers.

I do it all the time..


Ren?-Etienne Muscat wrote:
> But this will not remove the triggers from the tables! Is there a 
> clean way of getting this done or is this the only method?
>
> Thanks for your help,
> Rene
>
> On Wed, Aug 20, 2008 at 10:51 AM, Martin Eriksson 
> <m.eriksson@albourne.com <mailto:m.eriksson@albourne.com>> wrote:
>
>     on the slaves nodes issue:
>
>     drop schema <cluster name> cascade;
>
>     this will remove ALL slony stuff from the database it is run on.
>
>     /martin
>
>
>     Ren?-Etienne Muscat wrote:
>
>         Hi everyone, I have dropped the master cluster from pgadmin
>         and cannot do the same to the slave cluster.
>         What can I do to remove this replication set from the slave
>         and get the tables back to normal?
>         P.S. I have other replication sets working fine, so I do not
>         want to do any changes that would affect the other replication
>         sets.
>          Rene
>         ------------------------------------------------------------------------
>
>         _______________________________________________
>         Slony1-general mailing list
>         Slony1-general@lists.slony.info
>         <mailto:Slony1-general@lists.slony.info>
>         http://lists.slony.info/mailman/listinfo/slony1-general
>          
>
>
>     _______________________________________________
>     Slony1-general mailing list
>     Slony1-general@lists.slony.info
>     <mailto:Slony1-general@lists.slony.info>
>     http://lists.slony.info/mailman/listinfo/slony1-general
>
>

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


From remuscat at gmail.com  Wed Aug 20 09:02:17 2008
From: remuscat at gmail.com (=?ISO-8859-1?Q?Ren=E9-Etienne_Muscat?=)
Date: Wed Aug 20 09:02:26 2008
Subject: [Slony1-general] Dropped Master Cluster
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB2204A2D6DF@mail-1.rf.lan>
References: <46d2bc8f0808191024y3c759ef3m7a57dac7eb765c1a@mail.gmail.com>
	<48ABDB11.9040608@albourne.com>
	<46d2bc8f0808200243n40bc9b5aldc4a8d07a749e6a4@mail.gmail.com>
	<48ABE80F.5070805@albourne.com>
	<082D8A131DF72A4D88C908A1AD3DEB2204A2D6DF@mail-1.rf.lan>
Message-ID: <46d2bc8f0808200902j5b6e29f6h39d840cf6b5eb1a@mail.gmail.com>

Thanks both of you for your support. I will try it on a test machine before
doing it live.
Best Regards,
Rene


On 8/20/08, Shahaf Abileah <shahaf@redfin.com> wrote:
>
> I agree.  It does remove all the triggers.  I also do it regularly.  More
> specifically, I generally use slonik commands to unsubscribe and remove
> slave nodes, and then I use the "drop schema" command to clean up the mas=
ter
> node.
>
> --S
>
> -----Original Message-----
> From: slony1-general-bounces@lists.slony.info [mailto:
> slony1-general-bounces@lists.slony.info] On Behalf Of Martin Eriksson
> Sent: Wednesday, August 20, 2008 2:47 AM
> Cc: slony1-general@lists.slony.info
> Subject: Re: [Slony1-general] Dropped Master Cluster
>
> Yes it will,
>
> the cascade will remove ALL slony triggers.
>
> I do it all the time..
>
>
> Ren=E9-Etienne Muscat wrote:
> > But this will not remove the triggers from the tables! Is there a
> > clean way of getting this done or is this the only method?
> >
> > Thanks for your help,
> > Rene
> >
> > On Wed, Aug 20, 2008 at 10:51 AM, Martin Eriksson
> > <m.eriksson@albourne.com <mailto:m.eriksson@albourne.com>> wrote:
> >
> >     on the slaves nodes issue:
> >
> >     drop schema <cluster name> cascade;
> >
> >     this will remove ALL slony stuff from the database it is run on.
> >
> >     /martin
> >
> >
> >     Ren=E9-Etienne Muscat wrote:
> >
> >         Hi everyone, I have dropped the master cluster from pgadmin
> >         and cannot do the same to the slave cluster.
> >         What can I do to remove this replication set from the slave
> >         and get the tables back to normal?
> >         P.S. I have other replication sets working fine, so I do not
> >         want to do any changes that would affect the other replication
> >         sets.
> >          Rene
> >
> ------------------------------------------------------------------------
> >
> >         _______________________________________________
> >         Slony1-general mailing list
> >         Slony1-general@lists.slony.info
> >         <mailto:Slony1-general@lists.slony.info>
> >         http://lists.slony.info/mailman/listinfo/slony1-general
> >
> >
> >
> >     _______________________________________________
> >     Slony1-general mailing list
> >     Slony1-general@lists.slony.info
> >     <mailto:Slony1-general@lists.slony.info>
> >     http://lists.slony.info/mailman/listinfo/slony1-general
> >
> >
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080820/=
8337b054/attachment-0001.htm
From cbbrowne at ca.afilias.info  Wed Aug 20 10:37:32 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Aug 20 10:37:48 2008
Subject: [Slony1-general] Dropped Master Cluster
In-Reply-To: <46d2bc8f0808200243n40bc9b5aldc4a8d07a749e6a4@mail.gmail.com>
	(=?iso-8859-1?Q?Ren=E9-Etienne?= Muscat's message of "Wed, 20 Aug 2008
	11:43:46 +0200")
References: <46d2bc8f0808191024y3c759ef3m7a57dac7eb765c1a@mail.gmail.com>
	<48ABDB11.9040608@albourne.com>
	<46d2bc8f0808200243n40bc9b5aldc4a8d07a749e6a4@mail.gmail.com>
Message-ID: <8763pvmvtv.fsf@dba2.int.libertyrms.com>

"Ren?-Etienne Muscat" <remuscat@gmail.com> writes:
> But this will not remove the triggers from the tables!

Sure it does.

It drops the functions that the triggers are to run, and therefore
drops out the triggers.
-- 
(reverse (concatenate 'string "ofni.sesabatadxunil" "@" "enworbbc"))
http://www3.sympatico.ca/cbbrowne/multiplexor.html
Rules of the Evil Overlord #1. "My Legions of Terror will have helmets
with   clear    plexiglass   visors,   not    face-concealing   ones."
<http://www.eviloverlord.com/>
From kevin at kevinkempterllc.com  Wed Aug 20 13:24:15 2008
From: kevin at kevinkempterllc.com (kevin kempter)
Date: Wed Aug 20 13:24:33 2008
Subject: [Slony1-general] OID # does not exist
Message-ID: <899C5B50-8EA7-46E8-8698-849D38E77695@kevinkempterllc.com>

Our application login's are failing with this error:

Caused by: org.postgresql.util.PSQLException: ERROR: relation with OID
405096 does not exist

We're running SLONY but I'm not sure if that's the issue


Anyone familiar with this ?


Thanks in advance
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080820/52226f94/attachment.htm
From ajs at crankycanuck.ca  Wed Aug 20 13:26:35 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Aug 20 13:26:50 2008
Subject: [Slony1-general] OID # does not exist
In-Reply-To: <899C5B50-8EA7-46E8-8698-849D38E77695@kevinkempterllc.com>
References: <899C5B50-8EA7-46E8-8698-849D38E77695@kevinkempterllc.com>
Message-ID: <20080820202634.GD72091@crankycanuck.ca>

On Wed, Aug 20, 2008 at 02:24:15PM -0600, kevin kempter wrote:
> Our application login's are failing with this error:
>
> Caused by: org.postgresql.util.PSQLException: ERROR: relation with OID
> 405096 does not exist

It looks like it's trying to reach a table that's not there.  Did you
drop one?

a


-- 
Andrew Sullivan
ajs@commandprompt.com
+1 503 667 4564 x104
http://www.commandprompt.com/
From kevin at kevinkempterllc.com  Wed Aug 20 13:28:19 2008
From: kevin at kevinkempterllc.com (kevin kempter)
Date: Wed Aug 20 13:28:35 2008
Subject: [Slony1-general] OID # does not exist
In-Reply-To: <20080820202634.GD72091@crankycanuck.ca>
References: <899C5B50-8EA7-46E8-8698-849D38E77695@kevinkempterllc.com>
	<20080820202634.GD72091@crankycanuck.ca>
Message-ID: <603FAC81-B7F1-48D1-A199-506CB3F9E2CB@kevinkempterllc.com>

no we didn't AFAIK drop any tables

On Aug 20, 2008, at 2:26 PM, Andrew Sullivan wrote:

> On Wed, Aug 20, 2008 at 02:24:15PM -0600, kevin kempter wrote:
>> Our application login's are failing with this error:
>>
>> Caused by: org.postgresql.util.PSQLException: ERROR: relation with  
>> OID
>> 405096 does not exist
>
> It looks like it's trying to reach a table that's not there.  Did you
> drop one?
>
> a
>
>
> -- 
> Andrew Sullivan
> ajs@commandprompt.com
> +1 503 667 4564 x104
> http://www.commandprompt.com/
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

From shahaf at redfin.com  Wed Aug 20 14:07:09 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Wed Aug 20 14:07:25 2008
Subject: [Slony1-general] OID # does not exist
In-Reply-To: <603FAC81-B7F1-48D1-A199-506CB3F9E2CB@kevinkempterllc.com>
References: <899C5B50-8EA7-46E8-8698-849D38E77695@kevinkempterllc.com><20080820202634.GD72091@crankycanuck.ca>
	<603FAC81-B7F1-48D1-A199-506CB3F9E2CB@kevinkempterllc.com>
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB2204A2DAC4@mail-1.rf.lan>

Is it possible you did some kind of table swap?  E.g. create a new table
and then perform a couple of renames such that the new table has the
correct table name?  If so, then the new table will have a different OID
and this will break slony replication.

--S

-----Original Message-----
From: slony1-general-bounces@lists.slony.info
[mailto:slony1-general-bounces@lists.slony.info] On Behalf Of kevin
kempter
Sent: Wednesday, August 20, 2008 1:28 PM
To: Andrew Sullivan
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] OID # does not exist

no we didn't AFAIK drop any tables

On Aug 20, 2008, at 2:26 PM, Andrew Sullivan wrote:

> On Wed, Aug 20, 2008 at 02:24:15PM -0600, kevin kempter wrote:
>> Our application login's are failing with this error:
>>
>> Caused by: org.postgresql.util.PSQLException: ERROR: relation with  
>> OID
>> 405096 does not exist
>
> It looks like it's trying to reach a table that's not there.  Did you
> drop one?
>
> a
>
>
> -- 
> Andrew Sullivan
> ajs@commandprompt.com
> +1 503 667 4564 x104
> http://www.commandprompt.com/
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


From jeff at frostconsultingllc.com  Wed Aug 20 14:13:20 2008
From: jeff at frostconsultingllc.com (Jeff Frost)
Date: Wed Aug 20 14:13:48 2008
Subject: [Slony1-general] slony and autovacuum
Message-ID: <Pine.LNX.4.64.0808201411550.14879@discord.home.frostconsultingllc.com>

I went looking for the section about Interaction with PostgreSQL autovacuum 
and noticed it's no longer in the docs.  I found it on a cached version of the 
docs here:

http://209.85.141.104/search?q=cache:jAQjR0Q9t1EJ:cbbrowne.com/info/maintenance.html+slony+vacrelid&hl=en&ct=clnk&cd=5&gl=us

Is it no longer recommended to disable autovacuum for the slony tables?

-- 
Jeff Frost, Owner 	<jeff@frostconsultingllc.com>
Frost Consulting, LLC 	http://www.frostconsultingllc.com/
Phone: 916-647-6411	FAX: 916-405-4032
From ajs at crankycanuck.ca  Thu Aug 21 05:28:50 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Aug 21 05:28:55 2008
Subject: [Slony1-general] OID # does not exist
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB2204A2DAC4@mail-1.rf.lan>
References: <603FAC81-B7F1-48D1-A199-506CB3F9E2CB@kevinkempterllc.com>
	<082D8A131DF72A4D88C908A1AD3DEB2204A2DAC4@mail-1.rf.lan>
Message-ID: <20080821122850.GA40283@crankycanuck.ca>

On Wed, Aug 20, 2008 at 02:07:09PM -0700, Shahaf Abileah wrote:
> Is it possible you did some kind of table swap?  E.g. create a new table
> and then perform a couple of renames such that the new table has the
> correct table name?  If so, then the new table will have a different OID
> and this will break slony replication.

It's not a slony error, it's an application error.  Is the target
database the origin for all tables, and has it always been?

A


-- 
Andrew Sullivan
ajs@commandprompt.com
+1 503 667 4564 x104
http://www.commandprompt.com/
From ajs at crankycanuck.ca  Thu Aug 21 05:35:53 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Aug 21 05:35:57 2008
Subject: [Slony1-general] slony and autovacuum
In-Reply-To: <Pine.LNX.4.64.0808201411550.14879@discord.home.frostconsultingllc.com>
References: <Pine.LNX.4.64.0808201411550.14879@discord.home.frostconsultingllc.com>
Message-ID: <20080821123553.GB40283@crankycanuck.ca>

On Wed, Aug 20, 2008 at 02:13:20PM -0700, Jeff Frost wrote:
>
> Is it no longer recommended to disable autovacuum for the slony tables?

Recent versions of autovacuum, particularly in 8.3, are smarter about
how they work, so the slony-triggered vacuuming and the
autovac-triggered vacuuming won't collide, I think.  (I have no idea
if this is the basis for the doc change, however.)

A

-- 
Andrew Sullivan
ajs@commandprompt.com
+1 503 667 4564 x104
http://www.commandprompt.com/
From kevin at kevinkempterllc.com  Thu Aug 21 08:20:49 2008
From: kevin at kevinkempterllc.com (kevin kempter)
Date: Thu Aug 21 08:21:55 2008
Subject: [Slony1-general] OID # does not exist
In-Reply-To: <20080821122850.GA40283@crankycanuck.ca>
References: <603FAC81-B7F1-48D1-A199-506CB3F9E2CB@kevinkempterllc.com>
	<082D8A131DF72A4D88C908A1AD3DEB2204A2DAC4@mail-1.rf.lan>
	<20080821122850.GA40283@crankycanuck.ca>
Message-ID: <53EB4FF6-CA63-41F4-B96B-5F30A6F192B2@kevinkempterllc.com>


On Aug 21, 2008, at 6:28 AM, Andrew Sullivan wrote:

> On Wed, Aug 20, 2008 at 02:07:09PM -0700, Shahaf Abileah wrote:
>> Is it possible you did some kind of table swap?  E.g. create a new  
>> table
>> and then perform a couple of renames such that the new table has the
>> correct table name?  If so, then the new table will have a  
>> different OID
>> and this will break slony replication.

no


>>
>
> It's not a slony error, it's an application error.  Is the target
> database the origin for all tables, and has it always been?

Yes to both questions..


>
>
> A
>
>
> -- 
> Andrew Sullivan
> ajs@commandprompt.com
> +1 503 667 4564 x104
> http://www.commandprompt.com/
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

From msteben at autorevenue.com  Thu Aug 21 13:29:04 2008
From: msteben at autorevenue.com (Mark Steben)
Date: Thu Aug 21 13:28:45 2008
Subject: [Slony1-general] Slony over a WAN?
Message-ID: <004a01c903cc$8def4610$14050a0a@dei26g028575>


Looking for advice on how to proceed.  We are running Postgres 8.2.5 in Lee,
Massachusetts.  We will be installing same in Norfolk Virginia in the not
too distant future.  Our need is to replicate roughly 60 - 70 percent
Of our 80 GB database in Lee over a WAN to Norfolk for reporting purposes.
In reading 'Slony-1 Best Practices' on the Website I came across the
following statement:
     
  slon processes should run in the same "network context" as the node that
each is responsible for managing so that the connection to that node is a
"local" one. 
     Do not run such links across a WAN.

Does this still hold true?  If not I would like to hear experiences of
anyone engaging in a 'Slony-1 long distance relationship'
Any other alternatives to consider?  I do run Slony-1.2.14 in development
with everything encompassed in Lee.  However 
we will be opening another office 35 miles west in West Springfield that I
will be operating out of.  I plan on employing Slony-1
To provide replication between these 2 'not so long distance' locations.

Any comments welcome.  Thanks


Mark Steben?Database Administrator? @utoRevenueT
480 Pleasant Street, Suite B200, Lee, MA 01238 
413-243-4800 x1512 (Phone) ? 413-243-4809 (Fax)
A Division of Dominion Enterprises

 
 
 
 
 

From wmoran at collaborativefusion.com  Thu Aug 21 13:43:38 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Thu Aug 21 13:43:33 2008
Subject: [Slony1-general] Slony over a WAN?
In-Reply-To: <004a01c903cc$8def4610$14050a0a@dei26g028575>
References: <004a01c903cc$8def4610$14050a0a@dei26g028575>
Message-ID: <20080821164338.a4925fc8.wmoran@collaborativefusion.com>

In response to "Mark Steben" <msteben@autorevenue.com>:

> 
> Looking for advice on how to proceed.  We are running Postgres 8.2.5 in Lee,
> Massachusetts.  We will be installing same in Norfolk Virginia in the not
> too distant future.  Our need is to replicate roughly 60 - 70 percent
> Of our 80 GB database in Lee over a WAN to Norfolk for reporting purposes.
> In reading 'Slony-1 Best Practices' on the Website I came across the
> following statement:
>      
>   slon processes should run in the same "network context" as the node that
> each is responsible for managing so that the connection to that node is a
> "local" one. 
>      Do not run such links across a WAN.
> 
> Does this still hold true?  If not I would like to hear experiences of
> anyone engaging in a 'Slony-1 long distance relationship'
> Any other alternatives to consider?  I do run Slony-1.2.14 in development
> with everything encompassed in Lee.  However 
> we will be opening another office 35 miles west in West Springfield that I
> will be operating out of.  I plan on employing Slony-1
> To provide replication between these 2 'not so long distance' locations.
> 
> Any comments welcome.  Thanks

I suspect that you're misinterpreting that statement to mean that Slony
doesn't work over a WAN.  It works very well over a WAN in my experience.

That statement is saying that the individual slon daemons should run
on the same machine or a machine very close to the machine that they
are connecting to as their primary system.  I.e. if you're running a
slon daemon in Mass, it should not have in it's slon.conf a connection
string to the server in Virginia.  There should be one slon running
in Mass that connects to the Mass DB, and a second one running in
Norfolk that connects to the Norfolk DB.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From msteben at autorevenue.com  Thu Aug 21 13:53:26 2008
From: msteben at autorevenue.com (Mark Steben)
Date: Thu Aug 21 13:53:05 2008
Subject: [Slony1-general] Slony over a WAN?
In-Reply-To: <20080821164338.a4925fc8.wmoran@collaborativefusion.com>
Message-ID: <004e01c903cf$f56b66e0$14050a0a@dei26g028575>

Thank you Bill and Vivek for your prompt responses.

Mark
 
 

-----Original Message-----
From: Bill Moran [mailto:wmoran@collaborativefusion.com] 
Sent: Thursday, August 21, 2008 4:44 PM
To: Mark Steben
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] Slony over a WAN?

In response to "Mark Steben" <msteben@autorevenue.com>:

> 
> Looking for advice on how to proceed.  We are running Postgres 8.2.5 in
Lee,
> Massachusetts.  We will be installing same in Norfolk Virginia in the not
> too distant future.  Our need is to replicate roughly 60 - 70 percent
> Of our 80 GB database in Lee over a WAN to Norfolk for reporting purposes.
> In reading 'Slony-1 Best Practices' on the Website I came across the
> following statement:
>      
>   slon processes should run in the same "network context" as the node that
> each is responsible for managing so that the connection to that node is a
> "local" one. 
>      Do not run such links across a WAN.
> 
> Does this still hold true?  If not I would like to hear experiences of
> anyone engaging in a 'Slony-1 long distance relationship'
> Any other alternatives to consider?  I do run Slony-1.2.14 in development
> with everything encompassed in Lee.  However 
> we will be opening another office 35 miles west in West Springfield that I
> will be operating out of.  I plan on employing Slony-1
> To provide replication between these 2 'not so long distance' locations.
> 
> Any comments welcome.  Thanks

I suspect that you're misinterpreting that statement to mean that Slony
doesn't work over a WAN.  It works very well over a WAN in my experience.

That statement is saying that the individual slon daemons should run
on the same machine or a machine very close to the machine that they
are connecting to as their primary system.  I.e. if you're running a
slon daemon in Mass, it should not have in it's slon.conf a connection
string to the server in Virginia.  There should be one slon running
in Mass that connects to the Mass DB, and a second one running in
Norfolk that connects to the Norfolk DB.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023

From shahaf at redfin.com  Thu Aug 21 15:29:32 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Thu Aug 21 15:29:50 2008
Subject: [Slony1-general] Slony over a WAN?
In-Reply-To: <004e01c903cf$f56b66e0$14050a0a@dei26g028575>
References: <20080821164338.a4925fc8.wmoran@collaborativefusion.com>
	<004e01c903cf$f56b66e0$14050a0a@dei26g028575>
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB2204A92AEB@mail-1.rf.lan>

Actually, in my case Slony does work well in LAN but does not work at
all over WAN.  As I understand it, at least one of the slon processes
(perhaps both) need to communicate with both the master and the slave
DB.  If the slon process runs in the same location as the master, then
it needs to communicate with the slave over WAN.  If it runs in the same
location as the slave then it needs to communicate with the master over
WAN.  One way or another you'll have a slon process talking across the
WAN to some DB.  And in our case that simply didn't work.  There was
enough flakiness in the network between the slon process and the DB that
replication never got going.

As I understand it, if you have a WAN scenario, you need to use slony
*logshipping* (not to be confused with "standard" slony replication).
I've successfully set up slony logshipping across the WAN, but it's a
bit of a pain to set up.

--S


-----Original Message-----
From: slony1-general-bounces@lists.slony.info
[mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Mark
Steben
Sent: Thursday, August 21, 2008 1:53 PM
To: 'Bill Moran'; 'Vivek Khera'
Cc: slony1-general@lists.slony.info
Subject: RE: [Slony1-general] Slony over a WAN?

Thank you Bill and Vivek for your prompt responses.

Mark
 
 

-----Original Message-----
From: Bill Moran [mailto:wmoran@collaborativefusion.com] 
Sent: Thursday, August 21, 2008 4:44 PM
To: Mark Steben
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] Slony over a WAN?

In response to "Mark Steben" <msteben@autorevenue.com>:

> 
> Looking for advice on how to proceed.  We are running Postgres 8.2.5
in
Lee,
> Massachusetts.  We will be installing same in Norfolk Virginia in the
not
> too distant future.  Our need is to replicate roughly 60 - 70 percent
> Of our 80 GB database in Lee over a WAN to Norfolk for reporting
purposes.
> In reading 'Slony-1 Best Practices' on the Website I came across the
> following statement:
>      
>   slon processes should run in the same "network context" as the node
that
> each is responsible for managing so that the connection to that node
is a
> "local" one. 
>      Do not run such links across a WAN.
> 
> Does this still hold true?  If not I would like to hear experiences of
> anyone engaging in a 'Slony-1 long distance relationship'
> Any other alternatives to consider?  I do run Slony-1.2.14 in
development
> with everything encompassed in Lee.  However 
> we will be opening another office 35 miles west in West Springfield
that I
> will be operating out of.  I plan on employing Slony-1
> To provide replication between these 2 'not so long distance'
locations.
> 
> Any comments welcome.  Thanks

I suspect that you're misinterpreting that statement to mean that Slony
doesn't work over a WAN.  It works very well over a WAN in my
experience.

That statement is saying that the individual slon daemons should run
on the same machine or a machine very close to the machine that they
are connecting to as their primary system.  I.e. if you're running a
slon daemon in Mass, it should not have in it's slon.conf a connection
string to the server in Virginia.  There should be one slon running
in Mass that connects to the Mass DB, and a second one running in
Norfolk that connects to the Norfolk DB.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


From ahodgson at simkin.ca  Thu Aug 21 15:48:14 2008
From: ahodgson at simkin.ca (Alan Hodgson)
Date: Thu Aug 21 15:48:33 2008
Subject: [Slony1-general] Slony over a WAN?
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB2204A92AEB@mail-1.rf.lan>
References: <20080821164338.a4925fc8.wmoran@collaborativefusion.com>
	<004e01c903cf$f56b66e0$14050a0a@dei26g028575>
	<082D8A131DF72A4D88C908A1AD3DEB2204A92AEB@mail-1.rf.lan>
Message-ID: <200808211548.14372@hal.medialogik.com>

On Thursday 21 August 2008, Shahaf Abileah <shahaf@redfin.com> wrote:
> As I understand it, if you have a WAN scenario, you need to use slony
> *logshipping* (not to be confused with "standard" slony replication).
> I've successfully set up slony logshipping across the WAN, but it's a
> bit of a pain to set up.

I use the standard replication over an Internet link from Vancouver to the 
Bahamas. It works pretty well, although it does tend to stop every now and 
then if communications get interrupted. 

Our monitoring picks that up pretty quick, and it's not terribly 
time-sensitive in any event.

-- 
Alan
From shahaf at redfin.com  Thu Aug 21 16:45:29 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Thu Aug 21 16:46:04 2008
Subject: [Slony1-general] bandwidth limit on slony replication
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB2204A92BBE@mail-1.rf.lan>

Is there a way to limit the amount of bandwidth that slony uses?

 

Within our data center we don't really care how much network is used to
replicate DB's from one server to another.  However, we'd like to set up
slaves in other locations, and we can get slapped with huge fees if we
have big spikes of network bandwidth usage (e.g. when you first set up a
slave and you need to bulk-copy all the tables).

 

Thanks,

 

--S

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080821/6d80d5ba/attachment.htm
From m.eriksson at albourne.com  Fri Aug 22 00:54:12 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Fri Aug 22 00:54:30 2008
Subject: [Slony1-general] Slony over a WAN?
In-Reply-To: <004a01c903cc$8def4610$14050a0a@dei26g028575>
References: <004a01c903cc$8def4610$14050a0a@dei26g028575>
Message-ID: <48AE70A4.6010901@albourne.com>

Hi, we do slony over WAN where W = World hehe

Our master Database is in London, then we got one node in Cyprus, one 
node in San Fransisco, and then one node in Frankfurt and about to add 
another node in Hong Kong. Our database is only around 8 gigs though.

we got a bit of a special setup in terms of the applications using the 
dbs as we want our application to read locally but write to the master 
so we a pretty advanced cache system for handling if Bill is sitting In 
San Fransisco writes to the db and then look at the data he sees the 
data he just wrote so does everyone else in that office even though it 
might not have been replicated to his local db yet. this work pretty 
good, we never have more then 5-15 seconds delay on slony event across 
the globe so in worst case if someone tries to change something that 
already been changed it wont work and they just reset their cache (which 
takes 2 minutes) and then they continue working.

and we do have an AWFUL line london <-> Cyprus that on average does 
40kbytes/s which is horribly low. But it still works 100 times better 
then having both read and write going to London all the time from across 
the world..

Of course if the shit hits the fan so to say when we do DLL changes 
which happens every 3 months for release and we cant recover it will 
take up to 72 hours to replicate to all nodes (which is not really an 
option)

but if you got good bandwidth between your nodes its not a problem.

Though if you got a 80 gig db you might want to consider not replicating 
it across the WAN as it will take quite a long time and might cost your 
a lot depending on your ISP i guess.. well if you got a gigabit line and 
dont mind using it then I guess you are fine :) but if you bandwidth is 
limited you could do as we do sometime,

setup a second db instance on your masternode (assuming node and master 
will be in the same hardware architecture) do a replication to the local 
instance once done shut down that instance move the whole /data 
directory of that instance onto so movable disk, and drive down to the 
other node load it up, modify path to the node using slony store path 
and fire it up and let it catch up on the last hour or so of the new data.

 slon processes should run in the same "network context" as the node that
each is responsible for managing so that the connection to that node is a
"local" one. 
     Do not run such links across a WAN.



Its already been covered but I'll add to it. Yes you should run the slon 
process on the node in question do not run them all on the master node, 
not only because it will work better also because when you do have 
multiple slon process running on one machine it can get VERY confusing 
figure out which one goes where and which one is having connections 
open. Life is much easier if one slon process run on each node machine.

good luck!
Martin


Mark Steben wrote:
> Looking for advice on how to proceed.  We are running Postgres 8.2.5 in Lee,
> Massachusetts.  We will be installing same in Norfolk Virginia in the not
> too distant future.  Our need is to replicate roughly 60 - 70 percent
> Of our 80 GB database in Lee over a WAN to Norfolk for reporting purposes.
> In reading 'Slony-1 Best Practices' on the Website I came across the
> following statement:
>      
>   slon processes should run in the same "network context" as the node that
> each is responsible for managing so that the connection to that node is a
> "local" one. 
>      Do not run such links across a WAN.
>
> Does this still hold true?  If not I would like to hear experiences of
> anyone engaging in a 'Slony-1 long distance relationship'
> Any other alternatives to consider?  I do run Slony-1.2.14 in development
> with everything encompassed in Lee.  However 
> we will be opening another office 35 miles west in West Springfield that I
> will be operating out of.  I plan on employing Slony-1
> To provide replication between these 2 'not so long distance' locations.
>
> Any comments welcome.  Thanks
>
>
> Mark Steben?Database Administrator? @utoRevenueT
> 480 Pleasant Street, Suite B200, Lee, MA 01238 
> 413-243-4800 x1512 (Phone) ? 413-243-4809 (Fax)
> A Division of Dominion Enterprises
>
>  
>  
>  
>  
>  
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>   

From m.eriksson at albourne.com  Fri Aug 22 01:05:39 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Fri Aug 22 01:05:54 2008
Subject: [Slony1-general] bandwidth limit on slony replication
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB2204A92BBE@mail-1.rf.lan>
References: <082D8A131DF72A4D88C908A1AD3DEB2204A92BBE@mail-1.rf.lan>
Message-ID: <48AE7353.8020206@albourne.com>

I don't know of any such ways in slony, there might be?

even if there is no such way in slony you could always throttle it on a 
lower level on the network level, pleanty of software available to do that..

but there are other ways to handle the initial huge bulk load as well..

e.g. replicate to a new instance in your data center (assuming its same 
hardware architecture (32bit vs 64bit , intel vs AMD etc, postgreSQL 
version) as the new node) once replication is done shut down the 
instance and do a physical copy of the data directory of the new 
instance. if you b2zip it will become about 5-7 times smaller and then 
transfer that and use that data directory for the new node, modify the 
slon path with slony "store path" to correspond with the new ip etc. 
then just let it catch up on the data since the shutdown.

this would yield 4-7 times less bandwidth usage, other way is to put it 
on a removable disk and bring it physical to the new location.

I've done both ways in our slony cluster and never had any problem. But 
then again we always make sure all our db machines are exactly the same 
in terms of hardware and software.

Martin


Shahaf Abileah wrote:
>
> Is there a way to limit the amount of bandwidth that slony uses?
>
> Within our data center we don?t really care how much network is used 
> to replicate DB?s from one server to another. However, we?d like to 
> set up slaves in other locations, and we can get slapped with huge 
> fees if we have big spikes of network bandwidth usage (e.g. when you 
> first set up a slave and you need to bulk-copy all the tables).
>
> Thanks,
>
> --S
>
> ------------------------------------------------------------------------
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>   

From wmoran at collaborativefusion.com  Fri Aug 22 03:33:20 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Fri Aug 22 03:33:33 2008
Subject: [Slony1-general] Slony over a WAN?
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB2204A92AEB@mail-1.rf.lan>
References: <20080821164338.a4925fc8.wmoran@collaborativefusion.com>
	<004e01c903cf$f56b66e0$14050a0a@dei26g028575>
	<082D8A131DF72A4D88C908A1AD3DEB2204A92AEB@mail-1.rf.lan>
Message-ID: <20080822063320.d3d1ac1e.wmoran@collaborativefusion.com>

In response to Shahaf Abileah <shahaf@redfin.com>:

> Actually, in my case Slony does work well in LAN but does not work at
> all over WAN.  As I understand it, at least one of the slon processes
> (perhaps both) need to communicate with both the master and the slave
> DB.  If the slon process runs in the same location as the master, then
> it needs to communicate with the slave over WAN.  If it runs in the same
> location as the slave then it needs to communicate with the master over
> WAN.  One way or another you'll have a slon process talking across the
> WAN to some DB.  And in our case that simply didn't work.  There was
> enough flakiness in the network between the slon process and the DB that
> replication never got going.

It _does_ work, but the tolerance for network flakiness is only so high.
If your network is constantly dropping, you probably won't be able to
get it going, and you'll have to fall back on something like log
shipping.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From shahaf at redfin.com  Fri Aug 22 07:24:07 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Fri Aug 22 07:24:17 2008
Subject: [Slony1-general] bandwidth limit on slony replication
In-Reply-To: <48AE7353.8020206@albourne.com>
References: <082D8A131DF72A4D88C908A1AD3DEB2204A92BBE@mail-1.rf.lan>
	<48AE7353.8020206@albourne.com>
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB2204A92D7E@mail-1.rf.lan>

I haven't tried to update the path on a given node before.  That's
clever.  Do you think it would work to do a pg_dump / pg_restore?  Or
would that create different OID's and therefore break replication?

We have copied files over using rsync (which has a bwlimit option) and
that seems to work very well.

--S


-----Original Message-----
From: slony1-general-bounces@lists.slony.info
[mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Martin
Eriksson
Sent: Friday, August 22, 2008 1:06 AM
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] bandwidth limit on slony replication

I don't know of any such ways in slony, there might be?

even if there is no such way in slony you could always throttle it on a 
lower level on the network level, pleanty of software available to do
that..

but there are other ways to handle the initial huge bulk load as well..

e.g. replicate to a new instance in your data center (assuming its same 
hardware architecture (32bit vs 64bit , intel vs AMD etc, postgreSQL 
version) as the new node) once replication is done shut down the 
instance and do a physical copy of the data directory of the new 
instance. if you b2zip it will become about 5-7 times smaller and then 
transfer that and use that data directory for the new node, modify the 
slon path with slony "store path" to correspond with the new ip etc. 
then just let it catch up on the data since the shutdown.

this would yield 4-7 times less bandwidth usage, other way is to put it 
on a removable disk and bring it physical to the new location.

I've done both ways in our slony cluster and never had any problem. But 
then again we always make sure all our db machines are exactly the same 
in terms of hardware and software.

Martin


Shahaf Abileah wrote:
>
> Is there a way to limit the amount of bandwidth that slony uses?
>
> Within our data center we don't really care how much network is used 
> to replicate DB's from one server to another. However, we'd like to 
> set up slaves in other locations, and we can get slapped with huge 
> fees if we have big spikes of network bandwidth usage (e.g. when you 
> first set up a slave and you need to bulk-copy all the tables).
>
> Thanks,
>
> --S
>
>
------------------------------------------------------------------------
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>   

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


From shahaf at redfin.com  Fri Aug 22 07:29:53 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Fri Aug 22 07:30:08 2008
Subject: [Slony1-general] Slony over a WAN?
In-Reply-To: <48AE70A4.6010901@albourne.com>
References: <004a01c903cc$8def4610$14050a0a@dei26g028575>
	<48AE70A4.6010901@albourne.com>
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB2204A92D81@mail-1.rf.lan>

Thanks for the great info Martin.

If you don't mind me asking, what technology do you use for your cache?  How do you combine the data in a local site's cache with the (potentially different) data in that local site's slave DB?

Thanks,

--S

-----Original Message-----
From: slony1-general-bounces@lists.slony.info [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Martin Eriksson
Sent: Friday, August 22, 2008 12:54 AM
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] Slony over a WAN?

Hi, we do slony over WAN where W = World hehe

Our master Database is in London, then we got one node in Cyprus, one 
node in San Fransisco, and then one node in Frankfurt and about to add 
another node in Hong Kong. Our database is only around 8 gigs though.

we got a bit of a special setup in terms of the applications using the 
dbs as we want our application to read locally but write to the master 
so we a pretty advanced cache system for handling if Bill is sitting In 
San Fransisco writes to the db and then look at the data he sees the 
data he just wrote so does everyone else in that office even though it 
might not have been replicated to his local db yet. this work pretty 
good, we never have more then 5-15 seconds delay on slony event across 
the globe so in worst case if someone tries to change something that 
already been changed it wont work and they just reset their cache (which 
takes 2 minutes) and then they continue working.

and we do have an AWFUL line london <-> Cyprus that on average does 
40kbytes/s which is horribly low. But it still works 100 times better 
then having both read and write going to London all the time from across 
the world..

Of course if the shit hits the fan so to say when we do DLL changes 
which happens every 3 months for release and we cant recover it will 
take up to 72 hours to replicate to all nodes (which is not really an 
option)

but if you got good bandwidth between your nodes its not a problem.

Though if you got a 80 gig db you might want to consider not replicating 
it across the WAN as it will take quite a long time and might cost your 
a lot depending on your ISP i guess.. well if you got a gigabit line and 
dont mind using it then I guess you are fine :) but if you bandwidth is 
limited you could do as we do sometime,

setup a second db instance on your masternode (assuming node and master 
will be in the same hardware architecture) do a replication to the local 
instance once done shut down that instance move the whole /data 
directory of that instance onto so movable disk, and drive down to the 
other node load it up, modify path to the node using slony store path 
and fire it up and let it catch up on the last hour or so of the new data.

 slon processes should run in the same "network context" as the node that
each is responsible for managing so that the connection to that node is a
"local" one. 
     Do not run such links across a WAN.



Its already been covered but I'll add to it. Yes you should run the slon 
process on the node in question do not run them all on the master node, 
not only because it will work better also because when you do have 
multiple slon process running on one machine it can get VERY confusing 
figure out which one goes where and which one is having connections 
open. Life is much easier if one slon process run on each node machine.

good luck!
Martin


Mark Steben wrote:
> Looking for advice on how to proceed.  We are running Postgres 8.2.5 in Lee,
> Massachusetts.  We will be installing same in Norfolk Virginia in the not
> too distant future.  Our need is to replicate roughly 60 - 70 percent
> Of our 80 GB database in Lee over a WAN to Norfolk for reporting purposes.
> In reading 'Slony-1 Best Practices' on the Website I came across the
> following statement:
>      
>   slon processes should run in the same "network context" as the node that
> each is responsible for managing so that the connection to that node is a
> "local" one. 
>      Do not run such links across a WAN.
>
> Does this still hold true?  If not I would like to hear experiences of
> anyone engaging in a 'Slony-1 long distance relationship'
> Any other alternatives to consider?  I do run Slony-1.2.14 in development
> with everything encompassed in Lee.  However 
> we will be opening another office 35 miles west in West Springfield that I
> will be operating out of.  I plan on employing Slony-1
> To provide replication between these 2 'not so long distance' locations.
>
> Any comments welcome.  Thanks
>
>
> Mark Steben?Database Administrator? @utoRevenueT
> 480 Pleasant Street, Suite B200, Lee, MA 01238 
> 413-243-4800 x1512 (Phone) ? 413-243-4809 (Fax)
> A Division of Dominion Enterprises
>
>  
>  
>  
>  
>  
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>   

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


From cbbrowne at ca.afilias.info  Fri Aug 22 07:46:24 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Aug 22 07:46:33 2008
Subject: [Slony1-general] Slony over a WAN?
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB2204A92AEB@mail-1.rf.lan> (Shahaf
	Abileah's message of "Thu, 21 Aug 2008 15:29:32 -0700")
References: <20080821164338.a4925fc8.wmoran@collaborativefusion.com>
	<004e01c903cf$f56b66e0$14050a0a@dei26g028575>
	<082D8A131DF72A4D88C908A1AD3DEB2204A92AEB@mail-1.rf.lan>
Message-ID: <87k5e9kszj.fsf@dba2.int.libertyrms.com>

Shahaf Abileah <shahaf@redfin.com> writes:
> Actually, in my case Slony does work well in LAN but does not work
> at all over WAN.  As I understand it, at least one of the slon
> processes (perhaps both) need to communicate with both the master
> and the slave DB.  If the slon process runs in the same location as
> the master, then it needs to communicate with the slave over WAN.
> If it runs in the same location as the slave then it needs to
> communicate with the master over WAN.  One way or another you'll
> have a slon process talking across the WAN to some DB.  And in our
> case that simply didn't work.  There was enough flakiness in the
> network between the slon process and the DB that replication never
> got going.

Ouch!  Yes, if the WAN connection is really flakey, it won't turn out
well.  Fortunately (maybe!) that will show up immediately, as the part
that suffers most from this flakiness is the subscription process.  If
that gets interrupted, that requires restarting the copying of data.
(You could subscribe 1 table at a time, so that the "restart" is only
going back to the table Slony-I's working on, but if you've got one
"dominant" large table, as is fairly common, the difference isn't too
material...)

Once you have a subscription working, "network flakiness" should only
be an inconvenience.

We've been running Slony-I across a WAN ever since the very first day
it was put into production.  The comments about network flakiness in
the "best practices" do come from behaviour that I have directly seen.

FYI, I wouldn't expect to find that log shipping helps terribly much;
it writes out the same quantity of data in similarly sized "blocks."
If you can't get the initial subscription to go off across the WAN, I
wouldn't expect you'd find it that much easier to get the initial
logshipping data dump across the WAN, either.
-- 
(format nil "~S@~S" "cbbrowne" "acm.org")
http://cbbrowne.com/info/emacs.html
He's not dead. He's electroencephalographically challenged. 
From cbbrowne at ca.afilias.info  Fri Aug 22 07:51:03 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Aug 22 07:51:09 2008
Subject: [Slony1-general] bandwidth limit on slony replication
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB2204A92BBE@mail-1.rf.lan> (Shahaf
	Abileah's message of "Thu, 21 Aug 2008 16:45:29 -0700")
References: <082D8A131DF72A4D88C908A1AD3DEB2204A92BBE@mail-1.rf.lan>
Message-ID: <87fxoxksrs.fsf@dba2.int.libertyrms.com>

Shahaf Abileah <shahaf@redfin.com> writes:
> :o="urn:schemas-microsoft-com:office:office" xmlns:w="urn:schemas-microsoft-com:office:word" xmlns="http://www.w3.org/TR/REC-html40">
>
> Is there a way to limit the amount of bandwidth that slony uses?:p>
>
> :p>?
>
> Within our data center we don't really care how much network is used to replicate DB's from one server to another.? However, we'd like to set up
> slaves in other locations, and we can get slapped with huge fees if we have big spikes of network bandwidth usage (e.g. when you first set up a
> slave and you need to bulk-copy all the tables).:p>

I'd use some sort of network appliance (whether physical, or a Linux
kernel extension) to handle this.

Slony-I doesn't "gild itself" with this sort of functionality; it's
not a network management system, it's not a connection broker, it
won't make you coffee and toast in the morning, it's neither a floor
wax nor a dessert topping.

That sort of thing is terribly environment-specific, and there are
tools that can handle traffic shaping much better than Slony-I ever
could by itself.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/unix.html
Rules  of  the  Evil  Overlord  #24.  "I  will  maintain  a  realistic
assessment of my strengths and weaknesses. Even though this takes some
of the fun out  of the job, at least I will  never utter the line "No,
this  cannot be!  I AM  INVINCIBLE!!!" (After  that, death  is usually
instantaneous.)" <http://www.eviloverlord.com/>
From shahaf at redfin.com  Fri Aug 22 08:02:19 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Fri Aug 22 08:02:44 2008
Subject: [Slony1-general] Slony over a WAN?
In-Reply-To: <87k5e9kszj.fsf@dba2.int.libertyrms.com>
References: <20080821164338.a4925fc8.wmoran@collaborativefusion.com><004e01c903cf$f56b66e0$14050a0a@dei26g028575><082D8A131DF72A4D88C908A1AD3DEB2204A92AEB@mail-1.rf.lan>
	<87k5e9kszj.fsf@dba2.int.libertyrms.com>
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB2204A92D99@mail-1.rf.lan>

"the part that suffers most from this flakiness is the subscription
process.  If that gets interrupted, that requires restarting the copying
of data"

That's exactly what we noticed.  It kept restarting over and over, and
never made it through the initial bulk-copy process.  My DB is in the
tens of gigabytes and we have all tables in a single big set.  However,
even if I broke it up into multiple sets it still wouldn't work.  For
one, some of my tables have foreign key relationships to each other so
technically they need to be in the same set.  That aside, I do have some
pretty large tables that would likely fail on their own.

Other people suggested doing the initial subscription on some machine in
the LAN, then copying the entire /data directory to the new location
(across the WAN using rsync or whatnot), then updating the path info on
the master and kicking off replication to catch up on all the changes
that happened during the move.  That sounds like a great idea - I bet
it'll work.

--S


-----Original Message-----
From: Christopher Browne [mailto:cbbrowne@ca.afilias.info] 
Sent: Friday, August 22, 2008 7:46 AM
To: Shahaf Abileah
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] Slony over a WAN?

Shahaf Abileah <shahaf@redfin.com> writes:
> Actually, in my case Slony does work well in LAN but does not work
> at all over WAN.  As I understand it, at least one of the slon
> processes (perhaps both) need to communicate with both the master
> and the slave DB.  If the slon process runs in the same location as
> the master, then it needs to communicate with the slave over WAN.
> If it runs in the same location as the slave then it needs to
> communicate with the master over WAN.  One way or another you'll
> have a slon process talking across the WAN to some DB.  And in our
> case that simply didn't work.  There was enough flakiness in the
> network between the slon process and the DB that replication never
> got going.

Ouch!  Yes, if the WAN connection is really flakey, it won't turn out
well.  Fortunately (maybe!) that will show up immediately, as the part
that suffers most from this flakiness is the subscription process.  If
that gets interrupted, that requires restarting the copying of data.
(You could subscribe 1 table at a time, so that the "restart" is only
going back to the table Slony-I's working on, but if you've got one
"dominant" large table, as is fairly common, the difference isn't too
material...)

Once you have a subscription working, "network flakiness" should only
be an inconvenience.

We've been running Slony-I across a WAN ever since the very first day
it was put into production.  The comments about network flakiness in
the "best practices" do come from behaviour that I have directly seen.

FYI, I wouldn't expect to find that log shipping helps terribly much;
it writes out the same quantity of data in similarly sized "blocks."
If you can't get the initial subscription to go off across the WAN, I
wouldn't expect you'd find it that much easier to get the initial
logshipping data dump across the WAN, either.
-- 
(format nil "~S@~S" "cbbrowne" "acm.org")
http://cbbrowne.com/info/emacs.html
He's not dead. He's electroencephalographically challenged. 


From m.eriksson at albourne.com  Fri Aug 22 08:03:30 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Fri Aug 22 08:03:24 2008
Subject: [Slony1-general] bandwidth limit on slony replication
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB2204A92D7E@mail-1.rf.lan>
References: <082D8A131DF72A4D88C908A1AD3DEB2204A92BBE@mail-1.rf.lan>
	<48AE7353.8020206@albourne.com>
	<082D8A131DF72A4D88C908A1AD3DEB2204A92D7E@mail-1.rf.lan>
Message-ID: <48AED542.2020803@albourne.com>

i've never managed to get it to work using pg_dump due to OIDs... I 
always do a physical copy of the postgres data directory when server is 
shut down.. only way to do it reliably and be sure everything is there 
and exactly the same.

using b2zip as i think rsync has some optimisation for the p2zip as 
well.. and it got your bandwidth limiter as well :)


Shahaf Abileah wrote:
> I haven't tried to update the path on a given node before.  That's
> clever.  Do you think it would work to do a pg_dump / pg_restore?  Or
> would that create different OID's and therefore break replication?
>
> We have copied files over using rsync (which has a bwlimit option) and
> that seems to work very well.
>
> --S
>
>
> -----Original Message-----
> From: slony1-general-bounces@lists.slony.info
> [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Martin
> Eriksson
> Sent: Friday, August 22, 2008 1:06 AM
> Cc: slony1-general@lists.slony.info
> Subject: Re: [Slony1-general] bandwidth limit on slony replication
>
> I don't know of any such ways in slony, there might be?
>
> even if there is no such way in slony you could always throttle it on a 
> lower level on the network level, pleanty of software available to do
> that..
>
> but there are other ways to handle the initial huge bulk load as well..
>
> e.g. replicate to a new instance in your data center (assuming its same 
> hardware architecture (32bit vs 64bit , intel vs AMD etc, postgreSQL 
> version) as the new node) once replication is done shut down the 
> instance and do a physical copy of the data directory of the new 
> instance. if you b2zip it will become about 5-7 times smaller and then 
> transfer that and use that data directory for the new node, modify the 
> slon path with slony "store path" to correspond with the new ip etc. 
> then just let it catch up on the data since the shutdown.
>
> this would yield 4-7 times less bandwidth usage, other way is to put it 
> on a removable disk and bring it physical to the new location.
>
> I've done both ways in our slony cluster and never had any problem. But 
> then again we always make sure all our db machines are exactly the same 
> in terms of hardware and software.
>
> Martin
>
>
> Shahaf Abileah wrote:
>   
>> Is there a way to limit the amount of bandwidth that slony uses?
>>
>> Within our data center we don't really care how much network is used 
>> to replicate DB's from one server to another. However, we'd like to 
>> set up slaves in other locations, and we can get slapped with huge 
>> fees if we have big spikes of network bandwidth usage (e.g. when you 
>> first set up a slave and you need to bulk-copy all the tables).
>>
>> Thanks,
>>
>> --S
>>
>>
>>     
> ------------------------------------------------------------------------
>   
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>   
>>     
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
>   

From m.eriksson at albourne.com  Fri Aug 22 08:12:38 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Fri Aug 22 08:12:31 2008
Subject: [Slony1-general] Slony over a WAN?
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB2204A92D81@mail-1.rf.lan>
References: <004a01c903cc$8def4610$14050a0a@dei26g028575>
	<48AE70A4.6010901@albourne.com>
	<082D8A131DF72A4D88C908A1AD3DEB2204A92D81@mail-1.rf.lan>
Message-ID: <48AED766.7020109@albourne.com>

our cache server is all in-house written specifically for our 
application/database, (as not all data is cached only things likely to 
be access often etc)

but we do use java as language of choice, and the cache server is using 
the sun java rmiregistry

cache server updated as soon as it picks up changes that come in from 
the replication, its not perfect but it works pretty well. worst case 
user will get an error message when trying to write to master db, but 
then it will try to update the cache and by the time the cache is 
updated the write will go through..




Shahaf Abileah wrote:
> Thanks for the great info Martin.
>
> If you don't mind me asking, what technology do you use for your cache?  How do you combine the data in a local site's cache with the (potentially different) data in that local site's slave DB?
>
> Thanks,
>
> --S
>
> -----Original Message-----
> From: slony1-general-bounces@lists.slony.info [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Martin Eriksson
> Sent: Friday, August 22, 2008 12:54 AM
> Cc: slony1-general@lists.slony.info
> Subject: Re: [Slony1-general] Slony over a WAN?
>
> Hi, we do slony over WAN where W = World hehe
>
> Our master Database is in London, then we got one node in Cyprus, one 
> node in San Fransisco, and then one node in Frankfurt and about to add 
> another node in Hong Kong. Our database is only around 8 gigs though.
>
> we got a bit of a special setup in terms of the applications using the 
> dbs as we want our application to read locally but write to the master 
> so we a pretty advanced cache system for handling if Bill is sitting In 
> San Fransisco writes to the db and then look at the data he sees the 
> data he just wrote so does everyone else in that office even though it 
> might not have been replicated to his local db yet. this work pretty 
> good, we never have more then 5-15 seconds delay on slony event across 
> the globe so in worst case if someone tries to change something that 
> already been changed it wont work and they just reset their cache (which 
> takes 2 minutes) and then they continue working.
>
> and we do have an AWFUL line london <-> Cyprus that on average does 
> 40kbytes/s which is horribly low. But it still works 100 times better 
> then having both read and write going to London all the time from across 
> the world..
>
> Of course if the shit hits the fan so to say when we do DLL changes 
> which happens every 3 months for release and we cant recover it will 
> take up to 72 hours to replicate to all nodes (which is not really an 
> option)
>
> but if you got good bandwidth between your nodes its not a problem.
>
> Though if you got a 80 gig db you might want to consider not replicating 
> it across the WAN as it will take quite a long time and might cost your 
> a lot depending on your ISP i guess.. well if you got a gigabit line and 
> dont mind using it then I guess you are fine :) but if you bandwidth is 
> limited you could do as we do sometime,
>
> setup a second db instance on your masternode (assuming node and master 
> will be in the same hardware architecture) do a replication to the local 
> instance once done shut down that instance move the whole /data 
> directory of that instance onto so movable disk, and drive down to the 
> other node load it up, modify path to the node using slony store path 
> and fire it up and let it catch up on the last hour or so of the new data.
>
>  slon processes should run in the same "network context" as the node that
> each is responsible for managing so that the connection to that node is a
> "local" one. 
>      Do not run such links across a WAN.
>
>
>
> Its already been covered but I'll add to it. Yes you should run the slon 
> process on the node in question do not run them all on the master node, 
> not only because it will work better also because when you do have 
> multiple slon process running on one machine it can get VERY confusing 
> figure out which one goes where and which one is having connections 
> open. Life is much easier if one slon process run on each node machine.
>
> good luck!
> Martin
>
>
> Mark Steben wrote:
>   
>> Looking for advice on how to proceed.  We are running Postgres 8.2.5 in Lee,
>> Massachusetts.  We will be installing same in Norfolk Virginia in the not
>> too distant future.  Our need is to replicate roughly 60 - 70 percent
>> Of our 80 GB database in Lee over a WAN to Norfolk for reporting purposes.
>> In reading 'Slony-1 Best Practices' on the Website I came across the
>> following statement:
>>      
>>   slon processes should run in the same "network context" as the node that
>> each is responsible for managing so that the connection to that node is a
>> "local" one. 
>>      Do not run such links across a WAN.
>>
>> Does this still hold true?  If not I would like to hear experiences of
>> anyone engaging in a 'Slony-1 long distance relationship'
>> Any other alternatives to consider?  I do run Slony-1.2.14 in development
>> with everything encompassed in Lee.  However 
>> we will be opening another office 35 miles west in West Springfield that I
>> will be operating out of.  I plan on employing Slony-1
>> To provide replication between these 2 'not so long distance' locations.
>>
>> Any comments welcome.  Thanks
>>
>>
>> Mark Steben?Database Administrator? @utoRevenueT
>> 480 Pleasant Street, Suite B200, Lee, MA 01238 
>> 413-243-4800 x1512 (Phone) ? 413-243-4809 (Fax)
>> A Division of Dominion Enterprises
>>
>>  
>>  
>>  
>>  
>>  
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>   
>>     
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
>   

From shahaf at redfin.com  Fri Aug 22 08:15:32 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Fri Aug 22 08:15:50 2008
Subject: [Slony1-general] Slony over a WAN?
In-Reply-To: <48AED766.7020109@albourne.com>
References: <004a01c903cc$8def4610$14050a0a@dei26g028575><48AE70A4.6010901@albourne.com><082D8A131DF72A4D88C908A1AD3DEB2204A92D81@mail-1.rf.lan>
	<48AED766.7020109@albourne.com>
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB2204A92DA3@mail-1.rf.lan>

Do you use hibernate or some other O-R mapping layer?  Do you cache your data as domain objects or are some Record representation?

--S

-----Original Message-----
From: slony1-general-bounces@lists.slony.info [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Martin Eriksson
Sent: Friday, August 22, 2008 8:13 AM
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] Slony over a WAN?

our cache server is all in-house written specifically for our 
application/database, (as not all data is cached only things likely to 
be access often etc)

but we do use java as language of choice, and the cache server is using 
the sun java rmiregistry

cache server updated as soon as it picks up changes that come in from 
the replication, its not perfect but it works pretty well. worst case 
user will get an error message when trying to write to master db, but 
then it will try to update the cache and by the time the cache is 
updated the write will go through..




Shahaf Abileah wrote:
> Thanks for the great info Martin.
>
> If you don't mind me asking, what technology do you use for your cache?  How do you combine the data in a local site's cache with the (potentially different) data in that local site's slave DB?
>
> Thanks,
>
> --S
>
> -----Original Message-----
> From: slony1-general-bounces@lists.slony.info [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Martin Eriksson
> Sent: Friday, August 22, 2008 12:54 AM
> Cc: slony1-general@lists.slony.info
> Subject: Re: [Slony1-general] Slony over a WAN?
>
> Hi, we do slony over WAN where W = World hehe
>
> Our master Database is in London, then we got one node in Cyprus, one 
> node in San Fransisco, and then one node in Frankfurt and about to add 
> another node in Hong Kong. Our database is only around 8 gigs though.
>
> we got a bit of a special setup in terms of the applications using the 
> dbs as we want our application to read locally but write to the master 
> so we a pretty advanced cache system for handling if Bill is sitting In 
> San Fransisco writes to the db and then look at the data he sees the 
> data he just wrote so does everyone else in that office even though it 
> might not have been replicated to his local db yet. this work pretty 
> good, we never have more then 5-15 seconds delay on slony event across 
> the globe so in worst case if someone tries to change something that 
> already been changed it wont work and they just reset their cache (which 
> takes 2 minutes) and then they continue working.
>
> and we do have an AWFUL line london <-> Cyprus that on average does 
> 40kbytes/s which is horribly low. But it still works 100 times better 
> then having both read and write going to London all the time from across 
> the world..
>
> Of course if the shit hits the fan so to say when we do DLL changes 
> which happens every 3 months for release and we cant recover it will 
> take up to 72 hours to replicate to all nodes (which is not really an 
> option)
>
> but if you got good bandwidth between your nodes its not a problem.
>
> Though if you got a 80 gig db you might want to consider not replicating 
> it across the WAN as it will take quite a long time and might cost your 
> a lot depending on your ISP i guess.. well if you got a gigabit line and 
> dont mind using it then I guess you are fine :) but if you bandwidth is 
> limited you could do as we do sometime,
>
> setup a second db instance on your masternode (assuming node and master 
> will be in the same hardware architecture) do a replication to the local 
> instance once done shut down that instance move the whole /data 
> directory of that instance onto so movable disk, and drive down to the 
> other node load it up, modify path to the node using slony store path 
> and fire it up and let it catch up on the last hour or so of the new data.
>
>  slon processes should run in the same "network context" as the node that
> each is responsible for managing so that the connection to that node is a
> "local" one. 
>      Do not run such links across a WAN.
>
>
>
> Its already been covered but I'll add to it. Yes you should run the slon 
> process on the node in question do not run them all on the master node, 
> not only because it will work better also because when you do have 
> multiple slon process running on one machine it can get VERY confusing 
> figure out which one goes where and which one is having connections 
> open. Life is much easier if one slon process run on each node machine.
>
> good luck!
> Martin
>
>
> Mark Steben wrote:
>   
>> Looking for advice on how to proceed.  We are running Postgres 8.2.5 in Lee,
>> Massachusetts.  We will be installing same in Norfolk Virginia in the not
>> too distant future.  Our need is to replicate roughly 60 - 70 percent
>> Of our 80 GB database in Lee over a WAN to Norfolk for reporting purposes.
>> In reading 'Slony-1 Best Practices' on the Website I came across the
>> following statement:
>>      
>>   slon processes should run in the same "network context" as the node that
>> each is responsible for managing so that the connection to that node is a
>> "local" one. 
>>      Do not run such links across a WAN.
>>
>> Does this still hold true?  If not I would like to hear experiences of
>> anyone engaging in a 'Slony-1 long distance relationship'
>> Any other alternatives to consider?  I do run Slony-1.2.14 in development
>> with everything encompassed in Lee.  However 
>> we will be opening another office 35 miles west in West Springfield that I
>> will be operating out of.  I plan on employing Slony-1
>> To provide replication between these 2 'not so long distance' locations.
>>
>> Any comments welcome.  Thanks
>>
>>
>> Mark Steben?Database Administrator? @utoRevenueT
>> 480 Pleasant Street, Suite B200, Lee, MA 01238 
>> 413-243-4800 x1512 (Phone) ? 413-243-4809 (Fax)
>> A Division of Dominion Enterprises
>>
>>  
>>  
>>  
>>  
>>  
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>   
>>     
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
>   

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


From m.eriksson at albourne.com  Fri Aug 22 09:06:09 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Fri Aug 22 09:06:10 2008
Subject: [Slony1-general] Slony over a WAN?
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB2204A92DA3@mail-1.rf.lan>
References: <004a01c903cc$8def4610$14050a0a@dei26g028575><48AE70A4.6010901@albourne.com><082D8A131DF72A4D88C908A1AD3DEB2204A92D81@mail-1.rf.lan>
	<48AED766.7020109@albourne.com>
	<082D8A131DF72A4D88C908A1AD3DEB2204A92DA3@mail-1.rf.lan>
Message-ID: <48AEE3F1.3080705@albourne.com>

I'm dont know fully how the cache server works as i'm not really a 
developer as such and i've never looked closely at the code, but I know 
that its storing objects and not rows. and no we don't use hibernate.

Shahaf Abileah wrote:
> Do you use hibernate or some other O-R mapping layer?  Do you cache your data as domain objects or are some Record representation?
>
> --S
>
> -----Original Message-----
> From: slony1-general-bounces@lists.slony.info [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Martin Eriksson
> Sent: Friday, August 22, 2008 8:13 AM
> Cc: slony1-general@lists.slony.info
> Subject: Re: [Slony1-general] Slony over a WAN?
>
> our cache server is all in-house written specifically for our 
> application/database, (as not all data is cached only things likely to 
> be access often etc)
>
> but we do use java as language of choice, and the cache server is using 
> the sun java rmiregistry
>
> cache server updated as soon as it picks up changes that come in from 
> the replication, its not perfect but it works pretty well. worst case 
> user will get an error message when trying to write to master db, but 
> then it will try to update the cache and by the time the cache is 
> updated the write will go through..
>
>
>
>
> Shahaf Abileah wrote:
>   
>> Thanks for the great info Martin.
>>
>> If you don't mind me asking, what technology do you use for your cache?  How do you combine the data in a local site's cache with the (potentially different) data in that local site's slave DB?
>>
>> Thanks,
>>
>> --S
>>
>> -----Original Message-----
>> From: slony1-general-bounces@lists.slony.info [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Martin Eriksson
>> Sent: Friday, August 22, 2008 12:54 AM
>> Cc: slony1-general@lists.slony.info
>> Subject: Re: [Slony1-general] Slony over a WAN?
>>
>> Hi, we do slony over WAN where W = World hehe
>>
>> Our master Database is in London, then we got one node in Cyprus, one 
>> node in San Fransisco, and then one node in Frankfurt and about to add 
>> another node in Hong Kong. Our database is only around 8 gigs though.
>>
>> we got a bit of a special setup in terms of the applications using the 
>> dbs as we want our application to read locally but write to the master 
>> so we a pretty advanced cache system for handling if Bill is sitting In 
>> San Fransisco writes to the db and then look at the data he sees the 
>> data he just wrote so does everyone else in that office even though it 
>> might not have been replicated to his local db yet. this work pretty 
>> good, we never have more then 5-15 seconds delay on slony event across 
>> the globe so in worst case if someone tries to change something that 
>> already been changed it wont work and they just reset their cache (which 
>> takes 2 minutes) and then they continue working.
>>
>> and we do have an AWFUL line london <-> Cyprus that on average does 
>> 40kbytes/s which is horribly low. But it still works 100 times better 
>> then having both read and write going to London all the time from across 
>> the world..
>>
>> Of course if the shit hits the fan so to say when we do DLL changes 
>> which happens every 3 months for release and we cant recover it will 
>> take up to 72 hours to replicate to all nodes (which is not really an 
>> option)
>>
>> but if you got good bandwidth between your nodes its not a problem.
>>
>> Though if you got a 80 gig db you might want to consider not replicating 
>> it across the WAN as it will take quite a long time and might cost your 
>> a lot depending on your ISP i guess.. well if you got a gigabit line and 
>> dont mind using it then I guess you are fine :) but if you bandwidth is 
>> limited you could do as we do sometime,
>>
>> setup a second db instance on your masternode (assuming node and master 
>> will be in the same hardware architecture) do a replication to the local 
>> instance once done shut down that instance move the whole /data 
>> directory of that instance onto so movable disk, and drive down to the 
>> other node load it up, modify path to the node using slony store path 
>> and fire it up and let it catch up on the last hour or so of the new data.
>>
>>  slon processes should run in the same "network context" as the node that
>> each is responsible for managing so that the connection to that node is a
>> "local" one. 
>>      Do not run such links across a WAN.
>>
>>
>>
>> Its already been covered but I'll add to it. Yes you should run the slon 
>> process on the node in question do not run them all on the master node, 
>> not only because it will work better also because when you do have 
>> multiple slon process running on one machine it can get VERY confusing 
>> figure out which one goes where and which one is having connections 
>> open. Life is much easier if one slon process run on each node machine.
>>
>> good luck!
>> Martin
>>
>>
>> Mark Steben wrote:
>>   
>>     
>>> Looking for advice on how to proceed.  We are running Postgres 8.2.5 in Lee,
>>> Massachusetts.  We will be installing same in Norfolk Virginia in the not
>>> too distant future.  Our need is to replicate roughly 60 - 70 percent
>>> Of our 80 GB database in Lee over a WAN to Norfolk for reporting purposes.
>>> In reading 'Slony-1 Best Practices' on the Website I came across the
>>> following statement:
>>>      
>>>   slon processes should run in the same "network context" as the node that
>>> each is responsible for managing so that the connection to that node is a
>>> "local" one. 
>>>      Do not run such links across a WAN.
>>>
>>> Does this still hold true?  If not I would like to hear experiences of
>>> anyone engaging in a 'Slony-1 long distance relationship'
>>> Any other alternatives to consider?  I do run Slony-1.2.14 in development
>>> with everything encompassed in Lee.  However 
>>> we will be opening another office 35 miles west in West Springfield that I
>>> will be operating out of.  I plan on employing Slony-1
>>> To provide replication between these 2 'not so long distance' locations.
>>>
>>> Any comments welcome.  Thanks
>>>
>>>
>>> Mark Steben?Database Administrator? @utoRevenueT
>>> 480 Pleasant Street, Suite B200, Lee, MA 01238 
>>> 413-243-4800 x1512 (Phone) ? 413-243-4809 (Fax)
>>> A Division of Dominion Enterprises
>>>
>>>  
>>>  
>>>  
>>>  
>>>  
>>>
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general@lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>   
>>>     
>>>       
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>>
>>   
>>     
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
>   

From Ow.Mun.Heng at wdc.com  Sun Aug 24 21:25:08 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Sun Aug 24 21:25:41 2008
Subject: [Slony1-general] Can I perform CLUSTER on a slony DB?
Message-ID: <1219638308.2765.19.camel@neuromancer.home.net>

Just wondering if it's possible to do a cluster on a slony replicated DB
in either master/slave??


From bpineau at elma.fr  Mon Aug 25 01:28:52 2008
From: bpineau at elma.fr (Benjamin Pineau)
Date: Mon Aug 25 01:29:23 2008
Subject: [Slony1-general] Replication node suddenly lagging,
	CPU bound postmaster
In-Reply-To: <20080820140409.GB72091@crankycanuck.ca>
References: <20080820101520.GA4523@mailer.elma.fr>
	<20080820124628.GA72091@crankycanuck.ca>
	<20080820134140.GA29856@mailer.elma.fr>
	<20080820140409.GB72091@crankycanuck.ca>
Message-ID: <20080825082852.GA23323@mailer.elma.fr>

Thanks for your replies, Andrew and Alan,

On Wed, Aug 20, 2008 at 10:04:09AM -0400, Andrew Sullivan wrote:
> 
> Wju are you running manual vacuums and autovacuum too?  You shouldn't
> need the full vacuum.  Anyway, assuming this is a release after 8.1 (I

Hmm, I'm not sure but I guess it could be a leftover from a previous 
PostgreSQL installation (ie. a pre-autovacuum release). Or maybe it's 
just because, as you said, autovac is not that reliable in 8.1 (yes, 
I use 8.1.10).

> > Thank you for the tip, it does rings a bell (ie. since I had an "almost
> > disk full" situation just before the replication problem, maybe pg may
> > have launched an emergency autovacuum of some sort? I need to explore).
> 
> No, PG won't do that (the emergency autovac happens in 8.3 if you're
> about to roll over your xid space); but it does suggest the table
> bloat I'm supposing, from inadequate vacuuming.
> 
> To solve it, you could do VACUUM FULL and REINDEX.  Just be prepared
> to wait a long while. 

I ran a "VACUUM FULL VERBOSE ANALYSE", and you were right about the bloat
(I had not complaints about fsm though). But this did not solve the
problem : the replication delta continue to increase (this node is now
over 5 days behind master). Well, I didn't dare to run a REINDEX, maybe
I should have. Or could this be a PostgreSQL or Slony-I bug ?


Thanks again.

From ajs at crankycanuck.ca  Mon Aug 25 07:08:26 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon Aug 25 07:08:33 2008
Subject: [Slony1-general] Replication node suddenly lagging, CPU bound
	postmaster
In-Reply-To: <20080825082852.GA23323@mailer.elma.fr>
References: <20080820101520.GA4523@mailer.elma.fr>
	<20080820124628.GA72091@crankycanuck.ca>
	<20080820134140.GA29856@mailer.elma.fr>
	<20080820140409.GB72091@crankycanuck.ca>
	<20080825082852.GA23323@mailer.elma.fr>
Message-ID: <20080825140826.GA7742@crankycanuck.ca>

On Mon, Aug 25, 2008 at 10:28:52AM +0200, Benjamin Pineau wrote:

> I ran a "VACUUM FULL VERBOSE ANALYSE", and you were right about the bloat
> (I had not complaints about fsm though). But this did not solve the
> problem : the replication delta continue to increase (this node is now
> over 5 days behind master). Well, I didn't dare to run a REINDEX, maybe
> I should have. Or could this be a PostgreSQL or Slony-I bug ?

No, this is a problem in your node.  Did you stop the slon when you
did VACUUM FULL?  (I assume so, since it apparently finished.)  At
this point, I'd drop the node from replication, rebuild it with
REINDEX or just dump and reload, and then subscribe it back fresh.
Check your FSM settings.

A

-- 
Andrew Sullivan
ajs@commandprompt.com
+1 503 667 4564 x104
http://www.commandprompt.com/
From bnichols at ca.afilias.info  Mon Aug 25 07:09:31 2008
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Mon Aug 25 07:09:36 2008
Subject: [Slony1-general] Can I perform CLUSTER on a slony DB?
In-Reply-To: <1219638308.2765.19.camel@neuromancer.home.net>
References: <1219638308.2765.19.camel@neuromancer.home.net>
Message-ID: <1219673371.6179.61.camel@bnicholson-desktop>

On Mon, 2008-08-25 at 12:25 +0800, Ow Mun Heng wrote:
> Just wondering if it's possible to do a cluster on a slony replicated DB
> in either master/slave??

You can, with a couple of caveats.

1: Cluster will take an access exclusive lock the table in question and
replication will fall behind for the duration of the cluster if anything
tries to write to the table be being clustered.

2: Prior to 8.3, cluster is not MVCC safe, so it is very possible to
break things by clustering.

-- 
?Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.

From dlambert at bmtcarhaul.com  Mon Aug 25 13:30:59 2008
From: dlambert at bmtcarhaul.com (David Lee Lambert)
Date: Mon Aug 25 13:31:16 2008
Subject: [Slony1-general] Building against multiple Postgres versions
Message-ID: <200808251631.00519.dlambert@bmtcarhaul.com>

We're thinking about using Slony to replicate from a production server running 
Postgres 8.0 (installed from the Gentoo package) to a read-only server 
running Postgres 8.2 (based on Ubuntu, I'm not sure exactly what version; 
it's actually a VMware virtual appliance).  Based on my reading so far,  
there shouldn't be a big problem with this;  one use-case mentioned in the 
documentation is a minimal-downtime upgrade to a newer server.

However,  I'm running into trouble getting it it work on my test workstation,  
Ubuntu Hardy with an 8.0 instance and an 8.2 instance installed from Ubuntu 
packages (8.0 is from "dapper-backports").  There's a "postgresql-8.3-slony1" 
package containing the "xxid.so" and "slony1_funcs.so", but of course those 
won't work with older servers because of API changes.  I decided to build 
those files only from Slony source.

My first problem was that the "./configure" script was claiming that the 
Postgres version was 8.3, even though the 8.3 server was not currently 
installed.  I ended up editing "config.h" and invoking gcc manually to build 
the two .so files.

The next problem,  on which I'm stuck, seems to be an error in the way SLony 
is referring to a sequence:

<stdin>:7: PGRES_FATAL_ERROR 
select "_davidswaybill".initializeLocalNode(1, 'koios'); 
select "_davidswaybill".enableNode(1);  - ERROR:  function setval("unknown", 
bigint) does not exist
HINT:  No function matches the given name and argument types. You may need to 
add explicit type casts.
CONTEXT:  SQL statement "SELECT  setval('"_davidswaybill".sl_rowid_seq',  
$1 ::int8 * '1000000000000000'::int8)"
PL/pgSQL function "initializelocalnode" line 26 at perform
<stdin>:7: ERROR: no admin conninfo for node 134598976

Has anyone seen this kind of error before?  Any idea how to fix it?


-- 

David L. Lambert
  Software Developer,  Precision Motor Transport Group, LLC
  Work phone 517-349-3011 x215
  Cell phone 586-873-8813
From vostorga at gmail.com  Mon Aug 25 16:28:27 2008
From: vostorga at gmail.com (=?ISO-8859-1?Q?V=EDctor?=)
Date: Mon Aug 25 16:28:45 2008
Subject: [Slony1-general] Building against multiple Postgres versions
In-Reply-To: <200808251631.00519.dlambert@bmtcarhaul.com>
References: <200808251631.00519.dlambert@bmtcarhaul.com>
Message-ID: <2b391080808251628y2dc4de49o8a0f5c739db08f63@mail.gmail.com>

The full ./configure output will be a good starting point to solve your
problem.

On Mon, Aug 25, 2008 at 2:30 PM, David Lee Lambert
<dlambert@bmtcarhaul.com>wrote:

>
> My first problem was that the "./configure" script was claiming that the
> Postgres version was 8.3, even though the 8.3 server was not currently
> installed.




V=EDctor Ostorga
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080825/=
55149e5e/attachment.htm
From phoenix.kiula at gmail.com  Mon Aug 25 23:39:16 2008
From: phoenix.kiula at gmail.com (Phoenix Kiula)
Date: Mon Aug 25 23:39:46 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
Message-ID: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>

HI

On the PG-General list, I was recommended to look at Slony as I wish
to upgrade from PG 8.2.3 to the latest 8.3.3 without significant
downtime.

I have a very typical CentOS 4.4 Linux server with Cpanel/WHM.
Postgresql is used to work with Apache and PHP n a very busy site.

I am looking for simple instructions to

1. Install Slony

2. Install PG 8.3.3, and make sure it is working with same settings in
"pg_hba.conf" and "postgresql.conf" as my current working db

3. Replicate my current working db of PG 8.2.3 to this new installed
8.3.3 using Slony? (I am not familiar with master and slave stuff, so
simple instructions would be nice)

4. When all data is replicated and working, just switch the master and
slave, so that the new 8.3.3 becomes the main database

Are there any simple straightforward instructions for this that don't
assume that I'm a super techsavvy DBA with 300 years of experience?

Thanks! for any advice or pointers.
From m.eriksson at albourne.com  Tue Aug 26 00:02:33 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Tue Aug 26 00:02:48 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
Message-ID: <48B3AA89.9010708@albourne.com>

Hi,

slony can do that for you yes, where you have a maximum downtime of 
maybe 2-6 minutes. (if you want to keep new database on same machine and 
on same port)

I'm currently doing this migration though from 8.2.4 -> 8.3.3 though im 
doing it on 5 nodes..

pg_hba.conf is more or less the same between version so no problem 
there, for the postgresql.conf there are some changes.. so that you will 
need to investigate a bit more, you cant just copy over the 
postgresql.conf (the pg-General should know more on this)

I've written a rather detailed document on how to do migration using 
slony, though it does contain some company specific steps so I can not 
just put it out there, i would need to clean it up a bit first. Will see 
if i got the time time to do it today or tomorrow.

but if you never heard of slony I would recommend read up a bit on it 
first to understand the fundamentals.

regards
Martin


Phoenix Kiula wrote:
> HI
>
> On the PG-General list, I was recommended to look at Slony as I wish
> to upgrade from PG 8.2.3 to the latest 8.3.3 without significant
> downtime.
>
> I have a very typical CentOS 4.4 Linux server with Cpanel/WHM.
> Postgresql is used to work with Apache and PHP n a very busy site.
>
> I am looking for simple instructions to
>
> 1. Install Slony
>
> 2. Install PG 8.3.3, and make sure it is working with same settings in
> "pg_hba.conf" and "postgresql.conf" as my current working db
>
> 3. Replicate my current working db of PG 8.2.3 to this new installed
> 8.3.3 using Slony? (I am not familiar with master and slave stuff, so
> simple instructions would be nice)
>
> 4. When all data is replicated and working, just switch the master and
> slave, so that the new 8.3.3 becomes the main database
>
> Are there any simple straightforward instructions for this that don't
> assume that I'm a super techsavvy DBA with 300 years of experience?
>
> Thanks! for any advice or pointers.
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>   

From rod at iol.ie  Tue Aug 26 04:07:20 2008
From: rod at iol.ie (Raymond O'Donnell)
Date: Tue Aug 26 04:07:57 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
Message-ID: <48B3E3E8.9090003@iol.ie>

On 26/08/2008 07:39, Phoenix Kiula wrote:

> I am looking for simple instructions to
> 
> 1. Install Slony
> 
> 2. Install PG 8.3.3, and make sure it is working with same settings in
> "pg_hba.conf" and "postgresql.conf" as my current working db
> 
> 3. Replicate my current working db of PG 8.2.3 to this new installed
> 8.3.3 using Slony? (I am not familiar with master and slave stuff, so
> simple instructions would be nice)

You may have found this already, but I found the following very helpful,
particularly the sample slonik scripts:

http://www.slony.info/documentation/firstdb.html

Ray.


------------------------------------------------------------------
Raymond O'Donnell, Director of Music, Galway Cathedral, Ireland
rod@iol.ie
Galway Cathedral Recitals: http://www.galwaycathedral.org/recitals
------------------------------------------------------------------
From phoenix.kiula at gmail.com  Tue Aug 26 04:36:35 2008
From: phoenix.kiula at gmail.com (Phoenix Kiula)
Date: Tue Aug 26 04:37:13 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <48B3E3E8.9090003@iol.ie>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
	<48B3E3E8.9090003@iol.ie>
Message-ID: <e373d31e0808260436t7120cd02rb7bbcba3210f1218@mail.gmail.com>

On 8/26/08, Raymond O'Donnell <rod@iol.ie> wrote:
>
> You may have found this already, but I found the following very helpful,
>  particularly the sample slonik scripts:
>
>  http://www.slony.info/documentation/firstdb.html



Thank you. But the very beginning presumptions are showstoppers:

"You have enabled access in your cluster(s) via pg_hba.conf"

How to enable clusters in pg_hba.conf? What the heck are clusters? So
I have to read up a whole new bunch of docs. This is the problem with
the PG world--a wild labyrinth of reading materials.

How difficult would it have been to show a demo pg_hba.conf in this
page's example? This is my pg_hba.conf currently:


---
local   all         all                                              md5
host    all         all         127.0.0.1          255.255.255.255   md5
---


Where in there should I specify clusters?

Is there any clearer tutorial out there? This tutorial presumes I have
"master" and "slave" working. I.e., I have two databases out there. I
currently have only one PG database running with the "postgres" user
in charge. I have however installed Slony 1.2. But don't know what to
do next.

Any advice or pointers will be great. Thanks!
From phoenix.kiula at gmail.com  Tue Aug 26 04:45:25 2008
From: phoenix.kiula at gmail.com (Phoenix Kiula)
Date: Tue Aug 26 04:46:01 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <48B3AA89.9010708@albourne.com>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
	<48B3AA89.9010708@albourne.com>
Message-ID: <e373d31e0808260445m23996febs67164b5f1a72417@mail.gmail.com>

On 8/26/08, Martin Eriksson <m.eriksson@albourne.com> wrote:

>  I've written a rather detailed document on how to do migration using slony,
> though it does contain some company specific steps so I can not just put it
> out there, i would need to clean it up a bit first. Will see if i got the
> time time to do it today or tomorrow.


Thanks Martin. I would really appreciate that. All I need is one
database with 10 tables, some RULES, and 5 functions. That's it. But
the data size is close to about 30GB already as the tables are from a
very busy website, which is why I am bothering with Slony to begin
with, as pg_dump etc takes a LOT of time.

You can remove your corporate information. All I am looking for is
some simple steps to:

1. Install Slony
2. Install new PG on same server (localhost) with 8.3.3 version
3. Replicate all structure and data, everything, from old DB to new DB
4. Once it is all working, switch old DB to new DB

Between step 3 and 4, if there is any way in which I can check if the
new DB is also working with my Apache and PHP, that would be nice.

Thanks!
From dlambert at bmtcarhaul.com  Tue Aug 26 05:23:33 2008
From: dlambert at bmtcarhaul.com (David Lee Lambert)
Date: Tue Aug 26 05:23:37 2008
Subject: [Slony1-general] Building against multiple Postgres versions
In-Reply-To: <2b391080808251628y2dc4de49o8a0f5c739db08f63@mail.gmail.com>
References: <200808251631.00519.dlambert@bmtcarhaul.com>
	<2b391080808251628y2dc4de49o8a0f5c739db08f63@mail.gmail.com>
Message-ID: <200808260823.33573.dlambert@bmtcarhaul.com>
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From glynastill at yahoo.co.uk  Tue Aug 26 06:30:52 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue Aug 26 06:31:00 2008
Subject: [Slony1-general] PgPoolII and slony
Message-ID: <363109.41253.qm@web25805.mail.ukl.yahoo.com>

Hi people,

I did post this to the pgpool list but it seems to be pretty dead, so sorry for the slightly off topic post on the slony lists, but I thought this was probably the best place. Is anyone on here using Pgpool in the "Master/Slave Mode" that the docs mention?

My original message that went to the pgpool list is below.


We have a setup using Slony-I that currently consists of 1 origin server with 2 subscribers.  I'd like to use pgpool to manage connections in Master/Slave Mode, however looking at the docs (http://pgpool.projects.postgresql.org/) theres not that much info on this.

The docs state:

"This mode is for using pgpool-II with another master/slave replication software (like Slony-I), so it really does the actual data replication. DB nodes' information must be set as the replication mode. In addtion to that, set master_slave_mode and load_balance_mode to true."

And I'm afraid I just can't get my head around exactly what I'm supposed to do from that, I could do with a little more verbose instruction.

I've tried pgpool, and just set it up in Connection Pool Mode and it works well.  I guess we could just use it in this mode, then in the case we move our set or failover we just repoint the pool.  However, the Master/Slave Mode sounds very interesting.

Is there anywhere I can get better info on setting up this mode?

Thanks
Glyn

Send instant messages to your online friends http://uk.messenger.yahoo.com 
From ajs at crankycanuck.ca  Tue Aug 26 06:43:22 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue Aug 26 06:43:28 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
Message-ID: <20080826134322.GA10943@crankycanuck.ca>

On Tue, Aug 26, 2008 at 02:39:16PM +0800, Phoenix Kiula wrote:
> 
> 1. Install Slony

Follow the instructions available from slony.info.  They're not short,
but they're complete.  You will need to be patient and read the
documentation.  Slony is a complicated beast with lots of pointy
edges.  Sorry.
 
> 2. Install PG 8.3.3, and make sure it is working with same settings in
> "pg_hba.conf" and "postgresql.conf" as my current working db

It can't have the same settings exactly, because the 8.3 and 8.2
releases are different, and therefore need different tuning.  Install 8.3
on your machine and run it on a different port.  If you are running
a Debian-based distribution of Linux, you can install from packages.
Otherwise, you'll need to do this from source, because most other
distributions make it hard to run multiple versions.  How to configure
the pg_hba.conf is explained in painful detail in the postgres manual,
and you'll have to read how to do it there.
 
> 3. Replicate my current working db of PG 8.2.3 to this new installed
> 8.3.3 using Slony? (I am not familiar with master and slave stuff, so
> simple instructions would be nice)

Sorry, you _need_ to read the introductory material in Slony's manual.
The "simple instructions" you seek won't help you.  It's just not a
simple system, because this isn't all it was designed to do.
 
> 4. When all data is replicated and working, just switch the master and
> slave, so that the new 8.3.3 becomes the main database

This is easy, in Slony terms, but still puzzling if you know nothing
about Slony.  You use a slonik script to perform a switchover,
pointing the origin of all data sets from the 8.2 installation to the
8.3 installation.  This has the additional benefit that the 8.2 system
becomes a replica, so that if things blow up after you come up on 8.3
(you tested 8.3, right?  There are big changes in 8.3, and things that
used to work sometimes have stopped working ever in order to solve a
class of bugs that were obscured by the "sometimes"), you can switch
back (i.e. you get a "rollback plan").
 
> Are there any simple straightforward instructions for this that don't
> assume that I'm a super techsavvy DBA with 300 years of experience?

Yes, but there aren't any that don't require you read a lot of
material and understand it.  You have to become a reasonably competent
Postgres dba.  If you want something as simple as a filesystem, you
need not to use Postgres.

If this is too difficult or time-consuming for you, you could pay
someone (my employer, for instance) to help you with this.

A

-- 
Andrew Sullivan
ajs@commandprompt.com
+1 503 667 4564 x104
http://www.commandprompt.com/
From ajs at crankycanuck.ca  Tue Aug 26 06:45:57 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue Aug 26 06:46:03 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <e373d31e0808260436t7120cd02rb7bbcba3210f1218@mail.gmail.com>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
	<48B3E3E8.9090003@iol.ie>
	<e373d31e0808260436t7120cd02rb7bbcba3210f1218@mail.gmail.com>
Message-ID: <20080826134556.GB10943@crankycanuck.ca>

On Tue, Aug 26, 2008 at 07:36:35PM +0800, Phoenix Kiula wrote:

> Thank you. But the very beginning presumptions are showstoppers:
> 
> "You have enabled access in your cluster(s) via pg_hba.conf"
> 
> How to enable clusters in pg_hba.conf? What the heck are clusters? 

http://slony.info/documentation/concepts.html

I know it sucks to get "RTFM", but actually the Slony manual, while
verbose, is fairly complete.  Unfortunately, you need to read it in
order to operate the software.  

A

-- 
Andrew Sullivan
ajs@commandprompt.com
+1 503 667 4564 x104
http://www.commandprompt.com/
From ajs at crankycanuck.ca  Tue Aug 26 07:06:23 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue Aug 26 07:06:29 2008
Subject: [Slony1-general] PgPoolII and slony
In-Reply-To: <363109.41253.qm@web25805.mail.ukl.yahoo.com>
References: <363109.41253.qm@web25805.mail.ukl.yahoo.com>
Message-ID: <20080826140622.GC10943@crankycanuck.ca>

On Tue, Aug 26, 2008 at 01:30:52PM +0000, Glyn Astill wrote:
> The docs state:
> 
> "This mode is for using pgpool-II with another master/slave replication software (like Slony-I), so it really does the actual data replication. DB nodes' information must be set as the replication mode. In addtion to that, set master_slave_mode and load_balance_mode to true."
> 
> And I'm afraid I just can't get my head around exactly what I'm supposed to do from that, I could do with a little more verbose instruction.

Follow the instructions (I know, they're a little terse) to set the
pool software up in replication mode.  Then, turn on master_slave_mode
and load_balance_mode.  That's how it's supposed to work, anyway.

A


-- 
Andrew Sullivan
ajs@commandprompt.com
+1 503 667 4564 x104
http://www.commandprompt.com/
From JanWieck at Yahoo.com  Tue Aug 26 08:01:26 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Tue Aug 26 08:02:37 2008
Subject: [Slony1-general] Dropped Master Cluster
In-Reply-To: <48ABE80F.5070805@albourne.com>
References: <46d2bc8f0808191024y3c759ef3m7a57dac7eb765c1a@mail.gmail.com>		<48ABDB11.9040608@albourne.com>	<46d2bc8f0808200243n40bc9b5aldc4a8d07a749e6a4@mail.gmail.com>
	<48ABE80F.5070805@albourne.com>
Message-ID: <48B41AC6.3080107@Yahoo.com>

On 8/20/2008 5:46 AM, Martin Eriksson wrote:
> Yes it will,
> 
> the cascade will remove ALL slony triggers.
> 
> I do it all the time..

Note that before Slony 2.0 this is a perfect recipe for disaster as this 
method does not restore the system catalog to a sane state in case Slony 
had disabled user triggers, foreign key constraints or rewrite rules.

The recommended way to remove Slony from a subscriber is to use the 
slonik command UNINSTALL NODE.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From richard.broersma at gmail.com  Tue Aug 26 08:08:07 2008
From: richard.broersma at gmail.com (Richard Broersma)
Date: Tue Aug 26 08:08:14 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <e373d31e0808260436t7120cd02rb7bbcba3210f1218@mail.gmail.com>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
	<48B3E3E8.9090003@iol.ie>
	<e373d31e0808260436t7120cd02rb7bbcba3210f1218@mail.gmail.com>
Message-ID: <396486430808260808y38fc4f84v98906651d9904f4f@mail.gmail.com>

On Tue, Aug 26, 2008 at 4:36 AM, Phoenix Kiula <phoenix.kiula@gmail.com> wrote:

> What the heck are clusters?

This artile should answer your question:
http://www.postgresql.org/docs/8.3/interactive/creating-cluster.html


-- 
Regards,
Richard Broersma Jr.

Visit the Los Angeles PostgreSQL Users Group (LAPUG)
http://pugs.postgresql.org/lapug
From ajs at crankycanuck.ca  Tue Aug 26 08:10:24 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue Aug 26 08:10:32 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <396486430808260808y38fc4f84v98906651d9904f4f@mail.gmail.com>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
	<48B3E3E8.9090003@iol.ie>
	<e373d31e0808260436t7120cd02rb7bbcba3210f1218@mail.gmail.com>
	<396486430808260808y38fc4f84v98906651d9904f4f@mail.gmail.com>
Message-ID: <20080826151024.GE10943@crankycanuck.ca>

On Tue, Aug 26, 2008 at 08:08:07AM -0700, Richard Broersma wrote:
> On Tue, Aug 26, 2008 at 4:36 AM, Phoenix Kiula <phoenix.kiula@gmail.com> wrote:
> 
> > What the heck are clusters?
> 
> This artile should answer your question:
> http://www.postgresql.org/docs/8.3/interactive/creating-cluster.html

Only sort of, which is why I pointed to the Slony docs as well.
Unhappily, the term "cluster" is overloaded in the database world :(

A

-- 
Andrew Sullivan
ajs@commandprompt.com
+1 503 667 4564 x104
http://www.commandprompt.com/
From vostorga at gmail.com  Tue Aug 26 08:12:31 2008
From: vostorga at gmail.com (=?ISO-8859-1?Q?V=EDctor?=)
Date: Tue Aug 26 08:12:38 2008
Subject: [Slony1-general] Building against multiple Postgres versions
In-Reply-To: <200808260823.33573.dlambert@bmtcarhaul.com>
References: <200808251631.00519.dlambert@bmtcarhaul.com>
	<2b391080808251628y2dc4de49o8a0f5c739db08f63@mail.gmail.com>
	<200808260823.33573.dlambert@bmtcarhaul.com>
Message-ID: <2b391080808260812q5d244e7dwcd06a103c626d68e@mail.gmail.com>

Watching the following error in the output of make:

slon.c:33:22: error: libpq-fe.h: No such file or directory


You should install packages libpq-dev and postgresql-server-dev in your
ubuntu workstation. If the same error occurs in your Gentoo box you should
emerge dev-db/libpq.

Regards,

-- =

V=EDctor Ostorga
http://vostorga.org
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080826/=
13d1ebbc/attachment.htm
From richard.broersma at gmail.com  Tue Aug 26 08:52:11 2008
From: richard.broersma at gmail.com (Richard Broersma)
Date: Tue Aug 26 08:52:20 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <48B3AA89.9010708@albourne.com>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
	<48B3AA89.9010708@albourne.com>
Message-ID: <396486430808260852p521d76d5n98f9e319948edc21@mail.gmail.com>

On Tue, Aug 26, 2008 at 12:02 AM, Martin Eriksson
<m.eriksson@albourne.com> wrote:

> I've written a rather detailed document on how to do migration using slony,
> though it does contain some company specific steps so I can not just put it
> out there, i would need to clean it up a bit first. Will see if i got the
> time time to do it today or tomorrow.

Would you be willing to post this on wiki.postgresql.org as a guide
for major version upgrade "Hot-Cut-Over"?


-- 
Regards,
Richard Broersma Jr.

Visit the Los Angeles PostgreSQL Users Group (LAPUG)
http://pugs.postgresql.org/lapug
From phoenix.kiula at gmail.com  Tue Aug 26 10:37:22 2008
From: phoenix.kiula at gmail.com (Phoenix Kiula)
Date: Tue Aug 26 10:37:32 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <20080826134556.GB10943@crankycanuck.ca>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
	<48B3E3E8.9090003@iol.ie>
	<e373d31e0808260436t7120cd02rb7bbcba3210f1218@mail.gmail.com>
	<20080826134556.GB10943@crankycanuck.ca>
Message-ID: <e373d31e0808261037s6d5a0a86t2311ac844c11f61d@mail.gmail.com>

On 8/26/08, Andrew Sullivan <ajs@crankycanuck.ca> wrote:
>
>  I know it sucks to get "RTFM", but actually the Slony manual, while
>  verbose, is fairly complete.  Unfortunately, you need to read it in
>  order to operate the software.



It does not suck to get an RTFM if the M is written in an intelligible
manner. The Slony documentation is horribly incomplete for people who
are new to the system. The purpose of documentation is not to have
people sit around for a month to figure out what works where how.

A simple scenario -- typical one! -- would go a long way in making
Slony docs useful. I have been saying this throughout this thread:

1. Install Slony (Done. This was okay).

2. Install new instance of PG. Take a typical PG install. Include
thoughts on installing in a NEW DIRECTORY so that it doesn't conflict
with existing PG.

3. Do the master/slave stuff. I find it hard to believe that there is
NO default situation. There has to be a default config, right? Even if
the performance is bad or whatever, it at least serves as a starting
point. This would illustrate whatever is needed for setting up
"clusters" (a fun feast of reading) and whatnot.

4. When the new install is working, how to switch between new and old.

5. How to delete old.

The "concepts" page tells me nothing. I have read it, and the ENTIRE
slony documentation, plus some google pages here and there. Not a
single thing that explains how to do stuff.

I am not interested in being a Slony expert or a fabulous DBA. I just
want something that does the job of letting me upgrade a DB. (I am not
looking for regular replication, but I was forwarded from the
PG-GENERAL list to come here and ask how to use Slony to perform the
anal experience that is the PG upgrade. I am not impressed so far!)

Thanks for sharing your thoughts anyway. I'd appreciate some specific
help from others.
From salmanb at quietcaresystems.com  Tue Aug 26 10:56:10 2008
From: salmanb at quietcaresystems.com (salman)
Date: Tue Aug 26 10:56:21 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <e373d31e0808261037s6d5a0a86t2311ac844c11f61d@mail.gmail.com>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>	<48B3E3E8.9090003@iol.ie>	<e373d31e0808260436t7120cd02rb7bbcba3210f1218@mail.gmail.com>	<20080826134556.GB10943@crankycanuck.ca>
	<e373d31e0808261037s6d5a0a86t2311ac844c11f61d@mail.gmail.com>
Message-ID: <48B443BA.7000502@quietcaresystems.com>


> It does not suck to get an RTFM if the M is written in an intelligible
> manner. The Slony documentation is horribly incomplete for people who
> are new to the system. The purpose of documentation is not to have
> people sit around for a month to figure out what works where how.
> 

Not every project is going to be able to provide you an entry level 
how-to and full support. If that is what you require, then perhaps a 
commercial product is more to your liking.

> A simple scenario -- typical one! -- would go a long way in making
> Slony docs useful. I have been saying this throughout this thread:
> 
> 1. Install Slony (Done. This was okay).
> 
> 2. Install new instance of PG. Take a typical PG install. Include
> thoughts on installing in a NEW DIRECTORY so that it doesn't conflict
> with existing PG.
> 
> 3. Do the master/slave stuff. I find it hard to believe that there is
> NO default situation. There has to be a default config, right? Even if
> the performance is bad or whatever, it at least serves as a starting
> point. This would illustrate whatever is needed for setting up
> "clusters" (a fun feast of reading) and whatnot.
> 
> 4. When the new install is working, how to switch between new and old.
> 
> 5. How to delete old.
> 

This all *is* in the Slony documentation but you are also correct when 
you say that it's not 'simple'. The manual assumes familiarity with 
certain concepts. However, it is not too difficult to figure out what to 
do with some careful reading and not simply browsing for steps.

> The "concepts" page tells me nothing. I have read it, and the ENTIRE
> slony documentation, plus some google pages here and there. Not a
> single thing that explains how to do stuff.
> 
> I am not interested in being a Slony expert or a fabulous DBA. I just
> want something that does the job of letting me upgrade a DB. (I am not
> looking for regular replication, but I was forwarded from the
> PG-GENERAL list to come here and ask how to use Slony to perform the
> anal experience that is the PG upgrade. I am not impressed so far!)
> 

(Assuming you already have slony installed and configured properly)
1) Install the new version of postgres on your machine.
2) Create the appropriate users and databases in the new instance.
3) Dump schema for your database from the old PG instance, into the new 
instance using pg_dump
4) Add new PG instance to your slony master and create the appropriate 
sets, subscriptions, etc.
5) Wait for the data to sync
6) Stop all write activity to the old database and switch master and 
slave roles between your two PG instances
7) Shutoff old PG and put the new instance up on the same port/IP as the 
old one

I believe that pretty much covers all the steps.

-salman
From ajs at crankycanuck.ca  Tue Aug 26 11:15:39 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue Aug 26 11:15:52 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <e373d31e0808261037s6d5a0a86t2311ac844c11f61d@mail.gmail.com>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
	<48B3E3E8.9090003@iol.ie>
	<e373d31e0808260436t7120cd02rb7bbcba3210f1218@mail.gmail.com>
	<20080826134556.GB10943@crankycanuck.ca>
	<e373d31e0808261037s6d5a0a86t2311ac844c11f61d@mail.gmail.com>
Message-ID: <20080826181539.GA12248@crankycanuck.ca>

On Wed, Aug 27, 2008 at 01:37:22AM +0800, Phoenix Kiula wrote:

> A simple scenario -- typical one! -- would go a long way in making
                                     ^^^^^^^^^

There _is_ a typical scenario in there.  There's a significant
tutorial at the beginning.  Did you do it?

> 2. Install new instance of PG. Take a typical PG install. Include
> thoughts on installing in a NEW DIRECTORY so that it doesn't conflict
> with existing PG.

This is perfectly clear in the Postgres documentation.  Slony is an
add-on to Postgres.  Not a replacement.  You need to have two Postgres
instances.  Why should the Slony docs reproduce how to do that?  If
you need Slony, you already have some Postgresses around, it's
assumed.  I don't think that assumption strains credulity.
 
> 3. Do the master/slave stuff. I find it hard to believe that there is
> NO default situation. There has to be a default config, right? Even if

No, there doesn't, and there isn't, and that's exactly what everyone
is trying to tell you.  Even the term "master/slave" is wrong.  This
is why I said you need to go read the manual, long though it may be.
Alternatively, as I suggested in another thread, you might want to
look at Londiste, which has many fewer features precisely because
those people found Slony's surfeit of features too complicated.  As,
apparently, you do.  I understand that: I often find it too
complicated, too, and I was around from the moment the thing was
imagined.  

> The "concepts" page tells me nothing. I have read it, and the ENTIRE
> slony documentation, plus some google pages here and there. Not a
> single thing that explains how to do stuff.

It is entirely possible that you have run your eyes over the ENTIRE
Slony documentation, but I find it impossible to believe you "read" it
(in the sense of having understood all the words), but be able to
claim it contains "not a single thing that explains how to do stuff".
I have some issues with the way the documents are, too.  But how to
set up and run your first slony instance certainly is in there.  I
have personally run through the steps, and I am sure they work.  That
manual does explain how to do stuff.

It doesn't tell you what to do for your particular case, because it's
a general purpose tool.  You have the tools to build a house, and all
you want is to fix your faucet.  I get that.  The point we've been
trying to make is that you have two choices:

1.  Learn how to do enough home repair with the tools you have that
you can fix the faucet.

2.  Call a plumber.  (I've also noted that I am in fact such a
plumber, and if you want such help, you can call Command Prompt or
other companies who specialise in supporting cases like yours.  Somone
else pointed you to a community-supplied list of such companies.  Take
your pick.)
 
> looking for regular replication, but I was forwarded from the
> PG-GENERAL list to come here and ask how to use Slony to perform the
> anal experience that is the PG upgrade. I am not impressed so far!)

I think if you'd continued to read in that thread, you'd have noticed
that I actually suggested that learning Slony might be a big deal for
someone like you, who was surprised that dump and restore was the
standard Postgres upgrade approach.  (I confess that I find it more
than a little depressing that people who can't afford 8 hours of
outage don't know about these problems before they run into them in
production, but I guess we get the Internet we deserve.)

A

-- 
Andrew Sullivan
ajs@commandprompt.com
+1 503 667 4564 x104
http://www.commandprompt.com/
From richard.broersma at gmail.com  Tue Aug 26 12:28:01 2008
From: richard.broersma at gmail.com (Richard Broersma)
Date: Tue Aug 26 12:28:14 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <20080826181539.GA12248@crankycanuck.ca>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
	<48B3E3E8.9090003@iol.ie>
	<e373d31e0808260436t7120cd02rb7bbcba3210f1218@mail.gmail.com>
	<20080826134556.GB10943@crankycanuck.ca>
	<e373d31e0808261037s6d5a0a86t2311ac844c11f61d@mail.gmail.com>
	<20080826181539.GA12248@crankycanuck.ca>
Message-ID: <396486430808261228n33312984k7b96a56d384cef61@mail.gmail.com>

On Tue, Aug 26, 2008 at 11:15 AM, Andrew Sullivan <ajs@crankycanuck.ca> wrote:

> 1.  Learn how to do enough home repair with the tools you have that
> you can fix the faucet.
>
> 2.  Call a plumber.  (I've also noted that I am in fact such a
> plumber, and if you want such help, you can call Command Prompt or
> other companies who specialise in supporting cases like yours.  Somone
> else pointed you to a community-supplied list of such companies.  Take
> your pick.)

Another option is "nearly do nothing", keep version 8.2 only apply 8.2
updates.  This will keep the downtime minimal since no dump and
restore is needed.  And assuming that 8.2 will be supported from at
least the next 2 - 4 years, there is plenty of time to learn and plan
a version change for the future.


-- 
Regards,
Richard Broersma Jr.

Visit the Los Angeles PostgreSQL Users Group (LAPUG)
http://pugs.postgresql.org/lapug
From ajs at crankycanuck.ca  Tue Aug 26 12:57:09 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue Aug 26 12:57:23 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <396486430808261228n33312984k7b96a56d384cef61@mail.gmail.com>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
	<48B3E3E8.9090003@iol.ie>
	<e373d31e0808260436t7120cd02rb7bbcba3210f1218@mail.gmail.com>
	<20080826134556.GB10943@crankycanuck.ca>
	<e373d31e0808261037s6d5a0a86t2311ac844c11f61d@mail.gmail.com>
	<20080826181539.GA12248@crankycanuck.ca>
	<396486430808261228n33312984k7b96a56d384cef61@mail.gmail.com>
Message-ID: <20080826195708.GD12248@crankycanuck.ca>

On Tue, Aug 26, 2008 at 12:28:01PM -0700, Richard Broersma wrote:

> Another option is "nearly do nothing", keep version 8.2 only apply 8.2
> updates. 

This is an excellent point for another reason: if you are unfamiliar
with Postgres, I have a sinking feeling that you won't have performed
adequate tests on your application to be sure it still works with
8.3.  It is stricter about some things, and even the Postgres project
got bitten by this one time.  Failing to do those tests will make your
upgrade path painful, particularly if you discover the incompatibility
after running in 8.3 for several hours or days.

A
-- 
Andrew Sullivan
ajs@commandprompt.com
+1 503 667 4564 x104
http://www.commandprompt.com/
From troy at troywolf.com  Tue Aug 26 14:02:19 2008
From: troy at troywolf.com (Troy Wolf)
Date: Tue Aug 26 14:02:33 2008
Subject: [Slony1-general] Solution: table is an index corruption
Message-ID: <e0d7c3f50808261402s14f9f402q27eb3f4bd4eaba2b@mail.gmail.com>

Postgres 8.2
Slony 1.2.12
Two databases (one origin, one subscriber)
One replication set

We have a table (let's call it 'Foo') that has a foreign key relationship to
a column in another table (let's call it 'Bar'). Bar has a PRIMARY KEY
CONSTRAINT on this column (let's call it bar_pk).

Foo is not replicated. Bar is replicated. Foo is a table we no longer
use--this is why we never added it to the replication set.

Today, we dropped Foo from the origin database. No problem. When we tried to
drop Foo from the subscriber database, we got this error:

  bar_pk is an index

Lesson, although you may think it is safe to drop a non-replicated table
without using Slony's EXECUTE SCRIPT, you'd be wrong if that table has a
foreign-key relationship to a table that is replicated. If you find yourself
in this postion, you can use EXECUTE SCRIPT to drop the table from the
subscriber database by using the "EXECUTE ONLY ON" parameter of EXECUTE
SCRIPT.

EXECUTE SCRIPT
http://www.slony.info/documentation/stmtddlscript.html
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080826/=
708433ab/attachment.htm
From dlambert at bmtcarhaul.com  Tue Aug 26 14:25:39 2008
From: dlambert at bmtcarhaul.com (David Lee Lambert)
Date: Tue Aug 26 14:25:54 2008
Subject: [Slony1-general] Building against multiple Postgres versions
In-Reply-To: <2b391080808260812q5d244e7dwcd06a103c626d68e@mail.gmail.com>
References: <200808251631.00519.dlambert@bmtcarhaul.com>
	<200808260823.33573.dlambert@bmtcarhaul.com>
	<2b391080808260812q5d244e7dwcd06a103c626d68e@mail.gmail.com>
Message-ID: <200808261725.39552.dlambert@bmtcarhaul.com>

On Tuesday 26 August 2008 11:12:31 am V?ctor wrote:
> Watching the following error in the output of make:
>
> slon.c:33:22: error: libpq-fe.h: No such file or directory
>
>
> You should install packages libpq-dev and postgresql-server-dev in your
> ubuntu workstation. If the same error occurs in your Gentoo box you should
> emerge dev-db/libpq.

OK,  libpq-dev was already installed (version 8.3.3-0ubuntu0.8.04).  I 
installed postgresql-server-dev-8.2 (version 8.2.7-1; no -8.0 was available),  
and re-ran 'configure' as such:

./configure --with-pgbindir=/usr/lib/postgresql/8.0/bin/ --with-pgincludedir=/usr/include/postgresql --with-pgincludeserverdir=/usr/src/postgresql-8.0-8.0.7/build-tree/postgresql-8.0.7/src/include/ --with-pgpkglibdir=/usr/lib/postgresql/8.0/lib/

I ran 'make' and copied the .so files into the Postgres 8.0 "lib" directory,  
and tried to invoke "slonik" again.  I still get an error similar to what I 
saw before:

$ slonik <try1.slonik
<stdin>:7: PGRES_FATAL_ERROR 
select "_davidswaybill".initializeLocalNode(1, 'koios'); 
select "_davidswaybill".enableNode(1);  - ERROR:  function setval("unknown", 
bigint) does not exist
HINT:  No function matches the given name and argument types. You may need to 
add explicit type casts.
CONTEXT:  SQL statement "SELECT  setval('"_davidswaybill".sl_rowid_seq',  
$1 ::int8 * '1000000000000000'::int8)"
PL/pgSQL function "initializelocalnode" line 26 at perform
<stdin>:7: ERROR: no admin conninfo for node 134598976




-- 
David L. Lambert
  Software Developer,  Precision Motor Transport Group, LLC
  Work phone 517-349-3011 x215
  Cell phone 586-873-8813
From JanWieck at Yahoo.com  Tue Aug 26 21:33:20 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Tue Aug 26 21:33:35 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <e373d31e0808261037s6d5a0a86t2311ac844c11f61d@mail.gmail.com>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>	<48B3E3E8.9090003@iol.ie>	<e373d31e0808260436t7120cd02rb7bbcba3210f1218@mail.gmail.com>	<20080826134556.GB10943@crankycanuck.ca>
	<e373d31e0808261037s6d5a0a86t2311ac844c11f61d@mail.gmail.com>
Message-ID: <48B4D910.2040902@Yahoo.com>

On 8/26/2008 1:37 PM, Phoenix Kiula wrote:
> I am not interested in being a Slony expert or a fabulous DBA. I just
> want something that does the job of letting me upgrade a DB. (I am not
> looking for regular replication, but I was forwarded from the
> PG-GENERAL list to come here and ask how to use Slony to perform the
> anal experience that is the PG upgrade. I am not impressed so far!)

I am fully aware of your problem and am in the process of addressing it. 
It just won't happen in a week or two, that's for sure.

In my ignorant and naive manner I originally thought that providing 
nothing but this brutally horrible, noob-unfriendly "slonik" thing would 
cause someone to step up and build something useful. Didn't happen.

Next thought ... maybe it could be integrated into pgAdmin-III? Already 
a GUI tool, perfect! Well, that didn't get far either and in hindsight, 
it was a terrible thought to begin with because pgAdmin is the LAST 
thing you want on a production server. pgAdmin is despite its name a 
development tool that lets you create, alter and drop just about each 
and every database object with the click of a button ... kinda like 
having a full development environment and typing "make install". Neat, 
but not on a production server where dropping the wrong table is a 
disaster. So integrating basic replication administration into such a 
tool is a bad idea.

I am currently drafting requirements for a graphical configuration and 
status monitoring tool. The DBA's here at Afilias are over and over 
again drawing little "cluster diagrams" on a white board or just paper, 
in order to get an "overview" of the current configuration. And that, I 
think, some program can do much faster and more accurate.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From phoenix.kiula at gmail.com  Tue Aug 26 23:37:51 2008
From: phoenix.kiula at gmail.com (Phoenix Kiula)
Date: Tue Aug 26 23:38:19 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <20080826195708.GD12248@crankycanuck.ca>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
	<48B3E3E8.9090003@iol.ie>
	<e373d31e0808260436t7120cd02rb7bbcba3210f1218@mail.gmail.com>
	<20080826134556.GB10943@crankycanuck.ca>
	<e373d31e0808261037s6d5a0a86t2311ac844c11f61d@mail.gmail.com>
	<20080826181539.GA12248@crankycanuck.ca>
	<396486430808261228n33312984k7b96a56d384cef61@mail.gmail.com>
	<20080826195708.GD12248@crankycanuck.ca>
Message-ID: <e373d31e0808262337g59fa0fb3u99062c5c6a6889ce@mail.gmail.com>

>> Another option is "nearly do nothing", keep version 8.2 only apply 8.2
>> updates.



Thanks for this. In that case, I'll worry about Slony et al when 8.2
is retired I suppose. Hopefully PG will have a humanly decodable
upgrade mechanism by then.

But where can I find instructions to update 8.2.x without downtime?

I go to postgresql.org and search for "How to update 8.2":
http://search.postgresql.org/search?q=how+to+update+8.2&a=1&submit=Search

As always, the website's search engine returns useless stuff. If I
search google it comes up with links to the SQL "UPDATE" command.

I manually scoured the labyrinth of docs and came up with this Upgrade link:
http://www.postgresql.org/docs/current/static/install-upgrading.html

But again, as always, this assumes "pg_dumpall".

Perhaps this instruction assumes that I am upgrading between major
versions, but I want to stay clear of pg_dumpall -- it locks the
database and causes downtime. How can I simply upgrade the database
and leave the data alone. I thought an upgrade was merely PG's "system
files" being updated?

Any pointers deeply appreciated.
From dpage at pgadmin.org  Wed Aug 27 00:44:24 2008
From: dpage at pgadmin.org (Dave Page)
Date: Wed Aug 27 00:44:51 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <48B4D910.2040902@Yahoo.com>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
	<48B3E3E8.9090003@iol.ie>
	<e373d31e0808260436t7120cd02rb7bbcba3210f1218@mail.gmail.com>
	<20080826134556.GB10943@crankycanuck.ca>
	<e373d31e0808261037s6d5a0a86t2311ac844c11f61d@mail.gmail.com>
	<48B4D910.2040902@Yahoo.com>
Message-ID: <937d27e10808270044y79abc44bo279655d2a87c81d7@mail.gmail.com>

On Wed, Aug 27, 2008 at 5:33 AM, Jan Wieck <JanWieck@yahoo.com> wrote:

> Next thought ... maybe it could be integrated into pgAdmin-III? Already a
> GUI tool, perfect! Well, that didn't get far either and in hindsight, it was
> a terrible thought to begin with because pgAdmin is the LAST thing you want
> on a production server. pgAdmin is despite its name a development tool that
> lets you create, alter and drop just about each and every database object
> with the click of a button ... kinda like having a full development
> environment and typing "make install". Neat, but not on a production server
> where dropping the wrong table is a disaster. So integrating basic
> replication administration into such a tool is a bad idea.

I'm never entirely sure when you're being sarcastic Jan :-), but just
in case, I guess you're not aware that pgAdmin already has support for
Slony administration - work which was commissioned as part of the
contract we had for the win32 port a few years back.

http://people.planetpostgresql.org/dpage/index.php?/archives/51-Setting-up-Slony-I-with-pgAdmin.html


/D

-- 
Dave Page
EnterpriseDB UK:   http://www.enterprisedb.com
From glynastill at yahoo.co.uk  Wed Aug 27 01:41:28 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Wed Aug 27 01:41:58 2008
Subject: [Slony1-general] PgPoolII and slony
In-Reply-To: <20080826140622.GC10943@crankycanuck.ca>
Message-ID: <576670.41063.qm@web25806.mail.ukl.yahoo.com>

--- On Tue, 26/8/08, Andrew Sullivan <ajs@crankycanuck.ca> wrote:
>
> Follow the instructions (I know, they're a little
> terse) to set the
> pool software up in replication mode.  Then, turn on
> master_slave_mode
> and load_balance_mode.  That's how it's supposed to
> work, anyway.
> 

Thanks Andrew.  I'm guessing this is going to be one of those suck-it-and-see things. The docs don't say anything about how it works at all, nothing about how it sends queries, how it ensures any replication lag is accounted for... nothing.  So I jut wondered if anyone had actually used it.

The docs are a bit inconsistent too, for example they mention the hostnames as backend_hostname0/1/2 etc, but the actual conf file has master/secondary_backend_host_name etc, so whats the third hostname parameter?

I'm just trying to minimise the chance of having to wipe sh*t off my face here.



Send instant messages to your online friends http://uk.messenger.yahoo.com 
From richard.broersma at gmail.com  Wed Aug 27 06:28:14 2008
From: richard.broersma at gmail.com (Richard Broersma)
Date: Wed Aug 27 06:28:19 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <e373d31e0808262337g59fa0fb3u99062c5c6a6889ce@mail.gmail.com>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
	<48B3E3E8.9090003@iol.ie>
	<e373d31e0808260436t7120cd02rb7bbcba3210f1218@mail.gmail.com>
	<20080826134556.GB10943@crankycanuck.ca>
	<e373d31e0808261037s6d5a0a86t2311ac844c11f61d@mail.gmail.com>
	<20080826181539.GA12248@crankycanuck.ca>
	<396486430808261228n33312984k7b96a56d384cef61@mail.gmail.com>
	<20080826195708.GD12248@crankycanuck.ca>
	<e373d31e0808262337g59fa0fb3u99062c5c6a6889ce@mail.gmail.com>
Message-ID: <396486430808270628j123e9417yc9e297767c9784c0@mail.gmail.com>

On Tue, Aug 26, 2008 at 11:37 PM, Phoenix Kiula <phoenix.kiula@gmail.com> wrote:


> I manually scoured the labyrinth of docs and came up with this Upgrade link:
> http://www.postgresql.org/docs/current/static/install-upgrading.html

I have two thoughts on this point.

1) Here is the opening sentence of the 8.2 manual:
http://www.postgresql.org/docs/8.2/interactive/install-upgrading.html
"The internal data storage format changes with new releases of
PostgreSQL. Therefore, if you are upgrading an existing installation
that does not have a version number "8.2.x", you must back up and
restore your data as shown here."

So in one sense I see the confusion, the manual could have said, "If
you are only upgrading from and older version of 8.2.x, the steps to
back up and restore of your data is not necessary and could be
skipped."

2) regarding the labyrinth of docs:
I don't really understand why you've continued to use the word
"labyrinth" to describe the postgresql documentation.  There is really
only one place anyone needs to go to find the official PG manuals and
navigating to this place to me is very easy when you visit the
Postgresql.org home page because the link to documentation takes you
to this page:

http://www.postgresql.org/docs/

>From this link you only need to select the on-line manual for the
version of Postgresql that you are using.

2.b)  Also there are two ways that you can "file" a complaint to the
postgreSQL community.
The first is: "I don't like the layout of postgresql's documentation,
it stinks ( or use some other negative phrase like 'labyrinth')!"

The other is: "When I try the following actions to find the postgresql
official documentation, it is too easy to get lost at step X. Or at
link y, it isn't clear how to get to the official documentation.  If
you make to following Z changes, it could be improved."
Also when filing such a request, it is best to choose the most
appropriate mailing list.  For example, if you find a way to "untie
the PostgreSQL documentation labyrinth", why not suggest it to the
pg-WWW mailing list.  They would be happy to make the website easier
to use.

I am sure that everyone in the community wants you to find a workable
solution to your postgresql administration needs.  Also most persons
in the community are not Postgresql developers but rather users like
yourself.  To me as a fellow PG user, many of your emails are connote
a tone of sarcasm, criticality, and general dislike for many project
tools.   If your tone is really expressing actual unhappiness with
PostgreSQL perhaps PostgreSQL was the wrong choice for RDBMS.
<Sarcasm> If you were to choose a difference RDBMS flavor in the
future, who knows, you might find one that satisfies all of you needs
and then users like myself will not have to hear repeated complaints
that we do not necessary agree with and then users like myself will
not have to hear repeated complaint that we do not necessarily agree
with.  It could be a win-win for all of us. </Sarcasm>

-- 
Regards,
Richard Broersma Jr.

Visit the Los Angeles PostgreSQL Users Group (LAPUG)
http://pugs.postgresql.org/lapug
From ajs at crankycanuck.ca  Wed Aug 27 09:38:39 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Aug 27 09:38:50 2008
Subject: [Slony1-general] Simple, step by step instructions to upgrade PG
In-Reply-To: <e373d31e0808262337g59fa0fb3u99062c5c6a6889ce@mail.gmail.com>
References: <e373d31e0808252339h7162fd52o9ffdf9c5492e9c16@mail.gmail.com>
	<48B3E3E8.9090003@iol.ie>
	<e373d31e0808260436t7120cd02rb7bbcba3210f1218@mail.gmail.com>
	<20080826134556.GB10943@crankycanuck.ca>
	<e373d31e0808261037s6d5a0a86t2311ac844c11f61d@mail.gmail.com>
	<20080826181539.GA12248@crankycanuck.ca>
	<396486430808261228n33312984k7b96a56d384cef61@mail.gmail.com>
	<20080826195708.GD12248@crankycanuck.ca>
	<e373d31e0808262337g59fa0fb3u99062c5c6a6889ce@mail.gmail.com>
Message-ID: <20080827163839.GA16756@crankycanuck.ca>

On Wed, Aug 27, 2008 at 02:37:51PM +0800, Phoenix Kiula wrote:
> 
> But where can I find instructions to update 8.2.x without downtime?

With _no_ downtime?  You can't.  You have to restart the server in
order to change the binary.  But you can do it with minimal downtime:
install the new binaries, then restart (not just reload) Postgres.
This is how the Debian (and RPM-based, IIRC) packages do it, actually.

There's no need to dump and restore or anything.  (The release notes
will tell you if you need a pg_dump, BTW.  I'm not sure how clear the
manuals are on this topic, if you're new to Postgres.)

This is now off-topic for slony, so if you need continued help on this
particular topic, please ask on the pgsql-general or pgsql-admin list
(and not both, please).

A

-- 
Andrew Sullivan
ajs@commandprompt.com
+1 503 667 4564 x104
http://www.commandprompt.com/
From dlambert at bmtcarhaul.com  Wed Aug 27 10:02:08 2008
From: dlambert at bmtcarhaul.com (David Lee Lambert)
Date: Wed Aug 27 10:02:18 2008
Subject: [Slony1-general] Building against multiple Postgres versions
	[SOLVED WONTFIX]
In-Reply-To: <200808261725.39552.dlambert@bmtcarhaul.com>
References: <200808251631.00519.dlambert@bmtcarhaul.com>
	<2b391080808260812q5d244e7dwcd06a103c626d68e@mail.gmail.com>
	<200808261725.39552.dlambert@bmtcarhaul.com>
Message-ID: <200808271302.08649.dlambert@bmtcarhaul.com>

On Tuesday 26 August 2008 05:25:39 pm David Lee Lambert wrote:
> On Tuesday 26 August 2008 11:12:31 am V?ctor wrote:
> > Watching the following error in the output of make:
> >
> > slon.c:33:22: error: libpq-fe.h: No such file or directory
> >
> >
> > You should install packages libpq-dev and postgresql-server-dev in your
> > ubuntu workstation. [...]


It looks like some other people already ran into this problem and that there's 
no easy solution:


https://bugs.launchpad.net/ubuntu/+source/slony1/+bug/236749/


The Postgres release for the slave server doesn't have to be 8.2;  two options 
I can see immediately are to upgrade it to 8.3 and install Slony from Ubuntu 
packages against that,  or to uninstall Postgres and build 8.0 with Slony 
from source.  

-- 
David L. Lambert
  Software Developer,  Precision Motor Transport Group, LLC
  Work phone 517-349-3011 x215
  Cell phone 586-873-8813
From dlambert at bmtcarhaul.com  Wed Aug 27 12:18:11 2008
From: dlambert at bmtcarhaul.com (David Lee Lambert)
Date: Wed Aug 27 12:18:25 2008
Subject: [Slony1-general] Simple replication taking eons to complete
Message-ID: <200808271518.12624.dlambert@bmtcarhaul.com>

Yesterday afternoon I installed Slony 1.2.10 against Postgres 8.0 on a Gentoo 
box,  and set up two pgbench databases with it,  "pgbench_master" 
and "pgbench_slave".  With the two "slon" processes running, I 
ran "pgbench -i" and "pgbench" from another host.  As of right now,  the data 
still isn't replicated:

 pgbench_master=# SELECT aid,bid,abalance FROM accounts ORDER BY abalance DESC 
LIMIT 10;
   aid   | bid | abalance 
---------+-----+----------
 1541195 |  16 |      938
 1131104 |  12 |      929
  230915 |   3 |      906
...
pgbench_slave=#  SELECT aid,bid,abalance FROM accounts ORDER BY abalance DESC 
LIMIT 10;
  aid  | bid | abalance 
-------+-----+----------
 29533 |   1 |     1747
 23957 |   1 |     1704
 99989 |   1 |     1624
...

The system is a single-CPU Pentium 4 2.8GHz with 1G of RAM, not running any 
other active applications.  Is this sort of delay normal with Slony?  If it's 
stuck,  how do I unstick it?

-- 

David L. Lambert
  Software Developer,  Precision Motor Transport Group, LLC
  Work phone 517-349-3011 x215
  Cell phone 586-873-8813
From wmoran at collaborativefusion.com  Wed Aug 27 12:23:42 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Wed Aug 27 12:23:31 2008
Subject: [Slony1-general] Simple replication taking eons to complete
In-Reply-To: <200808271518.12624.dlambert@bmtcarhaul.com>
References: <200808271518.12624.dlambert@bmtcarhaul.com>
Message-ID: <20080827152342.f07cc073.wmoran@collaborativefusion.com>

In response to David Lee Lambert <dlambert@bmtcarhaul.com>:

> Yesterday afternoon I installed Slony 1.2.10 against Postgres 8.0 on a Gentoo 
> box,  and set up two pgbench databases with it,  "pgbench_master" 
> and "pgbench_slave".  With the two "slon" processes running, I 
> ran "pgbench -i" and "pgbench" from another host.  As of right now,  the data 
> still isn't replicated:
> 
>  pgbench_master=# SELECT aid,bid,abalance FROM accounts ORDER BY abalance DESC 
> LIMIT 10;
>    aid   | bid | abalance 
> ---------+-----+----------
>  1541195 |  16 |      938
>  1131104 |  12 |      929
>   230915 |   3 |      906
> ...
> pgbench_slave=#  SELECT aid,bid,abalance FROM accounts ORDER BY abalance DESC 
> LIMIT 10;
>   aid  | bid | abalance 
> -------+-----+----------
>  29533 |   1 |     1747
>  23957 |   1 |     1704
>  99989 |   1 |     1624
> ...
> 
> The system is a single-CPU Pentium 4 2.8GHz with 1G of RAM, not running any 
> other active applications.  Is this sort of delay normal with Slony?

Not unless your network is unbelievably slow.

> If it's 
> stuck,  how do I unstick it?

Have a look at the log files.  Slony is pretty good about logging why
it's unable to replicate.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023

From glynastill at yahoo.co.uk  Wed Aug 27 13:32:44 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Wed Aug 27 13:32:58 2008
Subject: [Slony1-general] Simple replication taking eons to complete
In-Reply-To: <20080827152342.f07cc073.wmoran@collaborativefusion.com>
Message-ID: <642847.21862.qm@web25801.mail.ukl.yahoo.com>

> 
> > If it's 
> > stuck,  how do I unstick it?
> 
> Have a look at the log files.  Slony is pretty good about
> logging why
> it's unable to replicate.
> 

Also a peek in pg_stat_activity and a "ps -ax" may shed some light...

Send instant messages to your online friends http://uk.messenger.yahoo.com 
From dlambert at bmtcarhaul.com  Wed Aug 27 14:07:36 2008
From: dlambert at bmtcarhaul.com (David Lee Lambert)
Date: Wed Aug 27 14:07:49 2008
Subject: [Slony1-general] Simple replication taking eons to complete
In-Reply-To: <642847.21862.qm@web25801.mail.ukl.yahoo.com>
References: <642847.21862.qm@web25801.mail.ukl.yahoo.com>
Message-ID: <200808271707.36602.dlambert@bmtcarhaul.com>

On Wednesday 27 August 2008 04:32:44 pm Glyn Astill wrote:
> > > If it's
> > > stuck,  how do I unstick it?
> >
> > Have a look at the log files.  Slony is pretty good about
> > logging why
> > it's unable to replicate.
>
> Also a peek in pg_stat_activity and a "ps -ax" may shed some light...

Here's a screenful of the intermixed logfiles.  I notice that I have about 
10 "slon" processes per node;  is that normal?  Nothing jumps out as 
obviously being a problem in the logs...

==> slony1/node2/pgbench_slave-2008-08-27_16:55:57.log <==
2008-08-27 16:59:50 EDT DEBUG2 remoteListenThread_1: queue event 1,6996 SYNC
2008-08-27 16:59:50 EDT DEBUG2 remoteWorkerThread_1: Received event 1,6996 
SYNC
2008-08-27 16:59:50 EDT DEBUG2 calc sync size - last time: 1 last length: 
17357 ideal: 3 proposed size: 3
2008-08-27 16:59:50 EDT DEBUG2 remoteWorkerThread_1: SYNC 6996 processing
2008-08-27 16:59:50 EDT DEBUG2 remoteWorkerThread_1: syncing set 1 with 4 
table(s) from provider 1
2008-08-27 16:59:50 EDT DEBUG2  ssy_action_list length: 0
2008-08-27 16:59:50 EDT DEBUG2 remoteWorkerThread_1: current local log_status 
is 0
2008-08-27 16:59:50 EDT DEBUG2 remoteWorkerThread_1_1: current remote 
log_status = 0
2008-08-27 16:59:50 EDT DEBUG2 remoteHelperThread_1_1: 0.001 seconds delay for 
first row
2008-08-27 16:59:50 EDT DEBUG2 remoteHelperThread_1_1: 0.001 seconds until 
close cursor
2008-08-27 16:59:50 EDT DEBUG2 remoteHelperThread_1_1: inserts=0 updates=0 
deletes=0
2008-08-27 16:59:50 EDT DEBUG2 remoteWorkerThread_1: new sl_rowid_seq value: 
1000000000002442
2008-08-27 16:59:50 EDT DEBUG2 remoteWorkerThread_1: SYNC 6996 done in 0.006 
seconds

==> postgresql.log <==
[2008-08-27 16:59:53 EDT] 48b5bf5d.62d8.14 (postgres@) LOG:  duration: 
2890.479 ms  statement: notify "_replication_Event"; insert 
into "_replication".sl_event     (ev_origin, ev_seqno, ev_timestamp,      
ev_minxid, ev_maxxid, ev_xip, ev_type     ) values ('2', '6770', '2008-08-27 
16:59:43.239217', '2046164', '2046166', '''2046164''', 'SYNC'); insert 
into "_replication".sl_confirm       (con_origin, con_received, con_seqno, 
con_timestamp)    values (2, 1, '6770', now()); commit transaction;
[2008-08-27 16:59:53 EDT] 48b5bf5d.62fe.15 (postgres@) LOG:  duration: 
2878.348 ms  statement: notify "_replication_Event"; insert 
into "_replication".sl_event     (ev_origin, ev_seqno, ev_timestamp,      
ev_minxid, ev_maxxid, ev_xip, ev_type     ) values ('1', '6996', '2008-08-27 
16:59:43.235193', '2046164', '2046165', '', 'SYNC'); insert 
into "_replication".sl_confirm  (con_origin, con_received, con_seqno, 
con_timestamp)    values (1, 2, '6996', now()); commit transaction;

==> slony1/node1/pgbench_master-2008-08-27_16:55:52.log <==
2008-08-27 16:59:53 EDT DEBUG2 syncThread: new sl_action_seq 12203 - SYNC 6997
2008-08-27 16:59:54 EDT DEBUG2 localListenThread: Received event 1,6997 SYNC
2008-08-27 16:59:54 EDT DEBUG2 remoteListenThread_2: queue event 2,6771 SYNC
2008-08-27 16:59:54 EDT DEBUG2 remoteWorkerThread_2: Received event 2,6771 
SYNC
2008-08-27 16:59:54 EDT DEBUG2 calc sync size - last time: 1 last length: 3781 
ideal: 15 proposed size: 3
2008-08-27 16:59:54 EDT DEBUG2 remoteWorkerThread_2: SYNC 6771 processing
2008-08-27 16:59:54 EDT DEBUG2 remoteWorkerThread_2: no sets need syncing for 
this event

==> slony1/node2/pgbench_slave-2008-08-27_16:55:57.log <==
2008-08-27 16:59:53 EDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 6771
2008-08-27 16:59:54 EDT DEBUG2 remoteListenThread_1: queue event 1,6997 SYNC
2008-08-27 16:59:54 EDT DEBUG2 remoteWorkerThread_1: Received event 1,6997 
SYNC
2008-08-27 16:59:54 EDT DEBUG2 calc sync size - last time: 1 last length: 3776 
ideal: 15 proposed size: 3
2008-08-27 16:59:54 EDT DEBUG2 remoteWorkerThread_1: SYNC 6997 processing
2008-08-27 16:59:54 EDT DEBUG2 remoteWorkerThread_1: syncing set 1 with 4 
table(s) from provider 1
2008-08-27 16:59:54 EDT DEBUG2  ssy_action_list length: 0
2008-08-27 16:59:54 EDT DEBUG2 remoteWorkerThread_1: current local log_status 
is 0
2008-08-27 16:59:54 EDT DEBUG2 remoteWorkerThread_1_1: current remote 
log_status = 0
2008-08-27 16:59:54 EDT DEBUG2 remoteHelperThread_1_1: 0.001 seconds delay for 
first row
2008-08-27 16:59:54 EDT DEBUG2 remoteHelperThread_1_1: 0.001 seconds until 
close cursor
2008-08-27 16:59:54 EDT DEBUG2 remoteHelperThread_1_1: inserts=0 updates=0 
deletes=0
2008-08-27 16:59:54 EDT DEBUG2 remoteWorkerThread_1: new sl_rowid_seq value: 
1000000000002442
2008-08-27 16:59:54 EDT DEBUG2 remoteWorkerThread_1: SYNC 6997 done in 0.006 
seconds
2008-08-27 16:59:54 EDT DEBUG2 localListenThread: Received event 2,6771 SYNC

==> postgresql.log <==
[2008-08-27 16:59:58 EDT] 48b5bf5d.62d8.15 (postgres@) LOG:  duration: 
3901.927 ms  statement: notify "_replication_Event"; insert 
into "_replication".sl_event     (ev_origin, ev_seqno, ev_timestamp,      
ev_minxid, ev_maxxid, ev_xip, ev_type     ) values ('2', '6771', '2008-08-27 
16:59:53.94613', '2046352', '2046353', '', 'SYNC'); insert 
into "_replication".sl_confirm   (con_origin, con_received, con_seqno, 
con_timestamp)    values (2, 1, '6771', now()); commit transaction;
[2008-08-27 16:59:58 EDT] 48b5bf5d.62fe.16 (postgres@) LOG:  duration: 
3930.953 ms  statement: notify "_replication_Event"; insert 
into "_replication".sl_event     (ev_origin, ev_seqno, ev_timestamp,      
ev_minxid, ev_maxxid, ev_xip, ev_type     ) values ('1', '6997', '2008-08-27 
16:59:53.928121', '2046351', '2046352', '', 'SYNC'); insert 
into "_replication".sl_confirm  (con_origin, con_received, con_seqno, 
con_timestamp)    values (1, 2, '6997', now()); commit transaction;

==> slony1/node1/pgbench_master-2008-08-27_16:55:52.log <==
2008-08-27 16:59:58 EDT DEBUG2 remoteWorkerThread_2: forward confirm 1,6996 
received by 2

==> slony1/node2/pgbench_slave-2008-08-27_16:55:57.log <==
2008-08-27 16:59:58 EDT DEBUG2 remoteWorkerThread_1: forward confirm 2,6770 
received by 1


-- 

David L. Lambert
  Software Developer,  Precision Motor Transport Group, LLC
  Work phone 517-349-3011 x215
  Cell phone 586-873-8813
From Ow.Mun.Heng at wdc.com  Wed Aug 27 19:19:19 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Wed Aug 27 19:19:50 2008
Subject: [Slony1-general] Can I perform CLUSTER on a slony DB?
In-Reply-To: <1219673371.6179.61.camel@bnicholson-desktop>
References: <1219638308.2765.19.camel@neuromancer.home.net>
	<1219673371.6179.61.camel@bnicholson-desktop>
Message-ID: <1219889959.6034.10.camel@neuromancer.home.net>

On Mon, 2008-08-25 at 10:09 -0400, Brad Nicholson wrote:
> On Mon, 2008-08-25 at 12:25 +0800, Ow Mun Heng wrote:
> > Just wondering if it's possible to do a cluster on a slony replicated DB
> > in either master/slave??
> 
> You can, with a couple of caveats.
> 
> 1: Cluster will take an access exclusive lock the table in question and
> replication will fall behind for the duration of the cluster if anything
> tries to write to the table be being clustered.

That's understandable.
> 
> 2: Prior to 8.3, cluster is not MVCC safe, so it is very possible to
> break things by clustering.

Break as in break the replication or break PG?

Hmm..

But, all in all, I can still do the cluster individually right? And not
through slonik
From Ow.Mun.Heng at wdc.com  Wed Aug 27 19:21:48 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Wed Aug 27 19:22:18 2008
Subject: [Slony1-general] Partition Tables - How does slony replicate it
Message-ID: <1219890108.6034.14.camel@neuromancer.home.net>

Considering doing partitioning/table inheritance on a master/slave Db
replicated by slony.

I'm not sure how it handles the table inheritance. 
Do I need to manually add the tables to be inherited? what about
removing it from inheritance?
Is there an automated way? 

Can anyone shed some light into how slony deals with this?
From jennifer.spencer at stanford.edu  Wed Aug 27 19:46:06 2008
From: jennifer.spencer at stanford.edu (Jennifer Spencer)
Date: Wed Aug 27 19:46:27 2008
Subject: [Slony1-general] How to define clusters - please help
Message-ID: <48B6116E.5020504@stanford.edu>

I need help figuring out how to create a smart cluster layout to log-ship my data.
Please, oh please, help me understand a couple of things about nodes & clusters.  What is possible and 
what is not, specifically.

Existing situation:
--I have a master db (node 1) on port 5432, a slave (node 2) on 5430, all running slony proper.
--I have one cluster, jsoc_example, between nodes 1 & 2
--I am successfully log-shipping jsoc_example to /mylogdir using a slon -a command on my slon for node 
2.

****************Problem to be solved:
1.  The boss wants minimal drag on the production database (e.g. minimal triggers, minimal number of 
overlapping clusters).
2.  We have about 10 future subscriber sites that want sometimes the same data tables, sometimes 
different data tables from the same database (arg!).
3.  All subscribers will be log-ship subscribers.

I figure I can do one cluster per table, or I can parse one cluster's logs.  <--What do you suggest 
here???
From ajs at crankycanuck.ca  Thu Aug 28 07:51:51 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Aug 28 07:52:00 2008
Subject: [Slony1-general] How to define clusters - please help
In-Reply-To: <48B6116E.5020504@stanford.edu>
References: <48B6116E.5020504@stanford.edu>
Message-ID: <20080828145151.GA23592@crankycanuck.ca>

On Wed, Aug 27, 2008 at 07:46:06PM -0700, Jennifer Spencer wrote:
> I need help figuring out how to create a smart cluster layout to log-ship my data.
> Please, oh please, help me understand a couple of things about nodes & 
> clusters.  What is possible and what is not, specifically.
>

I think you're missing a concept you need, which is "sets".

> 1.  The boss wants minimal drag on the production database (e.g. minimal 
> triggers, minimal number of overlapping clusters).

Additional sets should not produce detectable drag.

> 2.  We have about 10 future subscriber sites that want sometimes the same 
> data tables, sometimes different data tables from the same database (arg!).

Put the minimal set of tables that could be wanted at any one site
into one set.  That site subscribes to that set, and not the others.

A


-- 
Andrew Sullivan
ajs@commandprompt.com
+1 503 667 4564 x104
http://www.commandprompt.com/
From cbbrowne at ca.afilias.info  Thu Aug 28 08:53:32 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Aug 28 08:53:42 2008
Subject: [Slony1-general] Can I perform CLUSTER on a slony DB?
In-Reply-To: <1219889959.6034.10.camel@neuromancer.home.net> (Ow Mun Heng's
	message of "Thu, 28 Aug 2008 10:19:19 +0800")
References: <1219638308.2765.19.camel@neuromancer.home.net>
	<1219673371.6179.61.camel@bnicholson-desktop>
	<1219889959.6034.10.camel@neuromancer.home.net>
Message-ID: <878wuh9lvn.fsf@dba2.int.libertyrms.com>

Ow Mun Heng <Ow.Mun.Heng@wdc.com> writes:
> On Mon, 2008-08-25 at 10:09 -0400, Brad Nicholson wrote:
>> On Mon, 2008-08-25 at 12:25 +0800, Ow Mun Heng wrote:
>> > Just wondering if it's possible to do a cluster on a slony replicated DB
>> > in either master/slave??
>> 
>> You can, with a couple of caveats.
>> 
>> 1: Cluster will take an access exclusive lock the table in question and
>> replication will fall behind for the duration of the cluster if anything
>> tries to write to the table be being clustered.
>
> That's understandable.
>> 
>> 2: Prior to 8.3, cluster is not MVCC safe, so it is very possible to
>> break things by clustering.
>
> Break as in break the replication or break PG?

It *shouldn't* break replication, but there could be some odd caveats...  

CLUSTER, prior to 8.3, has the habit of dropping tuples that are dead,
irrespective of them still being visible to some sessions.

There are two *directly* relevant bits for Slony-I purposes, which
don't look to break replication:

 - If you CLUSTER any of Slony-I's tables (e.g. - such as sl_log_1/2),
   deleted tuples would go away Right Away.

   Fortunately, tuples don't get deleted from sl_log_* until they are
   known to have been replicated *everywhere*, so CLUSTER won't
   adversely affect deletions from sl_log_*.

 - The other side to this is in how data gets *into* sl_log_*.

   If you CLUSTER a replicated table, deleted data could go away
   despite there being transactions still pointing to it.

   Fortunately, that won't adversely affect Slony-I, either, as there
   are two possibilities:

   1.  The transaction deleting data from the replicated table may not
   have committed yet, but in that case CLUSTER won't be able to run
   (since it needs an exclusive lock).  So we can't lose data here.

   2.  The transaction deleting data from the replicated table may
   have committed; if so, then entries are committed to sl_log_*, and
   consistency of replication data remains.

In effect, I don't expect "cruel and unusual" use of CLUSTER to break
replication.

It may break your *application*; if you have old queries that might
look at data that has been deleted, then CLUSTER can cause those
queries to get inconsistent data.  Slony-I doesn't have patterns where
it would look for data in that sort of pattern, so replication
oughtn't break down.

> Hmm..
>
> But, all in all, I can still do the cluster individually right? And not
> through slonik

Right...  Indeed, I'd prefer NOT to use Slonik to do this.
-- 
output = ("cbbrowne" "@" "linuxfinances.info")
http://linuxdatabases.info/info/unix.html
"Well, I wish  you'd just  tell me rather   than trying to engage   my
enthusiasm, because I haven't got one." -- Marvin the Paranoid Android
From cbbrowne at ca.afilias.info  Thu Aug 28 09:03:19 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Aug 28 09:03:29 2008
Subject: [Slony1-general] Partition Tables - How does slony replicate it
In-Reply-To: <1219890108.6034.14.camel@neuromancer.home.net> (Ow Mun Heng's
	message of "Thu, 28 Aug 2008 10:21:48 +0800")
References: <1219890108.6034.14.camel@neuromancer.home.net>
Message-ID: <874p559lfc.fsf@dba2.int.libertyrms.com>

Ow Mun Heng <Ow.Mun.Heng@wdc.com> writes:

> Considering doing partitioning/table inheritance on a master/slave Db
> replicated by slony.
>
> I'm not sure how it handles the table inheritance. 
> Do I need to manually add the tables to be inherited? what about
> removing it from inheritance?
> Is there an automated way? 
>
> Can anyone shed some light into how slony deals with this?

There is a test in the test suite that uses inheritance-based partitioning:
  http://main.slony.info/viewcvs/viewvc.cgi/slony1-engine/tests/testpartition/?pathrev=REL_1_2_STABLE

It sets up 3 monthly partitions, and adds them to replication.

There are stored procedures in version 1.2 for this:

  add_empty_table_to_replication()
    which adds an empty table to a replication set (this is a general "tool")
  replicate_partition()
    which uses the above function to replicate an inherited partition, validating
    that it is a child of a replicated parent table

You can run replicate_partition() (passing in the appropriate
parameters) in a Slonik script; this will add a fresh, empty partition
to replication, in the same replication set as its parent.
-- 
"cbbrowne","@","cbbrowne.com"
http://www3.sympatico.ca/cbbrowne/x.html
Rules  of the  Evil Overlord  #214. "If  a malignant  being  demands a
sacrificial victim  have a  particular quality, I  will check  to make
sure said victim has this quality immediately before the sacrifice and
not rely on  earlier results. (Especially if the  quality is virginity
and the victim is the hero's girlfriend.)"
<http://www.eviloverlord.com/>
From msteben at autorevenue.com  Thu Aug 28 11:10:01 2008
From: msteben at autorevenue.com (Mark Steben)
Date: Thu Aug 28 11:10:26 2008
Subject: [Slony1-general] runtime configuration
Message-ID: <002c01c90939$4a02c090$14050a0a@dei26g028575>

Postgres has the PG_SETTINGS table to easily query for the current runtime
configurations.  Does Slony have something comparable?  I'm looking for
Any and all runtime configurations settings.  We are running Slony 1.2.14 
On Postgres 8.2.5.

Thanks, Mark 

From jennifer.spencer at stanford.edu  Thu Aug 28 13:10:46 2008
From: jennifer.spencer at stanford.edu (Jennifer Spencer)
Date: Thu Aug 28 13:11:02 2008
Subject: [Slony1-general] How to define clusters - please help
In-Reply-To: <20080828145151.GA23592@crankycanuck.ca>
References: <48B6116E.5020504@stanford.edu>
	<20080828145151.GA23592@crankycanuck.ca>
Message-ID: <48B70646.7010203@stanford.edu>

Thank you for replying!

The idea of sets (duh, yeah, of course) is a good one.  I didn't clue in on that.

How exactly to apply that idea with two nodes (total) where node 2 is essentially only in place as a 
log-ship-creating slave?  Still have the problem with all-inclusive-content log files as output, right?

How can I get the correct subscribed content of my logs to my users?  Node 2 is outputting log files 
every minute (presumably for oh, 50 sets, in your model, with one cluster).  How to go about 
separating those logs in a way that user A gets 25 sets and user B gets a different 10 sets and user C 
gets some intersection of what A & B are getting?

See what I mean?

Thanks again,
Jennifer


Andrew Sullivan wrote:
> On Wed, Aug 27, 2008 at 07:46:06PM -0700, Jennifer Spencer wrote:
>> I need help figuring out how to create a smart cluster layout to log-ship my data.
>> Please, oh please, help me understand a couple of things about nodes & 
>> clusters.  What is possible and what is not, specifically.
>>
> 
> I think you're missing a concept you need, which is "sets".
> 
>> 1.  The boss wants minimal drag on the production database (e.g. minimal 
>> triggers, minimal number of overlapping clusters).
> 
> Additional sets should not produce detectable drag.
> 
>> 2.  We have about 10 future subscriber sites that want sometimes the same 
>> data tables, sometimes different data tables from the same database (arg!).
> 
> Put the minimal set of tables that could be wanted at any one site
> into one set.  That site subscribes to that set, and not the others.
> 
> A
> 
> 
From ajs at crankycanuck.ca  Thu Aug 28 13:31:37 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Aug 28 13:31:53 2008
Subject: [Slony1-general] How to define clusters - please help
In-Reply-To: <48B70646.7010203@stanford.edu>
References: <48B6116E.5020504@stanford.edu>
	<20080828145151.GA23592@crankycanuck.ca>
	<48B70646.7010203@stanford.edu>
Message-ID: <20080828203137.GA27287@crankycanuck.ca>

On Thu, Aug 28, 2008 at 01:10:46PM -0700, Jennifer Spencer wrote:
> Thank you for replying!
>
> The idea of sets (duh, yeah, of course) is a good one.  I didn't clue in on that.

My turn to "duh":

> How exactly to apply that idea with two nodes (total) where node 2 is 
> essentially only in place as a log-ship-creating slave?  Still have the 
> problem with all-inclusive-content log files as output, right?

Right.  The log-shipping trick copies the data that arrives at that
node, which means that (alas) you have everything in one gigantic
file.  So that doesn't work.

Now, here's an incredibly ugly kludge I just thought of and haven't
tested that would make this possible.  You create a bunch more targets
on node 2, each of which subscribes to the target set.  Then you put
_those_ into echo mode.  If I remember correctly, each one will only
get the data destined to it, so you can break the sets up that way.
Essentially, you'd replicate from 1->2, and then from 2->2a, 2->2b,
&c., one target per set.  Then you deliver the pieces relevant to each
user by giving them the files for a,d,f (for instance).

That said, this makes my teeth hurt just to suggest it.  But the log
shipping today doesn't really do what you want.  I know there was some
intended work to allow filtering in 2.x.  I don't know if it got off
the ground.  Chris?

A


-- 
Andrew Sullivan
ajs@commandprompt.com
+1 503 667 4564 x104
http://www.commandprompt.com/
From cbbrowne at ca.afilias.info  Fri Aug 29 06:12:18 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Aug 29 06:12:25 2008
Subject: [Slony1-general] runtime configuration
In-Reply-To: <002c01c90939$4a02c090$14050a0a@dei26g028575> (Mark Steben's
	message of "Thu, 28 Aug 2008 14:10:01 -0400")
References: <002c01c90939$4a02c090$14050a0a@dei26g028575>
Message-ID: <87skso7yod.fsf@dba2.int.libertyrms.com>

"Mark Steben" <msteben@autorevenue.com> writes:
> Postgres has the PG_SETTINGS table to easily query for the current runtime
> configurations.  Does Slony have something comparable?  I'm looking for
> Any and all runtime configurations settings.  We are running Slony 1.2.14 
> On Postgres 8.2.5.
>
> Thanks, Mark 

Well, nearly all configuration is in the database schema that lives in
each database, which should be pretty useful.

There hasn't traditionally been a way to query the slon configuration
(e.g. - the parameters passed in on command line and in slon.conf
file).  "ps" should help with the command line options :-).  If
slon.conf changes, it mightn't match the in-memory conditions.

I have a change in 2.0 where the runtime config is dumped to the logs
at startup time; you're obviously not running that yet :-).
-- 
select 'cbbrowne' || '@' || 'acm.org';
http://linuxfinances.info/info/advocacy.html
Signs of a Klingon Programmer - 19.  "My program has just dumped Stova
Core!"
From cbbrowne at ca.afilias.info  Fri Aug 29 07:22:26 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Aug 29 07:22:32 2008
Subject: [Slony1-general] How to define clusters - please help
In-Reply-To: <20080828203137.GA27287@crankycanuck.ca> (Andrew Sullivan's
	message of "Thu, 28 Aug 2008 16:31:37 -0400")
References: <48B6116E.5020504@stanford.edu>
	<20080828145151.GA23592@crankycanuck.ca>
	<48B70646.7010203@stanford.edu>
	<20080828203137.GA27287@crankycanuck.ca>
Message-ID: <87myiv99zx.fsf@dba2.int.libertyrms.com>

Andrew Sullivan <ajs@crankycanuck.ca> writes:
> That said, this makes my teeth hurt just to suggest it.  But the log
> shipping today doesn't really do what you want.  I know there was some
> intended work to allow filtering in 2.x.  I don't know if it got off
> the ground.  Chris?

Jan's looking at implementing an admin tool to be rather friendlier
than what we have now; the notion of doing filtering seems to be a
good couple of steps further down the road.

That's smelling like "3.x", at this point.  We have loosely talked
about having "smarter triggers" that store more of the metadata in
more intelligent form, so that you could indeed pick and choose and
even transform on the way through.

It has gotten steps further, conceptually; Jan, Simon and I had a chat
at PGCon about a "wildly more intelligent COPY" that would get us the
kinds of efficiencies that COPY does for I/U/D updates.  A key thing
about that would be making it generally useful as a "Data Load Tool"
(e.g. - presumably with the potential to be popular for data
warehouse-like applications) so that it wouldn't hang as a
Slony-I-only "bag on the side."  But implementation hasn't started,
and *that* would be a logical prerequisite to filtering.
-- 
"cbbrowne","@","cbbrowne.com"
http://linuxdatabases.info/info/nonrdbms.html
Rules of  the Evil Overlord  #228.  "If the  hero claims he  wishes to
confess  in public  or to  me  personally, I  will remind  him that  a
notarized deposition will serve just as well."
<http://www.eviloverlord.com/>
From slony at rvt.dds.nl  Fri Aug 29 12:57:17 2008
From: slony at rvt.dds.nl (ries van Twisk)
Date: Fri Aug 29 12:57:38 2008
Subject: [Slony1-general] Merging sets in Slony 2.0 RC1
Message-ID: <F2F01763-C731-4D79-8985-79982F393F33@rvt.dds.nl>

Hey all,

this is my first mail on this list and have a question about merging  
sets

When I execute this command to slonik : merge set (id=1, add id = 999,  
origin=1);

I set the set perfectly merged to my master DB, but in my slave DB I  
don't see
the set merged. Is this correct or should I execute the merge set  
command for each
slave node?

This is on slony 2.0 RC1


Also in the file slon_watchdog I noticed a semi-colon was missing on  
line 47...

thanks,

Ries


-------------------------------------------------------------------------------------------------
Ries van Twisk
tags: Freelance TYPO3 Glassfish JasperReports JasperETL Flex Blaze-DS  
WebORB PostgreSQL DB-Architect
email: ries@vantwisk.nl
web:   http://www.rvantwisk.nl/
skype: callto://r.vantwisk







From slony at rvt.dds.nl  Fri Aug 29 13:06:52 2008
From: slony at rvt.dds.nl (ries van Twisk)
Date: Fri Aug 29 13:07:12 2008
Subject: [Slony1-general] Merging sets in Slony 2.0 RC1
In-Reply-To: <F2F01763-C731-4D79-8985-79982F393F33@rvt.dds.nl>
References: <F2F01763-C731-4D79-8985-79982F393F33@rvt.dds.nl>
Message-ID: <BCB2D2F2-2569-4048-91EA-D0447A13EAAE@rvt.dds.nl>


On Aug 29, 2008, at 2:57 PM, ries van Twisk wrote:

> Hey all,
>
> this is my first mail on this list and have a question about merging  
> sets
>
> When I execute this command to slonik : merge set (id=1, add id =  
> 999, origin=1);
>
> I set the set perfectly merged to my master DB, but in my slave DB I  
> don't see
> the set merged. Is this correct or should I execute the merge set  
> command for each
> slave node?
>
> This is on slony 2.0 RC1
>
>
> Also in the file slon_watchdog I noticed a semi-colon was missing on  
> line 47...
>
> thanks,
>
> Ries
>



TO add more info, I get this in my log...

2008-08-29 15:02:34 COT DEBUG2 remoteListenThread_1: queue event 1,133  
STORE_SET
2008-08-29 15:02:34 COT DEBUG2 remoteListenThread_1: queue event 1,134  
SET_ADD_TABLE
2008-08-29 15:02:34 COT DEBUG2 remoteListenThread_1: queue event 1,135  
SUBSCRIBE_SET
2008-08-29 15:02:34 COT DEBUG2 remoteListenThread_1: queue event 1,136  
ENABLE_SUBSCRIPTION
2008-08-29 15:02:34 COT DEBUG2 remoteListenThread_1: queue event 1,137  
SYNC
2008-08-29 15:02:34 COT DEBUG1 subscriber_1_provider_1 "host=127.0.0.1  
dbname=ace user=postgres port=5432": backend pid = 2331
2008-08-29 15:02:34 COT DEBUG2 remoteListenThread_1: queue event 1,138  
SYNC
2008-08-29 15:02:34 COT DEBUG2 remoteListenThread_1: queue event 1,139  
SYNC
2008-08-29 15:02:34 COT DEBUG1 remoteWorkerThread_1: connected to data  
provider 1 on 'host=127.0.0.1 dbname=ace user=postgres port=5432'
2008-08-29 15:02:34 COT INFO   remoteWorkerThread_1: syncing set 1  
with 1 table(s) from provider 1
2008-08-29 15:02:34 COT DEBUG2  ssy_action_list length: 0
2008-08-29 15:02:34 COT INFO   remoteWorkerThread_1: syncing set 999  
with 1 table(s) from provider 1
2008-08-29 15:02:35 COT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 87
slon(2324,0xb0185000) malloc: *** mmap(size=2147483648) failed (error  
code=12)
*** error: can't allocate region
*** set a breakpoint in malloc_error_break to debug
2008-08-29 15:02:36 COT DEBUG2 slon: child terminated status: 11; pid:  
2324, current worker pid: 2324
2008-08-29 15:02:36 COT DEBUG1 slon: restart of worker in 10 seconds

Apparently my slon daemon on my slave node dies (I got the log from  
node2)

This is on OSX leopard 64 bit. I can try CSV version of slony....


Ries





-------------------------------------------------------------------------------------------------
A: Because it messes up the order in which people normally read text.
Q: Why is top-posting such a bad thing?
A: Top-posting.
Q: What is the most annoying thing in e-mail?

From jennifer.spencer at stanford.edu  Fri Aug 29 13:12:17 2008
From: jennifer.spencer at stanford.edu (Jennifer Spencer)
Date: Fri Aug 29 13:12:45 2008
Subject: [Slony1-general] How to define clusters - please help
In-Reply-To: <87myiv99zx.fsf@dba2.int.libertyrms.com>
References: <48B6116E.5020504@stanford.edu>	<20080828145151.GA23592@crankycanuck.ca>	<48B70646.7010203@stanford.edu>	<20080828203137.GA27287@crankycanuck.ca>
	<87myiv99zx.fsf@dba2.int.libertyrms.com>
Message-ID: <48B85821.2070705@stanford.edu>

Thanks everyone for your responses and your help.  I needed the clarifications and thought 
provocations you provided before presenting the situation to our management.

After a long meeting and a lot of time at the white board, we've decided to try the following:
essentially, make clusters act as sets.

> Now, here's an incredibly ugly kludge I just thought of and haven't
> tested that would make this possible.  You create a bunch more targets
> on node 2, each of which subscribes to the target set. 
I considered going this route for a few minutes, until I began to draw it out on paper.  My problem 
with that is that some of the subscribers I have want some of the same (large, frequently updated) 
data tables, so we'd up with extra duplicates.  Bad.

Instead, we will figure out the most efficient division of frequently-desired data tables, and make 
each division into its own cluster (with one set).  We will run multiple clusters between nodes 1 & 2, 
having two slons going for each cluster, one with archiving on.  Each archive job will write to a 
different directory.  Different customers can pick up from whichever of those directories they want, 
being careful to manage their own directory structure so the log filenames don't overlap.  If you see 
any obvious problems with this, please feel free to let me know!

It was either that or write a parsing (filtering) software application for our log-shipping logs (in 
my copious spare time).

I am glad you guys were here to bounce these ideas off.  I kept thinking "maybe there's something I am 
missing, or overthinking...".
-Jennifer


Christopher Browne wrote:
> Andrew Sullivan <ajs@crankycanuck.ca> writes:
>> That said, this makes my teeth hurt just to suggest it.  But the log
>> shipping today doesn't really do what you want.  I know there was some
>> intended work to allow filtering in 2.x.  I don't know if it got off
>> the ground.  Chris?
> 
> Jan's looking at implementing an admin tool to be rather friendlier
> than what we have now; the notion of doing filtering seems to be a
> good couple of steps further down the road.
> 
> That's smelling like "3.x", at this point.  We have loosely talked
> about having "smarter triggers" that store more of the metadata in
> more intelligent form, so that you could indeed pick and choose and
> even transform on the way through.
> 
> It has gotten steps further, conceptually; Jan, Simon and I had a chat
> at PGCon about a "wildly more intelligent COPY" that would get us the
> kinds of efficiencies that COPY does for I/U/D updates.  A key thing
> about that would be making it generally useful as a "Data Load Tool"
> (e.g. - presumably with the potential to be popular for data
> warehouse-like applications) so that it wouldn't hang as a
> Slony-I-only "bag on the side."  But implementation hasn't started,
> and *that* would be a logical prerequisite to filtering.

From slony at rvt.dds.nl  Fri Aug 29 14:35:11 2008
From: slony at rvt.dds.nl (ries van Twisk)
Date: Fri Aug 29 14:35:35 2008
Subject: [Slony1-general] Merging sets in Slony 2.0 RC1
In-Reply-To: <BCB2D2F2-2569-4048-91EA-D0447A13EAAE@rvt.dds.nl>
References: <F2F01763-C731-4D79-8985-79982F393F33@rvt.dds.nl>
	<BCB2D2F2-2569-4048-91EA-D0447A13EAAE@rvt.dds.nl>
Message-ID: <DB2C4579-E264-46ED-A66D-56ABE1BAD6CA@rvt.dds.nl>


On Aug 29, 2008, at 3:06 PM, ries van Twisk wrote:

>
> On Aug 29, 2008, at 2:57 PM, ries van Twisk wrote:
>
>> Hey all,
>>
>> this is my first mail on this list and have a question about  
>> merging sets
>>
>> When I execute this command to slonik : merge set (id=1, add id =  
>> 999, origin=1);
>>
>> I set the set perfectly merged to my master DB, but in my slave DB  
>> I don't see
>> the set merged. Is this correct or should I execute the merge set  
>> command for each
>> slave node?
>>
>> This is on slony 2.0 RC1
>>
>>
>> Also in the file slon_watchdog I noticed a semi-colon was missing  
>> on line 47...
>>
>> thanks,
>>
>> Ries
>>
>
>
>
> TO add more info, I get this in my log...
>
> 2008-08-29 15:02:34 COT DEBUG2 remoteListenThread_1: queue event  
> 1,133 STORE_SET
> 2008-08-29 15:02:34 COT DEBUG2 remoteListenThread_1: queue event  
> 1,134 SET_ADD_TABLE
> 2008-08-29 15:02:34 COT DEBUG2 remoteListenThread_1: queue event  
> 1,135 SUBSCRIBE_SET
> 2008-08-29 15:02:34 COT DEBUG2 remoteListenThread_1: queue event  
> 1,136 ENABLE_SUBSCRIPTION
> 2008-08-29 15:02:34 COT DEBUG2 remoteListenThread_1: queue event  
> 1,137 SYNC
> 2008-08-29 15:02:34 COT DEBUG1 subscriber_1_provider_1  
> "host=127.0.0.1 dbname=ace user=postgres port=5432": backend pid =  
> 2331
> 2008-08-29 15:02:34 COT DEBUG2 remoteListenThread_1: queue event  
> 1,138 SYNC
> 2008-08-29 15:02:34 COT DEBUG2 remoteListenThread_1: queue event  
> 1,139 SYNC
> 2008-08-29 15:02:34 COT DEBUG1 remoteWorkerThread_1: connected to  
> data provider 1 on 'host=127.0.0.1 dbname=ace user=postgres port=5432'
> 2008-08-29 15:02:34 COT INFO   remoteWorkerThread_1: syncing set 1  
> with 1 table(s) from provider 1
> 2008-08-29 15:02:34 COT DEBUG2  ssy_action_list length: 0
> 2008-08-29 15:02:34 COT INFO   remoteWorkerThread_1: syncing set 999  
> with 1 table(s) from provider 1
> 2008-08-29 15:02:35 COT DEBUG2 syncThread: new sl_action_seq 1 -  
> SYNC 87
> slon(2324,0xb0185000) malloc: *** mmap(size=2147483648) failed  
> (error code=12)
> *** error: can't allocate region
> *** set a breakpoint in malloc_error_break to debug
> 2008-08-29 15:02:36 COT DEBUG2 slon: child terminated status: 11;  
> pid: 2324, current worker pid: 2324
> 2008-08-29 15:02:36 COT DEBUG1 slon: restart of worker in 10 seconds
>
> Apparently my slon daemon on my slave node dies (I got the log from  
> node2)
>
> This is on OSX leopard 64 bit. I can try CSV version of slony....
>
>
> Ries



To answer my own question,
my problem was this bugs entry : http://bugs.slony.info/bugzilla/show_bug.cgi?id=53



			regards, Ries van Twisk


-------------------------------------------------------------------------------------------------
Ries van Twisk
email: ries@vantwisk.nl
email:ries.vantwisk@bmkllc.com
blog:   http://www.vantwisk.nl/
skype: callto://r.vantwisk
Phone:+593 2 2237444

From slony at rvt.dds.nl  Sat Aug 30 06:12:49 2008
From: slony at rvt.dds.nl (ries van Twisk)
Date: Sat Aug 30 06:13:06 2008
Subject: [Slony1-general] replicate data once a day or week with slony???
Message-ID: <0AC48206-9B10-4236-91E4-2D39B3784989@rvt.dds.nl>

Hey All,

I have a general question about if I can slony use for the following  
situation.


I need to replicate a group of tables from server A to server B once a  
week (once a day for other tables).
This is because for this application they prepare the data during the  
days and at some point it needs to get
pushed out to server B.

Currently I have done using a a simple cron job that executes a stored  
procedure that copies
all data with DBLINK from server A to server B.

The problem is that if the schema changes I need to change it at 3  
locations, storedprocedure where
I execute DBLINK, server A and server B. To eliminate changing the  
stored procedure
I was thinking to use slony with the below slonik commands. Server B  
only handle select statements for reporting.



create set ( id = 998, origin = 1, comment = 'Temporary set' );
set add table ( set id = 998, origin = 1, id = 5, full qualified name  
= 'Table A', comment = '' );
set add table ( set id = 998, origin = 1, id = 6, full qualified name  
= 'Table B', comment = '' );
... more tables


subscribe set ( id = 998, provider = 1, receiver = 2, forward = yes);
sync (id=1);
wait for event (origin=1, confirmed=2, wait on=1);
merge set (id=1, add id = 998, origin=1);

set drop table (origin = 1, id = 3);
sync (id=1);


Would that be a good idea using slony or are there other tools that  
are more suited for this job?

Ries
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080830/6726cecd/attachment.htm
From slony at rvt.dds.nl  Sun Aug 31 17:00:40 2008
From: slony at rvt.dds.nl (ries van Twisk)
Date: Sun Aug 31 17:01:05 2008
Subject: [Slony1-general] execute scripts always fails on 2.0 RC1
Message-ID: <8317F339-B2EC-4628-8230-4B2F2ECC1D97@rvt.dds.nl>

hey All,

when I try to use execute script in slonik it always seems to fail  
with the following error :

bash-3.2$ ./alter-table
DDL script consisting of 3 SQL statements
DDL Statement 0: (0,59) [ALTER TABLE mytesttable ADD COLUMN test text;]
DDL Statement 1: (59,62) []
DDL Statement 2: (62,64) []
<stdin>:15: PGRES_FATAL_ERROR  - ERROR:  syntax error at or near ""
LINE 1:
         ^
Press any key to continue...



This is how my exec script command looks like :

     execute script (
         set id = 1,
         filename = '/tmp/alter',
         event node = 1
     );


Is this a bug do you think??

Ries








From vivek at khera.org  Sun Aug 31 20:21:08 2008
From: vivek at khera.org (Vivek Khera)
Date: Sun Aug 31 20:21:08 2008
Subject: [Slony1-general] Dropped node still in sl_status
Message-ID: <1220239268.6362.5.camel@Nokia-N810-23-14>

SSBoYWQgYSByZXBsaWNhdGlvbiBnb2luZyBmcm9tIG5vZGUgNCB0byAyIHRvIDEuICBJIGRpZCBh
IGRyb3Agbm9kZSBmb3Igbm9kZSAxLiAgSXQgaXMgZGlzYXBwZWFyZWQgZnJvbSBldmVyeXdoZXJl
IGV4Y2VwdCBzbF9zdGF0dXMuIFRoZSBsb2cgZm9yIHNsb24gb24gbm9kZSAxIHNob3dlZCBpdCB3
YXMgZHJvcHBlZCBhbmQgdGhlIHNsb24gZXhpdGVkLiAgSG93IGNhbiBJIGNsZWFyIHRoZSBzdGF0
dXMgbGluZT8gIEkgYWxyZWFkeSByZXN0YXJ0ZWQgdGhlIG90aGVyIHNsb25zLCBidXQgdGhhdCBk
aWRuJ3QgY2xlYXIgaXQuCgpIb3cgdG8gcHJvY2VlZD8KLS0tLS0tLS0tLS0tLS0gbmV4dCBwYXJ0
IC0tLS0tLS0tLS0tLS0tCkFuIEhUTUwgYXR0YWNobWVudCB3YXMgc2NydWJiZWQuLi4KVVJMOiBo
dHRwOi8vbGlzdHMuc2xvbnkuaW5mby9waXBlcm1haWwvc2xvbnkxLWdlbmVyYWwvYXR0YWNobWVu
dHMvMjAwODA4MzEvYWM4YmM4NDgvYXR0YWNobWVudC5odG0K
From chris at dba2.int.libertyrms.com  Tue Aug  5 12:17:48 2008
From: chris at dba2.int.libertyrms.com (chris)
Date: Thu Sep 25 08:27:02 2008
Subject: [Slony1-general] 	Strange behavior adding a new node, very,
	VERY slow
In-Reply-To: <48980FFD.5090408@albourne.com> (Martin Eriksson's message of
	"Tue, 05 Aug 2008 11:31:57 +0300")
References: <4897FEC5.1080905@albourne.com> <4897FF0C.503@albourne.com>
	<4898065A.1030904@postgresqlfr.org> <48980FFD.5090408@albourne.com>
Message-ID: <87abfruv8v.fsf@dba2.int.libertyrms.com>

Martin Eriksson <m.eriksson@albourne.com> writes:
> this is where it start getting weird,
>
> No error anywhere in any log.
>
> with
>
> pg_stat_activity
>
> I just got 3 transactions sitting in idle (to the three other nodes,
> Master, node2,3)
>
> It is using exactly the same schema as i was using for node 2 (as its
> on the same machine so used the same one!)
>
> and its not re-doing it, it is just being very very very slow, and
> about 99% of the time its doing nothing...
>
> its kind of freakish actually...
>
> :/

There should be two relevant connections to look at:

a) The connection to the data provider (e.g. - if the new node is
subscribing to node #1, then look at the conns on the DB for node #1),
which will be generating "COPY to stdout" requests, and

b) The connection to the subscriber, where there will tend to be three
things going on:

  1.  TRUNCATE against each table;
  2.  COPY to load data into each table;
  3.  REINDEX to regenerate the indices

Those are the 4 things that could be noticeable behaviour.

If none of those things are happening, anywhere, then presumably a
network connection has blown out somehow, or something of the sort.

But for sure, look at the behaviour on more databases, as the activity
may be elsewhere than where you expected it to be.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From seklecki at collaborativefusion.com  Sun Aug  3 14:07:55 2008
From: seklecki at collaborativefusion.com (Brian A. Seklecki)
Date: Thu Sep 25 08:27:30 2008
Subject: [Slony1-general] how to determine if the slave is caught up/in
	sync with the master
In-Reply-To: <2968dfd60808031318w3b02fe0flec083bd95e088258@mail.gmail.com>
References: <F1DC4D50-5B7A-45FA-B191-F1114734EBFC@kevinkempterllc.com>
	<2968dfd60808031318w3b02fe0flec083bd95e088258@mail.gmail.com>
Message-ID: <alpine.LFD.1.10.0808031707150.29816@soundwave.ws.pitbpa0.priv.collaborativefusion.com>


> On the master, run this SQL query, replacing _CLUSTERNAME with the
> name you selected for your cluster.
>
> select * from _CLUSTERNAME.sl_status;

Bill Moran @ CFI wrote some Nagios checks. ~BAS

From chris at dba2.int.libertyrms.com  Tue Aug  5 12:26:09 2008
From: chris at dba2.int.libertyrms.com (chris)
Date: Thu Sep 25 08:27:31 2008
Subject: [Slony1-general] how to determine if the slave is caught
	up/insync with the master
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB22047C7206@mail-1.rf.lan> (Shahaf
	Abileah's message of "Tue, 5 Aug 2008 10:41:25 -0700")
References: <F1DC4D50-5B7A-45FA-B191-F1114734EBFC@kevinkempterllc.com>
	<2968dfd60808031318w3b02fe0flec083bd95e088258@mail.gmail.com>
	<082D8A131DF72A4D88C908A1AD3DEB22047C7206@mail-1.rf.lan>
Message-ID: <874p5zuuuw.fsf@dba2.int.libertyrms.com>

Shahaf Abileah <shahaf@redfin.com> writes:
> I ended up using a different approach...
>
> 1. I created a table called replication_heartbeat that has two columns:
> id, last_modified
> 2. I added this table to the replication set
> 3. I pre-populated a single row into this table
> 4. I set up a cron job on the master that executes the following query
> once a minute: update replication_heartbeat set last_modified=now()
> 5. I created Nagios alerts on both the master and the slaves that checks
> this date.  On the master if the date goes stale then I know that
> something is wrong with the update command in cron.  On the slaves if
> the date is stale then I know something is wrong with replication.
>
> It's a poor man's approach and it's probably silly to do all this work
> when there's a built-in mechanism.  The main advantage is that it's dead
> simple to understand.

There's an advantage to each way:

a) With sl_status, you can easily hook up tools like MRTG to graph how
far behind replication tends to be.  Also, it adds NO extra
replication work.

b) The heartbeat actually tests an aspect of the end-to-end
functioning of the database as application platform.

They are both legitimate approaches.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From chris at dba2.int.libertyrms.com  Thu Aug  7 09:52:10 2008
From: chris at dba2.int.libertyrms.com (chris)
Date: Thu Sep 25 08:27:32 2008
Subject: [Slony1-general] IP address change
In-Reply-To: <214281.54752.qm@web25807.mail.ukl.yahoo.com> (Glyn Astill's
	message of "Thu, 7 Aug 2008 16:22:07 +0000 (GMT)")
References: <214281.54752.qm@web25807.mail.ukl.yahoo.com>
Message-ID: <873algu5sd.fsf@dba2.int.libertyrms.com>

Glyn Astill <glynastill@yahoo.co.uk> writes:
> It looks like at some point in the future I'm going to have to shift
> my production servers off onto another subnet.  Is there a set way
> to go about changing the IP addresses of slony nodes?  Or do I just
> go in and piddle with sl_path manually?

There are two parts to this:

1.  You need to change the configuration in sl_path on all nodes.

The *right* way to do this is by using the Slonik command STORE PATH
to submit requests that indicate the revised addresses.

Don't change the IPs manually in sl_path; use STORE PATH.

2.  You need to change the configuration used to get the slon
processes to connect to the nodes that they manage.

Where to change that depends on how you launch slon processes.
-- 
"cbbrowne","@","acm.org"
http://linuxdatabases.info/info/advocacy.html
"For those  of you who are  into writing programs that  are as obscure
and complicated  as possible, there are opportunities  for... real fun
here" -- Arthur Norman
From chris at dba2.int.libertyrms.com  Thu Aug  7 10:02:16 2008
From: chris at dba2.int.libertyrms.com (chris)
Date: Thu Sep 25 08:27:33 2008
Subject: [Slony1-general] How to determine how long it took for a slave to
	'catch up'
In-Reply-To: <5EDD09AE-7E16-41C7-B4E3-2E372753B5C3@kevinkempterllc.com> (kevin
	kempter's message of "Thu, 7 Aug 2008 10:46:06 -0600")
References: <5EDD09AE-7E16-41C7-B4E3-2E372753B5C3@kevinkempterllc.com>
Message-ID: <87y738sqr3.fsf@dba2.int.libertyrms.com>

kevin kempter <kevin@kevinkempterllc.com> writes:
> I setup a SLONY cluster yesterday and it worked without any
> issues. Can anyone point me to some documentation, or tell me how to
> determine how long it took for the slave to be completely caught up
> with the master?

This is a bit of a dynamic thing.

I'd expect to find this in the logs for the subscriber node.  It comes
in two portions:

  a) Firstly, you can check how long it took to do the initial subscription by
     searching in the logs for:

      "copy_set 17 done in 9234.442 seconds"

    (Obviously not expecting those particular numbers :-)!)

  b) Once the initial subscription is done, it will take some period of time
     to catch up and process all SYNCs that were supplied on the provider during 
     that period of time.

     In effect, you'll know when the subscriber is caught up when it stops
     processing multiple SYNCs at once, and processes just one SYNC, namely the
     latest one.

You can run the query, on the origin node:

   select  st_lag_num_events, st_lag_time from _myschema.sl_status where st_received = 3;

This assumes that the replication schema is _myschema, and the node
you're worried about is #3; change as needed.

Once st_lag_num_events falls to 0, the subscriber has caught up.  Of
course, it immediately falls behind again :-).
-- 
let name="cbbrowne" and tld="linuxdatabases.info" in String.concat "@" [name;tld];;
http://cbbrowne.com/info/lsf.html
Rules of the  Evil Overlord #104. "My undercover  agents will not have
tattoos identifying them as members  of my organization, nor will they
be  required to  wear  military boots  or  adhere to  any other  dress
codes." <http://www.eviloverlord.com/>
From chris at dba2.int.libertyrms.com  Mon Aug 11 07:56:03 2008
From: chris at dba2.int.libertyrms.com (chris)
Date: Thu Sep 25 08:27:34 2008
Subject: [Slony1-general] RE: node -1 error after restarting servers
In-Reply-To: <001601c8fbaa$85af4f10$14050a0a@dei26g028575> (Mark Steben's
	message of "Mon, 11 Aug 2008 08:05:18 -0400")
References: <001601c8fbaa$85af4f10$14050a0a@dei26g028575>
Message-ID: <87r68vr476.fsf@dba2.int.libertyrms.com>

"Mark Steben" <msteben@autorevenue.com> writes:
> I messed up and sent this originally as HTML.  Resending as plain text.

Thanks, that's helpful!

> Hi - hoping for some help.
> I'm running Slony 1.2.14 on a simple 1 master 1 slave configuration.? Each
> server is running Postgres 8.2.5.
> I had to restart both master and slave servers.? When I tried to restart the
> slons I got errors in the slave log that
> Table ids were already assigned within the set.? So I dropped the set,
> dropped the path and the listens,
> And recreated all with CREATE SET,? STORE PATH, and STORE LISTENS on the
> provider
>
> Now when I restart the slons and subscribe the newly defined set I get the
> following error in the logs
> On the slave:
> ?? ?node -1 not found in runtime configuration
>
> and the copy fails.
>
> When I query SL_NODE on the master I get:
>
> ? ?no_id | no_active |?? no_comment??? | no_spool 
> -------+-----------+-----------------+----------
> ???? 1 | t???????? | Master Node???? | f
> ???? 2 | f???????? | <event pending> | f
>
> And the same query on the slave gives:
>
> no_id | no_active |??????? no_comment??????? | no_spool 
> -------+-----------+--------------------------+----------
> ???? 1 | t???????? | Master Node?????????? ?????????| f
> ???? 2 | t???????? | subscriber node?????????????? ?| f
> (2 rows)
>
> Do I have to recreate the node(s) as well?? ?
>
> Any help would be appreciated.? thanks

That error message takes place at the beginning of the function
"copy_set", and indicates that the slon couldn't find the node in the
in-memory configuration.

That can commonly be rectified by restarting the slon, which will
cause the slon to reread its configuration.

Except, we should take a step back.  The trouble is that copy_set()
was trying to access a node that it wasn't properly aware of.

The data that you have provided about sl_node on both nodes looks
useful, actually.  It indicates that the "master" node hasn't yet
figured out its configuration, which seems quite plausible to cause
further troubles.

If I had to guess, I'd imagine that perhaps you hadn't run a slon
against the "master" node and that it isn't properly aware, yet, that
node #2 has been set up.

Before you start trying to set up subscriptions, make sure you can run
things like "STORE PATH" and see, in the logs, that this has been
processed by both nodes.
-- 
let name="cbbrowne" and tld="acm.org" in String.concat "@" [name;tld];;
http://cbbrowne.com/info/x.html
"What  you end  up with,  after  running an  operating system  concept
through these  many marketing coffee filters, is  something not unlike
plain hot water."  -- Matt Welsh
