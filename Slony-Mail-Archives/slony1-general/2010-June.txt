From JanWieck at Yahoo.com  Tue Jun  1 06:15:16 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Tue, 01 Jun 2010 09:15:16 -0400
Subject: [Slony1-general] 2.0.4 RC1
In-Reply-To: <4C004A41.6030800@ca.afilias.info>
References: <4BFFC8F4.20305@ca.afilias.info>	<AANLkTil6oottZSmC43fh9U7Py7OTXX--O81XjqSaWogv@mail.gmail.com>	<4C001197.4080502@ca.afilias.info>	<AANLkTinOicW_y6F4Qcp_pcGcJrj3-gs9n1OzOibWS-q8@mail.gmail.com>
	<4C004A41.6030800@ca.afilias.info>
Message-ID: <4C0507E4.3080304@Yahoo.com>

On 5/28/2010 6:57 PM, Steve Singer wrote:
> Gurjeet Singh wrote:
>> On Fri, May 28, 2010 at 2:55 PM, Steve Singer <ssinger at ca.afilias.info 
> 
>> One of our customers is looking to deploy this in an almost unattended 
>> environment, and having memory leaks in Slon will most definitely hurt 
>> their ability to deploy it with confidence.
>> 
>> I would prefer to see the patch incorporated in 2.0.4 and have people 
>> test the RC1 than have it included after 2.0.4 in the hopes that people 
>> would test the CVS version.
> 
> What do others think?
> 
> I take it your offering to give the leak memory hanges a good workout 
> before we tag 2.0.4 final?
> 
> 
> (A version of 2.0.4 RC1 + these changes can be found at 
> http://github.com/ssinger/slony/tree/203_memfixes_UlrichWeber)

Is that the memory leak from not freeing notifications? If so, let's 
test it with bombarding a slon with a gazillion notifies and if it 
survives, put it into 2.0.4.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From ssinger at ca.afilias.info  Tue Jun  1 06:31:02 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 01 Jun 2010 09:31:02 -0400
Subject: [Slony1-general] 2.0.4 RC1
In-Reply-To: <4C0507E4.3080304@Yahoo.com>
References: <4BFFC8F4.20305@ca.afilias.info>	<AANLkTil6oottZSmC43fh9U7Py7OTXX--O81XjqSaWogv@mail.gmail.com>	<4C001197.4080502@ca.afilias.info>	<AANLkTinOicW_y6F4Qcp_pcGcJrj3-gs9n1OzOibWS-q8@mail.gmail.com>
	<4C004A41.6030800@ca.afilias.info> <4C0507E4.3080304@Yahoo.com>
Message-ID: <4C050B96.5010104@ca.afilias.info>

Jan Wieck wrote:
> On 5/28/2010 6:57 PM, Steve Singer wrote:

>>
>> (A version of 2.0.4 RC1 + these changes can be found at 
>> http://github.com/ssinger/slony/tree/203_memfixes_UlrichWeber)
> 
> Is that the memory leak from not freeing notifications? If so, let's 
> test it with bombarding a slon with a gazillion notifies and if it 
> survives, put it into 2.0.4.
> 
> 

The patch frees memory in a number of places including the cleanup 
thread and during the set copy.  You can view the patch at the above URL.

The 1.2 version of the patch also removed the notify stuff (but that 
wasn't required with 2.0 the code was removed).   If we are happy with 
things we should apply it to 1.2 as well for the next time we do a 1.2.x 
release.

> Jan
> 


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From l.bolzani at gmail.com  Tue Jun  1 07:21:13 2010
From: l.bolzani at gmail.com (Lorenzo Bolzani)
Date: Tue, 1 Jun 2010 16:21:13 +0200
Subject: [Slony1-general] Slony with different schemas
In-Reply-To: <4C03E9D7.4080809@whitepages.com>
References: <AANLkTikgOh4Tmd1D1D7vl6FyW8d7ekcrlVmS-kSmrtXW@mail.gmail.com>
	<4C03E9D7.4080809@whitepages.com>
Message-ID: <AANLkTimhtNmZdaGluDRzgXpqWFrPSnbsVT7lMmxm_z_j@mail.gmail.com>

Steve, thanks for the pointer, I have the book but never found the time to
actually read it beyond the first chapters. i'll check it for ideas.


Michael, the scenario is simpler than the one you described.

There are three isolated installations located in three different cities.
Each one is upgraded when possible, strictly one at a time.
Each upgrade, as usual, could imply a data migration step from the old to
the new version.

The new requirement is to centralize part of the data, like the plant
personnel data, the vehicles data and the products catalogue (petroleum
materials). The rest of the data is unique for each installation.

All the editing on these items will be done connecting remotely to the
master installation. The changes should be replicated in almost real time to
the other plants (even to the same one where the changes are made from). We
expect very little activity on these items, something like a dozen of edits
per day on about 10 tables.

Slave plants have to be autonomous in case of network failure so all the
data has to be available locally.

There is not the requirement to change the plant that acts as the master.

Up to this point I would think slony is the ideal tool.


As you said, every time you upgrade any installation you need to test all
the new scenarios and this IS hard. The only thing mitigating this is the
expectation (hope?) for these tables to be typically quite stable. And the
schema differences checks could be automated with a simple script.


Accepting this, the only remaining issue are the differences in the schemas
that maybe could be fixed with a view/trigger "hack" as proposed. I do not
like this, but the option of a message based solution looks worse and
presents most of the same problems. BTW I do not now of any lightweight Java
data integration message framework.


Bye


Lorenzo
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100601/a5d7cece/attachment.htm 

From vivek at khera.org  Tue Jun  1 07:35:39 2010
From: vivek at khera.org (Vick Khera)
Date: Tue, 1 Jun 2010 10:35:39 -0400
Subject: [Slony1-general] Slony with different schemas
In-Reply-To: <4C03DB13.3070400@ca.afilias.info>
References: <AANLkTikgOh4Tmd1D1D7vl6FyW8d7ekcrlVmS-kSmrtXW@mail.gmail.com>
	<20100531154639.GA45740@shinkuro.com>
	<4C03DB13.3070400@ca.afilias.info>
Message-ID: <AANLkTilgd3qCFFgIHG0RP74ux9wQNkeR-MQ1w18UbTxb@mail.gmail.com>

On Mon, May 31, 2010 at 11:51 AM, Steve Singer <ssinger at ca.afilias.info> wrote:
> The idea is that you use a mixture of views and triggers to have 1
> schema that can be used by both versions of the applications that expect
> different schemas. ?You then upgrade all your applications and then you
> swap the schema out to one that only has support for the newer version
> of the application.
>
> This sounds like a lot of extra work

We regularly do this; however we limit the changes between releases.
Sometimes we'll do two or three releases within a week just to do the
migrations of the database.

Since we run our system as a service, there is really only one live
copy to worry about, not a bunch of copies at customers' sites.

From JanWieck at Yahoo.com  Tue Jun  1 08:02:26 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Tue, 01 Jun 2010 11:02:26 -0400
Subject: [Slony1-general] 2.0.4 RC1
In-Reply-To: <4C050B96.5010104@ca.afilias.info>
References: <4BFFC8F4.20305@ca.afilias.info>	<AANLkTil6oottZSmC43fh9U7Py7OTXX--O81XjqSaWogv@mail.gmail.com>	<4C001197.4080502@ca.afilias.info>	<AANLkTinOicW_y6F4Qcp_pcGcJrj3-gs9n1OzOibWS-q8@mail.gmail.com>	<4C004A41.6030800@ca.afilias.info>
	<4C0507E4.3080304@Yahoo.com> <4C050B96.5010104@ca.afilias.info>
Message-ID: <4C052102.3030300@Yahoo.com>

On 6/1/2010 9:31 AM, Steve Singer wrote:
> Jan Wieck wrote:
>> On 5/28/2010 6:57 PM, Steve Singer wrote:
> 
>>>
>>> (A version of 2.0.4 RC1 + these changes can be found at 
>>> http://github.com/ssinger/slony/tree/203_memfixes_UlrichWeber)
>> 
>> Is that the memory leak from not freeing notifications? If so, let's 
>> test it with bombarding a slon with a gazillion notifies and if it 
>> survives, put it into 2.0.4.
>> 
>> 
> 
> The patch frees memory in a number of places including the cleanup 
> thread and during the set copy.  You can view the patch at the above URL.
> 
> The 1.2 version of the patch also removed the notify stuff (but that 
> wasn't required with 2.0 the code was removed).   If we are happy with 
> things we should apply it to 1.2 as well for the next time we do a 1.2.x 
> release.

After a detailed line by line review of the patch, Steve and I decided 
to commit it for 2.0.4.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From msquires at whitepages.com  Tue Jun  1 08:15:53 2010
From: msquires at whitepages.com (Michael Squires)
Date: Tue, 1 Jun 2010 08:15:53 -0700
Subject: [Slony1-general] Slony with different schemas
In-Reply-To: <AANLkTimhtNmZdaGluDRzgXpqWFrPSnbsVT7lMmxm_z_j@mail.gmail.com>
References: <AANLkTikgOh4Tmd1D1D7vl6FyW8d7ekcrlVmS-kSmrtXW@mail.gmail.com>	<4C03E9D7.4080809@whitepages.com>
	<AANLkTimhtNmZdaGluDRzgXpqWFrPSnbsVT7lMmxm_z_j@mail.gmail.com>
Message-ID: <4C052429.2010201@whitepages.com>

Glad to hear that the situation is more constrained than first appeared.

At that point the one suggestion I'd make is to try to structure things such that you will upgrade
the slaves prior to or in conjunction with the master. That is, have no slave at a lower rev level
than the master. That allows you to structure the views such that the higher rev levels can
successfully read from views that are missing new columns, but the master isn't constrained to write
backward-compatible changes.

As far as a messaging framework is concerned, unless you have other needs for it, I think that slony
will be an easier path for you, even with the extra views. You'll be able to structure the system
just in database terms, without too much focus on the underlying messaging.

Good luck,
Michael

On 6/1/10 7:21 AM, Lorenzo Bolzani wrote:
> 
> Steve, thanks for the pointer, I have the book but never found the time
> to actually read it beyond the first chapters. i'll check it for ideas.
> 
> 
> Michael, the scenario is simpler than the one you described.
> 
> There are three isolated installations located in three different
> cities. Each one is upgraded when possible, strictly one at a time.
> Each upgrade, as usual, could imply a data migration step from the old
> to the new version.
> 
> The new requirement is to centralize part of the data, like the plant
> personnel data, the vehicles data and the products catalogue (petroleum
> materials). The rest of the data is unique for each installation.
> 
> All the editing on these items will be done connecting remotely to the
> master installation. The changes should be replicated in almost real
> time to the other plants (even to the same one where the changes are
> made from). We expect very little activity on these items, something
> like a dozen of edits per day on about 10 tables.
> 
> Slave plants have to be autonomous in case of network failure so all the
> data has to be available locally.
> 
> There is not the requirement to change the plant that acts as the master.
> 
> Up to this point I would think slony is the ideal tool.
> 
> 
> As you said, every time you upgrade any installation you need to test
> all the new scenarios and this IS hard. The only thing mitigating this
> is the expectation (hope?) for these tables to be typically quite
> stable. And the schema differences checks could be automated with a
> simple script.
> 
> 
> Accepting this, the only remaining issue are the differences in the
> schemas that maybe could be fixed with a view/trigger "hack" as
> proposed. I do not like this, but the option of a message based solution
> looks worse and presents most of the same problems. BTW I do not now of
> any lightweight Java data integration message framework.
> 
> 
> Bye
> 
> 
> Lorenzo
> 

From ssinger at ca.afilias.info  Tue Jun  1 08:25:38 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 01 Jun 2010 11:25:38 -0400
Subject: [Slony1-general] 2.0.4 RC2
Message-ID: <4C052672.9000207@ca.afilias.info>

I have tagged Slony-I 2.0.4 RC2

This is like 2.0.4 RC1 plus the memory leak fixes discussed on the list.

http://lists.slony.info/downloads/2.0/source/slony1-2.0.4.rc2.tar.bz2

It would be nice if people could test it and report back to the list.




-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From jason at merchantcircle.com  Tue Jun  1 09:18:58 2010
From: jason at merchantcircle.com (Jason Culverhouse)
Date: Tue, 1 Jun 2010 09:18:58 -0700
Subject: [Slony1-general] Replication Hung
Message-ID: <52A818E3-197E-4D4A-A28E-12F722CDCF18@merchantcircle.com>

I have a problem where my replication is hung,  I don't really know where to start....  This table is in set "38", most everything is in set "1", it looks like set 38 isn't "caught up"  in the ordering.
Any Idea's on how to repair this?  Is this problem because the set's are not merged? Can I merge the sets? Version is slony1-1.2.15 on postgres 8.3


2010-06-01 09:11:53 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 574847
2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_50: forward confirm 60,574847 received by 50
2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: SYNC 33381140 processing
2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 38 with 4 table(s) from provider 40
2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 1 with 139 table(s) from provider 40
2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: current local log_status is 0
2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40_40: current remote log_status = 1
2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.008 seconds delay for first row
2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: "update only "public"."review" set zipcode_id='80203' where id='1026409';
update only "public"."review" set zipcode_id='80203' where id='1026401';
update only "public"."review" set zipcode_id='80203' where id='1008795';
update only "public"."review" set zipcode_id='80203' where id='1008048';
update only "public"."review" set zipcode_id='80203' where id='1007445';
insert into "public"."company_zip_updated" (id,company_id,zipcode_id) values ('1205','30149282','80122');
update only "public"."merchant" set zipcode_id='80203' where id='905955';
update only "public"."advertisement" set zipcode_id='80203' where id='1467390';
update only "public"."advertisement" set zipcode_id='80203' where id='1375973';
update only "public"."advertisement" set zipcode_id='80203' where id='1389545';
" ERROR:  duplicate key value violates unique constraint "company_zip_updated_company_id_key"
 - qualification was: where log_origin = 40 and (  (
 log_tableid in (158,159,160,161)
    and (log_xid < '622094999')
    and (log_xid >= '622094987')
) or (
    log_tableid in (124,121,117,118,6,22,23,25,26,31,32,43,44,45,46,55,56,57,59,78,115,79,106,114,116,3,4,5,9,11,12,13,14,15,1,7,2,8,16,17,27,28,29,33,34,36,39,40,41,42,47,48,49,50,51,52,53,54,61,63,64,65,69,71,72,74,75,76,77,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,107,108,109,110,111,112,113,119,120,123,122,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157)
    and (log_xid < '622094999')
    and (log_xid >= '622094987')
) )
2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.016 seconds until close cursor
2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: inserts=1 updates=79 deletes=0
2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: SYNC aborted


Here is the table definition

 Table "public.company_zip_updated"
   Column   |  Type   |                            Modifiers                             
------------+---------+------------------------------------------------------------------
 id         | integer | not null default nextval('company_zip_updated_id_seq'::regclass)
 company_id | integer | not null
 zipcode_id | integer | not null
Indexes:
    "company_zip_updated_pkey" PRIMARY KEY, btree (id)
    "company_zip_updated_company_id_key" UNIQUE, btree (company_id)
Foreign-key constraints:
    "company_zip_updated_company_id_fkey" FOREIGN KEY (company_id) REFERENCES company(id) ON DELETE CASCADE
    "company_zip_updated_zipcode_id_fkey" FOREIGN KEY (zipcode_id) REFERENCES zipcode(zip) ON DELETE CASCADE
Triggers:
    _mc_cluster_logtrigger_161 AFTER INSERT OR DELETE OR UPDATE ON company_zip_updated FOR EACH ROW EXECUTE PROCEDURE _mc_cluster.logtrigger('_mc_cluster', '161', 'kvv')




From ssinger at ca.afilias.info  Tue Jun  1 09:48:42 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 01 Jun 2010 12:48:42 -0400
Subject: [Slony1-general] Replication Hung
In-Reply-To: <52A818E3-197E-4D4A-A28E-12F722CDCF18@merchantcircle.com>
References: <52A818E3-197E-4D4A-A28E-12F722CDCF18@merchantcircle.com>
Message-ID: <4C0539EA.5030306@ca.afilias.info>

Jason Culverhouse wrote:
> I have a problem where my replication is hung,  I don't really know where to start....  This table is in set "38", most everything is in set "1", it looks like set 38 isn't "caught up"  in the ordering.
> Any Idea's on how to repair this?  Is this problem because the set's are not merged? Can I merge the sets? Version is slony1-1.2.15 on postgres 8.3
> 
> 

Your problem appears to be that a row being inserted into your replica 
is already on your replica so it is failing.  This is bad and I am very 
curious to know how your cluster got into this state.

Have you mean making any schema changes to this cluster.  Was EXECUTE 
SCRIPT used or was it not used?

Have you tried making any data changes through EXECUTE SCRIPT (ie would 
someone have run a EXECUTE SCRIPT (..ONLY ON ..) the replica that 
inserst data into your company_zip_updated table).

Options for fixing it include

-Dropping this node from replication and rebuilding the replica. 
Depending on the size of your data this might be the simplest.

-Deleting the offending row of company_zip_updated from the replica 
using EXECUTE SCRIPT ONLY ON and letting replication progress.  I will 
warn you that this is treating the visible symptom only and does not 
address the cause.  This could make things worse for you (it is hard to 
say without knowing what the actual problem is)






> 2010-06-01 09:11:53 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 574847
> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_50: forward confirm 60,574847 received by 50
> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: SYNC 33381140 processing
> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 38 with 4 table(s) from provider 40
> 2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 1 with 139 table(s) from provider 40
> 2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: current local log_status is 0
> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40_40: current remote log_status = 1
> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.008 seconds delay for first row
> 2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: "update only "public"."review" set zipcode_id='80203' where id='1026409';
> update only "public"."review" set zipcode_id='80203' where id='1026401';
> update only "public"."review" set zipcode_id='80203' where id='1008795';
> update only "public"."review" set zipcode_id='80203' where id='1008048';
> update only "public"."review" set zipcode_id='80203' where id='1007445';
> insert into "public"."company_zip_updated" (id,company_id,zipcode_id) values ('1205','30149282','80122');
> update only "public"."merchant" set zipcode_id='80203' where id='905955';
> update only "public"."advertisement" set zipcode_id='80203' where id='1467390';
> update only "public"."advertisement" set zipcode_id='80203' where id='1375973';
> update only "public"."advertisement" set zipcode_id='80203' where id='1389545';
> " ERROR:  duplicate key value violates unique constraint "company_zip_updated_company_id_key"
>  - qualification was: where log_origin = 40 and (  (
>  log_tableid in (158,159,160,161)
>     and (log_xid < '622094999')
>     and (log_xid >= '622094987')
> ) or (
>     log_tableid in (124,121,117,118,6,22,23,25,26,31,32,43,44,45,46,55,56,57,59,78,115,79,106,114,116,3,4,5,9,11,12,13,14,15,1,7,2,8,16,17,27,28,29,33,34,36,39,40,41,42,47,48,49,50,51,52,53,54,61,63,64,65,69,71,72,74,75,76,77,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,107,108,109,110,111,112,113,119,120,123,122,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157)
>     and (log_xid < '622094999')
>     and (log_xid >= '622094987')
> ) )
> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.016 seconds until close cursor
> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: inserts=1 updates=79 deletes=0
> 2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: SYNC aborted
> 
> 
> Here is the table definition
> 
>  Table "public.company_zip_updated"
>    Column   |  Type   |                            Modifiers                             
> ------------+---------+------------------------------------------------------------------
>  id         | integer | not null default nextval('company_zip_updated_id_seq'::regclass)
>  company_id | integer | not null
>  zipcode_id | integer | not null
> Indexes:
>     "company_zip_updated_pkey" PRIMARY KEY, btree (id)
>     "company_zip_updated_company_id_key" UNIQUE, btree (company_id)
> Foreign-key constraints:
>     "company_zip_updated_company_id_fkey" FOREIGN KEY (company_id) REFERENCES company(id) ON DELETE CASCADE
>     "company_zip_updated_zipcode_id_fkey" FOREIGN KEY (zipcode_id) REFERENCES zipcode(zip) ON DELETE CASCADE
> Triggers:
>     _mc_cluster_logtrigger_161 AFTER INSERT OR DELETE OR UPDATE ON company_zip_updated FOR EACH ROW EXECUTE PROCEDURE _mc_cluster.logtrigger('_mc_cluster', '161', 'kvv')
> 
> 
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From jason at merchantcircle.com  Tue Jun  1 09:50:37 2010
From: jason at merchantcircle.com (Jason Culverhouse)
Date: Tue, 1 Jun 2010 09:50:37 -0700
Subject: [Slony1-general] Replication Hung
In-Reply-To: <52A818E3-197E-4D4A-A28E-12F722CDCF18@merchantcircle.com>
References: <52A818E3-197E-4D4A-A28E-12F722CDCF18@merchantcircle.com>
Message-ID: <05071C7C-37F4-4A85-9818-F90A01F55674@merchantcircle.com>

Ok...
So I just found out that a developer ran a
TRUNCATE TABLE on company_zip_updated

This table is in a set with 4 other tables.

Is the correct course of action to drop the set with the table and resubscribe?

Jason


On Jun 1, 2010, at 9:18 AM, Jason Culverhouse wrote:

> I have a problem where my replication is hung,  I don't really know where to start....  This table is in set "38", most everything is in set "1", it looks like set 38 isn't "caught up"  in the ordering.
> Any Idea's on how to repair this?  Is this problem because the set's are not merged? Can I merge the sets? Version is slony1-1.2.15 on postgres 8.3
> 
> 
> 2010-06-01 09:11:53 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 574847
> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_50: forward confirm 60,574847 received by 50
> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: SYNC 33381140 processing
> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 38 with 4 table(s) from provider 40
> 2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 1 with 139 table(s) from provider 40
> 2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: current local log_status is 0
> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40_40: current remote log_status = 1
> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.008 seconds delay for first row
> 2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: "update only "public"."review" set zipcode_id='80203' where id='1026409';
> update only "public"."review" set zipcode_id='80203' where id='1026401';
> update only "public"."review" set zipcode_id='80203' where id='1008795';
> update only "public"."review" set zipcode_id='80203' where id='1008048';
> update only "public"."review" set zipcode_id='80203' where id='1007445';
> insert into "public"."company_zip_updated" (id,company_id,zipcode_id) values ('1205','30149282','80122');
> update only "public"."merchant" set zipcode_id='80203' where id='905955';
> update only "public"."advertisement" set zipcode_id='80203' where id='1467390';
> update only "public"."advertisement" set zipcode_id='80203' where id='1375973';
> update only "public"."advertisement" set zipcode_id='80203' where id='1389545';
> " ERROR:  duplicate key value violates unique constraint "company_zip_updated_company_id_key"
> - qualification was: where log_origin = 40 and (  (
> log_tableid in (158,159,160,161)
>    and (log_xid < '622094999')
>    and (log_xid >= '622094987')
> ) or (
>    log_tableid in (124,121,117,118,6,22,23,25,26,31,32,43,44,45,46,55,56,57,59,78,115,79,106,114,116,3,4,5,9,11,12,13,14,15,1,7,2,8,16,17,27,28,29,33,34,36,39,40,41,42,47,48,49,50,51,52,53,54,61,63,64,65,69,71,72,74,75,76,77,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,107,108,109,110,111,112,113,119,120,123,122,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157)
>    and (log_xid < '622094999')
>    and (log_xid >= '622094987')
> ) )
> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.016 seconds until close cursor
> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: inserts=1 updates=79 deletes=0
> 2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: SYNC aborted
> 
> 
> Here is the table definition
> 
> Table "public.company_zip_updated"
>   Column   |  Type   |                            Modifiers                             
> ------------+---------+------------------------------------------------------------------
> id         | integer | not null default nextval('company_zip_updated_id_seq'::regclass)
> company_id | integer | not null
> zipcode_id | integer | not null
> Indexes:
>    "company_zip_updated_pkey" PRIMARY KEY, btree (id)
>    "company_zip_updated_company_id_key" UNIQUE, btree (company_id)
> Foreign-key constraints:
>    "company_zip_updated_company_id_fkey" FOREIGN KEY (company_id) REFERENCES company(id) ON DELETE CASCADE
>    "company_zip_updated_zipcode_id_fkey" FOREIGN KEY (zipcode_id) REFERENCES zipcode(zip) ON DELETE CASCADE
> Triggers:
>    _mc_cluster_logtrigger_161 AFTER INSERT OR DELETE OR UPDATE ON company_zip_updated FOR EACH ROW EXECUTE PROCEDURE _mc_cluster.logtrigger('_mc_cluster', '161', 'kvv')
> 
> 
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From singh.gurjeet at gmail.com  Tue Jun  1 10:42:41 2010
From: singh.gurjeet at gmail.com (Gurjeet Singh)
Date: Tue, 1 Jun 2010 13:42:41 -0400
Subject: [Slony1-general] 2.0.4 RC1
In-Reply-To: <4C052102.3030300@Yahoo.com>
References: <4BFFC8F4.20305@ca.afilias.info>
	<AANLkTil6oottZSmC43fh9U7Py7OTXX--O81XjqSaWogv@mail.gmail.com> 
	<4C001197.4080502@ca.afilias.info>
	<AANLkTinOicW_y6F4Qcp_pcGcJrj3-gs9n1OzOibWS-q8@mail.gmail.com> 
	<4C004A41.6030800@ca.afilias.info> <4C0507E4.3080304@Yahoo.com> 
	<4C050B96.5010104@ca.afilias.info> <4C052102.3030300@Yahoo.com>
Message-ID: <AANLkTimDCJ7EMfaoRMldnaag1Fb37YJk92I-TGKgNYkW@mail.gmail.com>

On Tue, Jun 1, 2010 at 11:02 AM, Jan Wieck <JanWieck at yahoo.com> wrote:

> On 6/1/2010 9:31 AM, Steve Singer wrote:
> > Jan Wieck wrote:
> >> On 5/28/2010 6:57 PM, Steve Singer wrote:
> >
> >>>
> >>> (A version of 2.0.4 RC1 + these changes can be found at
> >>> http://github.com/ssinger/slony/tree/203_memfixes_UlrichWeber)
> >>
> >> Is that the memory leak from not freeing notifications? If so, let's
> >> test it with bombarding a slon with a gazillion notifies and if it
> >> survives, put it into 2.0.4.
> >>
> >>
> >
> > The patch frees memory in a number of places including the cleanup
> > thread and during the set copy.  You can view the patch at the above URL.
> >
> > The 1.2 version of the patch also removed the notify stuff (but that
> > wasn't required with 2.0 the code was removed).   If we are happy with
> > things we should apply it to 1.2 as well for the next time we do a 1.2.x
> > release.
>
> After a detailed line by line review of the patch, Steve and I decided
> to commit it for 2.0.4.
>
>
Thank you :)

-- 
gurjeet.singh
@ EnterpriseDB - The Enterprise Postgres Company
http://www.EnterpriseDB.com

singh.gurjeet@{ gmail | yahoo }.com
Twitter/Skype: singh_gurjeet

Mail sent from my BlackLaptop device
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100601/be5646c7/attachment.htm 

From jason at merchantcircle.com  Tue Jun  1 10:56:14 2010
From: jason at merchantcircle.com (Jason Culverhouse)
Date: Tue, 1 Jun 2010 10:56:14 -0700
Subject: [Slony1-general] Replication Hung
In-Reply-To: <4C0539EA.5030306@ca.afilias.info>
References: <52A818E3-197E-4D4A-A28E-12F722CDCF18@merchantcircle.com>
	<4C0539EA.5030306@ca.afilias.info>
Message-ID: <F58F251C-AD51-4A23-8FA7-FF3E4882F807@merchantcircle.com>

Steve,
I tracked it down to a TRUNCATE TABLE on company_zip_updated.
This table is just a transient record keeping table, items get inserted, processed and deleted.

The question is, would a drop set on the set that contains the table repair the problem?
i.e, will this remove the statements that update that table from the log since the SET is removed?
OR
should I consider the EXECUTE SCRIPT ONLY ON to remove the offending rows in that table on the replicas?

I goal is to avoid replicating the whole database again. 
Jason

On Jun 1, 2010, at 9:48 AM, Steve Singer wrote:

> Jason Culverhouse wrote:
>> I have a problem where my replication is hung,  I don't really know where to start....  This table is in set "38", most everything is in set "1", it looks like set 38 isn't "caught up"  in the ordering.
>> Any Idea's on how to repair this?  Is this problem because the set's are not merged? Can I merge the sets? Version is slony1-1.2.15 on postgres 8.3
> 
> Your problem appears to be that a row being inserted into your replica is already on your replica so it is failing.  This is bad and I am very curious to know how your cluster got into this state.
> 
> Have you mean making any schema changes to this cluster.  Was EXECUTE SCRIPT used or was it not used?
> 
> Have you tried making any data changes through EXECUTE SCRIPT (ie would someone have run a EXECUTE SCRIPT (..ONLY ON ..) the replica that inserst data into your company_zip_updated table).
> 
> Options for fixing it include
> 
> -Dropping this node from replication and rebuilding the replica. Depending on the size of your data this might be the simplest.
> 
> -Deleting the offending row of company_zip_updated from the replica using EXECUTE SCRIPT ONLY ON and letting replication progress.  I will warn you that this is treating the visible symptom only and does not address the cause.  This could make things worse for you (it is hard to say without knowing what the actual problem is)
> 
> 
> 
> 
> 
> 
>> 2010-06-01 09:11:53 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 574847
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_50: forward confirm 60,574847 received by 50
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: SYNC 33381140 processing
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 38 with 4 table(s) from provider 40
>> 2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 1 with 139 table(s) from provider 40
>> 2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: current local log_status is 0
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40_40: current remote log_status = 1
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.008 seconds delay for first row
>> 2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: "update only "public"."review" set zipcode_id='80203' where id='1026409';
>> update only "public"."review" set zipcode_id='80203' where id='1026401';
>> update only "public"."review" set zipcode_id='80203' where id='1008795';
>> update only "public"."review" set zipcode_id='80203' where id='1008048';
>> update only "public"."review" set zipcode_id='80203' where id='1007445';
>> insert into "public"."company_zip_updated" (id,company_id,zipcode_id) values ('1205','30149282','80122');
>> update only "public"."merchant" set zipcode_id='80203' where id='905955';
>> update only "public"."advertisement" set zipcode_id='80203' where id='1467390';
>> update only "public"."advertisement" set zipcode_id='80203' where id='1375973';
>> update only "public"."advertisement" set zipcode_id='80203' where id='1389545';
>> " ERROR:  duplicate key value violates unique constraint "company_zip_updated_company_id_key"
>> - qualification was: where log_origin = 40 and (  (
>> log_tableid in (158,159,160,161)
>>    and (log_xid < '622094999')
>>    and (log_xid >= '622094987')
>> ) or (
>>    log_tableid in (124,121,117,118,6,22,23,25,26,31,32,43,44,45,46,55,56,57,59,78,115,79,106,114,116,3,4,5,9,11,12,13,14,15,1,7,2,8,16,17,27,28,29,33,34,36,39,40,41,42,47,48,49,50,51,52,53,54,61,63,64,65,69,71,72,74,75,76,77,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,107,108,109,110,111,112,113,119,120,123,122,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157)
>>    and (log_xid < '622094999')
>>    and (log_xid >= '622094987')
>> ) )
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.016 seconds until close cursor
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: inserts=1 updates=79 deletes=0
>> 2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: SYNC aborted
>> Here is the table definition
>> Table "public.company_zip_updated"
>>   Column   |  Type   |                            Modifiers                             ------------+---------+------------------------------------------------------------------
>> id         | integer | not null default nextval('company_zip_updated_id_seq'::regclass)
>> company_id | integer | not null
>> zipcode_id | integer | not null
>> Indexes:
>>    "company_zip_updated_pkey" PRIMARY KEY, btree (id)
>>    "company_zip_updated_company_id_key" UNIQUE, btree (company_id)
>> Foreign-key constraints:
>>    "company_zip_updated_company_id_fkey" FOREIGN KEY (company_id) REFERENCES company(id) ON DELETE CASCADE
>>    "company_zip_updated_zipcode_id_fkey" FOREIGN KEY (zipcode_id) REFERENCES zipcode(zip) ON DELETE CASCADE
>> Triggers:
>>    _mc_cluster_logtrigger_161 AFTER INSERT OR DELETE OR UPDATE ON company_zip_updated FOR EACH ROW EXECUTE PROCEDURE _mc_cluster.logtrigger('_mc_cluster', '161', 'kvv')
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 
> -- 
> Steve Singer
> Afilias Canada
> Data Services Developer
> 416-673-1142


From scott.marlowe at gmail.com  Tue Jun  1 11:09:46 2010
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Tue, 1 Jun 2010 12:09:46 -0600
Subject: [Slony1-general] Replication Hung
In-Reply-To: <F58F251C-AD51-4A23-8FA7-FF3E4882F807@merchantcircle.com>
References: <52A818E3-197E-4D4A-A28E-12F722CDCF18@merchantcircle.com>
	<4C0539EA.5030306@ca.afilias.info>
	<F58F251C-AD51-4A23-8FA7-FF3E4882F807@merchantcircle.com>
Message-ID: <AANLkTilv1QgtopKBGTM36pS3Kuj7qCE06fgo_pRGJrva@mail.gmail.com>

I'd truncate it on both ends then run a delete without where on the
master and see if that fixed it.

On Tue, Jun 1, 2010 at 11:56 AM, Jason Culverhouse
<jason at merchantcircle.com> wrote:
> Steve,
> I tracked it down to a TRUNCATE TABLE on company_zip_updated.
> This table is just a transient record keeping table, items get inserted, processed and deleted.
>
> The question is, would a drop set on the set that contains the table repair the problem?
> i.e, will this remove the statements that update that table from the log since the SET is removed?
> OR
> should I consider the EXECUTE SCRIPT ONLY ON to remove the offending rows in that table on the replicas?
>
> I goal is to avoid replicating the whole database again.
> Jason
>
> On Jun 1, 2010, at 9:48 AM, Steve Singer wrote:
>
>> Jason Culverhouse wrote:
>>> I have a problem where my replication is hung, ?I don't really know where to start.... ?This table is in set "38", most everything is in set "1", it looks like set 38 isn't "caught up" ?in the ordering.
>>> Any Idea's on how to repair this? ?Is this problem because the set's are not merged? Can I merge the sets? Version is slony1-1.2.15 on postgres 8.3
>>
>> Your problem appears to be that a row being inserted into your replica is already on your replica so it is failing. ?This is bad and I am very curious to know how your cluster got into this state.
>>
>> Have you mean making any schema changes to this cluster. ?Was EXECUTE SCRIPT used or was it not used?
>>
>> Have you tried making any data changes through EXECUTE SCRIPT (ie would someone have run a EXECUTE SCRIPT (..ONLY ON ..) the replica that inserst data into your company_zip_updated table).
>>
>> Options for fixing it include
>>
>> -Dropping this node from replication and rebuilding the replica. Depending on the size of your data this might be the simplest.
>>
>> -Deleting the offending row of company_zip_updated from the replica using EXECUTE SCRIPT ONLY ON and letting replication progress. ?I will warn you that this is treating the visible symptom only and does not address the cause. ?This could make things worse for you (it is hard to say without knowing what the actual problem is)
>>
>>
>>
>>
>>
>>
>>> 2010-06-01 09:11:53 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 574847
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_50: forward confirm 60,574847 received by 50
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: SYNC 33381140 processing
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 38 with 4 table(s) from provider 40
>>> 2010-06-01 09:11:53 PDT DEBUG2 ?ssy_action_list length: 0
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 1 with 139 table(s) from provider 40
>>> 2010-06-01 09:11:53 PDT DEBUG2 ?ssy_action_list length: 0
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: current local log_status is 0
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40_40: current remote log_status = 1
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.008 seconds delay for first row
>>> 2010-06-01 09:11:53 PDT ERROR ?remoteWorkerThread_40: "update only "public"."review" set zipcode_id='80203' where id='1026409';
>>> update only "public"."review" set zipcode_id='80203' where id='1026401';
>>> update only "public"."review" set zipcode_id='80203' where id='1008795';
>>> update only "public"."review" set zipcode_id='80203' where id='1008048';
>>> update only "public"."review" set zipcode_id='80203' where id='1007445';
>>> insert into "public"."company_zip_updated" (id,company_id,zipcode_id) values ('1205','30149282','80122');
>>> update only "public"."merchant" set zipcode_id='80203' where id='905955';
>>> update only "public"."advertisement" set zipcode_id='80203' where id='1467390';
>>> update only "public"."advertisement" set zipcode_id='80203' where id='1375973';
>>> update only "public"."advertisement" set zipcode_id='80203' where id='1389545';
>>> " ERROR: ?duplicate key value violates unique constraint "company_zip_updated_company_id_key"
>>> - qualification was: where log_origin = 40 and ( ?(
>>> log_tableid in (158,159,160,161)
>>> ? ?and (log_xid < '622094999')
>>> ? ?and (log_xid >= '622094987')
>>> ) or (
>>> ? ?log_tableid in (124,121,117,118,6,22,23,25,26,31,32,43,44,45,46,55,56,57,59,78,115,79,106,114,116,3,4,5,9,11,12,13,14,15,1,7,2,8,16,17,27,28,29,33,34,36,39,40,41,42,47,48,49,50,51,52,53,54,61,63,64,65,69,71,72,74,75,76,77,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,107,108,109,110,111,112,113,119,120,123,122,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157)
>>> ? ?and (log_xid < '622094999')
>>> ? ?and (log_xid >= '622094987')
>>> ) )
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.016 seconds until close cursor
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: inserts=1 updates=79 deletes=0
>>> 2010-06-01 09:11:53 PDT ERROR ?remoteWorkerThread_40: SYNC aborted
>>> Here is the table definition
>>> Table "public.company_zip_updated"
>>> ? Column ? | ?Type ? | ? ? ? ? ? ? ? ? ? ? ? ? ? ?Modifiers ? ? ? ? ? ? ? ? ? ? ? ? ? ? ------------+---------+------------------------------------------------------------------
>>> id ? ? ? ? | integer | not null default nextval('company_zip_updated_id_seq'::regclass)
>>> company_id | integer | not null
>>> zipcode_id | integer | not null
>>> Indexes:
>>> ? ?"company_zip_updated_pkey" PRIMARY KEY, btree (id)
>>> ? ?"company_zip_updated_company_id_key" UNIQUE, btree (company_id)
>>> Foreign-key constraints:
>>> ? ?"company_zip_updated_company_id_fkey" FOREIGN KEY (company_id) REFERENCES company(id) ON DELETE CASCADE
>>> ? ?"company_zip_updated_zipcode_id_fkey" FOREIGN KEY (zipcode_id) REFERENCES zipcode(zip) ON DELETE CASCADE
>>> Triggers:
>>> ? ?_mc_cluster_logtrigger_161 AFTER INSERT OR DELETE OR UPDATE ON company_zip_updated FOR EACH ROW EXECUTE PROCEDURE _mc_cluster.logtrigger('_mc_cluster', '161', 'kvv')
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general at lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>>
>> --
>> Steve Singer
>> Afilias Canada
>> Data Services Developer
>> 416-673-1142
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>



-- 
When fascism comes to America, it will be intolerance sold as diversity.

From ssinger at ca.afilias.info  Tue Jun  1 11:16:30 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 01 Jun 2010 14:16:30 -0400
Subject: [Slony1-general] Replication Hung
In-Reply-To: <F58F251C-AD51-4A23-8FA7-FF3E4882F807@merchantcircle.com>
References: <52A818E3-197E-4D4A-A28E-12F722CDCF18@merchantcircle.com>
	<4C0539EA.5030306@ca.afilias.info>
	<F58F251C-AD51-4A23-8FA7-FF3E4882F807@merchantcircle.com>
Message-ID: <4C054E7E.6080708@ca.afilias.info>

Jason Culverhouse wrote:
> Steve,
> I tracked it down to a TRUNCATE TABLE on company_zip_updated.
> This table is just a transient record keeping table, items get inserted, processed and deleted.
> 
> The question is, would a drop set on the set that contains the table repair the problem?
> i.e, will this remove the statements that update that table from the log since the SET is removed?
> OR
> should I consider the EXECUTE SCRIPT ONLY ON to remove the offending rows in that table on the replicas?

A DROP SET won't fix your problem because the problem insert will still 
be sl_log.   (In the future we might want to change things somehow so 
the drop set deletes the pending data?)

Scott's idea of doing a TRUNCATE on both ends makes sense.   The 
TRUNCATE doesn't even need to be inside of the execute script (since the 
_deny() trigger won't fire)



> 
> I goal is to avoid replicating the whole database again. 
> Jason
> 
> On Jun 1, 2010, at 9:48 AM, Steve Singer wrote:
> 
>> Jason Culverhouse wrote:
>>> I have a problem where my replication is hung,  I don't really know where to start....  This table is in set "38", most everything is in set "1", it looks like set 38 isn't "caught up"  in the ordering.
>>> Any Idea's on how to repair this?  Is this problem because the set's are not merged? Can I merge the sets? Version is slony1-1.2.15 on postgres 8.3
>> Your problem appears to be that a row being inserted into your replica is already on your replica so it is failing.  This is bad and I am very curious to know how your cluster got into this state.
>>
>> Have you mean making any schema changes to this cluster.  Was EXECUTE SCRIPT used or was it not used?
>>
>> Have you tried making any data changes through EXECUTE SCRIPT (ie would someone have run a EXECUTE SCRIPT (..ONLY ON ..) the replica that inserst data into your company_zip_updated table).
>>
>> Options for fixing it include
>>
>> -Dropping this node from replication and rebuilding the replica. Depending on the size of your data this might be the simplest.
>>
>> -Deleting the offending row of company_zip_updated from the replica using EXECUTE SCRIPT ONLY ON and letting replication progress.  I will warn you that this is treating the visible symptom only and does not address the cause.  This could make things worse for you (it is hard to say without knowing what the actual problem is)
>>
>>
>>
>>
>>
>>
>>> 2010-06-01 09:11:53 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 574847
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_50: forward confirm 60,574847 received by 50
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: SYNC 33381140 processing
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 38 with 4 table(s) from provider 40
>>> 2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 1 with 139 table(s) from provider 40
>>> 2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: current local log_status is 0
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40_40: current remote log_status = 1
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.008 seconds delay for first row
>>> 2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: "update only "public"."review" set zipcode_id='80203' where id='1026409';
>>> update only "public"."review" set zipcode_id='80203' where id='1026401';
>>> update only "public"."review" set zipcode_id='80203' where id='1008795';
>>> update only "public"."review" set zipcode_id='80203' where id='1008048';
>>> update only "public"."review" set zipcode_id='80203' where id='1007445';
>>> insert into "public"."company_zip_updated" (id,company_id,zipcode_id) values ('1205','30149282','80122');
>>> update only "public"."merchant" set zipcode_id='80203' where id='905955';
>>> update only "public"."advertisement" set zipcode_id='80203' where id='1467390';
>>> update only "public"."advertisement" set zipcode_id='80203' where id='1375973';
>>> update only "public"."advertisement" set zipcode_id='80203' where id='1389545';
>>> " ERROR:  duplicate key value violates unique constraint "company_zip_updated_company_id_key"
>>> - qualification was: where log_origin = 40 and (  (
>>> log_tableid in (158,159,160,161)
>>>    and (log_xid < '622094999')
>>>    and (log_xid >= '622094987')
>>> ) or (
>>>    log_tableid in (124,121,117,118,6,22,23,25,26,31,32,43,44,45,46,55,56,57,59,78,115,79,106,114,116,3,4,5,9,11,12,13,14,15,1,7,2,8,16,17,27,28,29,33,34,36,39,40,41,42,47,48,49,50,51,52,53,54,61,63,64,65,69,71,72,74,75,76,77,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,107,108,109,110,111,112,113,119,120,123,122,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157)
>>>    and (log_xid < '622094999')
>>>    and (log_xid >= '622094987')
>>> ) )
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.016 seconds until close cursor
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: inserts=1 updates=79 deletes=0
>>> 2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: SYNC aborted
>>> Here is the table definition
>>> Table "public.company_zip_updated"
>>>   Column   |  Type   |                            Modifiers                             ------------+---------+------------------------------------------------------------------
>>> id         | integer | not null default nextval('company_zip_updated_id_seq'::regclass)
>>> company_id | integer | not null
>>> zipcode_id | integer | not null
>>> Indexes:
>>>    "company_zip_updated_pkey" PRIMARY KEY, btree (id)
>>>    "company_zip_updated_company_id_key" UNIQUE, btree (company_id)
>>> Foreign-key constraints:
>>>    "company_zip_updated_company_id_fkey" FOREIGN KEY (company_id) REFERENCES company(id) ON DELETE CASCADE
>>>    "company_zip_updated_zipcode_id_fkey" FOREIGN KEY (zipcode_id) REFERENCES zipcode(zip) ON DELETE CASCADE
>>> Triggers:
>>>    _mc_cluster_logtrigger_161 AFTER INSERT OR DELETE OR UPDATE ON company_zip_updated FOR EACH ROW EXECUTE PROCEDURE _mc_cluster.logtrigger('_mc_cluster', '161', 'kvv')
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general at lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>> -- 
>> Steve Singer
>> Afilias Canada
>> Data Services Developer
>> 416-673-1142
> 


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From brianf at consistentstate.com  Tue Jun  1 16:24:05 2010
From: brianf at consistentstate.com (Brian Fehrle)
Date: Tue, 01 Jun 2010 17:24:05 -0600
Subject: [Slony1-general] slony archive log monitoring questions.
Message-ID: <4C059695.1040000@consistentstate.com>

Hi all,

    I have a two node slony cluster and I have the slon daemon on the 
slave node to run with the -a command. I'm attempting to better 
understand how the logs work for slony log shipping, and have noticed a 
bit of "odd" behavior (perhaps just odd to me) that perhaps someone can 
explain to me. It's not just the log shipping that I'd like to better 
understand, but just how the slony slave and master communicate with 
eachother and make sure that the slave is in sync with the master, and 
receiving ALL data that it needs without missing any.

In each of the archive logs created by the slony slave with the -a 
command, it looks something like this:

---------------------------------------

------------------------------------------------------------------
-- Slony-I log shipping archive
-- Node 1, Event 555
------------------------------------------------------------------
start transaction;
select "_slony".archiveTracking_offline('22', '2010-06-01 12:53:11.214886');
-- end of log archiving header
------------------------------------------------------------------
-- start of Slony-I data
------------------------------------------------------------------
select "_slony".sequenceSetValue_offline(1,'38');


------------------------------------------------------------------
-- End Of Archive Log
------------------------------------------------------------------
commit;
vacuum analyze "_slony ".sl_archive_tracking;

---------------------------------------

Obviously, this log has no actual replication data in it, however there 
are two main things that I notice in this. First is the Event number, 
second is the number which is in the select statement for 
archiveTracking_offline (which is the same number as what is in the name 
of this particular file, slony1_log_2_00000000000000000022.sql).

For the slony log shipping to work, I understand that each of the log 
files are required in order, but I've noticed that looking through the 
actual files, the Events themselves sometimes skip a number, or several. 
Example,

---------------------------------------
# cat /path/to/slon_archive_logs/* | grep Event
-- Node 1, Event 554
-- Node 1, Event 555
-- Node 1, Event 556
-- Node 1, Event 558
-- Node 1, Event 559
---------------------------------------

Looking at this, event 557 is missing, however the numbering of the 
archive logs is not broken, each log appears with the expected numbering 
in the name. This happens often, and I originally thought that this was 
due to the log that contains the previous event (in this case 556) would 
contain the data for both events, and 557 would simply not appear. I've 
looked through every single archive log, and the event does not appear 
in any of them, nor does it appear later down the road.

I then thought that this event could be an event that isn't a sync, but 
rather perhaps something else that wouldn't make it into these archive 
logs (this might still bet he case). However, shortly after seeing that 
this event is missing, i took a look at the sl_event table and saw that 
the event is indeed there and is indeed a SYNC:

---------------------------------------
postgres=# select ev_seqno, ev_type from _slony.sl_event where ev_origin 
= '1';
 ev_seqno | ev_type
----------+---------
  554 | SYNC
  555 | SYNC
  556 | SYNC
  557 | SYNC
  558 | SYNC
  559 | SYNC
---------------------------------------


Basically what I want to do is write up a little script that will alert 
me via email if something goes wonky with slony replication. I had an 
event recently where data was missing from the slony slave, and all the 
searching I could do came up with showed that the data was never 
replicated, but slony never reported errors (this is in a recent email 
to this mailing list). It would be easy to raise an alert that says 
"woah event 557 was not found", however if it is normal behavior for 
events to be missing like this, then that wouldn't be a good approach to 
take.

I've seen there are a couple of slony monitoring tools, and I'll be 
checking them out to see if they offer anything that I could use. But 
any other suggestions, or even some clarification as to how some of this 
works would be greatly appreciated. If the situation happens again where 
my slony slave is missing data, I'd like a bit of logs to review and see 
when something may have gone wrong, even if i have to generate these 
logs myself with some sort of monitoring.

Thanks in advance,
    Brian F

From brianf at consistentstate.com  Wed Jun  2 09:09:45 2010
From: brianf at consistentstate.com (Brian Fehrle)
Date: Wed, 02 Jun 2010 10:09:45 -0600
Subject: [Slony1-general] slony archive log monitoring questions.
In-Reply-To: <4C059695.1040000@consistentstate.com>
References: <4C059695.1040000@consistentstate.com>
Message-ID: <4C068249.6040901@consistentstate.com>

My apologies, I realized I left out the info of my installation. I'm 
running slony 1.2.20, and postgres 8.4.2 on my slave, and 8.4.1 on my 
master.

- Brian

Brian Fehrle wrote:
> Hi all,
>
>     I have a two node slony cluster and I have the slon daemon on the 
> slave node to run with the -a command. I'm attempting to better 
> understand how the logs work for slony log shipping, and have noticed a 
> bit of "odd" behavior (perhaps just odd to me) that perhaps someone can 
> explain to me. It's not just the log shipping that I'd like to better 
> understand, but just how the slony slave and master communicate with 
> eachother and make sure that the slave is in sync with the master, and 
> receiving ALL data that it needs without missing any.
>
> In each of the archive logs created by the slony slave with the -a 
> command, it looks something like this:
>
> ---------------------------------------
>
> ------------------------------------------------------------------
> -- Slony-I log shipping archive
> -- Node 1, Event 555
> ------------------------------------------------------------------
> start transaction;
> select "_slony".archiveTracking_offline('22', '2010-06-01 12:53:11.214886');
> -- end of log archiving header
> ------------------------------------------------------------------
> -- start of Slony-I data
> ------------------------------------------------------------------
> select "_slony".sequenceSetValue_offline(1,'38');
>
>
> ------------------------------------------------------------------
> -- End Of Archive Log
> ------------------------------------------------------------------
> commit;
> vacuum analyze "_slony ".sl_archive_tracking;
>
> ---------------------------------------
>
> Obviously, this log has no actual replication data in it, however there 
> are two main things that I notice in this. First is the Event number, 
> second is the number which is in the select statement for 
> archiveTracking_offline (which is the same number as what is in the name 
> of this particular file, slony1_log_2_00000000000000000022.sql).
>
> For the slony log shipping to work, I understand that each of the log 
> files are required in order, but I've noticed that looking through the 
> actual files, the Events themselves sometimes skip a number, or several. 
> Example,
>
> ---------------------------------------
> # cat /path/to/slon_archive_logs/* | grep Event
> -- Node 1, Event 554
> -- Node 1, Event 555
> -- Node 1, Event 556
> -- Node 1, Event 558
> -- Node 1, Event 559
> ---------------------------------------
>
> Looking at this, event 557 is missing, however the numbering of the 
> archive logs is not broken, each log appears with the expected numbering 
> in the name. This happens often, and I originally thought that this was 
> due to the log that contains the previous event (in this case 556) would 
> contain the data for both events, and 557 would simply not appear. I've 
> looked through every single archive log, and the event does not appear 
> in any of them, nor does it appear later down the road.
>
> I then thought that this event could be an event that isn't a sync, but 
> rather perhaps something else that wouldn't make it into these archive 
> logs (this might still bet he case). However, shortly after seeing that 
> this event is missing, i took a look at the sl_event table and saw that 
> the event is indeed there and is indeed a SYNC:
>
> ---------------------------------------
> postgres=# select ev_seqno, ev_type from _slony.sl_event where ev_origin 
> = '1';
>  ev_seqno | ev_type
> ----------+---------
>   554 | SYNC
>   555 | SYNC
>   556 | SYNC
>   557 | SYNC
>   558 | SYNC
>   559 | SYNC
> ---------------------------------------
>
>
> Basically what I want to do is write up a little script that will alert 
> me via email if something goes wonky with slony replication. I had an 
> event recently where data was missing from the slony slave, and all the 
> searching I could do came up with showed that the data was never 
> replicated, but slony never reported errors (this is in a recent email 
> to this mailing list). It would be easy to raise an alert that says 
> "woah event 557 was not found", however if it is normal behavior for 
> events to be missing like this, then that wouldn't be a good approach to 
> take.
>
> I've seen there are a couple of slony monitoring tools, and I'll be 
> checking them out to see if they offer anything that I could use. But 
> any other suggestions, or even some clarification as to how some of this 
> works would be greatly appreciated. If the situation happens again where 
> my slony slave is missing data, I'd like a bit of logs to review and see 
> when something may have gone wrong, even if i have to generate these 
> logs myself with some sort of monitoring.
>
> Thanks in advance,
>     Brian F
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>   


From jason at merchantcircle.com  Wed Jun  2 09:24:35 2010
From: jason at merchantcircle.com (Jason Culverhouse)
Date: Wed, 2 Jun 2010 09:24:35 -0700
Subject: [Slony1-general] Replication Hung
In-Reply-To: <AANLkTilv1QgtopKBGTM36pS3Kuj7qCE06fgo_pRGJrva@mail.gmail.com>
References: <52A818E3-197E-4D4A-A28E-12F722CDCF18@merchantcircle.com>
	<4C0539EA.5030306@ca.afilias.info>
	<F58F251C-AD51-4A23-8FA7-FF3E4882F807@merchantcircle.com>
	<AANLkTilv1QgtopKBGTM36pS3Kuj7qCE06fgo_pRGJrva@mail.gmail.com>
Message-ID: <C82A565B-03C7-4A72-9877-60DFD65E0724@merchantcircle.com>

Just an FYI,
I went with the TRUNCATE TABLE on the on the slaves, it all worked.  Thanks.
Luckily the nature of this table's definition and usage allows this to work, only inserts and deletes. 
Jason


On Jun 1, 2010, at 11:09 AM, Scott Marlowe wrote:

> I'd truncate it on both ends then run a delete without where on the
> master and see if that fixed it.
> 
> On Tue, Jun 1, 2010 at 11:56 AM, Jason Culverhouse
> <jason at merchantcircle.com> wrote:
>> Steve,
>> I tracked it down to a TRUNCATE TABLE on company_zip_updated.
>> This table is just a transient record keeping table, items get inserted, processed and deleted.
>> 
>> The question is, would a drop set on the set that contains the table repair the problem?
>> i.e, will this remove the statements that update that table from the log since the SET is removed?
>> OR
>> should I consider the EXECUTE SCRIPT ONLY ON to remove the offending rows in that table on the replicas?
>> 
>> I goal is to avoid replicating the whole database again.
>> Jason
>> 
>> On Jun 1, 2010, at 9:48 AM, Steve Singer wrote:
>> 
>>> Jason Culverhouse wrote:
>>>> I have a problem where my replication is hung,  I don't really know where to start....  This table is in set "38", most everything is in set "1", it looks like set 38 isn't "caught up"  in the ordering.
>>>> Any Idea's on how to repair this?  Is this problem because the set's are not merged? Can I merge the sets? Version is slony1-1.2.15 on postgres 8.3
>>> 
>>> Your problem appears to be that a row being inserted into your replica is already on your replica so it is failing.  This is bad and I am very curious to know how your cluster got into this state.
>>> 
>>> Have you mean making any schema changes to this cluster.  Was EXECUTE SCRIPT used or was it not used?
>>> 
>>> Have you tried making any data changes through EXECUTE SCRIPT (ie would someone have run a EXECUTE SCRIPT (..ONLY ON ..) the replica that inserst data into your company_zip_updated table).
>>> 
>>> Options for fixing it include
>>> 
>>> -Dropping this node from replication and rebuilding the replica. Depending on the size of your data this might be the simplest.
>>> 
>>> -Deleting the offending row of company_zip_updated from the replica using EXECUTE SCRIPT ONLY ON and letting replication progress.  I will warn you that this is treating the visible symptom only and does not address the cause.  This could make things worse for you (it is hard to say without knowing what the actual problem is)
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>>> 2010-06-01 09:11:53 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 574847
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_50: forward confirm 60,574847 received by 50
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: SYNC 33381140 processing
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 38 with 4 table(s) from provider 40
>>>> 2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 1 with 139 table(s) from provider 40
>>>> 2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: current local log_status is 0
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40_40: current remote log_status = 1
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.008 seconds delay for first row
>>>> 2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: "update only "public"."review" set zipcode_id='80203' where id='1026409';
>>>> update only "public"."review" set zipcode_id='80203' where id='1026401';
>>>> update only "public"."review" set zipcode_id='80203' where id='1008795';
>>>> update only "public"."review" set zipcode_id='80203' where id='1008048';
>>>> update only "public"."review" set zipcode_id='80203' where id='1007445';
>>>> insert into "public"."company_zip_updated" (id,company_id,zipcode_id) values ('1205','30149282','80122');
>>>> update only "public"."merchant" set zipcode_id='80203' where id='905955';
>>>> update only "public"."advertisement" set zipcode_id='80203' where id='1467390';
>>>> update only "public"."advertisement" set zipcode_id='80203' where id='1375973';
>>>> update only "public"."advertisement" set zipcode_id='80203' where id='1389545';
>>>> " ERROR:  duplicate key value violates unique constraint "company_zip_updated_company_id_key"
>>>> - qualification was: where log_origin = 40 and (  (
>>>> log_tableid in (158,159,160,161)
>>>>    and (log_xid < '622094999')
>>>>    and (log_xid >= '622094987')
>>>> ) or (
>>>>    log_tableid in (124,121,117,118,6,22,23,25,26,31,32,43,44,45,46,55,56,57,59,78,115,79,106,114,116,3,4,5,9,11,12,13,14,15,1,7,2,8,16,17,27,28,29,33,34,36,39,40,41,42,47,48,49,50,51,52,53,54,61,63,64,65,69,71,72,74,75,76,77,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,107,108,109,110,111,112,113,119,120,123,122,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157)
>>>>    and (log_xid < '622094999')
>>>>    and (log_xid >= '622094987')
>>>> ) )
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.016 seconds until close cursor
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: inserts=1 updates=79 deletes=0
>>>> 2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: SYNC aborted
>>>> Here is the table definition
>>>> Table "public.company_zip_updated"
>>>>   Column   |  Type   |                            Modifiers                             ------------+---------+------------------------------------------------------------------
>>>> id         | integer | not null default nextval('company_zip_updated_id_seq'::regclass)
>>>> company_id | integer | not null
>>>> zipcode_id | integer | not null
>>>> Indexes:
>>>>    "company_zip_updated_pkey" PRIMARY KEY, btree (id)
>>>>    "company_zip_updated_company_id_key" UNIQUE, btree (company_id)
>>>> Foreign-key constraints:
>>>>    "company_zip_updated_company_id_fkey" FOREIGN KEY (company_id) REFERENCES company(id) ON DELETE CASCADE
>>>>    "company_zip_updated_zipcode_id_fkey" FOREIGN KEY (zipcode_id) REFERENCES zipcode(zip) ON DELETE CASCADE
>>>> Triggers:
>>>>    _mc_cluster_logtrigger_161 AFTER INSERT OR DELETE OR UPDATE ON company_zip_updated FOR EACH ROW EXECUTE PROCEDURE _mc_cluster.logtrigger('_mc_cluster', '161', 'kvv')
>>>> _______________________________________________
>>>> Slony1-general mailing list
>>>> Slony1-general at lists.slony.info
>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>> 
>>> 
>>> --
>>> Steve Singer
>>> Afilias Canada
>>> Data Services Developer
>>> 416-673-1142
>> 
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>> 
> 
> 
> 
> -- 
> When fascism comes to America, it will be intolerance sold as diversity.


From karllehenbauer at gmail.com  Thu Jun  3 01:14:51 2010
From: karllehenbauer at gmail.com (Karl Lehenbauer)
Date: Thu, 3 Jun 2010 03:14:51 -0500
Subject: [Slony1-general] getting log switch to sl_log_2 still in progress -
	tried everything
Message-ID: <AANLkTilYWiwxr2CsLy8ZNB96Y-JFBlLYiCGovoYaWla7@mail.gmail.com>

Hey...

We have been running slony reasonably effectively for a couple of years now.
 We're on PostgreSQL 8.3, FreeBSD, Slony 2.0.2.

We had this kind of setup.   A -> B -> C

B crashed with a weird ethernet problem.  It hung us up real bad so in an
emergency we killed all the slon daemons, shutdown postgresql on C and
dropped the cluster on A.

Now we're trying to get back going.  We have this well documented, did a
bunch of testing initially, and have done it multiple times successfully in
the past.  You know the drill, plus we have a tool to make sure everything
has a primary key or an acceptable unique key, generate the slon conf, etc.

We init the cluster, add the set, add the node, subscribe B to A, and we
start getting...

NOTICE:  Slony-I: log switch to sl_log_2 still in progress - sl_log_1 not
truncated

OK, so following the Slony FAQ we killed all of our long-running
connections.  No joy.  It never straightens itself out.

Finally we killed the slon daemons, dropped the schema, then we shut down
the database on the primary and start it back up again.  We then went
through the procedure to start up B, subscribe it to A, etc, and again we
get the error.

I am sure we shut the database down without a slony schema and with no slon
daemons, started it back up, initialized the cluster, added the set, started
the slon daemons, but when we subscribe the second node (node 5 by the way),
it starts giving that notice.

We use the slonik tools and I'm pretty familiar with them.

There can't be any connections with some old cached query plan unless such
query plans can survive database server restarts.  We nuked slony, as we've
done successfully in the past, and restarted postgres, yet still we get the
sl_log_1-not-truncated and the table gets huge and then slow.

Can anyone hazard a guess as to what's going on and, better yet, what we
should do to fix it?

Regards....

Karl
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100603/3aae6288/attachment.htm 

From vivek.gupta at globallogic.com  Thu Jun  3 02:43:16 2010
From: vivek.gupta at globallogic.com (Vivek Gupta)
Date: Thu, 3 Jun 2010 15:13:16 +0530
Subject: [Slony1-general] Known issues with Slony-I 2.0.3
Message-ID: <478174F395A7E34BBD3891A50303D602D91724@ex3-del1.synapse.com>

Hi,

 

Visiting http://www.slony.info/ I realized that "Slony-I 2.0.3 is not
usable in its current state. <http://www.slony.info/> " Can you please
apprise what are the known issues with 'Slony-I 2.0.3'? Also it would be
great if someone can additionally let us know the expected availability
of Slony-I 2.0.4.

 

Thanks in advance!!

 

With regards,

Vivek Gupta

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100603/8a1e7050/attachment.htm 

From shoaibmir at gmail.com  Thu Jun  3 03:23:02 2010
From: shoaibmir at gmail.com (Shoaib Mir)
Date: Thu, 3 Jun 2010 20:23:02 +1000
Subject: [Slony1-general] Known issues with Slony-I 2.0.3
In-Reply-To: <478174F395A7E34BBD3891A50303D602D91724@ex3-del1.synapse.com>
References: <478174F395A7E34BBD3891A50303D602D91724@ex3-del1.synapse.com>
Message-ID: <AANLkTikF5mMT6YNEJzJkSgmhSwcT58daqsFDqaJvqa0T@mail.gmail.com>

On Thu, Jun 3, 2010 at 7:43 PM, Vivek Gupta <vivek.gupta at globallogic.com>wrote:

>  Hi,
>
>
>
> Visiting http://www.slony.info/ I realized that ?Slony-I 2.0.3 is not
> usable in its current state. <http://www.slony.info/>? Can you please
> apprise what are the known issues with ?Slony-I 2.0.3?? Also it would be
> great if someone can additionally let us know the expected availability of
> Slony-I 2.0.4.
>

Slony 2.0.4 RC2 is already available at
http://www.slony.info/downloads/2.0/source/

I am not really sure about the exact problems with 2.0.3 but if I re-call
correctly there were some memory corruption and UTF8 related bugs.

-- 
Shoaib Mir
http://shoaibmir.wordpress.com/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100603/b32c9f7f/attachment.htm 

From ssinger at ca.afilias.info  Thu Jun  3 05:32:20 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 03 Jun 2010 08:32:20 -0400
Subject: [Slony1-general] Known issues with Slony-I 2.0.3
In-Reply-To: <478174F395A7E34BBD3891A50303D602D91724@ex3-del1.synapse.com>
References: <478174F395A7E34BBD3891A50303D602D91724@ex3-del1.synapse.com>
Message-ID: <4C07A0D4.605@ca.afilias.info>

Vivek Gupta wrote:
> Hi,
> 
>  
> 
> Visiting http://www.slony.info/ I realized that ?Slony-I 2.0.3 is not 
> usable in its current state. <http://www.slony.info/>? Can you please 
> apprise what are the known issues with ?Slony-I 2.0.3?? Also it would be 
> great if someone can additionally let us know the expected availability 
> of Slony-I 2.0.4.

http://lists.slony.info/pipermail/slony1-general/2010-April/010596.html 
   contains details of the issue with 2.0.3

2.0.4 rc2 is out, we will release 2.0.4 once we get some successful test 
reports from people.


> 
>  
> 
> Thanks in advance!!
> 
>  
> 
> With regards,
> 
> Vivek Gupta
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From JanWieck at Yahoo.com  Thu Jun  3 05:29:31 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu, 03 Jun 2010 08:29:31 -0400
Subject: [Slony1-general] Replication Hung
In-Reply-To: <F58F251C-AD51-4A23-8FA7-FF3E4882F807@merchantcircle.com>
References: <52A818E3-197E-4D4A-A28E-12F722CDCF18@merchantcircle.com>	<4C0539EA.5030306@ca.afilias.info>
	<F58F251C-AD51-4A23-8FA7-FF3E4882F807@merchantcircle.com>
Message-ID: <4C07A02B.4010901@Yahoo.com>

On 6/1/2010 1:56 PM, Jason Culverhouse wrote:
> Steve,
> I tracked it down to a TRUNCATE TABLE on company_zip_updated.
> This table is just a transient record keeping table, items get inserted, processed and deleted.
> 
> The question is, would a drop set on the set that contains the table repair the problem?
> i.e, will this remove the tatements that update that table from the log since the SET is removed?
> OR
> should I consider the EXECUTE SCRIPT ONLY ON to remove the offending rows in that table on the replicas?

You could remove the table from set 38, create a new, temporary set 
containing just that table, subscribe it to all the nodes that are 
subscribed to 38, then merge set. That would cause Slony to bring it 
back into sync.


Jan


> 
> I goal is to avoid replicating the whole database again. 
> Jason
> 
> On Jun 1, 2010, at 9:48 AM, Steve Singer wrote:
> 
>> Jason Culverhouse wrote:
>>> I have a problem where my replication is hung,  I don't really know where to start....  This table is in set "38", most everything is in set "1", it looks like set 38 isn't "caught up"  in the ordering.
>>> Any Idea's on how to repair this?  Is this problem because the set's are not merged? Can I merge the sets? Version is slony1-1.2.15 on postgres 8.3
>> 
>> Your problem appears to be that a row being inserted into your replica is already on your replica so it is failing.  This is bad and I am very curious to know how your cluster got into this state.
>> 
>> Have you mean making any schema changes to this cluster.  Was EXECUTE SCRIPT used or was it not used?
>> 
>> Have you tried making any data changes through EXECUTE SCRIPT (ie would someone have run a EXECUTE SCRIPT (..ONLY ON ..) the replica that inserst data into your company_zip_updated table).
>> 
>> Options for fixing it include
>> 
>> -Dropping this node from replication and rebuilding the replica. Depending on the size of your data this might be the simplest.
>> 
>> -Deleting the offending row of company_zip_updated from the replica using EXECUTE SCRIPT ONLY ON and letting replication progress.  I will warn you that this is treating the visible symptom only and does not address the cause.  This could make things worse for you (it is hard to say without knowing what the actual problem is)
>> 
>> 
>> 
>> 
>> 
>> 
>>> 2010-06-01 09:11:53 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 574847
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_50: forward confirm 60,574847 received by 50
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: SYNC 33381140 processing
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 38 with 4 table(s) from provider 40
>>> 2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 1 with 139 table(s) from provider 40
>>> 2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: current local log_status is 0
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40_40: current remote log_status = 1
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.008 seconds delay for first row
>>> 2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: "update only "public"."review" set zipcode_id='80203' where id='1026409';
>>> update only "public"."review" set zipcode_id='80203' where id='1026401';
>>> update only "public"."review" set zipcode_id='80203' where id='1008795';
>>> update only "public"."review" set zipcode_id='80203' where id='1008048';
>>> update only "public"."review" set zipcode_id='80203' where id='1007445';
>>> insert into "public"."company_zip_updated" (id,company_id,zipcode_id) values ('1205','30149282','80122');
>>> update only "public"."merchant" set zipcode_id='80203' where id='905955';
>>> update only "public"."advertisement" set zipcode_id='80203' where id='1467390';
>>> update only "public"."advertisement" set zipcode_id='80203' where id='1375973';
>>> update only "public"."advertisement" set zipcode_id='80203' where id='1389545';
>>> " ERROR:  duplicate key value violates unique constraint "company_zip_updated_company_id_key"
>>> - qualification was: where log_origin = 40 and (  (
>>> log_tableid in (158,159,160,161)
>>>    and (log_xid < '622094999')
>>>    and (log_xid >= '622094987')
>>> ) or (
>>>    log_tableid in (124,121,117,118,6,22,23,25,26,31,32,43,44,45,46,55,56,57,59,78,115,79,106,114,116,3,4,5,9,11,12,13,14,15,1,7,2,8,16,17,27,28,29,33,34,36,39,40,41,42,47,48,49,50,51,52,53,54,61,63,64,65,69,71,72,74,75,76,77,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,107,108,109,110,111,112,113,119,120,123,122,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157)
>>>    and (log_xid < '622094999')
>>>    and (log_xid >= '622094987')
>>> ) )
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.016 seconds until close cursor
>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: inserts=1 updates=79 deletes=0
>>> 2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: SYNC aborted
>>> Here is the table definition
>>> Table "public.company_zip_updated"
>>>   Column   |  Type   |                            Modifiers                             ------------+---------+------------------------------------------------------------------
>>> id         | integer | not null default nextval('company_zip_updated_id_seq'::regclass)
>>> company_id | integer | not null
>>> zipcode_id | integer | not null
>>> Indexes:
>>>    "company_zip_updated_pkey" PRIMARY KEY, btree (id)
>>>    "company_zip_updated_company_id_key" UNIQUE, btree (company_id)
>>> Foreign-key constraints:
>>>    "company_zip_updated_company_id_fkey" FOREIGN KEY (company_id) REFERENCES company(id) ON DELETE CASCADE
>>>    "company_zip_updated_zipcode_id_fkey" FOREIGN KEY (zipcode_id) REFERENCES zipcode(zip) ON DELETE CASCADE
>>> Triggers:
>>>    _mc_cluster_logtrigger_161 AFTER INSERT OR DELETE OR UPDATE ON company_zip_updated FOR EACH ROW EXECUTE PROCEDURE _mc_cluster.logtrigger('_mc_cluster', '161', 'kvv')
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general at lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>> 
>> 
>> -- 
>> Steve Singer
>> Afilias Canada
>> Data Services Developer
>> 416-673-1142
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From JanWieck at Yahoo.com  Thu Jun  3 05:32:06 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu, 03 Jun 2010 08:32:06 -0400
Subject: [Slony1-general] Replication Hung
In-Reply-To: <4C054E7E.6080708@ca.afilias.info>
References: <52A818E3-197E-4D4A-A28E-12F722CDCF18@merchantcircle.com>	<4C0539EA.5030306@ca.afilias.info>	<F58F251C-AD51-4A23-8FA7-FF3E4882F807@merchantcircle.com>
	<4C054E7E.6080708@ca.afilias.info>
Message-ID: <4C07A0C6.90206@Yahoo.com>

On 6/1/2010 2:16 PM, Steve Singer wrote:
> Jason Culverhouse wrote:
>> Steve,
>> I tracked it down to a TRUNCATE TABLE on company_zip_updated.
>> This table is just a transient record keeping table, items get inserted, processed and deleted.
>> 
>> The question is, would a drop set on the set that contains the table repair the problem?
>> i.e, will this remove the statements that update that table from the log since the SET is removed?
>> OR
>> should I consider the EXECUTE SCRIPT ONLY ON to remove the offending rows in that table on the replicas?
> 
> A DROP SET won't fix your problem because the problem insert will still 
> be sl_log.   (In the future we might want to change things somehow so 
> the drop set deletes the pending data?)

Ah right, forgot about that one.

Yes, but that also requires that some events move somewhat out of 
bounds. Which is a rather dramatic change in the overall design.


Jan

> 
> Scott's idea of doing a TRUNCATE on both ends makes sense.   The 
> TRUNCATE doesn't even need to be inside of the execute script (since the 
> _deny() trigger won't fire)
> 
> 
> 
>> 
>> I goal is to avoid replicating the whole database again. 
>> Jason
>> 
>> On Jun 1, 2010, at 9:48 AM, Steve Singer wrote:
>> 
>>> Jason Culverhouse wrote:
>>>> I have a problem where my replication is hung,  I don't really know where to start....  This table is in set "38", most everything is in set "1", it looks like set 38 isn't "caught up"  in the ordering.
>>>> Any Idea's on how to repair this?  Is this problem because the set's are not merged? Can I merge the sets? Version is slony1-1.2.15 on postgres 8.3
>>> Your problem appears to be that a row being inserted into your replica is already on your replica so it is failing.  This is bad and I am very curious to know how your cluster got into this state.
>>>
>>> Have you mean making any schema changes to this cluster.  Was EXECUTE SCRIPT used or was it not used?
>>>
>>> Have you tried making any data changes through EXECUTE SCRIPT (ie would someone have run a EXECUTE SCRIPT (..ONLY ON ..) the replica that inserst data into your company_zip_updated table).
>>>
>>> Options for fixing it include
>>>
>>> -Dropping this node from replication and rebuilding the replica. Depending on the size of your data this might be the simplest.
>>>
>>> -Deleting the offending row of company_zip_updated from the replica using EXECUTE SCRIPT ONLY ON and letting replication progress.  I will warn you that this is treating the visible symptom only and does not address the cause.  This could make things worse for you (it is hard to say without knowing what the actual problem is)
>>>
>>>
>>>
>>>
>>>
>>>
>>>> 2010-06-01 09:11:53 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 574847
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_50: forward confirm 60,574847 received by 50
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: SYNC 33381140 processing
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 38 with 4 table(s) from provider 40
>>>> 2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 1 with 139 table(s) from provider 40
>>>> 2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: current local log_status is 0
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40_40: current remote log_status = 1
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.008 seconds delay for first row
>>>> 2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: "update only "public"."review" set zipcode_id='80203' where id='1026409';
>>>> update only "public"."review" set zipcode_id='80203' where id='1026401';
>>>> update only "public"."review" set zipcode_id='80203' where id='1008795';
>>>> update only "public"."review" set zipcode_id='80203' where id='1008048';
>>>> update only "public"."review" set zipcode_id='80203' where id='1007445';
>>>> insert into "public"."company_zip_updated" (id,company_id,zipcode_id) values ('1205','30149282','80122');
>>>> update only "public"."merchant" set zipcode_id='80203' where id='905955';
>>>> update only "public"."advertisement" set zipcode_id='80203' where id='1467390';
>>>> update only "public"."advertisement" set zipcode_id='80203' where id='1375973';
>>>> update only "public"."advertisement" set zipcode_id='80203' where id='1389545';
>>>> " ERROR:  duplicate key value violates unique constraint "company_zip_updated_company_id_key"
>>>> - qualification was: where log_origin = 40 and (  (
>>>> log_tableid in (158,159,160,161)
>>>>    and (log_xid < '622094999')
>>>>    and (log_xid >= '622094987')
>>>> ) or (
>>>>    log_tableid in (124,121,117,118,6,22,23,25,26,31,32,43,44,45,46,55,56,57,59,78,115,79,106,114,116,3,4,5,9,11,12,13,14,15,1,7,2,8,16,17,27,28,29,33,34,36,39,40,41,42,47,48,49,50,51,52,53,54,61,63,64,65,69,71,72,74,75,76,77,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,107,108,109,110,111,112,113,119,120,123,122,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157)
>>>>    and (log_xid < '622094999')
>>>>    and (log_xid >= '622094987')
>>>> ) )
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.016 seconds until close cursor
>>>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: inserts=1 updates=79 deletes=0
>>>> 2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: SYNC aborted
>>>> Here is the table definition
>>>> Table "public.company_zip_updated"
>>>>   Column   |  Type   |                            Modifiers                             ------------+---------+------------------------------------------------------------------
>>>> id         | integer | not null default nextval('company_zip_updated_id_seq'::regclass)
>>>> company_id | integer | not null
>>>> zipcode_id | integer | not null
>>>> Indexes:
>>>>    "company_zip_updated_pkey" PRIMARY KEY, btree (id)
>>>>    "company_zip_updated_company_id_key" UNIQUE, btree (company_id)
>>>> Foreign-key constraints:
>>>>    "company_zip_updated_company_id_fkey" FOREIGN KEY (company_id) REFERENCES company(id) ON DELETE CASCADE
>>>>    "company_zip_updated_zipcode_id_fkey" FOREIGN KEY (zipcode_id) REFERENCES zipcode(zip) ON DELETE CASCADE
>>>> Triggers:
>>>>    _mc_cluster_logtrigger_161 AFTER INSERT OR DELETE OR UPDATE ON company_zip_updated FOR EACH ROW EXECUTE PROCEDURE _mc_cluster.logtrigger('_mc_cluster', '161', 'kvv')
>>>> _______________________________________________
>>>> Slony1-general mailing list
>>>> Slony1-general at lists.slony.info
>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>
>>> -- 
>>> Steve Singer
>>> Afilias Canada
>>> Data Services Developer
>>> 416-673-1142
>> 
> 
> 


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From JanWieck at Yahoo.com  Thu Jun  3 05:32:54 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu, 03 Jun 2010 08:32:54 -0400
Subject: [Slony1-general] Replication Hung
In-Reply-To: <05071C7C-37F4-4A85-9818-F90A01F55674@merchantcircle.com>
References: <52A818E3-197E-4D4A-A28E-12F722CDCF18@merchantcircle.com>
	<05071C7C-37F4-4A85-9818-F90A01F55674@merchantcircle.com>
Message-ID: <4C07A0F6.60304@Yahoo.com>

On 6/1/2010 12:50 PM, Jason Culverhouse wrote:
> Ok...
> So I just found out that a developer ran a
> TRUNCATE TABLE on company_zip_updated
> 
> This table is in a set with 4 other tables.
> 
> Is the correct course of action to drop the set with the table and resubscribe?
> 
> Jason

Slony-I version 2.1 should support TRUNCATE.


Jan

> 
> 
> On Jun 1, 2010, at 9:18 AM, Jason Culverhouse wrote:
> 
>> I have a problem where my replication is hung,  I don't really know where to start....  This table is in set "38", most everything is in set "1", it looks like set 38 isn't "caught up"  in the ordering.
>> Any Idea's on how to repair this?  Is this problem because the set's are not merged? Can I merge the sets? Version is slony1-1.2.15 on postgres 8.3
>> 
>> 
>> 2010-06-01 09:11:53 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 574847
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_50: forward confirm 60,574847 received by 50
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: SYNC 33381140 processing
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 38 with 4 table(s) from provider 40
>> 2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: syncing set 1 with 139 table(s) from provider 40
>> 2010-06-01 09:11:53 PDT DEBUG2  ssy_action_list length: 0
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40: current local log_status is 0
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteWorkerThread_40_40: current remote log_status = 1
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.008 seconds delay for first row
>> 2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: "update only "public"."review" set zipcode_id='80203' where id='1026409';
>> update only "public"."review" set zipcode_id='80203' where id='1026401';
>> update only "public"."review" set zipcode_id='80203' where id='1008795';
>> update only "public"."review" set zipcode_id='80203' where id='1008048';
>> update only "public"."review" set zipcode_id='80203' where id='1007445';
>> insert into "public"."company_zip_updated" (id,company_id,zipcode_id) values ('1205','30149282','80122');
>> update only "public"."merchant" set zipcode_id='80203' where id='905955';
>> update only "public"."advertisement" set zipcode_id='80203' where id='1467390';
>> update only "public"."advertisement" set zipcode_id='80203' where id='1375973';
>> update only "public"."advertisement" set zipcode_id='80203' where id='1389545';
>> " ERROR:  duplicate key value violates unique constraint "company_zip_updated_company_id_key"
>> - qualification was: where log_origin = 40 and (  (
>> log_tableid in (158,159,160,161)
>>    and (log_xid < '622094999')
>>    and (log_xid >= '622094987')
>> ) or (
>>    log_tableid in (124,121,117,118,6,22,23,25,26,31,32,43,44,45,46,55,56,57,59,78,115,79,106,114,116,3,4,5,9,11,12,13,14,15,1,7,2,8,16,17,27,28,29,33,34,36,39,40,41,42,47,48,49,50,51,52,53,54,61,63,64,65,69,71,72,74,75,76,77,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,107,108,109,110,111,112,113,119,120,123,122,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157)
>>    and (log_xid < '622094999')
>>    and (log_xid >= '622094987')
>> ) )
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: 0.016 seconds until close cursor
>> 2010-06-01 09:11:53 PDT DEBUG2 remoteHelperThread_40_40: inserts=1 updates=79 deletes=0
>> 2010-06-01 09:11:53 PDT ERROR  remoteWorkerThread_40: SYNC aborted
>> 
>> 
>> Here is the table definition
>> 
>> Table "public.company_zip_updated"
>>   Column   |  Type   |                            Modifiers                             
>> ------------+---------+------------------------------------------------------------------
>> id         | integer | not null default nextval('company_zip_updated_id_seq'::regclass)
>> company_id | integer | not null
>> zipcode_id | integer | not null
>> Indexes:
>>    "company_zip_updated_pkey" PRIMARY KEY, btree (id)
>>    "company_zip_updated_company_id_key" UNIQUE, btree (company_id)
>> Foreign-key constraints:
>>    "company_zip_updated_company_id_fkey" FOREIGN KEY (company_id) REFERENCES company(id) ON DELETE CASCADE
>>    "company_zip_updated_zipcode_id_fkey" FOREIGN KEY (zipcode_id) REFERENCES zipcode(zip) ON DELETE CASCADE
>> Triggers:
>>    _mc_cluster_logtrigger_161 AFTER INSERT OR DELETE OR UPDATE ON company_zip_updated FOR EACH ROW EXECUTE PROCEDURE _mc_cluster.logtrigger('_mc_cluster', '161', 'kvv')
>> 
>> 
>> 
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From ssinger at ca.afilias.info  Thu Jun  3 05:59:47 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 03 Jun 2010 08:59:47 -0400
Subject: [Slony1-general] getting log switch to sl_log_2 still in
 progress -	tried everything
In-Reply-To: <AANLkTilYWiwxr2CsLy8ZNB96Y-JFBlLYiCGovoYaWla7@mail.gmail.com>
References: <AANLkTilYWiwxr2CsLy8ZNB96Y-JFBlLYiCGovoYaWla7@mail.gmail.com>
Message-ID: <4C07A743.4000509@ca.afilias.info>

Karl Lehenbauer wrote:
> Hey...
> 
> We have been running slony reasonably effectively for a couple of years 
> now.  We're on PostgreSQL 8.3, FreeBSD, Slony 2.0.2.
> 
> We had this kind of setup.   A -> B -> C
> 
> B crashed with a weird ethernet problem.  It hung us up real bad so in 
> an emergency we killed all the slon daemons, shutdown postgresql on C 
> and dropped the cluster on A.
> 
> Now we're trying to get back going.  We have this well documented, did a 
> bunch of testing initially, and have done it multiple times successfully 
> in the past.  You know the drill, plus we have a tool to make sure 
> everything has a primary key or an acceptable unique key, generate the 
> slon conf, etc.
> 
> We init the cluster, add the set, add the node, subscribe B to A, and we 
> start getting...
> 
> NOTICE:  Slony-I: log switch to sl_log_2 still in progress - sl_log_1 
> not truncated

Does your initial subscribe (the COPY) ever finish? You don't seem to say.

The logswitch can't happen while there is data that still needs to be 
replicated to the other node.   If your initial copy hasn't finished yet 
then you have rows in sl_log_1 that still need to be replicated, and 
those won't replicate until after the copy finishes.

The other thing that can cause this is if you still have a node 
subscribed setup that ins't running any slons (so it isn't confirming 
events).

> 
> OK, so following the Slony FAQ we killed all of our long-running 
> connections.  No joy.  It never straightens itself out.
> 
> Finally we killed the slon daemons, dropped the schema, then we shut 
> down the database on the primary and start it back up again.  We then 
> went through the procedure to start up B, subscribe it to A, etc, and 
> again we get the error.
> 
> I am sure we shut the database down without a slony schema and with no 
> slon daemons, started it back up, initialized the cluster, added the 
> set, started the slon daemons, but when we subscribe the second node 
> (node 5 by the way), it starts giving that notice.
> 
> We use the slonik tools and I'm pretty familiar with them.
> 
> There can't be any connections with some old cached query plan unless 
> such query plans can survive database server restarts.  We nuked slony, 
> as we've done successfully in the past, and restarted postgres, yet 
> still we get the sl_log_1-not-truncated and the table gets huge and then 
> slow.
> 
> Can anyone hazard a guess as to what's going on and, better yet, what we 
> should do to fix it?
> 
> Regards....
> 
> Karl
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From JanWieck at Yahoo.com  Thu Jun  3 06:07:07 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu, 03 Jun 2010 09:07:07 -0400
Subject: [Slony1-general] getting log switch to sl_log_2 still in
 progress -	tried everything
In-Reply-To: <AANLkTilYWiwxr2CsLy8ZNB96Y-JFBlLYiCGovoYaWla7@mail.gmail.com>
References: <AANLkTilYWiwxr2CsLy8ZNB96Y-JFBlLYiCGovoYaWla7@mail.gmail.com>
Message-ID: <4C07A8FB.2030404@Yahoo.com>

On 6/3/2010 4:14 AM, Karl Lehenbauer wrote:
> Hey...
> 
> We have been running slony reasonably effectively for a couple of years 
> now.  We're on PostgreSQL 8.3, FreeBSD, Slony 2.0.2.
> 
> We had this kind of setup.   A -> B -> C
> 
> B crashed with a weird ethernet problem.  It hung us up real bad so in 
> an emergency we killed all the slon daemons, shutdown postgresql on C 
> and dropped the cluster on A.
> 
> Now we're trying to get back going.  We have this well documented, did a 
> bunch of testing initially, and have done it multiple times successfully 
> in the past.  You know the drill, plus we have a tool to make sure 
> everything has a primary key or an acceptable unique key, generate the 
> slon conf, etc.
> 
> We init the cluster, add the set, add the node, subscribe B to A, and we 
> start getting...
> 
> NOTICE:  Slony-I: log switch to sl_log_2 still in progress - sl_log_1 
> not truncated
> 
> OK, so following the Slony FAQ we killed all of our long-running 
> connections.  No joy.  It never straightens itself out.

That message alone isn't a good reason to go into full panic mode and 
tearing down the whole cluster.

Did you give the replica enough time to catch up?

Are long running transactions on the master preventing the log switch to 
succeed?


Jan


> 
> Finally we killed the slon daemons, dropped the schema, then we shut 
> down the database on the primary and start it back up again.  We then 
> went through the procedure to start up B, subscribe it to A, etc, and 
> again we get the error.
> 
> I am sure we shut the database down without a slony schema and with no 
> slon daemons, started it back up, initialized the cluster, added the 
> set, started the slon daemons, but when we subscribe the second node 
> (node 5 by the way), it starts giving that notice.
> 
> We use the slonik tools and I'm pretty familiar with them.
> 
> There can't be any connections with some old cached query plan unless 
> such query plans can survive database server restarts.  We nuked slony, 
> as we've done successfully in the past, and restarted postgres, yet 
> still we get the sl_log_1-not-truncated and the table gets huge and then 
> slow.
> 
> Can anyone hazard a guess as to what's going on and, better yet, what we 
> should do to fix it?
> 
> Regards....
> 
> Karl
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From karllehenbauer at gmail.com  Thu Jun  3 06:20:53 2010
From: karllehenbauer at gmail.com (Karl Lehenbauer)
Date: Thu, 3 Jun 2010 08:20:53 -0500
Subject: [Slony1-general] getting log switch to sl_log_2 still in
	progress - tried everything
In-Reply-To: <4C07A743.4000509@ca.afilias.info>
References: <AANLkTilYWiwxr2CsLy8ZNB96Y-JFBlLYiCGovoYaWla7@mail.gmail.com>
	<4C07A743.4000509@ca.afilias.info>
Message-ID: <AANLkTikKP_C3AxQNHo78gTk2xiec_VQTtHcScrA-OK9C@mail.gmail.com>

It does finish, and least I think it does.  I will look very carefully this
time to make sure (takes a few hours).

This morning I tore the cluster down and recreated it with a different
schema name, thinking that might work.  I saw the sl_log_1-not-truncated
messages and figured it was hosed, so seeing your message that the logswitch
can't happen while there is data that needs to be replicated was heartening.
 (It might be good to make a note in the FAQ that those messages are normal
when doing the initial subscribe.)

On Thu, Jun 3, 2010 at 7:59 AM, Steve Singer <ssinger at ca.afilias.info>wrote:

> Karl Lehenbauer wrote:
>
>> Hey...
>>
>> We have been running slony reasonably effectively for a couple of years
>> now.  We're on PostgreSQL 8.3, FreeBSD, Slony 2.0.2.
>>
>> We had this kind of setup.   A -> B -> C
>>
>> B crashed with a weird ethernet problem.  It hung us up real bad so in an
>> emergency we killed all the slon daemons, shutdown postgresql on C and
>> dropped the cluster on A.
>>
>> Now we're trying to get back going.  We have this well documented, did a
>> bunch of testing initially, and have done it multiple times successfully in
>> the past.  You know the drill, plus we have a tool to make sure everything
>> has a primary key or an acceptable unique key, generate the slon conf, etc.
>>
>> We init the cluster, add the set, add the node, subscribe B to A, and we
>> start getting...
>>
>> NOTICE:  Slony-I: log switch to sl_log_2 still in progress - sl_log_1 not
>> truncated
>>
>
> Does your initial subscribe (the COPY) ever finish? You don't seem to say.
>
> The logswitch can't happen while there is data that still needs to be
> replicated to the other node.   If your initial copy hasn't finished yet
> then you have rows in sl_log_1 that still need to be replicated, and those
> won't replicate until after the copy finishes.
>
> The other thing that can cause this is if you still have a node subscribed
> setup that ins't running any slons (so it isn't confirming events).
>
>
>> OK, so following the Slony FAQ we killed all of our long-running
>> connections.  No joy.  It never straightens itself out.
>>
>> Finally we killed the slon daemons, dropped the schema, then we shut down
>> the database on the primary and start it back up again.  We then went
>> through the procedure to start up B, subscribe it to A, etc, and again we
>> get the error.
>>
>> I am sure we shut the database down without a slony schema and with no
>> slon daemons, started it back up, initialized the cluster, added the set,
>> started the slon daemons, but when we subscribe the second node (node 5 by
>> the way), it starts giving that notice.
>>
>> We use the slonik tools and I'm pretty familiar with them.
>>
>> There can't be any connections with some old cached query plan unless such
>> query plans can survive database server restarts.  We nuked slony, as we've
>> done successfully in the past, and restarted postgres, yet still we get the
>> sl_log_1-not-truncated and the table gets huge and then slow.
>>
>> Can anyone hazard a guess as to what's going on and, better yet, what we
>> should do to fix it?
>>
>> Regards....
>>
>> Karl
>>
>>
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>
>
> --
> Steve Singer
> Afilias Canada
> Data Services Developer
> 416-673-1142
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100603/c32ee13e/attachment.htm 

From karllehenbauer at gmail.com  Thu Jun  3 06:27:28 2010
From: karllehenbauer at gmail.com (Karl Lehenbauer)
Date: Thu, 3 Jun 2010 08:27:28 -0500
Subject: [Slony1-general] getting log switch to sl_log_2 still in
	progress - tried everything
In-Reply-To: <4C07A743.4000509@ca.afilias.info>
References: <AANLkTilYWiwxr2CsLy8ZNB96Y-JFBlLYiCGovoYaWla7@mail.gmail.com>
	<4C07A743.4000509@ca.afilias.info>
Message-ID: <AANLkTikssl68GEs_pQoNhB6JuW0GaJ3w_IB1-PwiQ6cd@mail.gmail.com>

On Thu, Jun 3, 2010 at 7:59 AM, Steve Singer <ssinger at ca.afilias.info>wrote:

> Does your initial subscribe (the COPY) ever finish? You don't seem to say.
>

In a prior message I just said I didn't know if it was finishing but it must
be because when we load balance to the slave the data there is current.  I
presume it would be stale if the copy didn't finish.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100603/9308ae4e/attachment.htm 

From ssinger at ca.afilias.info  Thu Jun  3 06:42:29 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 03 Jun 2010 09:42:29 -0400
Subject: [Slony1-general] getting log switch to sl_log_2 still in
 progress - tried everything
In-Reply-To: <AANLkTikssl68GEs_pQoNhB6JuW0GaJ3w_IB1-PwiQ6cd@mail.gmail.com>
References: <AANLkTilYWiwxr2CsLy8ZNB96Y-JFBlLYiCGovoYaWla7@mail.gmail.com>	<4C07A743.4000509@ca.afilias.info>
	<AANLkTikssl68GEs_pQoNhB6JuW0GaJ3w_IB1-PwiQ6cd@mail.gmail.com>
Message-ID: <4C07B145.50905@ca.afilias.info>

Karl Lehenbauer wrote:
> On Thu, Jun 3, 2010 at 7:59 AM, Steve Singer <ssinger at ca.afilias.info 
> <mailto:ssinger at ca.afilias.info>> wrote:
> 
>     Does your initial subscribe (the COPY) ever finish? You don't seem
>     to say.
> 
> 
> In a prior message I just said I didn't know if it was finishing but it 
> must be because when we load balance to the slave the data there is 
> current.  I presume it would be stale if the copy didn't finish.


What does your sl_status say?

What events in sl_event are not confirmed in sl_event for all of your 
nodes?

Also what does
select * FROM sl_node say?

Do you have nodes other than the 2 you talk about? If so are they 
subscribed to any sets? (select * FROM sl_subscribe)

Do you have another node that ins't caught up (possibly because you 
didn't realize you have that node)

-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From JanWieck at Yahoo.com  Thu Jun  3 08:04:21 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu, 03 Jun 2010 11:04:21 -0400
Subject: [Slony1-general] getting log switch to sl_log_2 still
 in	progress - tried everything
In-Reply-To: <AANLkTikKP_C3AxQNHo78gTk2xiec_VQTtHcScrA-OK9C@mail.gmail.com>
References: <AANLkTilYWiwxr2CsLy8ZNB96Y-JFBlLYiCGovoYaWla7@mail.gmail.com>	<4C07A743.4000509@ca.afilias.info>
	<AANLkTikKP_C3AxQNHo78gTk2xiec_VQTtHcScrA-OK9C@mail.gmail.com>
Message-ID: <4C07C475.8050903@Yahoo.com>

On 6/3/2010 9:20 AM, Karl Lehenbauer wrote:
> It does finish, and least I think it does.  I will look very carefully 
> this time to make sure (takes a few hours).
> 
> This morning I tore the cluster down and recreated it with a different 
> schema name, thinking that might work.  I saw the sl_log_1-not-truncated 
> messages and figured it was hosed, so seeing your message that the 
> logswitch can't happen while there is data that needs to be replicated 
> was heartening.  (It might be good to make a note in the FAQ that those 
> messages are normal when doing the initial subscribe.)
> 

I am not sure where that idea, that tearing down the whole cluster, is a 
good response to this issue.

The reason, why Slony-I cannot finish the log switch, is because of either

1) there are still log rows in the segment that need to be replicated.

or

2) transactions that were in progress when the logswitch started are
    still in progress.

In case 2) it is possible that such transaction actually did create new 
log rows, which once the transaction commits of course will need to be 
replicated.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From karllehenbauer at gmail.com  Thu Jun  3 08:29:00 2010
From: karllehenbauer at gmail.com (Karl Lehenbauer)
Date: Thu, 3 Jun 2010 10:29:00 -0500
Subject: [Slony1-general] getting log switch to sl_log_2 still
	in	progress - tried everything
In-Reply-To: <4C07C475.8050903@Yahoo.com>
References: <AANLkTilYWiwxr2CsLy8ZNB96Y-JFBlLYiCGovoYaWla7@mail.gmail.com>	<4C07A743.4000509@ca.afilias.info>
	<AANLkTikKP_C3AxQNHo78gTk2xiec_VQTtHcScrA-OK9C@mail.gmail.com>
	<4C07C475.8050903@Yahoo.com>
Message-ID: <FF09CD82-5D77-44BA-8E0F-8652EA8CC3CE@gmail.com>

Well, I did not know that the message was normal while replication was starting.  When I was having the problem, like 10 hours after replicating, one of the sl_log tables had millions of rows and a slony postgres process was continuously at 80% cpu.  I acknowledge it may have been an overreaction to tear the cluster down but after losing the cluster due to a crash, I don't know, it's not very hard to drop and if we start having problems it kills the site pretty fast.

Anyway after regenerating the cluster with a different schema name, not that that had anything to do with it, and waiting, slony has caught up and is now properly truncating / flipping the sl_log_* tables.

Also while this was going on we tried a switchover, which gave us a config error for node -1 (!), and we tried a failover but it just hung.  Now that things seem to be working better, maybe the switchover will go better during tonight's maintenance window.



On Jun 3, 2010, at 10:04 AM, Jan Wieck wrote:

> On 6/3/2010 9:20 AM, Karl Lehenbauer wrote:
>> It does finish, and least I think it does.  I will look very carefully this time to make sure (takes a few hours).
>> This morning I tore the cluster down and recreated it with a different schema name, thinking that might work.  I saw the sl_log_1-not-truncated messages and figured it was hosed, so seeing your message that the logswitch can't happen while there is data that needs to be replicated was heartening.  (It might be good to make a note in the FAQ that those messages are normal when doing the initial subscribe.)
> 
> I am not sure where that idea, that tearing down the whole cluster, is a good response to this issue.
> 
> The reason, why Slony-I cannot finish the log switch, is because of either
> 
> 1) there are still log rows in the segment that need to be replicated.
> 
> or
> 
> 2) transactions that were in progress when the logswitch started are
>   still in progress.
> 
> In case 2) it is possible that such transaction actually did create new log rows, which once the transaction commits of course will need to be replicated.
> 
> 
> Jan
> 
> -- 
> Anyone who trades liberty for security deserves neither
> liberty nor security. -- Benjamin Franklin


From ssinger at ca.afilias.info  Thu Jun  3 08:52:58 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 03 Jun 2010 11:52:58 -0400
Subject: [Slony1-general] getting log switch to sl_log_2 still
 in	progress - tried everything
In-Reply-To: <FF09CD82-5D77-44BA-8E0F-8652EA8CC3CE@gmail.com>
References: <AANLkTilYWiwxr2CsLy8ZNB96Y-JFBlLYiCGovoYaWla7@mail.gmail.com>	<4C07A743.4000509@ca.afilias.info>
	<AANLkTikKP_C3AxQNHo78gTk2xiec_VQTtHcScrA-OK9C@mail.gmail.com>
	<4C07C475.8050903@Yahoo.com>
	<FF09CD82-5D77-44BA-8E0F-8652EA8CC3CE@gmail.com>
Message-ID: <4C07CFDA.5060006@ca.afilias.info>

Karl Lehenbauer wrote:
> Well, I did not know that the message was normal while replication was starting.  When I was having the problem, like 10 hours after replicating, one of the sl_log tables had millions of rows and a slony postgres process was continuously at 80% cpu.  I acknowledge it may have been an overreaction to tear the cluster down but after losing the cluster due to a crash, I don't know, it's not very hard to drop and if we start having problems it kills the site pretty fast.
> 
> Anyway after regenerating the cluster with a different schema name, not that that had anything to do with it, and waiting, slony has caught up and is now properly truncating / flipping the sl_log_* tables.
> 
> Also while this was going on we tried a switchover, which gave us a config error for node -1 (!), and we tried a failover but it just hung.  Now that things seem to be working better, maybe the switchover will go better during tonight's maintenance window.
> 

Trying a switchover while a subscription is in progress is probably not 
the greatest test (unless your goal is to go out and search for 
problems).  A MOVE SET shouldn't complete until after the subscription 
process is finished (so it will appear as hung).   I'm  not sure what a 
FAILOVER while the subscription process is in progress will do, I'll try 
to write up a test case to see what it actually does.

In some of the testing I seem to hit the node -1 issue sometimes with 
cascaded replicas as well.  I've opened some bugs to track it but have 
yet figure out exactly what is going on.



> 
> On Jun 3, 2010, at 10:04 AM, Jan Wieck wrote:
> 
>> On 6/3/2010 9:20 AM, Karl Lehenbauer wrote:
>>> It does finish, and least I think it does.  I will look very carefully this time to make sure (takes a few hours).
>>> This morning I tore the cluster down and recreated it with a different schema name, thinking that might work.  I saw the sl_log_1-not-truncated messages and figured it was hosed, so seeing your message that the logswitch can't happen while there is data that needs to be replicated was heartening.  (It might be good to make a note in the FAQ that those messages are normal when doing the initial subscribe.)
>> I am not sure where that idea, that tearing down the whole cluster, is a good response to this issue.
>>
>> The reason, why Slony-I cannot finish the log switch, is because of either
>>
>> 1) there are still log rows in the segment that need to be replicated.
>>
>> or
>>
>> 2) transactions that were in progress when the logswitch started are
>>   still in progress.
>>
>> In case 2) it is possible that such transaction actually did create new log rows, which once the transaction commits of course will need to be replicated.
>>
>>
>> Jan
>>
>> -- 
>> Anyone who trades liberty for security deserves neither
>> liberty nor security. -- Benjamin Franklin
> 


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From raghu.ram at enterprisedb.com  Thu Jun  3 13:46:56 2010
From: raghu.ram at enterprisedb.com (Raghu Ram)
Date: Fri, 4 Jun 2010 02:16:56 +0530
Subject: [Slony1-general] Replication issue
Message-ID: <AANLkTinsF1E0Pqh0frc1TsJ6n5wWVQEWPyMEe3MxP6w1@mail.gmail.com>

Hi,

Replication is setup performed between master to slave node, while syncing
operation going b/w the master to slave, we have observed the below errors
in slony log files and sync is aborted


2010-06-03 12:51:41 PDTDEBUG2 remoteListenThread_1: queue event 1,5000007435
SYNC
2010-06-03 12:51:42 PDTDEBUG2 remoteListenThread_1: queue event 1,5000007436
SYNC
2010-06-03 12:51:43 PDTDEBUG2 remoteListenThread_1: queue event 1,5000007437
SYNC
2010-06-03 12:51:44 PDTDEBUG2 remoteListenThread_1: queue event 1,5000007438
SYNC
2010-06-03 12:51:45 PDTDEBUG2 remoteListenThread_1: queue event 1,5000007439
SYNC
2010-06-03 12:51:45 PDTDEBUG2 remoteWorkerThread_1: SYNC 5000004861
processing
2010-06-03 12:51:45 PDTDEBUG1 about to monitor_subscriber_query - pulling
big actionid list for 1
2010-06-03 12:51:45 PDTINFO   remoteWorkerThread_1: syncing set 1 with 36
table(s) from provider 1
2010-06-03 12:51:45 PDTDEBUG2  ssy_action_list length: 0
2010-06-03 12:51:45 PDTDEBUG2 remoteWorkerThread_1: current local log_status
is 0
2010-06-03 12:51:45 PDTDEBUG2 remoteWorkerThread_1_1: current remote
log_status = 2
2010-06-03 12:51:46 PDTDEBUG1 remoteHelperThread_1_1: 0.272 seconds delay
for first row
2010-06-03 12:51:46 PDTDEBUG1 remoteHelperThread_1_1: 0.291 seconds until
close cursor
2010-06-03 12:51:46 PDTDEBUG1 remoteHelperThread_1_1: inserts=226
updates=173 deletes=0
2010-06-03 12:51:46 PDTDEBUG1 remoteWorkerThread_1: sync_helper timing:
 pqexec (s/count)- provider 0.274/4 - subscriber 0.001/4
2010-06-03 12:51:46 PDTDEBUG1 remoteWorkerThread_1: sync_helper timing:
 large tuples 0.003/1
2010-06-03 12:51:46 PDTERROR  remoteWorkerThread_1: "update only
"public"."jobs" set
"folder_path"='/download/es/ws_2B_PROCESSED_PREVIEW2WEB/=pressfourt_14249-B0154127094-001-5k-4.4.0.0-16PT-BCMATT-2X35-MIA'
where "job_id"='2022703';
insert into "public"."job_status"
("job_status_id","job_id","status_code","take","date_set","user_id","email_needed","email_sent","folder_path","folder_user","batch_number","job_alias","ship_tracker")
values ('37700155','2022703','100','1','2010-06-03
12:05:21','0','f','f','/download/es/ws_2B_PROCESSED_PREVIEW2WEB/=pressfourt_14249-B0154127094-001-5k-4.4.0.0-16PT-BCMATT-2X35-MIA','','','','');
update only "public"."jobs" set "job_id"='2022703' where "job_id"='2022703';
insert into "public"."job_tasks"
("job_task_id","job_id","completed","task","notes","date_added","failed","attempt","date_completed","task_id")
values
('2022249.S','2022249','f','send2prod|++=@@theprintin_14181-B0154116748-001-1k-4.4.0.0-14PT-RCBCMATT-2X35-MIA-RC|2B_PROCESSED_4SITE at GLN|5',NULL,'2010-06-03
12:05:20.767821','f','0',NULL,'3044361');
update only "public"."job_tasks" set
"completed"='t',"notes"='',"date_completed"='2010-06-03 12:05:20.688241'
where "task_id"='3044349';
insert into "public"."notes"
("note_id","cust_id","anything_id","id_type","date_added","user_id","text","from_dept_id","to_dept_id","auto_note_id","read_by","read_date")
values ('2031235','52069','2022695','','2010-06-03 12:05:21','0','HTML
Preview:
http://tools.4over.com/previews/,B0154/B0154127082-001_Ss4BcFlHqYVG2OS0TBdMWHxN.d
','','','',NULL,NULL);
insert into "public"."orders"
("order_id","source","source_order_id","date_ordered","date_added","cust_id","cust_company","cust_firstname","cust_lastname","cust_phone","cust_email","cust_bill_addr1","cust_bill_addr2","cust_bill_city","cust_bill_state","cust_bill_zip","cust_bill_country","pay_method","pay_grossprice","pay_netprice","pay_cclast4","pay_approval_code","pay_trans_id","discount_type","discount_id","discount_adjustment","discount_value","prod_status","instructions","xml_file")
values ('1283441','trade','B0154127118','2010-06-03 12:06:54','2010-06-03
12:06:41','3228','Calabasas Printing','Darrin','Cockerill','8185912935','
paul at calabasasprinting.com','23875 Ventura Blvd., Suite
105','','Calabasas','CA','91302','US','cc','42000','42000','5718','413050','3020152307','','','','','0',NULL,NULL);
update only "public"."orders" set " ERROR:  syntax error at end of input
LINE 8: update only "public"."orders" set
                                          ^
 - qualification was:
2010-06-03 12:51:46 PDTDEBUG2 remoteWorkerThread_1: cleanup
2010-06-03 12:51:46 PDTERROR  remoteWorkerThread_1: SYNC aborted
2010-06-03 12:51:46 PDTDEBUG2 remoteListenThread_1: queue event 1,5000007440
SYNC
2010-06-03 12:51:47 PDTDEBUG2 remoteListenThread_1: queue event 1,5000007441
SYNC
2010-06-03 12:51:48 PDTDEBUG2 remoteListenThread_1: queue event 1,5000007442
SYNC
2010-06-03 12:51:49 PDTDEBUG2 remoteListenThread_1: queue event 1,5000007443
SYNC
2010-06-03 12:51:50 PDTDEBUG2 remoteListenThread_1: queue event 1,5000007444
SYNC


Could you please suggest, how to rid out of above errors.

Slony version: 2.0.1

Structure of the table for Master and slave is fine.



-- 
Thanks & Regards
Raghu Ram
System Engineer
EnterpriseDB Corporation
The Enterprise Postgres Company
Phone: 732-331-1300 Ext- 2022
           020-30589493
          +91-9604766989
Website: www.enterprisedb.com
EnterpriseDB Blog: http://blogs.enterprisedb.com/
Follow us on Twitter: http://www.twitter.com/enterprisedb

This e-mail message (and any attachment) is intended for the use of the
individual or entity to whom it is addressed. This message contains
information from EnterpriseDB Corporation that may be privileged,
confidential, or exempt from disclosure under applicable law. If you are not
the intended recipient or authorized to receive this for the intended
recipient, any use, dissemination, distribution, retention, archiving, or
copying of this communication is strictly prohibited. If you have received
this e-mail in error, please notify the sender immediately by reply e-mail
and delete this message.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100604/96050283/attachment.htm 

From scott.marlowe at gmail.com  Thu Jun  3 14:03:30 2010
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Thu, 3 Jun 2010 15:03:30 -0600
Subject: [Slony1-general] Replication issue
In-Reply-To: <AANLkTinsF1E0Pqh0frc1TsJ6n5wWVQEWPyMEe3MxP6w1@mail.gmail.com>
References: <AANLkTinsF1E0Pqh0frc1TsJ6n5wWVQEWPyMEe3MxP6w1@mail.gmail.com>
Message-ID: <AANLkTiksy9iZdaocsi6IVF2-dD-QF9Xg_upbTnOJ5ZLP@mail.gmail.com>

On Thu, Jun 3, 2010 at 2:46 PM, Raghu Ram <raghu.ram at enterprisedb.com> wrote:
> Hi,
> 105','','Calabasas','CA','91302','US','cc','42000','42000','5718','413050','3020152307','','','','','0',NULL,NULL);
> update only "public"."orders" set " ERROR: ?syntax error at end of input
> LINE 8: update only "public"."orders" set
>
> Could you please suggest, how to rid out of above errors.
> Slony version: 2.0.1
> Structure of the table for Master and slave is fine.

1: Slony 2.0.x is NOT ready for production (but it's getting closer),
you'd be advised to run 1.2.latest there.
2: Slony 2.0.x is up to 2.0.4.rc2 and has a lot of bug fixes, so it's
likely you're being bitten by an already fixed bug.  Try a 2.0.4.rc2
or wait for 2.0.4 final release and try that.

From alexander.kolodziej at tactel.se  Fri Jun  4 06:35:35 2010
From: alexander.kolodziej at tactel.se (Alexander Kolodziej)
Date: Fri, 04 Jun 2010 15:35:35 +0200
Subject: [Slony1-general] child terminated status: 11 -> restart of
 worker in 10 seconds
In-Reply-To: <4BFBD55F.9000200@ca.afilias.info>
References: <4BFBB25B.5010709@tactel.se> <4BFBD55F.9000200@ca.afilias.info>
Message-ID: <4C090127.5030702@tactel.se>

Hello!

To summarize: I have a large DB (about 15gb on disk), where i can start
replication, and first it seems ok, but when the slave has almost caught
up (judging by the size on disk) the slon process starts to segfault.

I compiled a slon (same version, 1.2.15) with debugging symbols, did
"ulimit -c unlimited" and also "ulimit -n 32768", since i also got
errors about "Too many open files".

And now i finally got around to producing a core dump (see below).

Is there any hope? This is Ubuntu 9.04, Postgresql 8.3.7 and Slon 1.2.15.

There is a 1.2.16 version of Slony available for Postgres 8.3 in Ubuntu.
Any chance that that could solve the problem? Other options i see are
too dump/restore the master DB. Other than that... Any ideas? :)

  wbr / Alex

So it segfaults while logging?
===================================================================
Core was generated by `/usr/bin/slon -f /etc/slony1/bigdb/slon.conf -p
/var/run/slony1/bigdb'.
Program terminated with signal 11, Segmentation fault.
[New process 31964]
[New process 31965]
[New process 31966]
[New process 31949]
[New process 31967]
[New process 31971]
[New process 31954]
[New process 31952]
#0  0x00007fcfd5e37c40 in strlen () from /lib/libc.so.6
(gdb) bt
#0  0x00007fcfd5e37c40 in strlen () from /lib/libc.so.6
#1  0x00007fcfd5e0075e in vfprintf () from /lib/libc.so.6
#2  0x00007fcfd5eb3738 in __vsnprintf_chk () from /lib/libc.so.6
#3  0x0000000000418764 in slon_log (level=<value optimized out>,
fmt=0x420093 " ssy_action_list value: %s\n") at
/usr/include/bits/stdio2.h:78
#4  0x000000000040b4c5 in sync_event (node=0x1659f90, local_conn=<value
optimized out>, wd=0x1659920, event=0x7fcfcc008f20) at remote_worker.c:4334
#5  0x000000000040dcf8 in remoteWorkerThread_main (cdata=0x1659f90) at
remote_worker.c:630
#6  0x00007fcfd61303ba in start_thread () from /lib/libpthread.so.0
#7  0x00007fcfd5e9cfcd in clone () from /lib/libc.so.6
#8  0x0000000000000000 in ?? ()
(gdb)
===================================================================






Steve Singer wrote:
> Alexander Kolodziej wrote:
>> Hello!
>> ...
>> 2010-05-25 11:16:00 UTC DEBUG2 remoteListenThread_1: queue event
>> 1,2533 SYNC
>> 2010-05-25 11:16:00 UTC DEBUG2 slon: child terminated status: 11; pid:
>> 29027, current worker pid: 29027
>> 2010-05-25 11:16:00 UTC DEBUG1 slon: restart of worker in 10 seconds
>> --------------------------
>>
>> In syslog i see this on the slave (slon segfault errors every 10s):
>> --------------------------
>> May 25 11:16:30 semc-sh62 kernel: [20053518.436336] slon[29076]:
>> segfault at 273936 ip 00007fd69e8bac40 sp 00007fd69ad48698 error 4 in
>> libc-2.9.so[7fd69e83a000+168000]
>> May 25 11:16:40 semc-sh62 kernel: [20053528.548794] slon[29104]:
>> segfault at 273936 ip 00007f359f4f4c40 sp 00007f359b982698 error 4 in
>> libc-2.9.so[7f359f474000+168000]
>> --------------------------
>>
> 
> Can you rub gdb against a core file, or start slony up inside of gdb, so
> we can get a stack trace of what slon was doing went it died?
> 
> (from a build with debugging symbols would be even more useful)
> 
> 
>> What could cause this?
>> Looking at the size of /var/lib/postgresql/8.3/ i can see that it has
>> almost succeeded in replicating the DB, but something is going boink.
>>
>> Slon loglevel is set to 4.
>> Are there any slon sl_* tables i can look in for info?
>> Tried google but only get 8 results on: "child terminated status: 11" +
>> "restart of worker", and none of those provide a solution.
>>
>>   wbr / Alexander
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 


From ssinger at ca.afilias.info  Fri Jun  4 06:48:39 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 04 Jun 2010 09:48:39 -0400
Subject: [Slony1-general] child terminated status: 11 -> restart of
 worker in 10 seconds
In-Reply-To: <4C090127.5030702@tactel.se>
References: <4BFBB25B.5010709@tactel.se> <4BFBD55F.9000200@ca.afilias.info>
	<4C090127.5030702@tactel.se>
Message-ID: <4C090437.30205@ca.afilias.info>

Alexander Kolodziej wrote:
> Hello!

> There is a 1.2.16 version of Slony available for Postgres 8.3 in Ubuntu.
> Any chance that that could solve the problem? Other options i see are
> too dump/restore the master DB. Other than that... Any ideas? :)
> 
>   wbr / Alex
> 
> So it segfaults while logging?

This looks somewhat like bug # 55.

This was fixed in 1.2.21 try slony version 1.2.21 to see if it fixes 
your problem.


The other thing you might try doing is set the slon log level to 
something lower (less logging), to not trigger the bug.



> ===================================================================
> Core was generated by `/usr/bin/slon -f /etc/slony1/bigdb/slon.conf -p
> /var/run/slony1/bigdb'.
> Program terminated with signal 11, Segmentation fault.
> [New process 31964]
> [New process 31965]
> [New process 31966]
> [New process 31949]
> [New process 31967]
> [New process 31971]
> [New process 31954]
> [New process 31952]
> #0  0x00007fcfd5e37c40 in strlen () from /lib/libc.so.6
> (gdb) bt
> #0  0x00007fcfd5e37c40 in strlen () from /lib/libc.so.6
> #1  0x00007fcfd5e0075e in vfprintf () from /lib/libc.so.6
> #2  0x00007fcfd5eb3738 in __vsnprintf_chk () from /lib/libc.so.6
> #3  0x0000000000418764 in slon_log (level=<value optimized out>,
> fmt=0x420093 " ssy_action_list value: %s\n") at
> /usr/include/bits/stdio2.h:78
> #4  0x000000000040b4c5 in sync_event (node=0x1659f90, local_conn=<value
> optimized out>, wd=0x1659920, event=0x7fcfcc008f20) at remote_worker.c:4334
> #5  0x000000000040dcf8 in remoteWorkerThread_main (cdata=0x1659f90) at
> remote_worker.c:630
> #6  0x00007fcfd61303ba in start_thread () from /lib/libpthread.so.0
> #7  0x00007fcfd5e9cfcd in clone () from /lib/libc.so.6
> #8  0x0000000000000000 in ?? ()
> (gdb)
> ===================================================================
> 
> 
> 
> 

-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From alexander.kolodziej at tactel.se  Fri Jun  4 06:59:21 2010
From: alexander.kolodziej at tactel.se (Alexander Kolodziej)
Date: Fri, 04 Jun 2010 15:59:21 +0200
Subject: [Slony1-general] child terminated status: 11 -> restart of
 worker in 10 seconds
In-Reply-To: <4C090437.30205@ca.afilias.info>
References: <4BFBB25B.5010709@tactel.se> <4BFBD55F.9000200@ca.afilias.info>
	<4C090127.5030702@tactel.se> <4C090437.30205@ca.afilias.info>
Message-ID: <4C0906B9.5080907@tactel.se>

Thanks!

I just restarted with log level 1 and actually, so far, it seems ok:)
Will see if perhaps building a .deb pkg with 1.2.21 is an option.

  thanks again! / alex

Steve Singer wrote:
> Alexander Kolodziej wrote:
>> Hello!
> 
>> There is a 1.2.16 version of Slony available for Postgres 8.3 in Ubuntu.
>> Any chance that that could solve the problem? Other options i see are
>> too dump/restore the master DB. Other than that... Any ideas? :)
>>
>>   wbr / Alex
>>
>> So it segfaults while logging?
> 
> This looks somewhat like bug # 55.
> 
> This was fixed in 1.2.21 try slony version 1.2.21 to see if it fixes
> your problem.
> 
> 
> The other thing you might try doing is set the slon log level to
> something lower (less logging), to not trigger the bug.
> 
> 
> 
>> ===================================================================
>> Core was generated by `/usr/bin/slon -f /etc/slony1/bigdb/slon.conf -p
>> /var/run/slony1/bigdb'.
>> Program terminated with signal 11, Segmentation fault.
>> [New process 31964]
>> [New process 31965]
>> [New process 31966]
>> [New process 31949]
>> [New process 31967]
>> [New process 31971]
>> [New process 31954]
>> [New process 31952]
>> #0  0x00007fcfd5e37c40 in strlen () from /lib/libc.so.6
>> (gdb) bt
>> #0  0x00007fcfd5e37c40 in strlen () from /lib/libc.so.6
>> #1  0x00007fcfd5e0075e in vfprintf () from /lib/libc.so.6
>> #2  0x00007fcfd5eb3738 in __vsnprintf_chk () from /lib/libc.so.6
>> #3  0x0000000000418764 in slon_log (level=<value optimized out>,
>> fmt=0x420093 " ssy_action_list value: %s\n") at
>> /usr/include/bits/stdio2.h:78
>> #4  0x000000000040b4c5 in sync_event (node=0x1659f90, local_conn=<value
>> optimized out>, wd=0x1659920, event=0x7fcfcc008f20) at
>> remote_worker.c:4334
>> #5  0x000000000040dcf8 in remoteWorkerThread_main (cdata=0x1659f90) at
>> remote_worker.c:630
>> #6  0x00007fcfd61303ba in start_thread () from /lib/libpthread.so.0
>> #7  0x00007fcfd5e9cfcd in clone () from /lib/libc.so.6
>> #8  0x0000000000000000 in ?? ()
>> (gdb)
>> ===================================================================
>>
>>
>>
>>
> 


From machielr at rdc.co.za  Mon Jun  7 00:14:39 2010
From: machielr at rdc.co.za (Machiel Richards)
Date: Mon, 7 Jun 2010 09:14:39 +0200
Subject: [Slony1-general] slony config error
Message-ID: <015101cb0611$1afefbc0$50fcf340$@co.za>

Hi All

	

???? I am hoping that there are some guys out there that knows slony and
some things to look out for.


????? I would really appreciate the help.

???? I am busy testing the slony replication setup (master /slave) on two
virtual machines that I have setup on Debian.

?? I have installed the slony packages, tested basic ocnnectivity etc... and
all god so far.


??? Going through documentation I created a preamble.sk file and an
initcluster.sk file which I am trying to run in order to initialize the
cluster.

??? when running the initcluster.sk I get the following error message:

preamble.sk:5: ERROR: syntax error at or near =


? I have attached both files and hoping that someone could assist please.

Preamble.sk:

		define CLUSTER test;
		define DEBIAN1 1;
		define DEBIAN2 2;

		cluster = @CLUSTER;
		node @DEBIAN1 admin
	      conninfo = 'dbname=postgres host=debian1 user=slony';
		node @DEBIAN2 admin
	      conninfo = 'dbname=postgres host=debian2 user=slony';


Initcluster.sk:

		 #!/usr/bin/slonik

		# File: initcluster.sk

		include <preamble.sk>;

		init cluster ( id = @DEBIAN1, comment = 'primary node -
Debian1');

Regards
Machiel



From stephane.schildknecht at postgresql.fr  Mon Jun  7 01:17:43 2010
From: stephane.schildknecht at postgresql.fr (=?ISO-8859-15?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Mon, 07 Jun 2010 10:17:43 +0200
Subject: [Slony1-general] slony config error
In-Reply-To: <015101cb0611$1afefbc0$50fcf340$@co.za>
References: <015101cb0611$1afefbc0$50fcf340$@co.za>
Message-ID: <4C0CAB27.6010202@postgresql.fr>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi,

Le 07/06/2010 09:14, Machiel Richards a ?crit :
> Hi All
(...)

> 
>     when running the initcluster.sk I get the following error message:
> 
> preamble.sk:5: ERROR: syntax error at or near =
> 
> 
>   I have attached both files and hoping that someone could assist please.
> 
> Preamble.sk:
> 
> 		define CLUSTER test;
> 		define DEBIAN1 1;
> 		define DEBIAN2 2;
> 
> 		cluster = @CLUSTER;

The exact syntax is :

cluster name = @CLUSTER;

> 		node @DEBIAN1 admin
> 	      conninfo = 'dbname=postgres host=debian1 user=slony';
> 		node @DEBIAN2 admin
> 	      conninfo = 'dbname=postgres host=debian2 user=slony';
> 
> 
> Initcluster.sk:
> 
> 		 #!/usr/bin/slonik
> 
> 		# File: initcluster.sk
> 
> 		include <preamble.sk>;

There is no ';' at the end of this line.

Best regards,
St?phane Schildknecht
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.10 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org/

iEYEARECAAYFAkwMqycACgkQA+REPKWGI0GHsgCgi57W4+4Lh720vEI6sHZmwgkB
4QMAn0ZI2EJAqDp/o9jilQvPj/Jq17O9
=e7w8
-----END PGP SIGNATURE-----

From open at immo.ru  Tue Jun  8 02:41:17 2010
From: open at immo.ru (Alexander V Openkin)
Date: Tue, 08 Jun 2010 13:41:17 +0400
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
Message-ID: <4C0E103D.1070404@immo.ru>

Hi all!

I build rpm (slony1-2.0.4.rc2) with postgresql8.4.3 support, install in 
VPS (x86_64),
create simple cluster, and running slon daemon,
and ps auxf show me

postgres 30141  0.0  0.0  40636  1840 pts/0    S    10:05   0:00 
/usr/bin/slon
postgres 30145  0.0  0.0 1042736 1404 pts/0    Sl   10:05   0:00  \_ 
/usr/bin/slon

Virtual memory size is a 1G, it's nornal ?

PS i check on the other vervion of slony1 ( slony1-2.0.3 slony1-1.2.21 ) 
on the x86_64 architecture - i have same result,
the huge memory requirements, about 1-2G per instance.

wtf ?

From JanWieck at Yahoo.com  Tue Jun  8 06:48:24 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Tue, 08 Jun 2010 09:48:24 -0400
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <4C0E103D.1070404@immo.ru>
References: <4C0E103D.1070404@immo.ru>
Message-ID: <4C0E4A28.9010500@Yahoo.com>

On 6/8/2010 5:41 AM, Alexander V Openkin wrote:
> Hi all!
> 
> I build rpm (slony1-2.0.4.rc2) with postgresql8.4.3 support, install in 
> VPS (x86_64),
> create simple cluster, and running slon daemon,
> and ps auxf show me
> 
> postgres 30141  0.0  0.0  40636  1840 pts/0    S    10:05   0:00 
> /usr/bin/slon
> postgres 30145  0.0  0.0 1042736 1404 pts/0    Sl   10:05   0:00  \_ 
> /usr/bin/slon
> 
> Virtual memory size is a 1G, it's nornal ?
> 
> PS i check on the other vervion of slony1 ( slony1-2.0.3 slony1-1.2.21 ) 
> on the x86_64 architecture - i have same result,
> the huge memory requirements, about 1-2G per instance.

Does your test database contain very wide rows?


Jan


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From open at immo.ru  Tue Jun  8 07:00:10 2010
From: open at immo.ru (Alexander V Openkin)
Date: Tue, 08 Jun 2010 18:00:10 +0400
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <4C0E4A28.9010500@Yahoo.com>
References: <4C0E103D.1070404@immo.ru> <4C0E4A28.9010500@Yahoo.com>
Message-ID: <4C0E4CEA.5090608@immo.ru>

it was the empty slony1 cluster with 0 (zero :) ) sets, and 1 node (master)

the same database (but with 3 slave and ~10 tables in replication sets) 
on PgSQL 8.3.9 and slony 1.2.14,
but i686 architecture


postgres 17801 0.0 0.0 4904 1300 ? S 04:03 0:00 /usr/bin/slon
postgres 17805 0.0 0.1 89508 2064 ? Sl 04:03 0:08 \_ /usr/bin/slon

only 90??....
i have no ideas...


08.06.2010 17:48, Jan Wieck ?????:
> On 6/8/2010 5:41 AM, Alexander V Openkin wrote:
>> Hi all!
>>
>> I build rpm (slony1-2.0.4.rc2) with postgresql8.4.3 support, install 
>> in VPS (x86_64),
>> create simple cluster, and running slon daemon,
>> and ps auxf show me
>>
>> postgres 30141 0.0 0.0 40636 1840 pts/0 S 10:05 0:00 /usr/bin/slon
>> postgres 30145 0.0 0.0 1042736 1404 pts/0 Sl 10:05 0:00 \_ /usr/bin/slon
>>
>> Virtual memory size is a 1G, it's nornal ?
>>
>> PS i check on the other vervion of slony1 ( slony1-2.0.3 
>> slony1-1.2.21 ) on the x86_64 architecture - i have same result,
>> the huge memory requirements, about 1-2G per instance.
>
> Does your test database contain very wide rows?
>
>
> Jan
>
>


From scott.marlowe at gmail.com  Tue Jun  8 07:01:18 2010
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Tue, 8 Jun 2010 08:01:18 -0600
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <4C0E103D.1070404@immo.ru>
References: <4C0E103D.1070404@immo.ru>
Message-ID: <AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>

On Tue, Jun 8, 2010 at 3:41 AM, Alexander V Openkin <open at immo.ru> wrote:
> Hi all!
>
> I build rpm (slony1-2.0.4.rc2) with postgresql8.4.3 support, install in
> VPS (x86_64),
> create simple cluster, and running slon daemon,
> and ps auxf show me
>
> postgres 30141 ?0.0 ?0.0 ?40636 ?1840 pts/0 ? ?S ? ?10:05 ? 0:00
> /usr/bin/slon
> postgres 30145 ?0.0 ?0.0 1042736 1404 pts/0 ? ?Sl ? 10:05 ? 0:00 ?\_
> /usr/bin/slon
>
> Virtual memory size is a 1G, it's nornal ?

Yes

man top and / virt

From open at immo.ru  Tue Jun  8 07:05:19 2010
From: open at immo.ru (Alexander V Openkin)
Date: Tue, 08 Jun 2010 18:05:19 +0400
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>
References: <4C0E103D.1070404@immo.ru>
	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>
Message-ID: <4C0E4E1F.5020803@immo.ru>

it was readed )

i have VPS based on OpenVZ, and i see the excess memory limitation 
(current memory limit ~20G )

08.06.2010 18:01, Scott Marlowe ?????:
> On Tue, Jun 8, 2010 at 3:41 AM, Alexander V Openkin<open at immo.ru>  wrote:
>    
>> Hi all!
>>
>> I build rpm (slony1-2.0.4.rc2) with postgresql8.4.3 support, install in
>> VPS (x86_64),
>> create simple cluster, and running slon daemon,
>> and ps auxf show me
>>
>> postgres 30141  0.0  0.0  40636  1840 pts/0    S    10:05   0:00
>> /usr/bin/slon
>> postgres 30145  0.0  0.0 1042736 1404 pts/0    Sl   10:05   0:00  \_
>> /usr/bin/slon
>>
>> Virtual memory size is a 1G, it's nornal ?
>>      
> Yes
>
> man top and / virt
>
>    


From ssinger at ca.afilias.info  Tue Jun  8 07:21:57 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 08 Jun 2010 10:21:57 -0400
Subject: [Slony1-general] slony_logshipper utility
Message-ID: <4C0E5205.8030207@ca.afilias.info>


Is there anyone out there that is using the slony_logshipper utility 
program that is included in the shared distribution.

If so,  are you using it in daemon mode or foreground mode?  how well do 
you find it works in practice?  Which version of slony have you been 
using it with?



-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From open at immo.ru  Wed Jun  9 00:37:44 2010
From: open at immo.ru (Alexander V Openkin)
Date: Wed, 09 Jun 2010 11:37:44 +0400
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>
References: <4C0E103D.1070404@immo.ru>
	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>
Message-ID: <4C0F44C8.5070409@immo.ru>

08.06.2010 18:01, Scott Marlowe ?????:
> On Tue, Jun 8, 2010 at 3:41 AM, Alexander V Openkin<open at immo.ru>  wrote:
>    
>> Hi all!
>>
>> I build rpm (slony1-2.0.4.rc2) with postgresql8.4.3 support, install in
>> VPS (x86_64),
>> create simple cluster, and running slon daemon,
>> and ps auxf show me
>>
>> postgres 30141  0.0  0.0  40636  1840 pts/0    S    10:05   0:00
>> /usr/bin/slon
>> postgres 30145  0.0  0.0 1042736 1404 pts/0    Sl   10:05   0:00  \_
>> /usr/bin/slon
>>
>> Virtual memory size is a 1G, it's nornal ?
>>      
> Yes
>
> man top and / virt
>
>    
Tnx for the quick answer,
but if you run all slon process (for your replication cluster)
you must have standalone server with 32G ?
I think this is unnatural )

From scott.marlowe at gmail.com  Wed Jun  9 00:54:22 2010
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Wed, 9 Jun 2010 01:54:22 -0600
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <4C0F44C8.5070409@immo.ru>
References: <4C0E103D.1070404@immo.ru>
	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>
	<4C0F44C8.5070409@immo.ru>
Message-ID: <AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>

On Wed, Jun 9, 2010 at 1:37 AM, Alexander V Openkin <open at immo.ru> wrote:
>
> Tnx for the quick answer,
> but if you run all slon process (for your replication cluster)
> you must have standalone server with 32G ?
> I think this is unnatural )

No, definitely not.  VIRT is everything the process has ever touched,
including shared memory and all libs whether or not they've actually
been loaded or not.  It's not uncommon to have literally a hundred
processes with 8G+VIRT on my 32Gig db servers, because most of that 8G
is shred buffers.  It's not using that much memory individually, it's
using shared_memory, and each process reports that it has access to
and has touched that 8G.

From open at immo.ru  Wed Jun  9 02:12:55 2010
From: open at immo.ru (Alexander V Openkin)
Date: Wed, 09 Jun 2010 13:12:55 +0400
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>
References: <4C0E103D.1070404@immo.ru>	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>	<4C0F44C8.5070409@immo.ru>
	<AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>
Message-ID: <4C0F5B17.4000702@immo.ru>

09.06.2010 11:54, Scott Marlowe ?????:
> On Wed, Jun 9, 2010 at 1:37 AM, Alexander V Openkin<open at immo.ru>  wrote:
>    
>> Tnx for the quick answer,
>> but if you run all slon process (for your replication cluster)
>> you must have standalone server with 32G ?
>> I think this is unnatural )
>>      
> No, definitely not.  VIRT is everything the process has ever touched,
> including shared memory and all libs whether or not they've actually
> been loaded or not.  It's not uncommon to have literally a hundred
> processes with 8G+VIRT on my 32Gig db servers, because most of that 8G
> is shred buffers.  It's not using that much memory individually, it's
> using shared_memory, and each process reports that it has access to
> and has touched that 8G.
>
>    
small experement, the second instance for ather Db was added...

[root at vpsXXXX /]# /etc/init.d/slon stop
Stopping slon service: [ DONE ]
Stopping slon service: [ DONE ]
[root at vpsXXXX /]# ps axuf |grep slon
root 28419 0.0 0.0 6044 580 pts/0 S+ Jun08 0:00 \_ grep slon
[root at vps6147 /]# free
total used free shared buffers cached
Mem: 10485760 99192 10386568 0 0 0
-/+ buffers/cache: 99192 10386568
Swap: 0 0 0
[root at vpsXXX /]# /etc/init.d/slon start
Starting slon service: [ DONE ]
[root at vpsXXX /]# ps axuf |grep slon
root 29737 0.0 0.0 6040 584 pts/0 S+ Jun08 0:00 \_ grep slon
postgres 28555 0.0 0.0 40636 1832 pts/0 S Jun08 0:00 /usr/bin/slon
postgres 28556 0.0 0.0 4042880 1508 pts/0 Sl Jun08 0:00 \_ /usr/bin/slon
postgres 28592 0.0 0.0 40636 1836 pts/0 S Jun08 0:00 /usr/bin/slon
postgres 28594 0.0 0.0 4042880 1512 pts/0 Sl Jun08 0:00 \_ /usr/bin/slon
[root at vpsXXXX /]# free
total used free shared buffers cached
Mem: 10485760 8119136 2366624 0 0 0
-/+ buffers/cache: 8119136 2366624
Swap: 0 0 0
[root at vpsXXXX /]#

The different between "no slon process" and "2 pair slon process" ~8G, 
that we have a problem,
because it`s not a shared segment.....

Besides OpenVZ divide shared memory and resident memory

[root at vps6147 /]# cat /proc/user_beancounters |grep -E 
'privvmpages|shmpages'
privvmpages 2029830 2033439 2621440 2621440 5
shmpages 17632 17632 412000 412000 0
[root at vps6147 /]#

first column - the current value in 4k pages, it`s indicates very small 
shared segment and huge resident segment,

Do you have a expirience using slony1 on x86_64 servers ?
We using slony1 replication about 3 year on i686 architecture and we 
hav`t similar problem....

PS we using the same OpenVZ template for application servers, and 
probability error in template or in the current VPS is minimum.


From scott.marlowe at gmail.com  Wed Jun  9 07:27:26 2010
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Wed, 9 Jun 2010 08:27:26 -0600
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <4C0F5B17.4000702@immo.ru>
References: <4C0E103D.1070404@immo.ru>
	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>
	<4C0F44C8.5070409@immo.ru>
	<AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>
	<4C0F5B17.4000702@immo.ru>
Message-ID: <AANLkTimmqqrV5N-M9nVAx-ygfMrX-uBuGIFHYRyJuhLj@mail.gmail.com>

On Wed, Jun 9, 2010 at 3:12 AM, Alexander V Openkin <open at immo.ru> wrote:
> 09.06.2010 11:54, Scott Marlowe ?????:
>>
>> On Wed, Jun 9, 2010 at 1:37 AM, Alexander V Openkin<open at immo.ru> ?wrote:
>>
>>>
>>> Tnx for the quick answer,
>>> but if you run all slon process (for your replication cluster)
>>> you must have standalone server with 32G ?
>>> I think this is unnatural )
>>>
>>
>> No, definitely not. ?VIRT is everything the process has ever touched,
>> including shared memory and all libs whether or not they've actually
>> been loaded or not. ?It's not uncommon to have literally a hundred
>> processes with 8G+VIRT on my 32Gig db servers, because most of that 8G
>> is shred buffers. ?It's not using that much memory individually, it's
>> using shared_memory, and each process reports that it has access to
>> and has touched that 8G.
>>
>>
>
> small experement, the second instance for ather Db was added...
>
> [root at vpsXXXX /]# /etc/init.d/slon stop
> Stopping slon service: [ DONE ]
> Stopping slon service: [ DONE ]
> [root at vpsXXXX /]# ps axuf |grep slon
> root 28419 0.0 0.0 6044 580 pts/0 S+ Jun08 0:00 \_ grep slon
> [root at vps6147 /]# free
> total used free shared buffers cached
> Mem: 10485760 99192 10386568 0 0 0
> -/+ buffers/cache: 99192 10386568
> Swap: 0 0 0
> [root at vpsXXX /]# /etc/init.d/slon start
> Starting slon service: [ DONE ]
> [root at vpsXXX /]# ps axuf |grep slon
> root 29737 0.0 0.0 6040 584 pts/0 S+ Jun08 0:00 \_ grep slon
> postgres 28555 0.0 0.0 40636 1832 pts/0 S Jun08 0:00 /usr/bin/slon
> postgres 28556 0.0 0.0 4042880 1508 pts/0 Sl Jun08 0:00 \_ /usr/bin/slon
> postgres 28592 0.0 0.0 40636 1836 pts/0 S Jun08 0:00 /usr/bin/slon
> postgres 28594 0.0 0.0 4042880 1512 pts/0 Sl Jun08 0:00 \_ /usr/bin/slon
> [root at vpsXXXX /]# free
> total used free shared buffers cached
> Mem: 10485760 8119136 2366624 0 0 0
> -/+ buffers/cache: 8119136 2366624
> Swap: 0 0 0
> [root at vpsXXXX /]#
>
> The different between "no slon process" and "2 pair slon process" ~8G, that
> we have a problem,
> because it`s not a shared segment.....

Oh whoa, I thought you were talking about the postgres backend that
slony connects to using up that much memory.

I wonder if there's some accounting difference in how your vps works
versus running right on the server.

> Besides OpenVZ divide shared memory and resident memory
>
> [root at vps6147 /]# cat /proc/user_beancounters |grep -E
> 'privvmpages|shmpages'
> privvmpages 2029830 2033439 2621440 2621440 5
> shmpages 17632 17632 412000 412000 0
> [root at vps6147 /]#
>
> first column - the current value in 4k pages, it`s indicates very small
> shared segment and huge resident segment,

Yeah, that's different from what I was thinking was going on.

> Do you have a expirience using slony1 on x86_64 servers ?

Quite a bit actually.

> We using slony1 replication about 3 year on i686 architecture and we hav`t

Is that a "have" or "haven't" ?

> similar problem....
>
> PS we using the same OpenVZ template for application servers, and
> probability error in template or in the current VPS is minimum.

I've never run dbs inside vms before (seems counter productive to me)

From scott.marlowe at gmail.com  Wed Jun  9 07:29:20 2010
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Wed, 9 Jun 2010 08:29:20 -0600
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <4C0F5B17.4000702@immo.ru>
References: <4C0E103D.1070404@immo.ru>
	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>
	<4C0F44C8.5070409@immo.ru>
	<AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>
	<4C0F5B17.4000702@immo.ru>
Message-ID: <AANLkTilUY1qA1CWxNyDQwo4vlq7yB-GLaoymjPbbJn3I@mail.gmail.com>

Oh, another question, is this memory usage immediate on startup, or
does it take time to build up?  If so, how long?  I wonder if the
slons are trying to output log info, and don't have anywhere to put it
and it's using up that space or something.  Just a guess.

From open at immo.ru  Wed Jun  9 22:39:13 2010
From: open at immo.ru (Alexander V Openkin)
Date: Thu, 10 Jun 2010 09:39:13 +0400
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <AANLkTimmqqrV5N-M9nVAx-ygfMrX-uBuGIFHYRyJuhLj@mail.gmail.com>
References: <4C0E103D.1070404@immo.ru>	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>	<4C0F44C8.5070409@immo.ru>	<AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>	<4C0F5B17.4000702@immo.ru>
	<AANLkTimmqqrV5N-M9nVAx-ygfMrX-uBuGIFHYRyJuhLj@mail.gmail.com>
Message-ID: <4C107A81.2040508@immo.ru>

09.06.2010 18:27, Scott Marlowe ?????:
> On Wed, Jun 9, 2010 at 3:12 AM, Alexander V Openkin<open at immo.ru>  wrote:
>    
>> 09.06.2010 11:54, Scott Marlowe ?????:
>>      
>>> On Wed, Jun 9, 2010 at 1:37 AM, Alexander V Openkin<open at immo.ru>    wrote:
>>>
>>>        
>>>> Tnx for the quick answer,
>>>> but if you run all slon process (for your replication cluster)
>>>> you must have standalone server with 32G ?
>>>> I think this is unnatural )
>>>>
>>>>          
>>> No, definitely not.  VIRT is everything the process has ever touched,
>>> including shared memory and all libs whether or not they've actually
>>> been loaded or not.  It's not uncommon to have literally a hundred
>>> processes with 8G+VIRT on my 32Gig db servers, because most of that 8G
>>> is shred buffers.  It's not using that much memory individually, it's
>>> using shared_memory, and each process reports that it has access to
>>> and has touched that 8G.
>>>
>>>
>>>        
>> small experement, the second instance for ather Db was added...
>>
>> [root at vpsXXXX /]# /etc/init.d/slon stop
>> Stopping slon service: [ DONE ]
>> Stopping slon service: [ DONE ]
>> [root at vpsXXXX /]# ps axuf |grep slon
>> root 28419 0.0 0.0 6044 580 pts/0 S+ Jun08 0:00 \_ grep slon
>> [root at vps6147 /]# free
>> total used free shared buffers cached
>> Mem: 10485760 99192 10386568 0 0 0
>> -/+ buffers/cache: 99192 10386568
>> Swap: 0 0 0
>> [root at vpsXXX /]# /etc/init.d/slon start
>> Starting slon service: [ DONE ]
>> [root at vpsXXX /]# ps axuf |grep slon
>> root 29737 0.0 0.0 6040 584 pts/0 S+ Jun08 0:00 \_ grep slon
>> postgres 28555 0.0 0.0 40636 1832 pts/0 S Jun08 0:00 /usr/bin/slon
>> postgres 28556 0.0 0.0 4042880 1508 pts/0 Sl Jun08 0:00 \_ /usr/bin/slon
>> postgres 28592 0.0 0.0 40636 1836 pts/0 S Jun08 0:00 /usr/bin/slon
>> postgres 28594 0.0 0.0 4042880 1512 pts/0 Sl Jun08 0:00 \_ /usr/bin/slon
>> [root at vpsXXXX /]# free
>> total used free shared buffers cached
>> Mem: 10485760 8119136 2366624 0 0 0
>> -/+ buffers/cache: 8119136 2366624
>> Swap: 0 0 0
>> [root at vpsXXXX /]#
>>
>> The different between "no slon process" and "2 pair slon process" ~8G, that
>> we have a problem,
>> because it`s not a shared segment.....
>>      
> Oh whoa, I thought you were talking about the postgres backend that
> slony connects to using up that much memory.
>    
no, we tolking about slon processes, not about postgres backend.

> I wonder if there's some accounting difference in how your vps works
> versus running right on the server.
>    

I have ~five replication cluster on slony1-1.2.14 and postgresql-8.3.9 
on i686 architecture and
we never see such problem...
I think that no differents between running slony cluster on hardware 
server or VPS

>    
>> Besides OpenVZ divide shared memory and resident memory
>>
>> [root at vps6147 /]# cat /proc/user_beancounters |grep -E
>> 'privvmpages|shmpages'
>> privvmpages 2029830 2033439 2621440 2621440 5
>> shmpages 17632 17632 412000 412000 0
>> [root at vps6147 /]#
>>
>> first column - the current value in 4k pages, it`s indicates very small
>> shared segment and huge resident segment,
>>      
> Yeah, that's different from what I was thinking was going on.
>
>    
>> Do you have a expirience using slony1 on x86_64 servers ?
>>      
> Quite a bit actually.
>
>    
>> We using slony1 replication about 3 year on i686 architecture and we hav`t
>>      
> Is that a "have" or "haven't" ?
>
>    

i mean haven't.
i have a 3 year expirience with slon replication and postgresql8.{0,1,2,3}
on i686 architecture and i have never seen it before

yesterday i read a news on slony.info "Slony-I 2.0.3 is not usable in its current state."



>> similar problem....
>>
>> PS we using the same OpenVZ template for application servers, and
>> probability error in template or in the current VPS is minimum.
>>      
> I've never run dbs inside vms before (seems counter productive to me)
>
>    
PS sorry for my awful english, i am russian )

From scott.marlowe at gmail.com  Wed Jun  9 23:07:31 2010
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Thu, 10 Jun 2010 00:07:31 -0600
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <4C107A81.2040508@immo.ru>
References: <4C0E103D.1070404@immo.ru>
	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>
	<4C0F44C8.5070409@immo.ru>
	<AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>
	<4C0F5B17.4000702@immo.ru>
	<AANLkTimmqqrV5N-M9nVAx-ygfMrX-uBuGIFHYRyJuhLj@mail.gmail.com>
	<4C107A81.2040508@immo.ru>
Message-ID: <AANLkTin0e8gSBrYqTqBUJ7jJHldbkTEN2cT8BkZ8FcY7@mail.gmail.com>

2010/6/9 Alexander V Openkin <open at immo.ru>:
> 09.06.2010 18:27, Scott Marlowe ?????:
>>
>> Oh whoa, I thought you were talking about the postgres backend that
>> slony connects to using up that much memory.
>>
>
> no, we tolking about slon processes, not about postgres backend.
>
>> I wonder if there's some accounting difference in how your vps works
>> versus running right on the server.
>>
>
> I have ~five replication cluster on slony1-1.2.14 and postgresql-8.3.9 on
> i686 architecture and
> we never see such problem...
> I think that no differents between running slony cluster on hardware server
> or VPS

Is this the same OS as on hardware?  The accounting seems all kinds of
wrong to me.  I just can't see slony asking for and getting 4G or 8G
of ram.

>>> Besides OpenVZ divide shared memory and resident memory
>>>
>>> [root at vps6147 /]# cat /proc/user_beancounters |grep -E
>>> 'privvmpages|shmpages'
>>> privvmpages 2029830 2033439 2621440 2621440 5
>>> shmpages 17632 17632 412000 412000 0
>>> [root at vps6147 /]#
>>>
>>> first column - the current value in 4k pages, it`s indicates very small
>>> shared segment and huge resident segment,
>>>
>>
>> Yeah, that's different from what I was thinking was going on.
>>
>>
>>>
>>> Do you have a expirience using slony1 on x86_64 servers ?
>>>
>> Quite a bit actually.
>>>
>>> We using slony1 replication about 3 year on i686 architecture and we
>>> hav`t
>>
>> Is that a "have" or "haven't" ?
>
> i mean haven't.
> i have a 3 year expirience with slon replication and postgresql8.{0,1,2,3}
> on i686 architecture and i have never seen it before

I have mostly experience on x86_64 / AMD64 hardware.  A little in the
past on 32 bit pentium, but that was slony 1.0 days.

> yesterday i read a news on slony.info "Slony-I 2.0.3 is not usable in its
> current state."

Correct.  Like 2.0.4 will be close.  I tried it last year and it blew
up twice.  Luckily switching out 1.2.latest for 2.0.x is pretty easily
done.

>>> similar problem....
>>>
>>> PS we using the same OpenVZ template for application servers, and
>>> probability error in template or in the current VPS is minimum.
>>
>> I've never run dbs inside vms before (seems counter productive to me)
>
> PS sorry for my awful english, i am russian )

Your English is much better than my Russian, no need to apologize.

Have you tried switching it out for slony 1.2.latest?    I'm thinking
it won't help this memory usage issue, but if you're in production you
should really be on 1.2.latest not 2.0.x.

From open at immo.ru  Wed Jun  9 23:32:03 2010
From: open at immo.ru (Alexander V Openkin)
Date: Thu, 10 Jun 2010 10:32:03 +0400
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <AANLkTilUY1qA1CWxNyDQwo4vlq7yB-GLaoymjPbbJn3I@mail.gmail.com>
References: <4C0E103D.1070404@immo.ru>	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>	<4C0F44C8.5070409@immo.ru>	<AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>	<4C0F5B17.4000702@immo.ru>
	<AANLkTilUY1qA1CWxNyDQwo4vlq7yB-GLaoymjPbbJn3I@mail.gmail.com>
Message-ID: <4C1086E3.70508@immo.ru>

09.06.2010 18:29, Scott Marlowe ?????:
> Oh, another question, is this memory usage immediate on startup, or
> does it take time to build up?  If so, how long?  I wonder if the
> slons are trying to output log info, and don't have anywhere to put it
> and it's using up that space or something.  Just a guess.
>
>    

Slon process starting up about 1 sec, i have a log_level=4 in config
We have a usual debug info in log...


Jun 10 09:58:27 vps6147 test_db[8040]: [1-1] 2010-06-10 09:58:27 
MSD[8040] CONFIG main: slon version 2.0.4 starting up
Jun 10 09:58:27 vps6147 test_db[8040]: [2-1] 2010-06-10 09:58:27 
MSD[8040] INFO slon: watchdog process started
Jun 10 09:58:27 vps6147 test_db[8040]: [3-1] 2010-06-10 09:58:27 
MSD[8040] CONFIG slon: watchdog ready - pid = 8040
Jun 10 09:58:27 vps6147 test_db[8041]: [4-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: Integer option vac_frequency = 3
Jun 10 09:58:27 vps6147 test_db[8041]: [5-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: Integer option log_level = 4
Jun 10 09:58:27 vps6147 test_db[8041]: [6-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: Integer option sync_interval = 1000
Jun 10 09:58:27 vps6147 test_db[8041]: [7-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: Integer option sync_interval_timeout = 60000
Jun 10 09:58:27 vps6147 test_db[8041]: [8-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: Integer option sync_group_maxsize = 20
Jun 10 09:58:27 vps6147 test_db[8041]: [9-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: Integer option desired_sync_time = 60000
Jun 10 09:58:27 vps6147 test_db[8040]: [4-1] 2010-06-10 09:58:27 
MSD[8040] CONFIG slon: worker process created - pid = 8041
Jun 10 09:58:27 vps6147 test_db[8041]: [10-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: Integer option syslog = 2
Jun 10 09:58:27 vps6147 test_db[8041]: [11-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: Integer option quit_sync_provider = 0
Jun 10 09:58:27 vps6147 test_db[8041]: [12-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: Integer option quit_sync_finalsync = 0
Jun 10 09:58:27 vps6147 test_db[8041]: [13-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: Integer option sync_max_rowsize = 8192
Jun 10 09:58:27 vps6147 test_db[8041]: [14-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: Integer option sync_max_largemem = 5242880
Jun 10 09:58:27 vps6147 test_db[8041]: [15-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: Integer option remote_listen_timeout = 300
Jun 10 09:58:27 vps6147 test_db[8041]: [16-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: Boolean option log_pid = 1
Jun 10 09:58:27 vps6147 test_db[8041]: [17-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: Boolean option log_timestamp = 1
Jun 10 09:58:27 vps6147 test_db[8041]: [18-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: Boolean option cleanup_deletelogs = 0
Jun 10 09:58:27 vps6147 test_db[8041]: [19-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: Real option real_placeholder = 0.000000
Jun 10 09:58:27 vps6147 test_db[8041]: [20-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: String option cluster_name = test_db
Jun 10 09:58:27 vps6147 test_db[8041]: [21-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: String option conn_info = host=localhost 
port=5432 user=postgres password=123
Jun 10 09:58:27 vps6147 test_db[8041]: [21-2] dbname=test_db
Jun 10 09:58:27 vps6147 test_db[8041]: [22-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: String option pid_file = /var/run/slony1/test_db.pid
Jun 10 09:58:27 vps6147 test_db[8041]: [23-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: String option log_timestamp_format = %Y-%m-%d 
%H:%M:%S %Z
Jun 10 09:58:27 vps6147 test_db[8041]: [24-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: String option archive_dir = [NULL]
Jun 10 09:58:27 vps6147 test_db[8041]: [25-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: String option sql_on_connection = [NULL]
Jun 10 09:58:27 vps6147 test_db[8041]: [26-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: String option lag_interval = [NULL]
Jun 10 09:58:27 vps6147 test_db[8041]: [27-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: String option command_on_logarchive = [NULL]
Jun 10 09:58:27 vps6147 test_db[8041]: [28-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: String option syslog_facility = LOCAL3
Jun 10 09:58:27 vps6147 test_db[8041]: [29-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: String option syslog_ident = test_db
Jun 10 09:58:27 vps6147 test_db[8041]: [30-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: String option cleanup_interval = 10 minutes
Jun 10 09:58:27 vps6147 test_db[8041]: [31-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: local node id = 1
Jun 10 09:58:27 vps6147 test_db[8041]: [32-1] 2010-06-10 09:58:27 
MSD[8041] INFO main: main process started
Jun 10 09:58:27 vps6147 test_db[8041]: [33-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: launching sched_start_mainloop
Jun 10 09:58:27 vps6147 test_db[8041]: [34-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: loading current cluster configuration
Jun 10 09:58:27 vps6147 test_db[8041]: [35-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG storeSet: set_id=1 set_origin=1 set_comment='Set 1 for 
test_db'
Jun 10 09:58:27 vps6147 test_db[8041]: [37-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: last local event sequence = 5000000023
Jun 10 09:58:27 vps6147 test_db[8041]: [38-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG main: configuration complete - starting threads
Jun 10 09:58:27 vps6147 test_db[8041]: [39-1] 2010-06-10 09:58:27 
MSD[8041] INFO localListenThread: thread starts
Jun 10 09:58:27 vps6147 test_db[8041]: [40-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG version for "host=localhost port=5432 user=postgres 
password=123
Jun 10 09:58:27 vps6147 test_db[8041]: [40-2] dbname=test_db" is 80403
Jun 10 09:58:27 vps6147 test_db[8041]: [42-1] 2010-06-10 09:58:27 
MSD[8041] INFO main: running scheduler mainloop
Jun 10 09:58:27 vps6147 test_db[8041]: [43-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG cleanupThread: thread starts
Jun 10 09:58:27 vps6147 test_db[8041]: [44-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG cleanupThread: bias = 14563
Jun 10 09:58:27 vps6147 test_db[8041]: [45-1] 2010-06-10 09:58:27 
MSD[8041] INFO syncThread: thread starts
Jun 10 09:58:27 vps6147 test_db[8041]: [46-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG version for "host=localhost port=5432 user=postgres 
password=123
Jun 10 09:58:27 vps6147 test_db[8041]: [46-2] dbname=test_db" is 80403
Jun 10 09:58:27 vps6147 test_db[8041]: [48-1] 2010-06-10 09:58:27 
MSD[8041] CONFIG version for "host=localhost port=5432 user=postgres 
password=123
Jun 10 09:58:27 vps6147 test_db[8041]: [48-2] dbname=test_db" is 80403
Jun 10 10:09:54 vps6147 test_db[8041]: [63-1] 2010-06-10 10:09:54 
MSD[8041] INFO cleanupThread: 0.046 seconds for cleanupEvent()
~

From open at immo.ru  Wed Jun  9 23:46:33 2010
From: open at immo.ru (Alexander V Openkin)
Date: Thu, 10 Jun 2010 10:46:33 +0400
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <AANLkTin0e8gSBrYqTqBUJ7jJHldbkTEN2cT8BkZ8FcY7@mail.gmail.com>
References: <4C0E103D.1070404@immo.ru>	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>	<4C0F44C8.5070409@immo.ru>	<AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>	<4C0F5B17.4000702@immo.ru>	<AANLkTimmqqrV5N-M9nVAx-ygfMrX-uBuGIFHYRyJuhLj@mail.gmail.com>	<4C107A81.2040508@immo.ru>
	<AANLkTin0e8gSBrYqTqBUJ7jJHldbkTEN2cT8BkZ8FcY7@mail.gmail.com>
Message-ID: <4C108A49.1060002@immo.ru>

10.06.2010 10:07, Scott Marlowe ?????:
> 2010/6/9 Alexander V Openkin<open at immo.ru>:
>    
>> 09.06.2010 18:27, Scott Marlowe ?????:
>>      
>>> Oh whoa, I thought you were talking about the postgres backend that
>>> slony connects to using up that much memory.
>>>
>>>        
>> no, we tolking about slon processes, not about postgres backend.
>>
>>      
>>> I wonder if there's some accounting difference in how your vps works
>>> versus running right on the server.
>>>
>>>        
>> I have ~five replication cluster on slony1-1.2.14 and postgresql-8.3.9 on
>> i686 architecture and
>> we never see such problem...
>> I think that no differents between running slony cluster on hardware server
>> or VPS
>>      
> Is this the same OS as on hardware?  The accounting seems all kinds of
> wrong to me.  I just can't see slony asking for and getting 4G or 8G
> of ram.
>
>    
The same linux kernel,
on OpenVZ hardware server, we can run different OS (different linux 
distributions),
but the kernel will be same.

i run ps auxf on hardware server

[root at vz19 ~]# ps auxf |grep slon |grep cms
postgres  6973  0.0  0.0  40636  1836 ?        S    09:58   0:00  \_ 
/usr/bin/slon -f /etc/slony1.d/blabla
postgres  6974  0.0  0.0 4108420 1544 ?        Sl   09:58   0:00  |   \_ 
/usr/bin/slon -f /etc/slony1.d/blabla
postgres  7016  0.0  0.0  40640  1836 ?        S    09:58   0:00  \_ 
/usr/bin/slon -f /etc/slony1.d/blabla2
postgres  7017  0.0  0.0 4108424 1544 ?        Sl   09:58   0:00      \_ 
/usr/bin/slon -f /etc/slony1.d/blabla2
[root at vz19 ~]#

the fifth colunm is a VSZ (in kb) it show us two 4G segments...

>>>> Besides OpenVZ divide shared memory and resident memory
>>>>
>>>> [root at vps6147 /]# cat /proc/user_beancounters |grep -E
>>>> 'privvmpages|shmpages'
>>>> privvmpages 2029830 2033439 2621440 2621440 5
>>>> shmpages 17632 17632 412000 412000 0
>>>> [root at vps6147 /]#
>>>>
>>>> first column - the current value in 4k pages, it`s indicates very small
>>>> shared segment and huge resident segment,
>>>>
>>>>          
>>> Yeah, that's different from what I was thinking was going on.
>>>
>>>
>>>        
>>>> Do you have a expirience using slony1 on x86_64 servers ?
>>>>
>>>>          
>>> Quite a bit actually.
>>>        
>>>> We using slony1 replication about 3 year on i686 architecture and we
>>>> hav`t
>>>>          
>>> Is that a "have" or "haven't" ?
>>>        
>> i mean haven't.
>> i have a 3 year expirience with slon replication and postgresql8.{0,1,2,3}
>> on i686 architecture and i have never seen it before
>>      
> I have mostly experience on x86_64 / AMD64 hardware.  A little in the
> past on 32 bit pentium, but that was slony 1.0 days.
>
>    
>> yesterday i read a news on slony.info "Slony-I 2.0.3 is not usable in its
>> current state."
>>      
> Correct.  Like 2.0.4 will be close.  I tried it last year and it blew
> up twice.  Luckily switching out 1.2.latest for 2.0.x is pretty easily
> done.
>
>    
>>>> similar problem....
>>>>
>>>> PS we using the same OpenVZ template for application servers, and
>>>> probability error in template or in the current VPS is minimum.
>>>>          
>>> I've never run dbs inside vms before (seems counter productive to me)
>>>        
>> PS sorry for my awful english, i am russian )
>>      
> Your English is much better than my Russian, no need to apologize.
>
> Have you tried switching it out for slony 1.2.latest?    I'm thinking
> it won't help this memory usage issue, but if you're in production you
> should really be on 1.2.latest not 2.0.x.
>
>    
I'll try 1.2.latest, and show result's



From scott.marlowe at gmail.com  Thu Jun 10 00:07:19 2010
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Thu, 10 Jun 2010 01:07:19 -0600
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <4C108A49.1060002@immo.ru>
References: <4C0E103D.1070404@immo.ru>
	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>
	<4C0F44C8.5070409@immo.ru>
	<AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>
	<4C0F5B17.4000702@immo.ru>
	<AANLkTimmqqrV5N-M9nVAx-ygfMrX-uBuGIFHYRyJuhLj@mail.gmail.com>
	<4C107A81.2040508@immo.ru>
	<AANLkTin0e8gSBrYqTqBUJ7jJHldbkTEN2cT8BkZ8FcY7@mail.gmail.com>
	<4C108A49.1060002@immo.ru>
Message-ID: <AANLkTimDtrbBs9SnZj6ASBCdGzg41CbHNtnb7J_uxOht@mail.gmail.com>

2010/6/10 Alexander V Openkin <open at immo.ru>:
> 10.06.2010 10:07, Scott Marlowe ?????:
>>
>> 2010/6/9 Alexander V Openkin<open at immo.ru>:
>>
>>>
>>> 09.06.2010 18:27, Scott Marlowe ?????:
>>>
>>>>
>>>> Oh whoa, I thought you were talking about the postgres backend that
>>>> slony connects to using up that much memory.
>>>>
>>>>
>>>
>>> no, we tolking about slon processes, not about postgres backend.
>>>
>>>
>>>>
>>>> I wonder if there's some accounting difference in how your vps works
>>>> versus running right on the server.
>>>>
>>>>
>>>
>>> I have ~five replication cluster on slony1-1.2.14 and postgresql-8.3.9 on
>>> i686 architecture and
>>> we never see such problem...
>>> I think that no differents between running slony cluster on hardware
>>> server
>>> or VPS
>>>
>>
>> Is this the same OS as on hardware? ?The accounting seems all kinds of
>> wrong to me. ?I just can't see slony asking for and getting 4G or 8G
>> of ram.
>>
>>
>
> The same linux kernel,
> on OpenVZ hardware server, we can run different OS (different linux
> distributions),
> but the kernel will be same.
>
> i run ps auxf on hardware server
>
> [root at vz19 ~]# ps auxf |grep slon |grep cms
> postgres ?6973 ?0.0 ?0.0 ?40636 ?1836 ? ? ? ? ?S ? ?09:58 ? 0:00 ?\_
> /usr/bin/slon -f /etc/slony1.d/blabla
> postgres ?6974 ?0.0 ?0.0 4108420 1544 ? ? ? ? ?Sl ? 09:58 ? 0:00 ?| ? \_
> /usr/bin/slon -f /etc/slony1.d/blabla
> postgres ?7016 ?0.0 ?0.0 ?40640 ?1836 ? ? ? ? ?S ? ?09:58 ? 0:00 ?\_
> /usr/bin/slon -f /etc/slony1.d/blabla2
> postgres ?7017 ?0.0 ?0.0 4108424 1544 ? ? ? ? ?Sl ? 09:58 ? 0:00 ? ? ?\_
> /usr/bin/slon -f /etc/slony1.d/blabla2
> [root at vz19 ~]#
>
> the fifth colunm is a VSZ (in kb) it show us two 4G segments...
>
>>>>> Besides OpenVZ divide shared memory and resident memory
>>>>>
>>>>> [root at vps6147 /]# cat /proc/user_beancounters |grep -E
>>>>> 'privvmpages|shmpages'
>>>>> privvmpages 2029830 2033439 2621440 2621440 5
>>>>> shmpages 17632 17632 412000 412000 0
>>>>> [root at vps6147 /]#
>>>>>
>>>>> first column - the current value in 4k pages, it`s indicates very small
>>>>> shared segment and huge resident segment,
>>>>>
>>>>>
>>>>
>>>> Yeah, that's different from what I was thinking was going on.
>>>>
>>>>
>>>>
>>>>>
>>>>> Do you have a expirience using slony1 on x86_64 servers ?
>>>>>
>>>>>
>>>>
>>>> Quite a bit actually.
>>>>
>>>>>
>>>>> We using slony1 replication about 3 year on i686 architecture and we
>>>>> hav`t
>>>>>
>>>>
>>>> Is that a "have" or "haven't" ?
>>>>
>>>
>>> i mean haven't.
>>> i have a 3 year expirience with slon replication and
>>> postgresql8.{0,1,2,3}
>>> on i686 architecture and i have never seen it before
>>>
>>
>> I have mostly experience on x86_64 / AMD64 hardware. ?A little in the
>> past on 32 bit pentium, but that was slony 1.0 days.
>>
>>
>>>
>>> yesterday i read a news on slony.info "Slony-I 2.0.3 is not usable in its
>>> current state."
>>>
>>
>> Correct. ?Like 2.0.4 will be close. ?I tried it last year and it blew
>> up twice. ?Luckily switching out 1.2.latest for 2.0.x is pretty easily
>> done.
>>
>>
>>>>>
>>>>> similar problem....
>>>>>
>>>>> PS we using the same OpenVZ template for application servers, and
>>>>> probability error in template or in the current VPS is minimum.
>>>>>
>>>>
>>>> I've never run dbs inside vms before (seems counter productive to me)
>>>>
>>>
>>> PS sorry for my awful english, i am russian )
>>>
>>
>> Your English is much better than my Russian, no need to apologize.
>>
>> Have you tried switching it out for slony 1.2.latest? ? ?I'm thinking
>> it won't help this memory usage issue, but if you're in production you
>> should really be on 1.2.latest not 2.0.x.
>>
>>
>
> I'll try 1.2.latest, and show result's

Just wondering do you have any strange things about your setup, like
10000 tables in replication or 50,000 schemas in your db or something
like that?  I just keep wondering if you're running into some strange,
out of the ordinary corner case.

From open at immo.ru  Thu Jun 10 00:54:29 2010
From: open at immo.ru (Alexander V Openkin)
Date: Thu, 10 Jun 2010 11:54:29 +0400
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <AANLkTimDtrbBs9SnZj6ASBCdGzg41CbHNtnb7J_uxOht@mail.gmail.com>
References: <4C0E103D.1070404@immo.ru>	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>	<4C0F44C8.5070409@immo.ru>	<AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>	<4C0F5B17.4000702@immo.ru>	<AANLkTimmqqrV5N-M9nVAx-ygfMrX-uBuGIFHYRyJuhLj@mail.gmail.com>	<4C107A81.2040508@immo.ru>	<AANLkTin0e8gSBrYqTqBUJ7jJHldbkTEN2cT8BkZ8FcY7@mail.gmail.com>	<4C108A49.1060002@immo.ru>
	<AANLkTimDtrbBs9SnZj6ASBCdGzg41CbHNtnb7J_uxOht@mail.gmail.com>
Message-ID: <4C109A35.6010000@immo.ru>

10.06.2010 11:07, Scott Marlowe ?????:
> 2010/6/10 Alexander V Openkin<open at immo.ru>:
>    
>> 10.06.2010 10:07, Scott Marlowe ?????:
>>      
>>> 2010/6/9 Alexander V Openkin<open at immo.ru>:
>>>
>>>        
>>>> 09.06.2010 18:27, Scott Marlowe ?????:
>>>>
>>>>          
>>>>> Oh whoa, I thought you were talking about the postgres backend that
>>>>> slony connects to using up that much memory.
>>>>>
>>>>>
>>>>>            
>>>> no, we tolking about slon processes, not about postgres backend.
>>>>
>>>>
>>>>          
>>>>> I wonder if there's some accounting difference in how your vps works
>>>>> versus running right on the server.
>>>>>
>>>>>
>>>>>            
>>>> I have ~five replication cluster on slony1-1.2.14 and postgresql-8.3.9 on
>>>> i686 architecture and
>>>> we never see such problem...
>>>> I think that no differents between running slony cluster on hardware
>>>> server
>>>> or VPS
>>>>
>>>>          
>>> Is this the same OS as on hardware?  The accounting seems all kinds of
>>> wrong to me.  I just can't see slony asking for and getting 4G or 8G
>>> of ram.
>>>
>>>
>>>        
>> The same linux kernel,
>> on OpenVZ hardware server, we can run different OS (different linux
>> distributions),
>> but the kernel will be same.
>>
>> i run ps auxf on hardware server
>>
>> [root at vz19 ~]# ps auxf |grep slon |grep cms
>> postgres  6973  0.0  0.0  40636  1836 ?        S    09:58   0:00  \_
>> /usr/bin/slon -f /etc/slony1.d/blabla
>> postgres  6974  0.0  0.0 4108420 1544 ?        Sl   09:58   0:00  |   \_
>> /usr/bin/slon -f /etc/slony1.d/blabla
>> postgres  7016  0.0  0.0  40640  1836 ?        S    09:58   0:00  \_
>> /usr/bin/slon -f /etc/slony1.d/blabla2
>> postgres  7017  0.0  0.0 4108424 1544 ?        Sl   09:58   0:00      \_
>> /usr/bin/slon -f /etc/slony1.d/blabla2
>> [root at vz19 ~]#
>>
>> the fifth colunm is a VSZ (in kb) it show us two 4G segments...
>>
>>      
>>>>>> Besides OpenVZ divide shared memory and resident memory
>>>>>>
>>>>>> [root at vps6147 /]# cat /proc/user_beancounters |grep -E
>>>>>> 'privvmpages|shmpages'
>>>>>> privvmpages 2029830 2033439 2621440 2621440 5
>>>>>> shmpages 17632 17632 412000 412000 0
>>>>>> [root at vps6147 /]#
>>>>>>
>>>>>> first column - the current value in 4k pages, it`s indicates very small
>>>>>> shared segment and huge resident segment,
>>>>>>
>>>>>>
>>>>>>              
>>>>> Yeah, that's different from what I was thinking was going on.
>>>>>
>>>>>
>>>>>
>>>>>            
>>>>>> Do you have a expirience using slony1 on x86_64 servers ?
>>>>>>
>>>>>>
>>>>>>              
>>>>> Quite a bit actually.
>>>>>
>>>>>            
>>>>>> We using slony1 replication about 3 year on i686 architecture and we
>>>>>> hav`t
>>>>>>
>>>>>>              
>>>>> Is that a "have" or "haven't" ?
>>>>>
>>>>>            
>>>> i mean haven't.
>>>> i have a 3 year expirience with slon replication and
>>>> postgresql8.{0,1,2,3}
>>>> on i686 architecture and i have never seen it before
>>>>
>>>>          
>>> I have mostly experience on x86_64 / AMD64 hardware.  A little in the
>>> past on 32 bit pentium, but that was slony 1.0 days.
>>>
>>>
>>>        
>>>> yesterday i read a news on slony.info "Slony-I 2.0.3 is not usable in its
>>>> current state."
>>>>
>>>>          
>>> Correct.  Like 2.0.4 will be close.  I tried it last year and it blew
>>> up twice.  Luckily switching out 1.2.latest for 2.0.x is pretty easily
>>> done.
>>>
>>>
>>>        
>>>>>> similar problem....
>>>>>>
>>>>>> PS we using the same OpenVZ template for application servers, and
>>>>>> probability error in template or in the current VPS is minimum.
>>>>>>
>>>>>>              
>>>>> I've never run dbs inside vms before (seems counter productive to me)
>>>>>
>>>>>            
>>>> PS sorry for my awful english, i am russian )
>>>>
>>>>          
>>> Your English is much better than my Russian, no need to apologize.
>>>
>>> Have you tried switching it out for slony 1.2.latest?    I'm thinking
>>> it won't help this memory usage issue, but if you're in production you
>>> should really be on 1.2.latest not 2.0.x.
>>>
>>>
>>>        
>> I'll try 1.2.latest, and show result's
>>      
> Just wondering do you have any strange things about your setup, like
> 10000 tables in replication or 50,000 schemas in your db or something
> like that?  I just keep wondering if you're running into some strange,
> out of the ordinary corner case.
>
>    
no, one shema, ~10 tables

From open at immo.ru  Thu Jun 10 02:00:50 2010
From: open at immo.ru (Alexander V Openkin)
Date: Thu, 10 Jun 2010 13:00:50 +0400
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <4C1086E3.70508@immo.ru>
References: <4C0E103D.1070404@immo.ru>	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>	<4C0F44C8.5070409@immo.ru>	<AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>	<4C0F5B17.4000702@immo.ru>	<AANLkTilUY1qA1CWxNyDQwo4vlq7yB-GLaoymjPbbJn3I@mail.gmail.com>
	<4C1086E3.70508@immo.ru>
Message-ID: <4C10A9C2.6000203@immo.ru>

i build slony1-1.2.21 RPM for the  i686 architecture with 
postgresql8.4-server-8.4.4,
create test vps with the same linux distributive (but i686 arch..)

It's amazing but, the same test db...

[root at vps999 /]# ps auxf |grep slon
root     15397  0.0  0.0   1880   524 pts/0    S+   07:58   0:00      \_ 
grep slon
postgres 26415  0.0  0.0   5304  1376 pts/0    S    07:50   0:00 
/usr/bin/slon
postgres 26417  0.0  0.0  46448  1296 pts/0    Sl   07:50   0:00  \_ 
/usr/bin/slon
[root at vps999 /]#

VMZ 46mb....  it's wonderful


another experiment...

slony1-1.2.21 with postgresql8.4-server-8.4.4 on the x86_64 vps,
the same linux distributive, the same test Db, 1 schema about 10 tables,
the same slon config

[root at vps9999 /]# ps auxf |grep slon
root      9912  0.0  0.0   6040   584 pts/0    S+   11:41   0:00      \_ 
grep slon
postgres  7893  0.0  0.0  40640  1840 ?        S    11:39   0:00 
/usr/bin/slon
postgres  7897  0.0  0.0 4108420 1500 ?        Sl   11:39   0:00  \_ 
/usr/bin/slon
[root at vps9999 /]#

The same huge VMZ segment 4G...
i have ho idea where the bug...




10.06.2010 10:32, Alexander V Openkin ?????:
> 09.06.2010 18:29, Scott Marlowe ?????:
>    
>> Oh, another question, is this memory usage immediate on startup, or
>> does it take time to build up?  If so, how long?  I wonder if the
>> slons are trying to output log info, and don't have anywhere to put it
>> and it's using up that space or something.  Just a guess.
>>
>>
>>      
> Slon process starting up about 1 sec, i have a log_level=4 in config
> We have a usual debug info in log...
>
>
> Jun 10 09:58:27 vps6147 test_db[8040]: [1-1] 2010-06-10 09:58:27
> MSD[8040] CONFIG main: slon version 2.0.4 starting up
> Jun 10 09:58:27 vps6147 test_db[8040]: [2-1] 2010-06-10 09:58:27
> MSD[8040] INFO slon: watchdog process started
> Jun 10 09:58:27 vps6147 test_db[8040]: [3-1] 2010-06-10 09:58:27
> MSD[8040] CONFIG slon: watchdog ready - pid = 8040
> Jun 10 09:58:27 vps6147 test_db[8041]: [4-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: Integer option vac_frequency = 3
> Jun 10 09:58:27 vps6147 test_db[8041]: [5-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: Integer option log_level = 4
> Jun 10 09:58:27 vps6147 test_db[8041]: [6-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: Integer option sync_interval = 1000
> Jun 10 09:58:27 vps6147 test_db[8041]: [7-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: Integer option sync_interval_timeout = 60000
> Jun 10 09:58:27 vps6147 test_db[8041]: [8-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: Integer option sync_group_maxsize = 20
> Jun 10 09:58:27 vps6147 test_db[8041]: [9-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: Integer option desired_sync_time = 60000
> Jun 10 09:58:27 vps6147 test_db[8040]: [4-1] 2010-06-10 09:58:27
> MSD[8040] CONFIG slon: worker process created - pid = 8041
> Jun 10 09:58:27 vps6147 test_db[8041]: [10-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: Integer option syslog = 2
> Jun 10 09:58:27 vps6147 test_db[8041]: [11-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: Integer option quit_sync_provider = 0
> Jun 10 09:58:27 vps6147 test_db[8041]: [12-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: Integer option quit_sync_finalsync = 0
> Jun 10 09:58:27 vps6147 test_db[8041]: [13-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: Integer option sync_max_rowsize = 8192
> Jun 10 09:58:27 vps6147 test_db[8041]: [14-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: Integer option sync_max_largemem = 5242880
> Jun 10 09:58:27 vps6147 test_db[8041]: [15-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: Integer option remote_listen_timeout = 300
> Jun 10 09:58:27 vps6147 test_db[8041]: [16-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: Boolean option log_pid = 1
> Jun 10 09:58:27 vps6147 test_db[8041]: [17-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: Boolean option log_timestamp = 1
> Jun 10 09:58:27 vps6147 test_db[8041]: [18-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: Boolean option cleanup_deletelogs = 0
> Jun 10 09:58:27 vps6147 test_db[8041]: [19-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: Real option real_placeholder = 0.000000
> Jun 10 09:58:27 vps6147 test_db[8041]: [20-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: String option cluster_name = test_db
> Jun 10 09:58:27 vps6147 test_db[8041]: [21-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: String option conn_info = host=localhost
> port=5432 user=postgres password=123
> Jun 10 09:58:27 vps6147 test_db[8041]: [21-2] dbname=test_db
> Jun 10 09:58:27 vps6147 test_db[8041]: [22-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: String option pid_file = /var/run/slony1/test_db.pid
> Jun 10 09:58:27 vps6147 test_db[8041]: [23-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: String option log_timestamp_format = %Y-%m-%d
> %H:%M:%S %Z
> Jun 10 09:58:27 vps6147 test_db[8041]: [24-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: String option archive_dir = [NULL]
> Jun 10 09:58:27 vps6147 test_db[8041]: [25-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: String option sql_on_connection = [NULL]
> Jun 10 09:58:27 vps6147 test_db[8041]: [26-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: String option lag_interval = [NULL]
> Jun 10 09:58:27 vps6147 test_db[8041]: [27-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: String option command_on_logarchive = [NULL]
> Jun 10 09:58:27 vps6147 test_db[8041]: [28-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: String option syslog_facility = LOCAL3
> Jun 10 09:58:27 vps6147 test_db[8041]: [29-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: String option syslog_ident = test_db
> Jun 10 09:58:27 vps6147 test_db[8041]: [30-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: String option cleanup_interval = 10 minutes
> Jun 10 09:58:27 vps6147 test_db[8041]: [31-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: local node id = 1
> Jun 10 09:58:27 vps6147 test_db[8041]: [32-1] 2010-06-10 09:58:27
> MSD[8041] INFO main: main process started
> Jun 10 09:58:27 vps6147 test_db[8041]: [33-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: launching sched_start_mainloop
> Jun 10 09:58:27 vps6147 test_db[8041]: [34-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: loading current cluster configuration
> Jun 10 09:58:27 vps6147 test_db[8041]: [35-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG storeSet: set_id=1 set_origin=1 set_comment='Set 1 for
> test_db'
> Jun 10 09:58:27 vps6147 test_db[8041]: [37-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: last local event sequence = 5000000023
> Jun 10 09:58:27 vps6147 test_db[8041]: [38-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG main: configuration complete - starting threads
> Jun 10 09:58:27 vps6147 test_db[8041]: [39-1] 2010-06-10 09:58:27
> MSD[8041] INFO localListenThread: thread starts
> Jun 10 09:58:27 vps6147 test_db[8041]: [40-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG version for "host=localhost port=5432 user=postgres
> password=123
> Jun 10 09:58:27 vps6147 test_db[8041]: [40-2] dbname=test_db" is 80403
> Jun 10 09:58:27 vps6147 test_db[8041]: [42-1] 2010-06-10 09:58:27
> MSD[8041] INFO main: running scheduler mainloop
> Jun 10 09:58:27 vps6147 test_db[8041]: [43-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG cleanupThread: thread starts
> Jun 10 09:58:27 vps6147 test_db[8041]: [44-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG cleanupThread: bias = 14563
> Jun 10 09:58:27 vps6147 test_db[8041]: [45-1] 2010-06-10 09:58:27
> MSD[8041] INFO syncThread: thread starts
> Jun 10 09:58:27 vps6147 test_db[8041]: [46-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG version for "host=localhost port=5432 user=postgres
> password=123
> Jun 10 09:58:27 vps6147 test_db[8041]: [46-2] dbname=test_db" is 80403
> Jun 10 09:58:27 vps6147 test_db[8041]: [48-1] 2010-06-10 09:58:27
> MSD[8041] CONFIG version for "host=localhost port=5432 user=postgres
> password=123
> Jun 10 09:58:27 vps6147 test_db[8041]: [48-2] dbname=test_db" is 80403
> Jun 10 10:09:54 vps6147 test_db[8041]: [63-1] 2010-06-10 10:09:54
> MSD[8041] INFO cleanupThread: 0.046 seconds for cleanupEvent()
> ~
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>    


From stuart at stuartbishop.net  Thu Jun 10 03:50:10 2010
From: stuart at stuartbishop.net (Stuart Bishop)
Date: Thu, 10 Jun 2010 17:50:10 +0700
Subject: [Slony1-general] child terminated status: 11 -> restart of
	worker in 10 seconds
In-Reply-To: <4C0906B9.5080907@tactel.se>
References: <4BFBB25B.5010709@tactel.se> <4BFBD55F.9000200@ca.afilias.info>
	<4C090127.5030702@tactel.se> <4C090437.30205@ca.afilias.info>
	<4C0906B9.5080907@tactel.se>
Message-ID: <AANLkTik74YxYGc3YGB53dheChDIRgwsDwlDsKEpTSXhQ@mail.gmail.com>

On Fri, Jun 4, 2010 at 8:59 PM, Alexander Kolodziej
<alexander.kolodziej at tactel.se> wrote:
> Thanks!
>
> I just restarted with log level 1 and actually, so far, it seems ok:)
> Will see if perhaps building a .deb pkg with 1.2.21 is an option.

I'm migrating soon too, and have packages for 1.2.21 for both 8.3 and 8.4:

For hardy (8.04): https://launchpad.net/~maxb/+archive/launchpad
For lucid (10.04): https://launchpad.net/~launchpad/+archive/ppa

I've only tested the lucid packages so far - I'll need to test the
hardy packages on our staging server soon.


-- 
Stuart Bishop <stuart at stuartbishop.net>
http://www.stuartbishop.net/

From ssinger at ca.afilias.info  Thu Jun 10 05:59:49 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 10 Jun 2010 08:59:49 -0400
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <4C10A9C2.6000203@immo.ru>
References: <4C0E103D.1070404@immo.ru>	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>	<4C0F44C8.5070409@immo.ru>	<AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>	<4C0F5B17.4000702@immo.ru>	<AANLkTilUY1qA1CWxNyDQwo4vlq7yB-GLaoymjPbbJn3I@mail.gmail.com>	<4C1086E3.70508@immo.ru>
	<4C10A9C2.6000203@immo.ru>
Message-ID: <4C10E1C5.4030205@ca.afilias.info>

Alexander V Openkin wrote:
> i build slony1-1.2.21 RPM for the  i686 architecture with 
> postgresql8.4-server-8.4.4,
> create test vps with the same linux distributive (but i686 arch..)
> 
> It's amazing but, the same test db...
> 
> [root at vps999 /]# ps auxf |grep slon
> root     15397  0.0  0.0   1880   524 pts/0    S+   07:58   0:00      \_ 
> grep slon
> postgres 26415  0.0  0.0   5304  1376 pts/0    S    07:50   0:00 
> /usr/bin/slon
> postgres 26417  0.0  0.0  46448  1296 pts/0    Sl   07:50   0:00  \_ 
> /usr/bin/slon
> [root at vps999 /]#
> 
> VMZ 46mb....  it's wonderful
> 
> 
> another experiment...
> 
> slony1-1.2.21 with postgresql8.4-server-8.4.4 on the x86_64 vps,
> the same linux distributive, the same test Db, 1 schema about 10 tables,
> the same slon config
> 
> [root at vps9999 /]# ps auxf |grep slon
> root      9912  0.0  0.0   6040   584 pts/0    S+   11:41   0:00      \_ 
> grep slon
> postgres  7893  0.0  0.0  40640  1840 ?        S    11:39   0:00 
> /usr/bin/slon
> postgres  7897  0.0  0.0 4108420 1500 ?        Sl   11:39   0:00  \_ 
> /usr/bin/slon
> [root at vps9999 /]#
> 
> The same huge VMZ segment 4G...
> i have ho idea where the bug...
>

Can you do the same test on a x86_64 server that isn't vps?

On my x86_64 laptop with my test cluster of a non-optimized build (after 
slon has been running for a bit) I see (with 2.0.4)

ssinger.local at ssinger:~/src/clustertest/clustertest$ ps auxf |grep bin/slon
994     15851  0.0  0.0   7336   868 pts/0    S+   08:54   0:00  |   \_ 
grep bin/slon
9994      7845  0.0  0.0  10700   144 pts/0    S    Jun09   0:00 
/usr/local/pgsql84/bin/slon -d4 disorder_replica dbname=test2 
host=localhost user=slony password=slony
9994      9023  0.0  0.0 196016  1452 pts/0    Sl   Jun09   0:03  \_ 
/usr/local/pgsql84/bin/slon -d4 disorder_replica dbname=test2 
host=localhost user=slony password=slony

x86_64 will tend to give a bit bigger memory footprint due to the larger 
pointer size.  I am wondering if the 4GB your seeing is somehow vps related.





> 
> 
> 
> 10.06.2010 10:32, Alexander V Openkin ?????:
-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From open at immo.ru  Thu Jun 10 06:38:56 2010
From: open at immo.ru (Alexander V Openkin)
Date: Thu, 10 Jun 2010 17:38:56 +0400
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <4C10E1C5.4030205@ca.afilias.info>
References: <4C0E103D.1070404@immo.ru>	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>	<4C0F44C8.5070409@immo.ru>	<AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>	<4C0F5B17.4000702@immo.ru>	<AANLkTilUY1qA1CWxNyDQwo4vlq7yB-GLaoymjPbbJn3I@mail.gmail.com>	<4C1086E3.70508@immo.ru>
	<4C10A9C2.6000203@immo.ru> <4C10E1C5.4030205@ca.afilias.info>
Message-ID: <4C10EAF0.4060306@immo.ru>

An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100610/eca956b1/attachment.html 

From ssinger at ca.afilias.info  Thu Jun 10 06:40:05 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 10 Jun 2010 09:40:05 -0400
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <4C10EAF0.4060306@immo.ru>
References: <4C0E103D.1070404@immo.ru>	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>	<4C0F44C8.5070409@immo.ru>	<AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>	<4C0F5B17.4000702@immo.ru>	<AANLkTilUY1qA1CWxNyDQwo4vlq7yB-GLaoymjPbbJn3I@mail.gmail.com>	<4C1086E3.70508@immo.ru>
	<4C10A9C2.6000203@immo.ru> <4C10E1C5.4030205@ca.afilias.info>
	<4C10EAF0.4060306@immo.ru>
Message-ID: <4C10EB35.3060108@ca.afilias.info>

Alexander V Openkin wrote:

>>>
>>
>> Can you do the same test on a x86_64 server that isn't vps?
> yes, i'll test it tomorrow,
> show me please
> 
> *uname -a *
> 
> from your laptop
> 
>>

uname -a
Linux ssinger 2.6.31-19-generic #56-Ubuntu SMP Thu Jan 28 02:39:34 UTC 
2010 x86_64 GNU/Linux


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From open at immo.ru  Thu Jun 10 06:52:50 2010
From: open at immo.ru (Alexander V Openkin)
Date: Thu, 10 Jun 2010 17:52:50 +0400
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <4C10EB35.3060108@ca.afilias.info>
References: <4C0E103D.1070404@immo.ru>	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>	<4C0F44C8.5070409@immo.ru>	<AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>	<4C0F5B17.4000702@immo.ru>	<AANLkTilUY1qA1CWxNyDQwo4vlq7yB-GLaoymjPbbJn3I@mail.gmail.com>	<4C1086E3.70508@immo.ru>
	<4C10A9C2.6000203@immo.ru> <4C10E1C5.4030205@ca.afilias.info>
	<4C10EAF0.4060306@immo.ru> <4C10EB35.3060108@ca.afilias.info>
Message-ID: <4C10EE32.8090806@immo.ru>

ldd /usr/local/pgsql84/bin/slon
??

Do you compile slony1 "Slackware" method ? (./configure; make; make install)
show me the ./configure keys please

10.06.2010 17:40, Steve Singer ?????:
> Alexander V Openkin wrote:
>
>>>>
>>>
>>> Can you do the same test on a x86_64 server that isn't vps?
>> yes, i'll test it tomorrow,
>> show me please
>>
>> *uname -a *
>>
>> from your laptop
>>
>>>
>
> uname -a
> Linux ssinger 2.6.31-19-generic #56-Ubuntu SMP Thu Jan 28 02:39:34 UTC 
> 2010 x86_64 GNU/Linux
>
>


From ssinger at ca.afilias.info  Thu Jun 10 06:56:11 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 10 Jun 2010 09:56:11 -0400
Subject: [Slony1-general] huge virtual memory size of slony1 proccess
In-Reply-To: <4C10EE32.8090806@immo.ru>
References: <4C0E103D.1070404@immo.ru>	<AANLkTimuZkKAHSx7R5kmztDDQitxFMW0jA7lPaG8sRmK@mail.gmail.com>	<4C0F44C8.5070409@immo.ru>	<AANLkTim8uwKOTsMArpvpvuieB9IKGUXeHw8eLFamuNaZ@mail.gmail.com>	<4C0F5B17.4000702@immo.ru>	<AANLkTilUY1qA1CWxNyDQwo4vlq7yB-GLaoymjPbbJn3I@mail.gmail.com>	<4C1086E3.70508@immo.ru>
	<4C10A9C2.6000203@immo.ru> <4C10E1C5.4030205@ca.afilias.info>
	<4C10EAF0.4060306@immo.ru> <4C10EB35.3060108@ca.afilias.info>
	<4C10EE32.8090806@immo.ru>
Message-ID: <4C10EEFB.5040107@ca.afilias.info>

Alexander V Openkin wrote:
> ldd /usr/local/pgsql84/bin/slon
> ??
> 

ssinger.local at ssinger:~/src/slony_git/src$ ldd /usr/local/pgsql84/bin/slon
	linux-vdso.so.1 =>  (0x00007fff733ff000)
	libpq.so.5 => /usr/local/pgsql84/lib/libpq.so.5 (0x00007f0c65488000)
	libpthread.so.0 => /lib/libpthread.so.0 (0x00007f0c6526c000)
	libc.so.6 => /lib/libc.so.6 (0x00007f0c64efd000)
	libcrypt.so.1 => /lib/libcrypt.so.1 (0x00007f0c64cc4000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f0c656b1000)


> Do you compile slony1 "Slackware" method ? (./configure; make; make 
> install)
> show me the ./configure keys please
> 

Yes.


ssinger.local at ssinger:~/src/slony_git$ ./configure 
--with-pgconfigdir=/usr/local/pgsql84/bin/
checking build system type... x86_64-unknown-linux-gnu
checking host system type... x86_64-unknown-linux-gnu
checking which template to use... linux
configure: using CFLAGS=
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ISO C89... none needed
checking for ld used by GCC... /usr/bin/ld
checking if the linker (/usr/bin/ld) is GNU ld... yes
checking for perl... /usr/bin/perl
checking for tar... /bin/tar
checking for bison... bison -y
checking for sed... sed
checking for the pthreads library -lpthreads... no
checking whether pthreads work without any flags... no
checking whether pthreads work with -Kthread... no
checking whether pthreads work with -kthread... no
checking for the pthreads library -llthread... no
checking whether pthreads work with -pthread... yes
checking for joinable pthread attribute... PTHREAD_CREATE_JOINABLE
checking if more special flags are required for pthreads... no
checking for cc_r... gcc
checking how to run the C preprocessor... gcc -E
checking for grep that handles long lines and -e... /bin/grep
checking for egrep... /bin/grep -E
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking fcntl.h usability... yes
checking fcntl.h presence... yes
checking for fcntl.h... yes
checking limits.h usability... yes
checking limits.h presence... yes
checking for limits.h... yes
checking stddef.h usability... yes
checking stddef.h presence... yes
checking for stddef.h... yes
checking sys/socket.h usability... yes
checking sys/socket.h presence... yes
checking for sys/socket.h... yes
checking sys/time.h usability... yes
checking sys/time.h presence... yes
checking for sys/time.h... yes
checking for inttypes.h... (cached) yes
checking for gettimeofday... yes
checking for dup2... yes
checking for alarm... yes
checking for memset... yes
checking for select... yes
checking for strdup... yes
checking for strerror... yes
checking for strtol... yes
checking for strtoul... yes
checking for int32_t... yes
checking for uint32_t... yes
checking for u_int32_t... yes
checking for int64_t... yes
checking for uint64_t... yes
checking for u_int64_t... yes
checking for size_t... yes
checking for ssize_t... yes
checking for POSIX signal interface... yes
checking for flex... /usr/bin/flex
configure: using
checking if you have requested slony1-engine building... yes
checking for pg_config... /usr/local/pgsql84/bin//pg_config
pg_config says pg_bindir is /usr/local/pgsql84/bin/
pg_config says pg_libdir is /usr/local/pgsql84/lib/
pg_config says pg_includedir is /usr/local/pgsql84/include/
pg_config says pg_pkglibdir is /usr/local/pgsql84/lib/
pg_config says pg_includeserverdir is /usr/local/pgsql84/include/server/
checking for correct version of PostgreSQL... 8.4
8.4
pg_config says pg_sharedir is /usr/local/pgsql84/share/
checking for PQunescapeBytea in -lpq... yes
checking libpq-fe.h usability... yes
checking libpq-fe.h presence... yes
checking for libpq-fe.h... yes
checking postgres.h usability... yes
checking postgres.h presence... yes
checking for postgres.h... yes
checking for utils/typcache.h... yes
checking for plpgsql.so... yes
checking for postgresql.conf.sample... skipped due to override
checking for PQputCopyData in -lpq... yes
checking for PQsetNoticeReceiver in -lpq... yes
checking for PQfreemem in -lpq... yes
checking for ScanKeywordLookup... yes, and it takes  arguments
checking for typenameTypeId... checking for typenameTypeId... yes, and 
it takes 3 arguments
checking for GetActiveSnapshot... yes
checking for ScanKeywordLookup... no
checking for standard_conforming_strings... yes
checking whether GetTopTransactionId is declared... yes
checking if you have requested documentation building... no
configure: creating ./config.status
config.status: creating Makefile.global
config.status: WARNING:  'Makefile.global.in' seems to ignore the 
--datarootdir setting
config.status: creating GNUmakefile
config.status: creating slony1.spec
config.status: creating Makefile.port
config.status: creating config.h



> 10.06.2010 17:40, Steve Singer ?????:
>> Alexander V Openkin wrote:
>>
>>>>>
>>>>
>>>> Can you do the same test on a x86_64 server that isn't vps?
>>> yes, i'll test it tomorrow,
>>> show me please
>>>
>>> *uname -a *
>>>
>>> from your laptop
>>>
>>>>
>>
>> uname -a
>> Linux ssinger 2.6.31-19-generic #56-Ubuntu SMP Thu Jan 28 02:39:34 UTC 
>> 2010 x86_64 GNU/Linux
>>
>>
> 


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From guillaume at lelarge.info  Fri Jun 11 10:39:07 2010
From: guillaume at lelarge.info (Guillaume Lelarge)
Date: Fri, 11 Jun 2010 19:39:07 +0200
Subject: [Slony1-general] Having a version.rss would be great
In-Reply-To: <4BFFD39E.1070806@lelarge.info>
References: <4BFE70F7.6050500@lelarge.info>
	<4BFE884F.1020803@ca.afilias.info>	<4BFE9013.30707@ca.afilias.info>
	<4BFE909E.1080703@lelarge.info>	<4BFFCBAE.7030104@ca.afilias.info>
	<4BFFCD94.9080208@lelarge.info>	<4BFFCEAB.2040708@ca.afilias.info>
	<4BFFD39E.1070806@lelarge.info>
Message-ID: <4C1274BB.10706@lelarge.info>

Le 28/05/2010 16:30, Guillaume Lelarge a ?crit :
> Le 28/05/2010 16:09, Steve Singer a ?crit :
>>
>>>
>>> I would be interested in working on this, but I couldn't find the
>>> Makefile Christopher told us about.
>>>
>>>
>>
>> The current makefile is literally
>>
>> ------------------
>> all:
>>         md5sum * > MD5SUMS
>>
>> ---------------
>>
>> and sits inside of the downloads/2.0/source  and downloads/1.2/source
>> directories on the webserver.  It does not appear to be in cvs (but
>> probably should be in the slony1-www project).
>>
> 
> OK, working on it.
> 

Got something, but not really something to be proud of. See attached files.

rssall.sh should be installed in the downloads directory. Makefile and
rss.sh should be in downloads/1.2/source and downloads/2.0/source
directories. If you execute make in one of the source directory, you
should get a versions.rss file in the downloads directory. I also
attached a copy of it.

There are two big issues:

  * beta and rc releases will be published in the versions.rss file,
    and we don't want that (we only want final releases).
  * pubDate is hard to get in the good format. As a matter of fact, I
    have no idea how to get it on the right format.

Moreover, I don't find my solution elegant to say the least :-/

I think a better idea would be to simply echo "Remember to update the
versions.rss file, please." when one executes the makefile. It's the
simplest idea, something that does not ask for a lot of work to build
it, and to actually do it for each release.

Comments?


-- 
Guillaume
 http://www.postgresql.fr
 http://dalibo.com
-------------- next part --------------
A non-text attachment was scrubbed...
Name: rssall.sh
Type: application/x-sh
Size: 283 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20100611/48dd0375/attachment.sh 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Makefile
Url: http://lists.slony.info/pipermail/slony1-general/attachments/20100611/48dd0375/attachment.txt 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: rss.sh
Type: application/x-sh
Size: 717 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20100611/48dd0375/attachment-0001.sh 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: versions.rss
Type: application/rss+xml
Size: 1145 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20100611/48dd0375/attachment.rss 

From bablu_postgres at yahoo.com  Mon Jun 14 08:56:16 2010
From: bablu_postgres at yahoo.com (Dhaval Jaiswal)
Date: Mon, 14 Jun 2010 08:56:16 -0700 (PDT)
Subject: [Slony1-general] Huge lagging time
Message-ID: <926431.78372.qm@web111201.mail.gq1.yahoo.com>


I am working on PostgreSQL 8.0.2. with slony I. 

Whenever there is a update, insert, delete happened on primary it will take some time to replicate the same on slave. We came to know about this using sl_status table, where its lagging time showing 1 hr or 2 hrs. However, sl_confirm table shows last replicated events is before 5 mins.
We have also seen there is vacuum analyze running on replications schema.   

Can someone point me where should i look into and how to improve replication performance. 

As of now there is no chance for upgradation of version. 

--
Dj



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100614/2246948c/attachment.htm 

From vivek at khera.org  Mon Jun 14 12:29:20 2010
From: vivek at khera.org (Vick Khera)
Date: Mon, 14 Jun 2010 15:29:20 -0400
Subject: [Slony1-general] Huge lagging time
In-Reply-To: <926431.78372.qm@web111201.mail.gq1.yahoo.com>
References: <926431.78372.qm@web111201.mail.gq1.yahoo.com>
Message-ID: <AANLkTilSDy11fTT36hpELkHRRGVOOvwhYSV0pDxIaGyq@mail.gmail.com>

faster hardware.  possibly reindex if you have index bloat.  if you're
falling that far behind, your workload is too much for your hardware.
unless you're running a vacuum full, the vacuum will not block
operations.

On Mon, Jun 14, 2010 at 11:56 AM, Dhaval Jaiswal
<bablu_postgres at yahoo.com> wrote:
> Can someone point me where should i look into and how to improve replication
> performance.
>

From scott.marlowe at gmail.com  Mon Jun 14 13:01:50 2010
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Mon, 14 Jun 2010 14:01:50 -0600
Subject: [Slony1-general] Huge lagging time
In-Reply-To: <926431.78372.qm@web111201.mail.gq1.yahoo.com>
References: <926431.78372.qm@web111201.mail.gq1.yahoo.com>
Message-ID: <AANLkTikrbUckg5K_YJXgGfkwlk5Ad6Ve7Thf9ZHNmA-L@mail.gmail.com>

On Mon, Jun 14, 2010 at 9:56 AM, Dhaval Jaiswal
<bablu_postgres at yahoo.com> wrote:
>
> I am working on PostgreSQL 8.0.2. with slony I.
>
> Whenever there is a update, insert, delete happened on primary it will take
> some time to replicate the same on slave. We came to know about this using
> sl_status table, where its lagging time showing 1 hr or 2 hrs. However,
> sl_confirm table shows last replicated events is before 5 mins.
> We have also seen there is vacuum analyze running on replications schema.

Happens to me when there's too much IO for my hardware (which is quite
a bit on my hardware).

> Can someone point me where should i look into and how to improve replication
> performance.

More / faster drives and controllers.

> As of now there is no chance for upgradation of version.

That would be the first thing I'd recommend.  Since you can't do it,
you're gonna have to have faster hardware, specifically the IO
subsystem.

From hsaltiel at gmail.com  Mon Jun 14 13:25:36 2010
From: hsaltiel at gmail.com (Hernan Saltiel)
Date: Mon, 14 Jun 2010 17:25:36 -0300
Subject: [Slony1-general] Huge lagging time
In-Reply-To: <AANLkTikrbUckg5K_YJXgGfkwlk5Ad6Ve7Thf9ZHNmA-L@mail.gmail.com>
References: <926431.78372.qm@web111201.mail.gq1.yahoo.com>
	<AANLkTikrbUckg5K_YJXgGfkwlk5Ad6Ve7Thf9ZHNmA-L@mail.gmail.com>
Message-ID: <AANLkTin3aeV6CE44z7cSmwRHR66dspalU6uxuK3EYciJ@mail.gmail.com>

On Mon, Jun 14, 2010 at 5:01 PM, Scott Marlowe <scott.marlowe at gmail.com>wrote:

> On Mon, Jun 14, 2010 at 9:56 AM, Dhaval Jaiswal
> <bablu_postgres at yahoo.com> wrote:
> >
> > I am working on PostgreSQL 8.0.2. with slony I.
> >
> > Whenever there is a update, insert, delete happened on primary it will
> take
> > some time to replicate the same on slave. We came to know about this
> using
> > sl_status table, where its lagging time showing 1 hr or 2 hrs. However,
> > sl_confirm table shows last replicated events is before 5 mins.
> > We have also seen there is vacuum analyze running on replications schema.
>
> Happens to me when there's too much IO for my hardware (which is quite
> a bit on my hardware).
>

How can I meassure how much is too much use of my hardware when Slony is in
place?
I can meassure the CPU, memory, disk IO, and network use, but how much is
needed in order to let Slony work well?
Is there any way to calculate this on a transaction number and size basis?
Thanks!


>
> > Can someone point me where should i look into and how to improve
> replication
> > performance.
>
> More / faster drives and controllers.
>
> > As of now there is no chance for upgradation of version.
>
> That would be the first thing I'd recommend.  Since you can't do it,
> you're gonna have to have faster hardware, specifically the IO
> subsystem.
>  _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>



-- 
HeCSa
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100614/743e7648/attachment.htm 

From scott.marlowe at gmail.com  Mon Jun 14 13:28:47 2010
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Mon, 14 Jun 2010 14:28:47 -0600
Subject: [Slony1-general] Huge lagging time
In-Reply-To: <AANLkTin3aeV6CE44z7cSmwRHR66dspalU6uxuK3EYciJ@mail.gmail.com>
References: <926431.78372.qm@web111201.mail.gq1.yahoo.com>
	<AANLkTikrbUckg5K_YJXgGfkwlk5Ad6Ve7Thf9ZHNmA-L@mail.gmail.com>
	<AANLkTin3aeV6CE44z7cSmwRHR66dspalU6uxuK3EYciJ@mail.gmail.com>
Message-ID: <AANLkTimtcMyeJX0HsbxCQsVNitW8nvult4DnQ02ynKIQ@mail.gmail.com>

On Mon, Jun 14, 2010 at 2:25 PM, Hernan Saltiel <hsaltiel at gmail.com> wrote:
>
>
> On Mon, Jun 14, 2010 at 5:01 PM, Scott Marlowe <scott.marlowe at gmail.com>
> wrote:
>>
>> On Mon, Jun 14, 2010 at 9:56 AM, Dhaval Jaiswal
>> <bablu_postgres at yahoo.com> wrote:
>> >
>> > I am working on PostgreSQL 8.0.2. with slony I.
>> >
>> > Whenever there is a update, insert, delete happened on primary it will
>> > take
>> > some time to replicate the same on slave. We came to know about this
>> > using
>> > sl_status table, where its lagging time showing 1 hr or 2 hrs. However,
>> > sl_confirm table shows last replicated events is before 5 mins.
>> > We have also seen there is vacuum analyze running on replications
>> > schema.
>>
>> Happens to me when there's too much IO for my hardware (which is quite
>> a bit on my hardware).
>
>
> How can I meassure how much is too much use of my hardware when Slony is in
> place?
> I can meassure the CPU, memory,?disk IO, and network?use, but how much is
> needed in order to let Slony work well?
> Is there any way to calculate this on a transaction number and size basis?

size isn't so much important as how much disk io you're using, and
whether or not that's too much is very dependent on your hardware.  I
find that when

iostat -xd /dev/sd? 60

consistently shows 100% utilization, slony starts to fall behind.
Load testing / benchmarking your system will tell you how many
requests / second of your typical load it takes to hit that.  Once you
hit that load, the only solution is faster hardware, or reducing load,
often by optimizing your code.

From ssinger at ca.afilias.info  Mon Jun 14 14:01:50 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 14 Jun 2010 17:01:50 -0400
Subject: [Slony1-general] Huge lagging time
In-Reply-To: <AANLkTin3aeV6CE44z7cSmwRHR66dspalU6uxuK3EYciJ@mail.gmail.com>
References: <926431.78372.qm@web111201.mail.gq1.yahoo.com>	<AANLkTikrbUckg5K_YJXgGfkwlk5Ad6Ve7Thf9ZHNmA-L@mail.gmail.com>
	<AANLkTin3aeV6CE44z7cSmwRHR66dspalU6uxuK3EYciJ@mail.gmail.com>
Message-ID: <4C1698BE.1060407@ca.afilias.info>

Hernan Saltiel wrote:
> 
> How can I meassure how much is too much use of my hardware when Slony is 
> in place?
> I can meassure the CPU, memory, disk IO, and network use, but how much 
> is needed in order to let Slony work well?
> Is there any way to calculate this on a transaction number and size basis?
> Thanks!

The other thing you should look into is if your having performance 
issues on your database from improper/insufficient vacuuming.  Back in 
the 8.0 days vacuuming issues where pretty common (I think 8.0 was 
before auto-vacuum or at least before auto-vacuum got good).

Are your application tables bloated?
Are your slony tables bloated?
Are your vacuum processes taking a long time?
If your've had vacuum issues in the past have you exceeded the size 
you've allocated to the free-space map.

Vacuuming the entire database through a single "VACUUM" command launched 
from cron is somtimes not the best approach, sometimes you need to issue 
individual vacuum commands on a per table basis where some tables get 
hit frequently (maybe a few times an hour) while others might only get 
vacuumed once a week.   It all depends on the acccess patterns to the 
tables and with older versions of postgresql the DBA is often left to 
figure this out on their own.

Also slony should be issuing vacuum commands against the slony tables so 
you probably don't want 'other' vacuum commands that regularly get run 
to duplicate the work.





>  
> 
> 
>      > Can someone point me where should i look into and how to improve
>     replication
>      > performance.
> 
>     More / faster drives and controllers.
> 
>      > As of now there is no chance for upgradation of version.
> 
>     That would be the first thing I'd recommend.  Since you can't do it,
>     you're gonna have to have faster hardware, specifically the IO
>     subsystem.
>     _______________________________________________
>     Slony1-general mailing list
>     Slony1-general at lists.slony.info <mailto:Slony1-general at lists.slony.info>
>     http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 
> 
> 
> -- 
> HeCSa
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From hsaltiel at gmail.com  Mon Jun 14 16:24:12 2010
From: hsaltiel at gmail.com (Hernan Saltiel)
Date: Mon, 14 Jun 2010 20:24:12 -0300
Subject: [Slony1-general] Huge lagging time
In-Reply-To: <4C1698BE.1060407@ca.afilias.info>
References: <926431.78372.qm@web111201.mail.gq1.yahoo.com>
	<AANLkTikrbUckg5K_YJXgGfkwlk5Ad6Ve7Thf9ZHNmA-L@mail.gmail.com>
	<AANLkTin3aeV6CE44z7cSmwRHR66dspalU6uxuK3EYciJ@mail.gmail.com>
	<4C1698BE.1060407@ca.afilias.info>
Message-ID: <AANLkTikBZIqescfyD0t2Znwf0Bq-whBodLJNckqmp7xV@mail.gmail.com>

Thanks, Scott & Steve for this clues!
My situation is not the PgSQL 8.0.x one, I have 8.4.x x64 installed, and the
replication schema uses an internet connection between two DB's, a master &
a slave.
Sometimes, the replication seems to stop working, I'm trying to figure out
what's wrong, if there is something.
I'll start a shell script right now to check the master & slave systems
based on the clues you sent.
Thanks again, and best regards,

HeCSa.



On Mon, Jun 14, 2010 at 6:01 PM, Steve Singer <ssinger at ca.afilias.info>wrote:

> Hernan Saltiel wrote:
>
>>
>> How can I meassure how much is too much use of my hardware when Slony is
>> in place?
>> I can meassure the CPU, memory, disk IO, and network use, but how much is
>> needed in order to let Slony work well?
>> Is there any way to calculate this on a transaction number and size basis?
>> Thanks!
>>
>
> The other thing you should look into is if your having performance issues
> on your database from improper/insufficient vacuuming.  Back in the 8.0 days
> vacuuming issues where pretty common (I think 8.0 was before auto-vacuum or
> at least before auto-vacuum got good).
>
> Are your application tables bloated?
> Are your slony tables bloated?
> Are your vacuum processes taking a long time?
> If your've had vacuum issues in the past have you exceeded the size you've
> allocated to the free-space map.
>
> Vacuuming the entire database through a single "VACUUM" command launched
> from cron is somtimes not the best approach, sometimes you need to issue
> individual vacuum commands on a per table basis where some tables get hit
> frequently (maybe a few times an hour) while others might only get vacuumed
> once a week.   It all depends on the acccess patterns to the tables and with
> older versions of postgresql the DBA is often left to figure this out on
> their own.
>
> Also slony should be issuing vacuum commands against the slony tables so
> you probably don't want 'other' vacuum commands that regularly get run to
> duplicate the work.
>
>
>
>
>
>
>>
>>     > Can someone point me where should i look into and how to improve
>>    replication
>>     > performance.
>>
>>    More / faster drives and controllers.
>>
>>     > As of now there is no chance for upgradation of version.
>>
>>    That would be the first thing I'd recommend.  Since you can't do it,
>>    you're gonna have to have faster hardware, specifically the IO
>>    subsystem.
>>    _______________________________________________
>>    Slony1-general mailing list
>>    Slony1-general at lists.slony.info <mailto:
>> Slony1-general at lists.slony.info>
>>
>>    http://lists.slony.info/mailman/listinfo/slony1-general
>>
>>
>>
>>
>> --
>> HeCSa
>>
>>
>> ------------------------------------------------------------------------
>>
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>
>
> --
> Steve Singer
> Afilias Canada
> Data Services Developer
> 416-673-1142
>



-- 
HeCSa
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100614/a7b42661/attachment.htm 

From shaun.thomas.mccloud at gmail.com  Tue Jun 15 06:59:44 2010
From: shaun.thomas.mccloud at gmail.com (Shaun McCloud)
Date: Tue, 15 Jun 2010 08:59:44 -0500
Subject: [Slony1-general] Replicate multiple databases in Windows
Message-ID: <4c17873e.413adc0a.3c95.6135@mx.google.com>

Hello,

 

I am trying to replicate two databases in Windows (Web Server 2008R2 to be
exact) using Slony-I 1.2.15 & PostgreSQL 8.3 (my hands are tied on the
PostgreSQL version for right now).  The guide I found to go by is from the
kind folks at EnterpriseDB and works great for one database.  If I setup
separate services for each database in my pg_service.conf file I get event
viewer errors asking if I am using the correct database when trying to
subscribe the slaves to the first replication set.  I know it would be
easier to put the two tables (yes, that's all I am replicating total) into
the same database, but if I do that then we have to modify our customers
data and we would like to avoid that if at all possible.  Am I just trying
to do something that isn't possible?

 

My pg_service.conf on the master is as follows.

[sr_routing_01-slonik]

dbname=sr_routing

host=172.16.6.70

user=slony

password=slony

[sr_routing_02-slonik]

dbname=sr_routing

host=172.16.6.71

user=slony

password=slony

[sr_routing_03-slonik]

dbname=sr_routing

host=172.16.6.72

user=slony

password=slony

[sr_alt_routing_01-slonik]

dbname=sr_alt_routing

host=172.16.6.70

user=slony

password=slony

[sr_alt_routing_02-slonik]

dbname=sr_alt_routing

host=172.16.6.71

user=slony

password=slony

[sr_alt_routing_03-slonik]

dbname=sr_alt_routing

host=172.16.6.72

user=slony

password=slony

 

 

Shaun McCloud - Software Testing Analyst
GeoComm Inc.
601 W. Saint Germain St., Saint Cloud, MN 56301
Office: 320.240.0040 Fax: 320.240.2389 Toll Free: 888.436.2666
click here to visit  <http://www.geo-comm.com> www.geo-comm.com

Microsoft Certified Desktop Support Technician (MCDST)

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100615/011e875c/attachment-0001.htm 

From bnichols at ca.afilias.info  Tue Jun 15 09:15:25 2010
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Tue, 15 Jun 2010 12:15:25 -0400
Subject: [Slony1-general] Huge lagging time
In-Reply-To: <AANLkTikBZIqescfyD0t2Znwf0Bq-whBodLJNckqmp7xV@mail.gmail.com>
References: <926431.78372.qm@web111201.mail.gq1.yahoo.com>
	<AANLkTikrbUckg5K_YJXgGfkwlk5Ad6Ve7Thf9ZHNmA-L@mail.gmail.com>
	<AANLkTin3aeV6CE44z7cSmwRHR66dspalU6uxuK3EYciJ@mail.gmail.com>
	<4C1698BE.1060407@ca.afilias.info>
	<AANLkTikBZIqescfyD0t2Znwf0Bq-whBodLJNckqmp7xV@mail.gmail.com>
Message-ID: <1276618525.5372.34.camel@bnicholson-desktop>

On Mon, 2010-06-14 at 20:24 -0300, Hernan Saltiel wrote:
> Thanks, Scott & Steve for this clues!
> My situation is not the PgSQL 8.0.x one, I have 8.4.x x64 installed,
> and the replication schema uses an internet connection between two
> DB's, a master & a slave.
> Sometimes, the replication seems to stop working, I'm trying to figure
> out what's wrong, if there is something.
> I'll start a shell script right now to check the master & slave
> systems based on the clues you sent.
> Thanks again, and best regards,

Does your application do any commits where you are writing a lots of
data in a single transaction like batch inserts, deleting large tables
or updating a lot of rows?


>  
> On Mon, Jun 14, 2010 at 6:01 PM, Steve Singer
> <ssinger at ca.afilias.info> wrote:
>         Hernan Saltiel wrote:
>                 
>                 How can I meassure how much is too much use of my
>                 hardware when Slony is in place?
>                 I can meassure the CPU, memory, disk IO, and network
>                 use, but how much is needed in order to let Slony work
>                 well?
>                 Is there any way to calculate this on a transaction
>                 number and size basis?
>                 Thanks!
>         
>         
>         The other thing you should look into is if your having
>         performance issues on your database from improper/insufficient
>         vacuuming.  Back in the 8.0 days vacuuming issues where pretty
>         common (I think 8.0 was before auto-vacuum or at least before
>         auto-vacuum got good).
>         
>         Are your application tables bloated?
>         Are your slony tables bloated?
>         Are your vacuum processes taking a long time?
>         If your've had vacuum issues in the past have you exceeded the
>         size you've allocated to the free-space map.
>         
>         Vacuuming the entire database through a single "VACUUM"
>         command launched from cron is somtimes not the best approach,
>         sometimes you need to issue individual vacuum commands on a
>         per table basis where some tables get hit frequently (maybe a
>         few times an hour) while others might only get vacuumed once a
>         week.   It all depends on the acccess patterns to the tables
>         and with older versions of postgresql the DBA is often left to
>         figure this out on their own.
>         
>         Also slony should be issuing vacuum commands against the slony
>         tables so you probably don't want 'other' vacuum commands that
>         regularly get run to duplicate the work.
>         
>         
>         
>         
>         
>                  
>                 
>                     > Can someone point me where should i look into
>                 and how to improve
>                    replication
>                     > performance.
>                 
>                    More / faster drives and controllers.
>                 
>                     > As of now there is no chance for upgradation of
>                 version.
>                 
>                    That would be the first thing I'd recommend.  Since
>                 you can't do it,
>                    you're gonna have to have faster hardware,
>                 specifically the IO
>                    subsystem.
>                    _______________________________________________
>                    Slony1-general mailing list
>                 
>                    Slony1-general at lists.slony.info
>                 <mailto:Slony1-general at lists.slony.info> 
>                 
>                 
>                  http://lists.slony.info/mailman/listinfo/slony1-general
>                 
>                 
>                 
>                 
>                 -- 
>                 HeCSa
>                 
>                 
>                 
>                 ------------------------------------------------------------------------ 
>                 
>                 
>                 _______________________________________________
>                 Slony1-general mailing list
>                 Slony1-general at lists.slony.info
>                 http://lists.slony.info/mailman/listinfo/slony1-general
>                 
>         
>         
>         -- 
>         Steve Singer
>         Afilias Canada
>         Data Services Developer
>         416-673-1142
> 
> 
> 
> -- 
> HeCSa
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
-- 
Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.



From cbbrowne at ca.afilias.info  Tue Jun 15 09:29:58 2010
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue, 15 Jun 2010 12:29:58 -0400
Subject: [Slony1-general] Having a version.rss would be great
In-Reply-To: <4C1274BB.10706@lelarge.info> (Guillaume Lelarge's message of
	"Fri, 11 Jun 2010 19:39:07 +0200")
References: <4BFE70F7.6050500@lelarge.info> <4BFE884F.1020803@ca.afilias.info>
	<4BFE9013.30707@ca.afilias.info> <4BFE909E.1080703@lelarge.info>
	<4BFFCBAE.7030104@ca.afilias.info> <4BFFCD94.9080208@lelarge.info>
	<4BFFCEAB.2040708@ca.afilias.info> <4BFFD39E.1070806@lelarge.info>
	<4C1274BB.10706@lelarge.info>
Message-ID: <871vc8p8c9.fsf@cbbrowne-laptop.afilias-int.info>

Guillaume Lelarge <guillaume at lelarge.info> writes:
>   * pubDate is hard to get in the good format. As a matter of fact, I
>     have no idea how to get it on the right format.

How about...

PUBDATE=$(tar tfvj ${TARFILE} | head -1 | cut -d " " -f 4-5)
?

And how about
AUTHOR=$(tar tfvj ${TARFILE} | head -1 | cut -d " " -f 2 | cut -d "/" -f 1)
?

Those mayn't be totally optimal, but they're surely better than
nothing.

I'd suggest looking at the names for "rc", and having a little bit of
logic that assortedly reports:

 - "This is a release" if it doesn't say "rc"
 - "This is a mere release candidate" if it does.

I think it's a fine idea for an RSS feed to include release
candidates, particularly if it expressly tells the reader that that's
what they are!

It's all pretty dependent on the combination of:
  a) The format of output from tar.  But that shouldn't be *too* fragile, as 
     a LOT of people depend on its behaviour, including how it outputs things.

  b) The naming conventions for releases.

     But again, that's not *too* fragile - I get hatemail (not too
     hateful :-)) any time the names of tarballs vary from
     expectations.

     Apparently folks with package management systems (e.g. - like
     Debian dpkg, RPM, Ports) have some dependencies on how things are
     spelled.

I'm fine with the direction you seem to be going on this; if you like
my suggestions, feel free to add them in.

And I expect that the right answer for compiling the whole RSS is for
each Makefile to have a rule that essentially says:

   "rummage around in the nearby download directories and generate a
   new central RSS file based on the combination of all of the files."

It's "expensive" in one sense, since each time you run "make" in one
of the download directories, it regenerates the main rss file.  But
since that's just constructed by cat'ing a few files together, it's
not *REALLY* expensive.

We already need to run "make" in one of the download directories
whenever we deploy a new download, to generate checksums.  Getting
that to regenerate RSS at no visible cost is an eminently reasonable
price to pay.
-- 
"cbbrowne","@","ca.afilias.info"
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)

From cbbrowne at ca.afilias.info  Tue Jun 15 09:46:43 2010
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue, 15 Jun 2010 12:46:43 -0400
Subject: [Slony1-general] Huge lagging time
In-Reply-To: <926431.78372.qm@web111201.mail.gq1.yahoo.com> (Dhaval Jaiswal's
	message of "Mon, 14 Jun 2010 08:56:16 -0700 (PDT)")
References: <926431.78372.qm@web111201.mail.gq1.yahoo.com>
Message-ID: <87wru0nszw.fsf@cbbrowne-laptop.afilias-int.info>

Dhaval Jaiswal <bablu_postgres at yahoo.com> writes:
> 					      I am working on PostgreSQL 8.0.2. with slony I.
> Whenever there is a update, insert, delete happened on primary it will take some time to replicate the same on slave. We came to know about
> this using sl_status table, where its lagging time showing 1 hr or 2 hrs. However, sl_confirm table shows last replicated events is before 5
> 								   mins.
> 				We have also seen there is vacuum analyze running on replications schema. ?
> 			 Can someone point me where should i look into and how to improve replication performance.
> 					  As of now there is no chance for upgradation of version.

Others have given some good recommendations of things to poke at to see what may be wrong.

I'll suggest another...

I suggest doing "VACUUM VERBOSE pg_catalog.pg_listener;" to see if
there is any bloat there.  If that table has a huge number of dead
tuples that haven't been vacuumed out, that has historically been one
of the "major offenders" of things causing replication to slow down.

If that's the case, then running VACUUM FULL on pg_listener is quite
likely to be rather helpful.  It's a blocking activity, but if you
have drawn all the data into memory via running "VACUUM VERBOSE
pg_listener;", it should be *reasonably* quick.

If you're on PostgreSQL 8.0.2, that's more than plenty old enough that
you're likely not running autovacuum, and I could readily see
pg_listener bloat being an issue.
-- 
"cbbrowne","@","ca.afilias.info"
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)

From hsaltiel at gmail.com  Tue Jun 15 09:51:53 2010
From: hsaltiel at gmail.com (Hernan Saltiel)
Date: Tue, 15 Jun 2010 13:51:53 -0300
Subject: [Slony1-general] Huge lagging time
In-Reply-To: <1276618525.5372.34.camel@bnicholson-desktop>
References: <926431.78372.qm@web111201.mail.gq1.yahoo.com>
	<AANLkTikrbUckg5K_YJXgGfkwlk5Ad6Ve7Thf9ZHNmA-L@mail.gmail.com>
	<AANLkTin3aeV6CE44z7cSmwRHR66dspalU6uxuK3EYciJ@mail.gmail.com>
	<4C1698BE.1060407@ca.afilias.info>
	<AANLkTikBZIqescfyD0t2Znwf0Bq-whBodLJNckqmp7xV@mail.gmail.com>
	<1276618525.5372.34.camel@bnicholson-desktop>
Message-ID: <AANLkTikHk2ad6HsLWBr-DQbFVN7fH_G9g6oPaBWGRlrF@mail.gmail.com>

On Tue, Jun 15, 2010 at 1:15 PM, Brad Nicholson <bnichols at ca.afilias.info>wrote:

> On Mon, 2010-06-14 at 20:24 -0300, Hernan Saltiel wrote:
> > Thanks, Scott & Steve for this clues!
> > My situation is not the PgSQL 8.0.x one, I have 8.4.x x64 installed,
> > and the replication schema uses an internet connection between two
> > DB's, a master & a slave.
> > Sometimes, the replication seems to stop working, I'm trying to figure
> > out what's wrong, if there is something.
> > I'll start a shell script right now to check the master & slave
> > systems based on the clues you sent.
> > Thanks again, and best regards,
>
> Does your application do any commits where you are writing a lots of
> data in a single transaction like batch inserts, deleting large tables
> or updating a lot of rows?
>

Yes, it does!
We're working to insert massive records, almos 1 millon in a single batch.
This is why I'm concerned about performance issues.
Thanks a lot, and best regards,

HeCSa.


>
>
> >
> > On Mon, Jun 14, 2010 at 6:01 PM, Steve Singer
> > <ssinger at ca.afilias.info> wrote:
> >         Hernan Saltiel wrote:
> >
> >                 How can I meassure how much is too much use of my
> >                 hardware when Slony is in place?
> >                 I can meassure the CPU, memory, disk IO, and network
> >                 use, but how much is needed in order to let Slony work
> >                 well?
> >                 Is there any way to calculate this on a transaction
> >                 number and size basis?
> >                 Thanks!
> >
> >
> >         The other thing you should look into is if your having
> >         performance issues on your database from improper/insufficient
> >         vacuuming.  Back in the 8.0 days vacuuming issues where pretty
> >         common (I think 8.0 was before auto-vacuum or at least before
> >         auto-vacuum got good).
> >
> >         Are your application tables bloated?
> >         Are your slony tables bloated?
> >         Are your vacuum processes taking a long time?
> >         If your've had vacuum issues in the past have you exceeded the
> >         size you've allocated to the free-space map.
> >
> >         Vacuuming the entire database through a single "VACUUM"
> >         command launched from cron is somtimes not the best approach,
> >         sometimes you need to issue individual vacuum commands on a
> >         per table basis where some tables get hit frequently (maybe a
> >         few times an hour) while others might only get vacuumed once a
> >         week.   It all depends on the acccess patterns to the tables
> >         and with older versions of postgresql the DBA is often left to
> >         figure this out on their own.
> >
> >         Also slony should be issuing vacuum commands against the slony
> >         tables so you probably don't want 'other' vacuum commands that
> >         regularly get run to duplicate the work.
> >
> >
> >
> >
> >
> >
> >
> >                     > Can someone point me where should i look into
> >                 and how to improve
> >                    replication
> >                     > performance.
> >
> >                    More / faster drives and controllers.
> >
> >                     > As of now there is no chance for upgradation of
> >                 version.
> >
> >                    That would be the first thing I'd recommend.  Since
> >                 you can't do it,
> >                    you're gonna have to have faster hardware,
> >                 specifically the IO
> >                    subsystem.
> >                    _______________________________________________
> >                    Slony1-general mailing list
> >
> >                    Slony1-general at lists.slony.info
> >                 <mailto:Slony1-general at lists.slony.info>
> >
> >
> >                  http://lists.slony.info/mailman/listinfo/slony1-general
> >
> >
> >
> >
> >                 --
> >                 HeCSa
> >
> >
> >
> >
> ------------------------------------------------------------------------
> >
> >
> >                 _______________________________________________
> >                 Slony1-general mailing list
> >                 Slony1-general at lists.slony.info
> >                 http://lists.slony.info/mailman/listinfo/slony1-general
> >
> >
> >
> >         --
> >         Steve Singer
> >         Afilias Canada
> >         Data Services Developer
> >         416-673-1142
> >
> >
> >
> > --
> > HeCSa
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general at lists.slony.info
> > http://lists.slony.info/mailman/listinfo/slony1-general
> --
> Brad Nicholson  416-673-4106
> Database Administrator, Afilias Canada Corp.
>
>
>


-- 
HeCSa
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100615/f6049961/attachment.htm 

From cbbrowne at ca.afilias.info  Tue Jun 15 10:34:33 2010
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue, 15 Jun 2010 13:34:33 -0400
Subject: [Slony1-general] Huge lagging time
In-Reply-To: <AANLkTikHk2ad6HsLWBr-DQbFVN7fH_G9g6oPaBWGRlrF@mail.gmail.com>
	(Hernan Saltiel's message of "Tue, 15 Jun 2010 13:51:53 -0300")
References: <926431.78372.qm@web111201.mail.gq1.yahoo.com>
	<AANLkTikrbUckg5K_YJXgGfkwlk5Ad6Ve7Thf9ZHNmA-L@mail.gmail.com>
	<AANLkTin3aeV6CE44z7cSmwRHR66dspalU6uxuK3EYciJ@mail.gmail.com>
	<4C1698BE.1060407@ca.afilias.info>
	<AANLkTikBZIqescfyD0t2Znwf0Bq-whBodLJNckqmp7xV@mail.gmail.com>
	<1276618525.5372.34.camel@bnicholson-desktop>
	<AANLkTikHk2ad6HsLWBr-DQbFVN7fH_G9g6oPaBWGRlrF@mail.gmail.com>
Message-ID: <87sk4onqs6.fsf@cbbrowne-laptop.afilias-int.info>

Hernan Saltiel <hsaltiel at gmail.com> writes:
> We're working to insert massive records, almos 1 millon in a single batch.
> This is why I'm concerned about performance issues.

OK, that certainly provides some indication of why you'd observe
things getting pretty "bloated," and that points the focus towards
your system being just plain overloaded, and away from VACUUMs or
other oddities.

You should see that either:

a) The slon from the subscriber has a connection against the provider
that is totally busy pulling large masses of data from the provider,
or

b) The connection from the subscriber-managing slon process has a
connection to the subscriber that is totally busy submitting
INSERT/UPDATE/DELETE requests.

If things are behaving well, then one of those two connections should
be the dominant bottleneck, busy all the time.  And improving the
speed of that would be the appropriate focus of efforts.
-- 
"cbbrowne","@","ca.afilias.info"
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)

From ssinger at ca.afilias.info  Tue Jun 15 10:49:04 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 15 Jun 2010 13:49:04 -0400
Subject: [Slony1-general] Huge lagging time
In-Reply-To: <AANLkTikHk2ad6HsLWBr-DQbFVN7fH_G9g6oPaBWGRlrF@mail.gmail.com>
References: <926431.78372.qm@web111201.mail.gq1.yahoo.com>	<AANLkTikrbUckg5K_YJXgGfkwlk5Ad6Ve7Thf9ZHNmA-L@mail.gmail.com>	<AANLkTin3aeV6CE44z7cSmwRHR66dspalU6uxuK3EYciJ@mail.gmail.com>	<4C1698BE.1060407@ca.afilias.info>	<AANLkTikBZIqescfyD0t2Znwf0Bq-whBodLJNckqmp7xV@mail.gmail.com>	<1276618525.5372.34.camel@bnicholson-desktop>
	<AANLkTikHk2ad6HsLWBr-DQbFVN7fH_G9g6oPaBWGRlrF@mail.gmail.com>
Message-ID: <4C17BD10.7010306@ca.afilias.info>

Hernan Saltiel wrote:
> 
> 
> Yes, it does!
> We're working to insert massive records, almos 1 millon in a single batch.
> This is why I'm concerned about performance issues.
> Thanks a lot, and best regards,

Slony's performance when replicating a large insert batch isn't great.

Depending on the total size of the table (and other factors) you might 
want to drop the table from replication and resubscribe the table after 
you've added the rows to the master.   Slony replicates the initial data 
with a 'copy' which is faster than the the million individual SQL 
statements that will be otherwise performed.





> HeCSa.
>  
> 
> 
> 
>      >
>      > On Mon, Jun 14, 2010 at 6:01 PM, Steve Singer
>      > <ssinger at ca.afilias.info <mailto:ssinger at ca.afilias.info>> wrote:
>      >         Hernan Saltiel wrote:
>      >
>      >                 How can I meassure how much is too much use of my
>      >                 hardware when Slony is in place?
>      >                 I can meassure the CPU, memory, disk IO, and network
>      >                 use, but how much is needed in order to let Slony
>     work
>      >                 well?
>      >                 Is there any way to calculate this on a transaction
>      >                 number and size basis?
>      >                 Thanks!
>      >
>      >
>      >         The other thing you should look into is if your having
>      >         performance issues on your database from
>     improper/insufficient
>      >         vacuuming.  Back in the 8.0 days vacuuming issues where
>     pretty
>      >         common (I think 8.0 was before auto-vacuum or at least before
>      >         auto-vacuum got good).
>      >
>      >         Are your application tables bloated?
>      >         Are your slony tables bloated?
>      >         Are your vacuum processes taking a long time?
>      >         If your've had vacuum issues in the past have you
>     exceeded the
>      >         size you've allocated to the free-space map.
>      >
>      >         Vacuuming the entire database through a single "VACUUM"
>      >         command launched from cron is somtimes not the best approach,
>      >         sometimes you need to issue individual vacuum commands on a
>      >         per table basis where some tables get hit frequently (maybe a
>      >         few times an hour) while others might only get vacuumed
>     once a
>      >         week.   It all depends on the acccess patterns to the tables
>      >         and with older versions of postgresql the DBA is often
>     left to
>      >         figure this out on their own.
>      >
>      >         Also slony should be issuing vacuum commands against the
>     slony
>      >         tables so you probably don't want 'other' vacuum commands
>     that
>      >         regularly get run to duplicate the work.
>      >
>      >
>      >
>      >
>      >
>      >
>      >
>      >                     > Can someone point me where should i look into
>      >                 and how to improve
>      >                    replication
>      >                     > performance.
>      >
>      >                    More / faster drives and controllers.
>      >
>      >                     > As of now there is no chance for upgradation of
>      >                 version.
>      >
>      >                    That would be the first thing I'd recommend.
>      Since
>      >                 you can't do it,
>      >                    you're gonna have to have faster hardware,
>      >                 specifically the IO
>      >                    subsystem.
>      >                    _______________________________________________
>      >                    Slony1-general mailing list
>      >
>      >                    Slony1-general at lists.slony.info
>     <mailto:Slony1-general at lists.slony.info>
>      >                 <mailto:Slony1-general at lists.slony.info
>     <mailto:Slony1-general at lists.slony.info>>
>      >
>      >
>      >                
>      http://lists.slony.info/mailman/listinfo/slony1-general
>      >
>      >
>      >
>      >
>      >                 --
>      >                 HeCSa
>      >
>      >
>      >
>      >                
>     ------------------------------------------------------------------------
>      >
>      >
>      >                 _______________________________________________
>      >                 Slony1-general mailing list
>      >                 Slony1-general at lists.slony.info
>     <mailto:Slony1-general at lists.slony.info>
>      >                
>     http://lists.slony.info/mailman/listinfo/slony1-general
>      >
>      >
>      >
>      >         --
>      >         Steve Singer
>      >         Afilias Canada
>      >         Data Services Developer
>      >         416-673-1142
>      >
>      >
>      >
>      > --
>      > HeCSa
>      > _______________________________________________
>      > Slony1-general mailing list
>      > Slony1-general at lists.slony.info
>     <mailto:Slony1-general at lists.slony.info>
>      > http://lists.slony.info/mailman/listinfo/slony1-general
>     --
>     Brad Nicholson  416-673-4106
>     Database Administrator, Afilias Canada Corp.
> 
> 
> 
> 
> 
> -- 
> HeCSa


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From hsaltiel at gmail.com  Tue Jun 15 10:52:18 2010
From: hsaltiel at gmail.com (Hernan Saltiel)
Date: Tue, 15 Jun 2010 14:52:18 -0300
Subject: [Slony1-general] Huge lagging time
In-Reply-To: <87sk4onqs6.fsf@cbbrowne-laptop.afilias-int.info>
References: <926431.78372.qm@web111201.mail.gq1.yahoo.com>
	<AANLkTikrbUckg5K_YJXgGfkwlk5Ad6Ve7Thf9ZHNmA-L@mail.gmail.com>
	<AANLkTin3aeV6CE44z7cSmwRHR66dspalU6uxuK3EYciJ@mail.gmail.com>
	<4C1698BE.1060407@ca.afilias.info>
	<AANLkTikBZIqescfyD0t2Znwf0Bq-whBodLJNckqmp7xV@mail.gmail.com>
	<1276618525.5372.34.camel@bnicholson-desktop>
	<AANLkTikHk2ad6HsLWBr-DQbFVN7fH_G9g6oPaBWGRlrF@mail.gmail.com>
	<87sk4onqs6.fsf@cbbrowne-laptop.afilias-int.info>
Message-ID: <AANLkTinoDfkY7zUc9K5qLy59MqW3e7CncIp27BF6nYtF@mail.gmail.com>

On Tue, Jun 15, 2010 at 2:34 PM, Christopher Browne <
cbbrowne at ca.afilias.info> wrote:

> Hernan Saltiel <hsaltiel at gmail.com> writes:
> > We're working to insert massive records, almos 1 millon in a single
> batch.
> > This is why I'm concerned about performance issues.
>
> OK, that certainly provides some indication of why you'd observe
> things getting pretty "bloated," and that points the focus towards
> your system being just plain overloaded, and away from VACUUMs or
> other oddities.
>
> You should see that either:
>
> a) The slon from the subscriber has a connection against the provider
> that is totally busy pulling large masses of data from the provider,
> or
>
> b) The connection from the subscriber-managing slon process has a
> connection to the subscriber that is totally busy submitting
> INSERT/UPDATE/DELETE requests.
>


I'm using "bandwithd" (great open source product to check link usage!) to
check the used bandwidth, is there any place where I can see how busy of
INSERT/UPDATE/DELETE is the connection and/or any other parameter?
Thanks again, and best regards,

HeCSa.


>
> If things are behaving well, then one of those two connections should
> be the dominant bottleneck, busy all the time.  And improving the
> speed of that would be the appropriate focus of efforts.
>  --
> "cbbrowne","@","ca.afilias.info"
> <http://dba2.int.libertyrms.com/>
> Christopher Browne
> (416) 673-4124 (land)
>



-- 
HeCSa
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100615/e70f67ae/attachment.htm 

From ssinger at ca.afilias.info  Wed Jun 16 12:45:36 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 16 Jun 2010 15:45:36 -0400
Subject: [Slony1-general] 2.0.4 release?
Message-ID: <4C1929E0.6070600@ca.afilias.info>


The release candidate for 2.0.4 has been out for a few weeks now and 
we've received no reports of regressions or serious bugs with it.

We are thinking it might be time to do an actual 2.0.4 release soon. 
If anyone has been experiencing any bugs or strange behavior with 
2.0.4rc2 now would be a good time to let us know.



-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From ssinger at ca.afilias.info  Wed Jun 16 12:53:01 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 16 Jun 2010 15:53:01 -0400
Subject: [Slony1-general] Replicate multiple databases in Windows
In-Reply-To: <4c17873e.413adc0a.3c95.6135@mx.google.com>
References: <4c17873e.413adc0a.3c95.6135@mx.google.com>
Message-ID: <4C192B9D.4060606@ca.afilias.info>

Shaun McCloud wrote:
> Hello,
> 
>  
> 
> I am trying to replicate two databases in Windows (Web Server 2008R2 to 
> be exact) using Slony-I 1.2.15 & PostgreSQL 8.3 (my hands are tied on 
> the PostgreSQL version for right now).  The guide I found to go by is 
> from the kind folks at EnterpriseDB and works great for one database.  
> If I setup separate services for each database in my pg_service.conf 
> file I get event viewer errors asking if I am using the correct database 
> when trying to subscribe the slaves to the first replication set.  I 
> know it would be easier to put the two tables (yes, that?s all I am 
> replicating total) into the same database, but if I do that then we have 
> to modify our customers data and we would like to avoid that if at all 
> possible.  Am I just trying to do something that isn?t possible?

I should say that Slony under Win32 doesn't seem to get a lot of use, so 
   I can't rule out their being unreported problems under win32.

In theory you should be able to have both databases on the same server. 
  What service names are your slons using?  sr_routing_01-slonik and 
sr_routing_02-slonik are both pointing at the same database, is this 
what you intended?  I see lots of entries in that conf file but you only 
have 2 databases (that you have told us about) so I would only expect 
you to need to slon services.



> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From brianf at consistentstate.com  Wed Jun 16 16:40:51 2010
From: brianf at consistentstate.com (Brian Fehrle)
Date: Wed, 16 Jun 2010 17:40:51 -0600
Subject: [Slony1-general] Replicate multiple databases in Windows
In-Reply-To: <4C192B9D.4060606@ca.afilias.info>
References: <4c17873e.413adc0a.3c95.6135@mx.google.com>
	<4C192B9D.4060606@ca.afilias.info>
Message-ID: <4C196103.5060900@consistentstate.com>

Shaun,

 From looking at your config file, I am assuming you have this type of 
setup. Three physical postgres clusters, 6 total databases (two sets of 
3, where one database of each set is a master, the other two are slaves).

--------------pg Cluster A--------------  <-- slony master 172.16.6.70
----database X---- ----database Y----
------table D------- ------- table H------

--------------pg Cluster B--------------  <-- slony master 172.16.6.71
----database X---- ----database Y----
------table D------- ------- table H------

--------------pg Cluster C--------------  <-- slony master 172.16.6.72
----database X---- ----database Y----
------table D------- ------- table H------

----Replication set 1----
----------table D ---------

----Replication set 2----
----------table H ---------

I didn't see anywhere in the config file where you are defining a slony 
cluster, I don't have any familiarity with slony on windows, so not sure 
how this is handled. Are you trying to get all six nodes running in the 
same slony cluster? If so, you'll need to have two separate "master" or 
"origin" nodes on Cluster A. then for each of the x.71 and x.72 
clusters, you will need two slony nodes set up as subscribers for their 
respective replication sets.

---- Origin nodes ----
Database 'X' on Cluster A will be the origin node for 'replication set 1'
Database 'Y' on Cluster A will be the origin node for 'replication set 2'

---- Subscriber Nodes ----
Database 'X' on Cluster B will be subscribed to 'replication set 1'
Database 'Y' on Cluster B will be subscribed to 'replication set 2'

Database 'X' on Cluster C will be subscribed to 'replication set 1'
Database 'Y' on Cluster C will be subscribed to 'replication set 2'

Another (and probably easier) option is to have two separate slony 
clusters set up. With this, each slony cluster will only need to be 
connected to three nodes (you will still need 6 nodes and a 
service/daemon for each of them)

--- slony cluster sr_routing ---
pg Cluster A - Database X - replication set 1 <-- Origin node
pg Cluster B - Database X - replication set 1 <-- Subscriber node
pg Cluster C - Database X - replication set 1 <-- Subscriber node

--- slony cluster sr_alt_routing ---
pg Cluster A - Database Y - replication set 1 <-- Origin node
pg Cluster B - Database Y - replication set 1 <-- Subscriber node
pg Cluster C - Database Y - replication set 1 <-- Subscriber node

So I'm not quite sure which of these you're attempting to get set up, 
but if the error is something along the lines of asking "if I am using 
the correct database when trying to subscribe the slaves to the first 
replication set", I'd say to make sure that you are only subscribing the 
correct slaves to the first replication set. Since you will have two 
separate replication sets, one for each table that live on separate 
databases, you won't want to subscribe Database 'Y' on pg Cluster C to 
the replication set that Database 'X' on pg Cluster A is the origin node 
for.


I hope I got my assumptions right, as well as my conclusions. Any more 
information you can provide us will help out.

- Brian Fehrle

Steve Singer wrote:
> Shaun McCloud wrote:
>   
>> Hello,
>>
>>  
>>
>> I am trying to replicate two databases in Windows (Web Server 2008R2 to 
>> be exact) using Slony-I 1.2.15 & PostgreSQL 8.3 (my hands are tied on 
>> the PostgreSQL version for right now).  The guide I found to go by is 
>> from the kind folks at EnterpriseDB and works great for one database.  
>> If I setup separate services for each database in my pg_service.conf 
>> file I get event viewer errors asking if I am using the correct database 
>> when trying to subscribe the slaves to the first replication set.  I 
>> know it would be easier to put the two tables (yes, that?s all I am 
>> replicating total) into the same database, but if I do that then we have 
>> to modify our customers data and we would like to avoid that if at all 
>> possible.  Am I just trying to do something that isn?t possible?
>>     
>
> I should say that Slony under Win32 doesn't seem to get a lot of use, so 
>    I can't rule out their being unreported problems under win32.
>
> In theory you should be able to have both databases on the same server. 
>   What service names are your slons using?  sr_routing_01-slonik and 
> sr_routing_02-slonik are both pointing at the same database, is this 
> what you intended?  I see lots of entries in that conf file but you only 
> have 2 databases (that you have told us about) so I would only expect 
> you to need to slon services.
>
>
>   


>   
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>     
>
>
>   


From vivek.gupta at globallogic.com  Thu Jun 17 04:05:34 2010
From: vivek.gupta at globallogic.com (Vivek Gupta)
Date: Thu, 17 Jun 2010 16:35:34 +0530
Subject: [Slony1-general] High memory usage in 'Slon' process
Message-ID: <478174F395A7E34BBD3891A50303D602DCAE15@ex3-del1.synapse.com>

We have upgrade the Slony from 1.1.5 to 2.0.4rc2 and additionally the
platform has been migrated from 32-bit to 64-bit. To our strange 'slon'
process is taking ~250 MB of memory although its usage was in KBs
earlier. What could be the root cause either slony upgrade or the
platform change?

 

With regards,

Vivek Gupta

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100617/93826b7e/attachment.htm 

From shaun.thomas.mccloud at gmail.com  Thu Jun 17 06:11:30 2010
From: shaun.thomas.mccloud at gmail.com (Shaun McCloud)
Date: Thu, 17 Jun 2010 08:11:30 -0500
Subject: [Slony1-general] Replicate multiple databases in Windows
In-Reply-To: <4C196103.5060900@consistentstate.com>
References: <4c17873e.413adc0a.3c95.6135@mx.google.com>
	<4C192B9D.4060606@ca.afilias.info>
	<4C196103.5060900@consistentstate.com>
Message-ID: <4c1a1ef0.e879dc0a.761b.ffff968b@mx.google.com>

Brian,

The tutorial I found for Slony on Windows has you use a bunch of different
files to create the cluster.  You are correct, I have three database servers
each with two databases.  Both master databases are on the same server & am
I trying to use two replication groups to replicate the data.  I'm not sure
what I did Monday/Tuesday morning but I am now to the point where I can't
even replicate a single table.  Seems like I take 1 step forward on this and
then take 10 steps backwords.

The following is included in every file I pass to slonik when building the
cluster.

cluster name = sr_routing_cluster;
node 1 admin conninfo = 'service=sr_routing_01-slonik';
node 2 admin conninfo = 'service=sr_routing_02-slonik';
node 3 admin conninfo = 'service=sr_routing_03-slonik';

To initialize the cluster, I use a file containing
include <preamble.sk>;
init cluster (id=1,comment='hostname=172.16.6.70');

Adding the nodes
include <preamble.sk>;
store node (id=2, comment='hostname=172.16.6.71', event node=1);
store node (id=3, comment='hostname=172.16.6.72', event node=1);

Adding the paths
include <preamble.sk>;
store path (server=1, client=2, conninfo='service=sr_routing_02-slonik');
store path (server=1, client=3, conninfo='service=sr_routing_03-slonik');
store path (server=2, client=1, conninfo='service=sr_routing_01-slonik');
store path (server=3, client=1, conninfo='service=sr_routing_01-slonik');

Config file for the slony service
cluster_name='sr_routing_cluster'
conn_info='service=sr_routing_01-slonik'

Build the set
include <preamble.sk>;
create set (id=1, origin=1, comment='Spatial Router GIS Data');

Add the table(s)
include <preamble.sk>;
set add table(set id=1, origin=1, id=1, fully qualified name='sde.routing',
comment='Routing', key='routing_pk');

And finally subscribe the slaves
include <preamble.sk>;
subscribe set (id=1, provider=1, receiver=2, forward=yes);
sync(id=1);
wait for event (origin=1, confirmed=2, wait on=1, timeout=30000);
subscribe set (id=1, provider=1, receiver=3, forward=yes);
sync(id=1);
wait for event (origin=1, confirmed=3, wait on=1, timeout=30000);

-----Original Message-----
From: Brian Fehrle [mailto:brianf at consistentstate.com] 
Sent: Wednesday, June 16, 2010 18:41
Cc: Shaun McCloud; slony1-general at lists.slony.info
Subject: Re: [Slony1-general] Replicate multiple databases in Windows

Shaun,

 From looking at your config file, I am assuming you have this type of 
setup. Three physical postgres clusters, 6 total databases (two sets of 
3, where one database of each set is a master, the other two are slaves).

--------------pg Cluster A--------------  <-- slony master 172.16.6.70
----database X---- ----database Y----
------table D------- ------- table H------

--------------pg Cluster B--------------  <-- slony master 172.16.6.71
----database X---- ----database Y----
------table D------- ------- table H------

--------------pg Cluster C--------------  <-- slony master 172.16.6.72
----database X---- ----database Y----
------table D------- ------- table H------

----Replication set 1----
----------table D ---------

----Replication set 2----
----------table H ---------

I didn't see anywhere in the config file where you are defining a slony 
cluster, I don't have any familiarity with slony on windows, so not sure 
how this is handled. Are you trying to get all six nodes running in the 
same slony cluster? If so, you'll need to have two separate "master" or 
"origin" nodes on Cluster A. then for each of the x.71 and x.72 
clusters, you will need two slony nodes set up as subscribers for their 
respective replication sets.

---- Origin nodes ----
Database 'X' on Cluster A will be the origin node for 'replication set 1'
Database 'Y' on Cluster A will be the origin node for 'replication set 2'

---- Subscriber Nodes ----
Database 'X' on Cluster B will be subscribed to 'replication set 1'
Database 'Y' on Cluster B will be subscribed to 'replication set 2'

Database 'X' on Cluster C will be subscribed to 'replication set 1'
Database 'Y' on Cluster C will be subscribed to 'replication set 2'

Another (and probably easier) option is to have two separate slony 
clusters set up. With this, each slony cluster will only need to be 
connected to three nodes (you will still need 6 nodes and a 
service/daemon for each of them)

--- slony cluster sr_routing ---
pg Cluster A - Database X - replication set 1 <-- Origin node
pg Cluster B - Database X - replication set 1 <-- Subscriber node
pg Cluster C - Database X - replication set 1 <-- Subscriber node

--- slony cluster sr_alt_routing ---
pg Cluster A - Database Y - replication set 1 <-- Origin node
pg Cluster B - Database Y - replication set 1 <-- Subscriber node
pg Cluster C - Database Y - replication set 1 <-- Subscriber node

So I'm not quite sure which of these you're attempting to get set up, 
but if the error is something along the lines of asking "if I am using 
the correct database when trying to subscribe the slaves to the first 
replication set", I'd say to make sure that you are only subscribing the 
correct slaves to the first replication set. Since you will have two 
separate replication sets, one for each table that live on separate 
databases, you won't want to subscribe Database 'Y' on pg Cluster C to 
the replication set that Database 'X' on pg Cluster A is the origin node 
for.


I hope I got my assumptions right, as well as my conclusions. Any more 
information you can provide us will help out.

- Brian Fehrle

Steve Singer wrote:
> Shaun McCloud wrote:
>   
>> Hello,
>>
>>  
>>
>> I am trying to replicate two databases in Windows (Web Server 2008R2 to 
>> be exact) using Slony-I 1.2.15 & PostgreSQL 8.3 (my hands are tied on 
>> the PostgreSQL version for right now).  The guide I found to go by is 
>> from the kind folks at EnterpriseDB and works great for one database.  
>> If I setup separate services for each database in my pg_service.conf 
>> file I get event viewer errors asking if I am using the correct database 
>> when trying to subscribe the slaves to the first replication set.  I 
>> know it would be easier to put the two tables (yes, that's all I am 
>> replicating total) into the same database, but if I do that then we have 
>> to modify our customers data and we would like to avoid that if at all 
>> possible.  Am I just trying to do something that isn't possible?
>>     
>
> I should say that Slony under Win32 doesn't seem to get a lot of use, so 
>    I can't rule out their being unreported problems under win32.
>
> In theory you should be able to have both databases on the same server. 
>   What service names are your slons using?  sr_routing_01-slonik and 
> sr_routing_02-slonik are both pointing at the same database, is this 
> what you intended?  I see lots of entries in that conf file but you only 
> have 2 databases (that you have told us about) so I would only expect 
> you to need to slon services.
>
>
>   


>   
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>     
>
>
>   



From brianf at consistentstate.com  Thu Jun 17 08:31:43 2010
From: brianf at consistentstate.com (Brian Fehrle)
Date: Thu, 17 Jun 2010 09:31:43 -0600
Subject: [Slony1-general] Replicate multiple databases in Windows
In-Reply-To: <4c1a1ef0.e879dc0a.761b.ffff968b@mx.google.com>
References: <4c17873e.413adc0a.3c95.6135@mx.google.com>
	<4C192B9D.4060606@ca.afilias.info>
	<4C196103.5060900@consistentstate.com>
	<4c1a1ef0.e879dc0a.761b.ffff968b@mx.google.com>
Message-ID: <4C1A3FDF.2000609@consistentstate.com>

I don't think it's quite a show-stopper, but when you add the paths, you 
should have an entry from every node to every other node, even from the 
slaves to each other.

store path (server=1, client=2, conninfo='service=sr_routing_02-slonik');
store path (server=1, client=3, conninfo='service=sr_routing_03-slonik');
store path (server=2, client=1, conninfo='service=sr_routing_01-slonik');
store path (server=3, client=1, conninfo='service=sr_routing_01-slonik');
store path (server=2, client=3, conninfo='service=sr_routing_01-slonik');
store path (server=3, client=2, conninfo='service=sr_routing_01-slonik');

And actually, now that I look at that, it looks like you may be giving the store paths the wrong conninfo. The conn info entered should be the connection information for the server specified. Such as:

store path (server=1, client=2, conninfo='service=sr_routing_01-slonik');
store path (server=1, client=3, conninfo='service=sr_routing_01-slonik');
store path (server=2, client=1, conninfo='service=sr_routing_02-slonik');
store path (server=3, client=1, conninfo='service=sr_routing_03-slonik');
store path (server=2, client=3, conninfo='service=sr_routing_02-slonik');
store path (server=3, client=2, conninfo='service=sr_routing_03-slonik');


Also, if you have the ability to look at the service/daemon's output 
logs for each of the slave nodes, it should report anything that's going 
wrong, such as the inability to connect to a node, or unable to write 
data to the replicated table on the slave database. Perhaps something in 
there can point to what may be going wrong.

The documentation has a page that helps with looking at the logs, it's 
for 1.2.21, but should all/mostly be valid:
http://lists.slony.info/adminguide/1.2/doc/adminguide/loganalysis.html

- Brian Fehrle

Shaun McCloud wrote:
> Brian,
>
> The tutorial I found for Slony on Windows has you use a bunch of different
> files to create the cluster.  You are correct, I have three database servers
> each with two databases.  Both master databases are on the same server & am
> I trying to use two replication groups to replicate the data.  I'm not sure
> what I did Monday/Tuesday morning but I am now to the point where I can't
> even replicate a single table.  Seems like I take 1 step forward on this and
> then take 10 steps backwords.
>
> The following is included in every file I pass to slonik when building the
> cluster.
>
> cluster name = sr_routing_cluster;
> node 1 admin conninfo = 'service=sr_routing_01-slonik';
> node 2 admin conninfo = 'service=sr_routing_02-slonik';
> node 3 admin conninfo = 'service=sr_routing_03-slonik';
>
> To initialize the cluster, I use a file containing
> include <preamble.sk>;
> init cluster (id=1,comment='hostname=172.16.6.70');
>
> Adding the nodes
> include <preamble.sk>;
> store node (id=2, comment='hostname=172.16.6.71', event node=1);
> store node (id=3, comment='hostname=172.16.6.72', event node=1);
>
> Adding the paths
> include <preamble.sk>;
> store path (server=1, client=2, conninfo='service=sr_routing_02-slonik');
> store path (server=1, client=3, conninfo='service=sr_routing_03-slonik');
> store path (server=2, client=1, conninfo='service=sr_routing_01-slonik');
> store path (server=3, client=1, conninfo='service=sr_routing_01-slonik');
>
> Config file for the slony service
> cluster_name='sr_routing_cluster'
> conn_info='service=sr_routing_01-slonik'
>
> Build the set
> include <preamble.sk>;
> create set (id=1, origin=1, comment='Spatial Router GIS Data');
>
> Add the table(s)
> include <preamble.sk>;
> set add table(set id=1, origin=1, id=1, fully qualified name='sde.routing',
> comment='Routing', key='routing_pk');
>
> And finally subscribe the slaves
> include <preamble.sk>;
> subscribe set (id=1, provider=1, receiver=2, forward=yes);
> sync(id=1);
> wait for event (origin=1, confirmed=2, wait on=1, timeout=30000);
> subscribe set (id=1, provider=1, receiver=3, forward=yes);
> sync(id=1);
> wait for event (origin=1, confirmed=3, wait on=1, timeout=30000);
>
> -----Original Message-----
> From: Brian Fehrle [mailto:brianf at consistentstate.com] 
> Sent: Wednesday, June 16, 2010 18:41
> Cc: Shaun McCloud; slony1-general at lists.slony.info
> Subject: Re: [Slony1-general] Replicate multiple databases in Windows
>
> Shaun,
>
>  From looking at your config file, I am assuming you have this type of 
> setup. Three physical postgres clusters, 6 total databases (two sets of 
> 3, where one database of each set is a master, the other two are slaves).
>
> --------------pg Cluster A--------------  <-- slony master 172.16.6.70
> ----database X---- ----database Y----
> ------table D------- ------- table H------
>
> --------------pg Cluster B--------------  <-- slony master 172.16.6.71
> ----database X---- ----database Y----
> ------table D------- ------- table H------
>
> --------------pg Cluster C--------------  <-- slony master 172.16.6.72
> ----database X---- ----database Y----
> ------table D------- ------- table H------
>
> ----Replication set 1----
> ----------table D ---------
>
> ----Replication set 2----
> ----------table H ---------
>
> I didn't see anywhere in the config file where you are defining a slony 
> cluster, I don't have any familiarity with slony on windows, so not sure 
> how this is handled. Are you trying to get all six nodes running in the 
> same slony cluster? If so, you'll need to have two separate "master" or 
> "origin" nodes on Cluster A. then for each of the x.71 and x.72 
> clusters, you will need two slony nodes set up as subscribers for their 
> respective replication sets.
>
> ---- Origin nodes ----
> Database 'X' on Cluster A will be the origin node for 'replication set 1'
> Database 'Y' on Cluster A will be the origin node for 'replication set 2'
>
> ---- Subscriber Nodes ----
> Database 'X' on Cluster B will be subscribed to 'replication set 1'
> Database 'Y' on Cluster B will be subscribed to 'replication set 2'
>
> Database 'X' on Cluster C will be subscribed to 'replication set 1'
> Database 'Y' on Cluster C will be subscribed to 'replication set 2'
>
> Another (and probably easier) option is to have two separate slony 
> clusters set up. With this, each slony cluster will only need to be 
> connected to three nodes (you will still need 6 nodes and a 
> service/daemon for each of them)
>
> --- slony cluster sr_routing ---
> pg Cluster A - Database X - replication set 1 <-- Origin node
> pg Cluster B - Database X - replication set 1 <-- Subscriber node
> pg Cluster C - Database X - replication set 1 <-- Subscriber node
>
> --- slony cluster sr_alt_routing ---
> pg Cluster A - Database Y - replication set 1 <-- Origin node
> pg Cluster B - Database Y - replication set 1 <-- Subscriber node
> pg Cluster C - Database Y - replication set 1 <-- Subscriber node
>
> So I'm not quite sure which of these you're attempting to get set up, 
> but if the error is something along the lines of asking "if I am using 
> the correct database when trying to subscribe the slaves to the first 
> replication set", I'd say to make sure that you are only subscribing the 
> correct slaves to the first replication set. Since you will have two 
> separate replication sets, one for each table that live on separate 
> databases, you won't want to subscribe Database 'Y' on pg Cluster C to 
> the replication set that Database 'X' on pg Cluster A is the origin node 
> for.
>
>
> I hope I got my assumptions right, as well as my conclusions. Any more 
> information you can provide us will help out.
>
> - Brian Fehrle
>
> Steve Singer wrote:
>   
>> Shaun McCloud wrote:
>>   
>>     
>>> Hello,
>>>
>>>  
>>>
>>> I am trying to replicate two databases in Windows (Web Server 2008R2 to 
>>> be exact) using Slony-I 1.2.15 & PostgreSQL 8.3 (my hands are tied on 
>>> the PostgreSQL version for right now).  The guide I found to go by is 
>>> from the kind folks at EnterpriseDB and works great for one database.  
>>> If I setup separate services for each database in my pg_service.conf 
>>> file I get event viewer errors asking if I am using the correct database 
>>> when trying to subscribe the slaves to the first replication set.  I 
>>> know it would be easier to put the two tables (yes, that's all I am 
>>> replicating total) into the same database, but if I do that then we have 
>>> to modify our customers data and we would like to avoid that if at all 
>>> possible.  Am I just trying to do something that isn't possible?
>>>     
>>>       
>> I should say that Slony under Win32 doesn't seem to get a lot of use, so 
>>    I can't rule out their being unreported problems under win32.
>>
>> In theory you should be able to have both databases on the same server. 
>>   What service names are your slons using?  sr_routing_01-slonik and 
>> sr_routing_02-slonik are both pointing at the same database, is this 
>> what you intended?  I see lots of entries in that conf file but you only 
>> have 2 databases (that you have told us about) so I would only expect 
>> you to need to slon services.
>>
>>
>>   
>>     
>
>
>   
>>   
>>     
>>> ------------------------------------------------------------------------
>>>
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general at lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>     
>>>       
>>   
>>     
>
>
>   


From shaun.thomas.mccloud at gmail.com  Thu Jun 17 08:48:59 2010
From: shaun.thomas.mccloud at gmail.com (Shaun McCloud)
Date: Thu, 17 Jun 2010 10:48:59 -0500
Subject: [Slony1-general] Replicate multiple databases in Windows
In-Reply-To: <4C1A3FDF.2000609@consistentstate.com>
References: <4c17873e.413adc0a.3c95.6135@mx.google.com>
	<4C192B9D.4060606@ca.afilias.info>
	<4C196103.5060900@consistentstate.com>
	<4c1a1ef0.e879dc0a.761b.ffff968b@mx.google.com>
	<4C1A3FDF.2000609@consistentstate.com>
Message-ID: <4c1a43d9.cd3edc0a.66db.56cb@mx.google.com>

I figured the second part out just after I sent my last email.  I can
replicate one database without issue but I cannot get the second database to
replicate.  For now I am going to put it on the back burner and get back to
doing other stuff at work.  We can technically get by with one database, it
would just be nice to have the second one so we don't have to modify our
customers data to much.

-----Original Message-----
From: Brian Fehrle [mailto:brianf at consistentstate.com] 
Sent: Thursday, June 17, 2010 10:32
To: Shaun McCloud
Cc: slony1-general at lists.slony.info
Subject: Re: [Slony1-general] Replicate multiple databases in Windows

I don't think it's quite a show-stopper, but when you add the paths, you 
should have an entry from every node to every other node, even from the 
slaves to each other.

store path (server=1, client=2, conninfo='service=sr_routing_02-slonik');
store path (server=1, client=3, conninfo='service=sr_routing_03-slonik');
store path (server=2, client=1, conninfo='service=sr_routing_01-slonik');
store path (server=3, client=1, conninfo='service=sr_routing_01-slonik');
store path (server=2, client=3, conninfo='service=sr_routing_01-slonik');
store path (server=3, client=2, conninfo='service=sr_routing_01-slonik');

And actually, now that I look at that, it looks like you may be giving the
store paths the wrong conninfo. The conn info entered should be the
connection information for the server specified. Such as:

store path (server=1, client=2, conninfo='service=sr_routing_01-slonik');
store path (server=1, client=3, conninfo='service=sr_routing_01-slonik');
store path (server=2, client=1, conninfo='service=sr_routing_02-slonik');
store path (server=3, client=1, conninfo='service=sr_routing_03-slonik');
store path (server=2, client=3, conninfo='service=sr_routing_02-slonik');
store path (server=3, client=2, conninfo='service=sr_routing_03-slonik');


Also, if you have the ability to look at the service/daemon's output 
logs for each of the slave nodes, it should report anything that's going 
wrong, such as the inability to connect to a node, or unable to write 
data to the replicated table on the slave database. Perhaps something in 
there can point to what may be going wrong.

The documentation has a page that helps with looking at the logs, it's 
for 1.2.21, but should all/mostly be valid:
http://lists.slony.info/adminguide/1.2/doc/adminguide/loganalysis.html

- Brian Fehrle

Shaun McCloud wrote:
> Brian,
>
> The tutorial I found for Slony on Windows has you use a bunch of different
> files to create the cluster.  You are correct, I have three database
servers
> each with two databases.  Both master databases are on the same server &
am
> I trying to use two replication groups to replicate the data.  I'm not
sure
> what I did Monday/Tuesday morning but I am now to the point where I can't
> even replicate a single table.  Seems like I take 1 step forward on this
and
> then take 10 steps backwords.
>
> The following is included in every file I pass to slonik when building the
> cluster.
>
> cluster name = sr_routing_cluster;
> node 1 admin conninfo = 'service=sr_routing_01-slonik';
> node 2 admin conninfo = 'service=sr_routing_02-slonik';
> node 3 admin conninfo = 'service=sr_routing_03-slonik';
>
> To initialize the cluster, I use a file containing
> include <preamble.sk>;
> init cluster (id=1,comment='hostname=172.16.6.70');
>
> Adding the nodes
> include <preamble.sk>;
> store node (id=2, comment='hostname=172.16.6.71', event node=1);
> store node (id=3, comment='hostname=172.16.6.72', event node=1);
>
> Adding the paths
> include <preamble.sk>;
> store path (server=1, client=2, conninfo='service=sr_routing_02-slonik');
> store path (server=1, client=3, conninfo='service=sr_routing_03-slonik');
> store path (server=2, client=1, conninfo='service=sr_routing_01-slonik');
> store path (server=3, client=1, conninfo='service=sr_routing_01-slonik');
>
> Config file for the slony service
> cluster_name='sr_routing_cluster'
> conn_info='service=sr_routing_01-slonik'
>
> Build the set
> include <preamble.sk>;
> create set (id=1, origin=1, comment='Spatial Router GIS Data');
>
> Add the table(s)
> include <preamble.sk>;
> set add table(set id=1, origin=1, id=1, fully qualified
name='sde.routing',
> comment='Routing', key='routing_pk');
>
> And finally subscribe the slaves
> include <preamble.sk>;
> subscribe set (id=1, provider=1, receiver=2, forward=yes);
> sync(id=1);
> wait for event (origin=1, confirmed=2, wait on=1, timeout=30000);
> subscribe set (id=1, provider=1, receiver=3, forward=yes);
> sync(id=1);
> wait for event (origin=1, confirmed=3, wait on=1, timeout=30000);
>
> -----Original Message-----
> From: Brian Fehrle [mailto:brianf at consistentstate.com] 
> Sent: Wednesday, June 16, 2010 18:41
> Cc: Shaun McCloud; slony1-general at lists.slony.info
> Subject: Re: [Slony1-general] Replicate multiple databases in Windows
>
> Shaun,
>
>  From looking at your config file, I am assuming you have this type of 
> setup. Three physical postgres clusters, 6 total databases (two sets of 
> 3, where one database of each set is a master, the other two are slaves).
>
> --------------pg Cluster A--------------  <-- slony master 172.16.6.70
> ----database X---- ----database Y----
> ------table D------- ------- table H------
>
> --------------pg Cluster B--------------  <-- slony master 172.16.6.71
> ----database X---- ----database Y----
> ------table D------- ------- table H------
>
> --------------pg Cluster C--------------  <-- slony master 172.16.6.72
> ----database X---- ----database Y----
> ------table D------- ------- table H------
>
> ----Replication set 1----
> ----------table D ---------
>
> ----Replication set 2----
> ----------table H ---------
>
> I didn't see anywhere in the config file where you are defining a slony 
> cluster, I don't have any familiarity with slony on windows, so not sure 
> how this is handled. Are you trying to get all six nodes running in the 
> same slony cluster? If so, you'll need to have two separate "master" or 
> "origin" nodes on Cluster A. then for each of the x.71 and x.72 
> clusters, you will need two slony nodes set up as subscribers for their 
> respective replication sets.
>
> ---- Origin nodes ----
> Database 'X' on Cluster A will be the origin node for 'replication set 1'
> Database 'Y' on Cluster A will be the origin node for 'replication set 2'
>
> ---- Subscriber Nodes ----
> Database 'X' on Cluster B will be subscribed to 'replication set 1'
> Database 'Y' on Cluster B will be subscribed to 'replication set 2'
>
> Database 'X' on Cluster C will be subscribed to 'replication set 1'
> Database 'Y' on Cluster C will be subscribed to 'replication set 2'
>
> Another (and probably easier) option is to have two separate slony 
> clusters set up. With this, each slony cluster will only need to be 
> connected to three nodes (you will still need 6 nodes and a 
> service/daemon for each of them)
>
> --- slony cluster sr_routing ---
> pg Cluster A - Database X - replication set 1 <-- Origin node
> pg Cluster B - Database X - replication set 1 <-- Subscriber node
> pg Cluster C - Database X - replication set 1 <-- Subscriber node
>
> --- slony cluster sr_alt_routing ---
> pg Cluster A - Database Y - replication set 1 <-- Origin node
> pg Cluster B - Database Y - replication set 1 <-- Subscriber node
> pg Cluster C - Database Y - replication set 1 <-- Subscriber node
>
> So I'm not quite sure which of these you're attempting to get set up, 
> but if the error is something along the lines of asking "if I am using 
> the correct database when trying to subscribe the slaves to the first 
> replication set", I'd say to make sure that you are only subscribing the 
> correct slaves to the first replication set. Since you will have two 
> separate replication sets, one for each table that live on separate 
> databases, you won't want to subscribe Database 'Y' on pg Cluster C to 
> the replication set that Database 'X' on pg Cluster A is the origin node 
> for.
>
>
> I hope I got my assumptions right, as well as my conclusions. Any more 
> information you can provide us will help out.
>
> - Brian Fehrle
>
> Steve Singer wrote:
>   
>> Shaun McCloud wrote:
>>   
>>     
>>> Hello,
>>>
>>>  
>>>
>>> I am trying to replicate two databases in Windows (Web Server 2008R2 to 
>>> be exact) using Slony-I 1.2.15 & PostgreSQL 8.3 (my hands are tied on 
>>> the PostgreSQL version for right now).  The guide I found to go by is 
>>> from the kind folks at EnterpriseDB and works great for one database.  
>>> If I setup separate services for each database in my pg_service.conf 
>>> file I get event viewer errors asking if I am using the correct database

>>> when trying to subscribe the slaves to the first replication set.  I 
>>> know it would be easier to put the two tables (yes, that's all I am 
>>> replicating total) into the same database, but if I do that then we have

>>> to modify our customers data and we would like to avoid that if at all 
>>> possible.  Am I just trying to do something that isn't possible?
>>>     
>>>       
>> I should say that Slony under Win32 doesn't seem to get a lot of use, so 
>>    I can't rule out their being unreported problems under win32.
>>
>> In theory you should be able to have both databases on the same server. 
>>   What service names are your slons using?  sr_routing_01-slonik and 
>> sr_routing_02-slonik are both pointing at the same database, is this 
>> what you intended?  I see lots of entries in that conf file but you only 
>> have 2 databases (that you have told us about) so I would only expect 
>> you to need to slon services.
>>
>>
>>   
>>     
>
>
>   
>>   
>>     
>>> ------------------------------------------------------------------------
>>>
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general at lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>     
>>>       
>>   
>>     
>
>
>   



From jks at selectacast.net  Thu Jun 17 10:44:39 2010
From: jks at selectacast.net (Joseph S)
Date: Thu, 17 Jun 2010 13:44:39 -0400
Subject: [Slony1-general] 2.0.4 RC2
In-Reply-To: <4C052672.9000207@ca.afilias.info>
References: <4C052672.9000207@ca.afilias.info>
Message-ID: <4C1A5F07.9060700@selectacast.net>

Should I blow away my old slave database and rereplicate from scratch?  
Or can I just upgrade slony and start it up?

Steve Singer wrote:
> I have tagged Slony-I 2.0.4 RC2
>
> This is like 2.0.4 RC1 plus the memory leak fixes discussed on the list.
>
> http://lists.slony.info/downloads/2.0/source/slony1-2.0.4.rc2.tar.bz2
>
> It would be nice if people could test it and report back to the list.
>
>
>
>
>   

From ssinger at ca.afilias.info  Thu Jun 17 10:50:15 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 17 Jun 2010 13:50:15 -0400
Subject: [Slony1-general] 2.0.4 RC2
In-Reply-To: <4C1A5F07.9060700@selectacast.net>
References: <4C052672.9000207@ca.afilias.info>
	<4C1A5F07.9060700@selectacast.net>
Message-ID: <4C1A6057.30206@ca.afilias.info>

Joseph S wrote:
> Should I blow away my old slave database and rereplicate from scratch?  
> Or can I just upgrade slony and start it up?
> 
> Steve Singer wrote:
>> I have tagged Slony-I 2.0.4 RC2
>>
>> This is like 2.0.4 RC1 plus the memory leak fixes discussed on the list.
>>
>> http://lists.slony.info/downloads/2.0/source/slony1-2.0.4.rc2.tar.bz2
>>
>> It would be nice if people could test it and report back to the list.
>>
>>


As long as your test database was in a resonable state the upgrade from 
a previous 2.0.x release should work fine.  Please report any issues you 
encounter


>>
>>
>>   
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From guillaume at lelarge.info  Thu Jun 17 10:50:22 2010
From: guillaume at lelarge.info (Guillaume Lelarge)
Date: Thu, 17 Jun 2010 19:50:22 +0200
Subject: [Slony1-general] Having a version.rss would be great
In-Reply-To: <871vc8p8c9.fsf@cbbrowne-laptop.afilias-int.info>
References: <4BFE70F7.6050500@lelarge.info>
	<4BFE884F.1020803@ca.afilias.info>	<4BFE9013.30707@ca.afilias.info>
	<4BFE909E.1080703@lelarge.info>	<4BFFCBAE.7030104@ca.afilias.info>
	<4BFFCD94.9080208@lelarge.info>	<4BFFCEAB.2040708@ca.afilias.info>
	<4BFFD39E.1070806@lelarge.info>	<4C1274BB.10706@lelarge.info>
	<871vc8p8c9.fsf@cbbrowne-laptop.afilias-int.info>
Message-ID: <4C1A605E.9040505@lelarge.info>

Le 15/06/2010 18:29, Christopher Browne a ?crit :
> Guillaume Lelarge <guillaume at lelarge.info> writes:
>>   * pubDate is hard to get in the good format. As a matter of fact, I
>>     have no idea how to get it on the right format.
> 
> How about...
> 
> PUBDATE=$(tar tfvj ${TARFILE} | head -1 | cut -d " " -f 4-5)
> ?
> 
> And how about
> AUTHOR=$(tar tfvj ${TARFILE} | head -1 | cut -d " " -f 2 | cut -d "/" -f 1)
> ?
> 
> Those mayn't be totally optimal, but they're surely better than
> nothing.
> 

Sure. PUBDATE has a specific format, and this is not the good one.
People could start complaining about it once it is published.

> I'd suggest looking at the names for "rc", and having a little bit of
> logic that assortedly reports:
> 
>  - "This is a release" if it doesn't say "rc"
>  - "This is a mere release candidate" if it does.
> 
> I think it's a fine idea for an RSS feed to include release
> candidates, particularly if it expressly tells the reader that that's
> what they are!
> 

You mean, having for example the 1.2.21, the 2.0.3, the 2.0.4-rc in the
versions.rss. Or only the 1.2.21 and the 2.0.4-rc. I can work with the
former. The latter has absolutely no interest to me.

> It's all pretty dependent on the combination of:
>   a) The format of output from tar.  But that shouldn't be *too* fragile, as 
>      a LOT of people depend on its behaviour, including how it outputs things.
> 
>   b) The naming conventions for releases.
> 
>      But again, that's not *too* fragile - I get hatemail (not too
>      hateful :-)) any time the names of tarballs vary from
>      expectations.
> 
>      Apparently folks with package management systems (e.g. - like
>      Debian dpkg, RPM, Ports) have some dependencies on how things are
>      spelled.
> 
> I'm fine with the direction you seem to be going on this; if you like
> my suggestions, feel free to add them in.
> 

I added them. So new file attached. The other files don't change.

> And I expect that the right answer for compiling the whole RSS is for
> each Makefile to have a rule that essentially says:
> 
>    "rummage around in the nearby download directories and generate a
>    new central RSS file based on the combination of all of the files."
> 
> It's "expensive" in one sense, since each time you run "make" in one
> of the download directories, it regenerates the main rss file.  But
> since that's just constructed by cat'ing a few files together, it's
> not *REALLY* expensive.
> 
> We already need to run "make" in one of the download directories
> whenever we deploy a new download, to generate checksums.  Getting
> that to regenerate RSS at no visible cost is an eminently reasonable
> price to pay.

Thanks.


-- 
Guillaume
 http://www.postgresql.fr
 http://dalibo.com
-------------- next part --------------
A non-text attachment was scrubbed...
Name: rss.sh
Type: application/x-sh
Size: 1050 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20100617/f6e47048/attachment.sh 

From vivek at khera.org  Thu Jun 17 12:11:45 2010
From: vivek at khera.org (Vick Khera)
Date: Thu, 17 Jun 2010 15:11:45 -0400
Subject: [Slony1-general] High memory usage in 'Slon' process
In-Reply-To: <478174F395A7E34BBD3891A50303D602DCAE15@ex3-del1.synapse.com>
References: <478174F395A7E34BBD3891A50303D602DCAE15@ex3-del1.synapse.com>
Message-ID: <AANLkTiksfKXEocDqBIesSrHZGrppyZ-ZSHBaacmXY1G1@mail.gmail.com>

On Thu, Jun 17, 2010 at 7:05 AM, Vivek Gupta
<vivek.gupta at globallogic.com> wrote:
> We have upgrade the Slony from 1.1.5 to 2.0.4rc2 and additionally the
> platform has been migrated from 32-bit to 64-bit. To our strange ?slon?
> process is taking ~250 MB of memory although its usage was in KBs earlier.
> What could be the root cause either slony upgrade or the platform change?

You'll find in general that the 64-bit OS will use much more RAM.
Without knowing your OS and what the size was before it is hard to say
for sure if this accounts for all of your increase.

From scott.marlowe at gmail.com  Thu Jun 17 12:28:51 2010
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Thu, 17 Jun 2010 13:28:51 -0600
Subject: [Slony1-general] High memory usage in 'Slon' process
In-Reply-To: <AANLkTiksfKXEocDqBIesSrHZGrppyZ-ZSHBaacmXY1G1@mail.gmail.com>
References: <478174F395A7E34BBD3891A50303D602DCAE15@ex3-del1.synapse.com>
	<AANLkTiksfKXEocDqBIesSrHZGrppyZ-ZSHBaacmXY1G1@mail.gmail.com>
Message-ID: <AANLkTin9s4k70E7mRh-3t9oRMNHAm2JiBju9DB9jKQ7w@mail.gmail.com>

On Thu, Jun 17, 2010 at 1:11 PM, Vick Khera <vivek at khera.org> wrote:
> On Thu, Jun 17, 2010 at 7:05 AM, Vivek Gupta
> <vivek.gupta at globallogic.com> wrote:
>> We have upgrade the Slony from 1.1.5 to 2.0.4rc2 and additionally the
>> platform has been migrated from 32-bit to 64-bit. To our strange ?slon?
>> process is taking ~250 MB of memory although its usage was in KBs earlier.
>> What could be the root cause either slony upgrade or the platform change?
>
> You'll find in general that the 64-bit OS will use much more RAM.
> Without knowing your OS and what the size was before it is hard to say
> for sure if this accounts for all of your increase.

FYI, on 64 bit Centos 5.4 my slon daemons are using about 250k to 350k
resident (virt is about twice that).  When they are first restarted
they sit at ~200k VIRT and 3k RSS.

From ssinger at ca.afilias.info  Thu Jun 17 13:16:10 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 17 Jun 2010 16:16:10 -0400
Subject: [Slony1-general] High memory usage in 'Slon' process
In-Reply-To: <AANLkTin9s4k70E7mRh-3t9oRMNHAm2JiBju9DB9jKQ7w@mail.gmail.com>
References: <478174F395A7E34BBD3891A50303D602DCAE15@ex3-del1.synapse.com>	<AANLkTiksfKXEocDqBIesSrHZGrppyZ-ZSHBaacmXY1G1@mail.gmail.com>
	<AANLkTin9s4k70E7mRh-3t9oRMNHAm2JiBju9DB9jKQ7w@mail.gmail.com>
Message-ID: <4C1A828A.7080703@ca.afilias.info>

Scott Marlowe wrote:
> On Thu, Jun 17, 2010 at 1:11 PM, Vick Khera <vivek at khera.org> wrote:
>> On Thu, Jun 17, 2010 at 7:05 AM, Vivek Gupta
>> <vivek.gupta at globallogic.com> wrote:
>>> We have upgrade the Slony from 1.1.5 to 2.0.4rc2 and additionally the
>>> platform has been migrated from 32-bit to 64-bit. To our strange ?slon?
>>> process is taking ~250 MB of memory although its usage was in KBs earlier.
>>> What could be the root cause either slony upgrade or the platform change?
>> You'll find in general that the 64-bit OS will use much more RAM.
>> Without knowing your OS and what the size was before it is hard to say
>> for sure if this accounts for all of your increase.
> 
> FYI, on 64 bit Centos 5.4 my slon daemons are using about 250k to 350k
> resident (virt is about twice that).  When they are first restarted
> they sit at ~200k VIRT and 3k RSS.

Which version of slony 2.0.x or 1.2.x ?


> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From scott.marlowe at gmail.com  Thu Jun 17 13:21:46 2010
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Thu, 17 Jun 2010 14:21:46 -0600
Subject: [Slony1-general] High memory usage in 'Slon' process
In-Reply-To: <4C1A828A.7080703@ca.afilias.info>
References: <478174F395A7E34BBD3891A50303D602DCAE15@ex3-del1.synapse.com>
	<AANLkTiksfKXEocDqBIesSrHZGrppyZ-ZSHBaacmXY1G1@mail.gmail.com>
	<AANLkTin9s4k70E7mRh-3t9oRMNHAm2JiBju9DB9jKQ7w@mail.gmail.com>
	<4C1A828A.7080703@ca.afilias.info>
Message-ID: <AANLkTik8311ix1Bz6fbzs-zCER8dfmy5NlsUJ1kfIGky@mail.gmail.com>

On Thu, Jun 17, 2010 at 2:16 PM, Steve Singer <ssinger at ca.afilias.info> wrote:
> Scott Marlowe wrote:
>>
>> On Thu, Jun 17, 2010 at 1:11 PM, Vick Khera <vivek at khera.org> wrote:
>>>
>>> On Thu, Jun 17, 2010 at 7:05 AM, Vivek Gupta
>>> <vivek.gupta at globallogic.com> wrote:
>>>>
>>>> We have upgrade the Slony from 1.1.5 to 2.0.4rc2 and additionally the
>>>> platform has been migrated from 32-bit to 64-bit. To our strange ?slon?
>>>> process is taking ~250 MB of memory although its usage was in KBs
>>>> earlier.
>>>> What could be the root cause either slony upgrade or the platform
>>>> change?
>>>
>>> You'll find in general that the 64-bit OS will use much more RAM.
>>> Without knowing your OS and what the size was before it is hard to say
>>> for sure if this accounts for all of your increase.
>>
>> FYI, on 64 bit Centos 5.4 my slon daemons are using about 250k to 350k
>> resident (virt is about twice that). ?When they are first restarted
>> they sit at ~200k VIRT and 3k RSS.
>
> Which version of slony 2.0.x or 1.2.x ?

1.2.15 or so.  (whatever version RHEL ships)  I'm gonna upgrade to
1.2.latest when I get my new servers in a few weeks.  Want to test
2.0.4 when it comes out, but haven't had the chance yet.  Last year I
had catastrophic failures of 2.0.2 or so under testing, and it took
weeks for the problems to show up.

From jks at selectacast.net  Thu Jun 17 13:47:44 2010
From: jks at selectacast.net (Joseph S)
Date: Thu, 17 Jun 2010 16:47:44 -0400
Subject: [Slony1-general] 2.0.4 RC2
In-Reply-To: <4C1A6057.30206@ca.afilias.info>
References: <4C052672.9000207@ca.afilias.info>
	<4C1A5F07.9060700@selectacast.net> <4C1A6057.30206@ca.afilias.info>
Message-ID: <4C1A89F0.5060906@selectacast.net>

I can't compile 2.0.4 rc2

slony_logshipper.c: In function ?main?:
slony_logshipper.c:476: warning: ?destfname.data? may be used 
uninitialized in this function
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../.. 
-DPGSHARE="\"/usr/local/pgsql/share/\""  -I/usr/local/pgsql/include/ 
-I/usr/local/pgsql/include/server/  -c -o dbutil.o dbutil.c
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../.. 
-DPGSHARE="\"/usr/local/pgsql/share/\""  -I/usr/local/pgsql/include/ 
-I/usr/local/pgsql/include/server/  -c -o ipcutil.o ipcutil.c
bison -y -d  parser.y
mv -f y.tab.c parser.c
Missing flex scan.l scan.c
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../.. 
-DPGSHARE="\"/usr/local/pgsql/share/\""  -I/usr/local/pgsql/include/ 
-I/usr/local/pgsql/include/server/  -c -o parser.o parser.c
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../.. 
-DPGSHARE="\"/usr/local/pgsql/share/\""  -I/usr/local/pgsql/include/ 
-I/usr/local/pgsql/include/server/  -c -o scan.o scan.c
gcc: scan.c: No such file or directory
gcc: no input files
make[2]: *** [scan.o] Error 1
make[2]: Leaving directory 
`/local/download/src/slony1-2.0.4.rc2/src/slony_logshipper'
make[1]: *** [all] Error 2
make[1]: Leaving directory `/local/download/src/slony1-2.0.4.rc2/src'
make: *** [all] Error 2

There is a scan.c under src/slonik/

Steve Singer wrote:
> Joseph S wrote:
>> Should I blow away my old slave database and rereplicate from 
>> scratch?  Or can I just upgrade slony and start it up?
>>
>> Steve Singer wrote:
>>> I have tagged Slony-I 2.0.4 RC2
>>>
>>> This is like 2.0.4 RC1 plus the memory leak fixes discussed on the list.
>>>
>>> http://lists.slony.info/downloads/2.0/source/slony1-2.0.4.rc2.tar.bz2
>>>
>>> It would be nice if people could test it and report back to the list.
>>>
>>>
> 
> 
> As long as your test database was in a resonable state the upgrade from 
> a previous 2.0.x release should work fine.  Please report any issues you 
> encounter
> 
> 
>>>
>>>
>>>   
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 


From scott.marlowe at gmail.com  Thu Jun 17 14:00:25 2010
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Thu, 17 Jun 2010 15:00:25 -0600
Subject: [Slony1-general] 2.0.4 RC2
In-Reply-To: <4C1A89F0.5060906@selectacast.net>
References: <4C052672.9000207@ca.afilias.info>
	<4C1A5F07.9060700@selectacast.net> <4C1A6057.30206@ca.afilias.info>
	<4C1A89F0.5060906@selectacast.net>
Message-ID: <AANLkTilEGBS_KXcuAueS22mVgzDWQk4PF0186ABymaZn@mail.gmail.com>

On Thu, Jun 17, 2010 at 2:47 PM, Joseph S <jks at selectacast.net> wrote:
> I can't compile 2.0.4 rc2
>
> slony_logshipper.c: In function ?main?:
> slony_logshipper.c:476: warning: ?destfname.data? may be used
> uninitialized in this function
> gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../..
> -DPGSHARE="\"/usr/local/pgsql/share/\"" ?-I/usr/local/pgsql/include/
> -I/usr/local/pgsql/include/server/ ?-c -o dbutil.o dbutil.c
> gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../..
> -DPGSHARE="\"/usr/local/pgsql/share/\"" ?-I/usr/local/pgsql/include/
> -I/usr/local/pgsql/include/server/ ?-c -o ipcutil.o ipcutil.c
> bison -y -d ?parser.y
> mv -f y.tab.c parser.c
> Missing flex scan.l scan.c


Looks like you need bison or flexx -devel packages.

From ssinger at ca.afilias.info  Thu Jun 17 14:16:56 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 17 Jun 2010 17:16:56 -0400
Subject: [Slony1-general] 2.0.4 RC2
In-Reply-To: <4C1A89F0.5060906@selectacast.net>
References: <4C052672.9000207@ca.afilias.info>
	<4C1A5F07.9060700@selectacast.net> <4C1A6057.30206@ca.afilias.info>
	<4C1A89F0.5060906@selectacast.net>
Message-ID: <4C1A90C8.4050501@ca.afilias.info>

Joseph S wrote:
> I can't compile 2.0.4 rc2
> 
> make[2]: *** [scan.o] Error 1
> make[2]: Leaving directory 
> `/local/download/src/slony1-2.0.4.rc2/src/slony_logshipper'
> make[1]: *** [all] Error 2
> make[1]: Leaving directory `/local/download/src/slony1-2.0.4.rc2/src'
> make: *** [all] Error 2


This is actually a bug in the distclean rule in slony_logshipper.  Our 
policy has been to require bison/flex for cvs builds but to include the 
generated files in the tarfiles.  The distclean rule seems to be 
deleting the scan.l file.

If you install flex you can avoid that error.   We will include those 
files in the next tarfile (technically this means we should have a rc3 
but I'd like to see what else people find)

Thanks

> 
> There is a scan.c under src/slonik/
> 
> Steve Singer wrote:
>> Joseph S wrote:
>>> Should I blow away my old slave database and rereplicate from 
>>> scratch?  Or can I just upgrade slony and start it up?
>>>
>>> Steve Singer wrote:
>>>> I have tagged Slony-I 2.0.4 RC2
>>>>
>>>> This is like 2.0.4 RC1 plus the memory leak fixes discussed on the 
>>>> list.
>>>>
>>>> http://lists.slony.info/downloads/2.0/source/slony1-2.0.4.rc2.tar.bz2
>>>>
>>>> It would be nice if people could test it and report back to the list.
>>>>
>>>>
>>
>>
>> As long as your test database was in a resonable state the upgrade 
>> from a previous 2.0.x release should work fine.  Please report any 
>> issues you encounter
>>
>>
>>>>
>>>>
>>>>   
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general at lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>>
> 


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From jks at selectacast.net  Thu Jun 17 14:49:30 2010
From: jks at selectacast.net (Joseph S)
Date: Thu, 17 Jun 2010 17:49:30 -0400
Subject: [Slony1-general] 2.0.4 RC2
In-Reply-To: <4C1A90C8.4050501@ca.afilias.info>
References: <4C052672.9000207@ca.afilias.info>
	<4C1A5F07.9060700@selectacast.net> <4C1A6057.30206@ca.afilias.info>
	<4C1A89F0.5060906@selectacast.net>
	<4C1A90C8.4050501@ca.afilias.info>
Message-ID: <4C1A986A.2010004@selectacast.net>

I'm having a problem with 2.0.4 but it isn't related to the problem I 
was having with 2.0.3.  I had a table that I kept creating, adding to 
the slony replication set, and then dropping and starting over.  During 
this time I wasn't running slony because of the bugs in 2.0.3.  Now that 
2.0.4 rc2 is running it got stuck in a loop trying to create this table 
and failing because it existed already.  I kept dropping the table from 
my slave until that wasn't a problem anymore but now it is stuck in a 
loop doing this:


ERROR:  Slony-I: alterTableDropTriggers(): Table with id 41 not found
CONTEXT:  SQL statement "SELECT  "_devel".alterTableDropTriggers( $1 )"
         PL/pgSQL function "dropset_int" line 18 at PERFORM
STATEMENT:  select "_devel".dropSet_int(999); insert into 
"_devel".sl_event     (ev_origin, ev_seqno, ev_timestamp, 
ev_snapshot, ev_type , ev_data1    ) values ('1', '5000048246', 
'2010-04-27 18:44:50.666916', '720295:720295:', 'DROP_SET', '999'); 
insert into "_devel".sl_confirm   (con_origin, con_received, con_seqno, 
con_timestamp)    values (1, 2, '5000048246', now()); commit transaction;
LOG:  unexpected EOF on client connection

How can I break it out of this loop?

Steve Singer wrote:
> Joseph S wrote:
>> I can't compile 2.0.4 rc2
>>
>> make[2]: *** [scan.o] Error 1
>> make[2]: Leaving directory 
>> `/local/download/src/slony1-2.0.4.rc2/src/slony_logshipper'
>> make[1]: *** [all] Error 2
>> make[1]: Leaving directory `/local/download/src/slony1-2.0.4.rc2/src'
>> make: *** [all] Error 2
> 
> 
> This is actually a bug in the distclean rule in slony_logshipper.  Our 
> policy has been to require bison/flex for cvs builds but to include the 
> generated files in the tarfiles.  The distclean rule seems to be 
> deleting the scan.l file.
> 
> If you install flex you can avoid that error.   We will include those 
> files in the next tarfile (technically this means we should have a rc3 
> but I'd like to see what else people find)
> 
> Thanks
> 
>>
>> There is a scan.c under src/slonik/
>>
>> Steve Singer wrote:
>>> Joseph S wrote:
>>>> Should I blow away my old slave database and rereplicate from 
>>>> scratch?  Or can I just upgrade slony and start it up?
>>>>
>>>> Steve Singer wrote:
>>>>> I have tagged Slony-I 2.0.4 RC2
>>>>>
>>>>> This is like 2.0.4 RC1 plus the memory leak fixes discussed on the 
>>>>> list.
>>>>>
>>>>> http://lists.slony.info/downloads/2.0/source/slony1-2.0.4.rc2.tar.bz2
>>>>>
>>>>> It would be nice if people could test it and report back to the list.
>>>>>
>>>>>
>>>
>>>
>>> As long as your test database was in a resonable state the upgrade 
>>> from a previous 2.0.x release should work fine.  Please report any 
>>> issues you encounter
>>>
>>>
>>>>>
>>>>>
>>>>>   
>>>> _______________________________________________
>>>> Slony1-general mailing list
>>>> Slony1-general at lists.slony.info
>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>
>>>
>>
> 
> 


From jks at selectacast.net  Thu Jun 17 14:58:36 2010
From: jks at selectacast.net (Joseph S)
Date: Thu, 17 Jun 2010 17:58:36 -0400
Subject: [Slony1-general] 2.0.4 RC2
In-Reply-To: <4C1A986A.2010004@selectacast.net>
References: <4C052672.9000207@ca.afilias.info>	<4C1A5F07.9060700@selectacast.net>
	<4C1A6057.30206@ca.afilias.info>	<4C1A89F0.5060906@selectacast.net>	<4C1A90C8.4050501@ca.afilias.info>
	<4C1A986A.2010004@selectacast.net>
Message-ID: <4C1A9A8C.1030400@selectacast.net>

It eventually died with:

2010-06-17 17:53:06 EDTFATAL  localListenThread: "select 
"_devel".cleanupNodelock(); insert into "_devel".sl_nodelock values (    
2, 0, "pg_catalog".pg_backend_pid()); " - ERROR:  duplicate key value 
violates unique constraint "sl_nodelock-pkey"

2010-06-17 17:53:06 EDTINFO   slon: shutdown requested
2010-06-17 17:53:06 EDTINFO   slon: notify worker process to shutdown
2010-06-17 17:53:26 EDTINFO   slon: child termination timeout - kill child
2010-06-17 17:53:26 EDTCONFIG slon: child terminated status: 9; pid: 
23878, current worker pid: 23878
2010-06-17 17:53:26 EDTINFO   slon: done
2010-06-17 17:53:26 EDTINFO   slon: exit(0)


Joseph S wrote:
> I'm having a problem with 2.0.4 but it isn't related to the problem I 
> was having with 2.0.3.  I had a table that I kept creating, adding to 
> the slony replication set, and then dropping and starting over.  During 
> this time I wasn't running slony because of the bugs in 2.0.3.  Now that 
> 2.0.4 rc2 is running it got stuck in a loop trying to create this table 
> and failing because it existed already.  I kept dropping the table from 
> my slave until that wasn't a problem anymore but now it is stuck in a 
> loop doing this:
>
>
> ERROR:  Slony-I: alterTableDropTriggers(): Table with id 41 not found
> CONTEXT:  SQL statement "SELECT  "_devel".alterTableDropTriggers( $1 )"
>          PL/pgSQL function "dropset_int" line 18 at PERFORM
> STATEMENT:  select "_devel".dropSet_int(999); insert into 
> "_devel".sl_event     (ev_origin, ev_seqno, ev_timestamp, 
> ev_snapshot, ev_type , ev_data1    ) values ('1', '5000048246', 
> '2010-04-27 18:44:50.666916', '720295:720295:', 'DROP_SET', '999'); 
> insert into "_devel".sl_confirm   (con_origin, con_received, con_seqno, 
> con_timestamp)    values (1, 2, '5000048246', now()); commit transaction;
> LOG:  unexpected EOF on client connection
>
> How can I break it out of this loop?
>
> Steve Singer wrote:
>   
>> Joseph S wrote:
>>     
>>> I can't compile 2.0.4 rc2
>>>
>>> make[2]: *** [scan.o] Error 1
>>> make[2]: Leaving directory 
>>> `/local/download/src/slony1-2.0.4.rc2/src/slony_logshipper'
>>> make[1]: *** [all] Error 2
>>> make[1]: Leaving directory `/local/download/src/slony1-2.0.4.rc2/src'
>>> make: *** [all] Error 2
>>>       
>> This is actually a bug in the distclean rule in slony_logshipper.  Our 
>> policy has been to require bison/flex for cvs builds but to include the 
>> generated files in the tarfiles.  The distclean rule seems to be 
>> deleting the scan.l file.
>>
>> If you install flex you can avoid that error.   We will include those 
>> files in the next tarfile (technically this means we should have a rc3 
>> but I'd like to see what else people find)
>>
>> Thanks
>>
>>     
>>> There is a scan.c under src/slonik/
>>>
>>> Steve Singer wrote:
>>>       
>>>> Joseph S wrote:
>>>>         
>>>>> Should I blow away my old slave database and rereplicate from 
>>>>> scratch?  Or can I just upgrade slony and start it up?
>>>>>
>>>>> Steve Singer wrote:
>>>>>           
>>>>>> I have tagged Slony-I 2.0.4 RC2
>>>>>>
>>>>>> This is like 2.0.4 RC1 plus the memory leak fixes discussed on the 
>>>>>> list.
>>>>>>
>>>>>> http://lists.slony.info/downloads/2.0/source/slony1-2.0.4.rc2.tar.bz2
>>>>>>
>>>>>> It would be nice if people could test it and report back to the list.
>>>>>>
>>>>>>
>>>>>>             
>>>> As long as your test database was in a resonable state the upgrade 
>>>> from a previous 2.0.x release should work fine.  Please report any 
>>>> issues you encounter
>>>>
>>>>
>>>>         
>>>>>>   
>>>>>>             
>>>>> _______________________________________________
>>>>> Slony1-general mailing list
>>>>> Slony1-general at lists.slony.info
>>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>>>           
>>>>         
>>     
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>   

From ssinger at ca.afilias.info  Thu Jun 17 15:02:00 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 17 Jun 2010 18:02:00 -0400
Subject: [Slony1-general] 2.0.4 RC2
In-Reply-To: <4C1A986A.2010004@selectacast.net>
References: <4C052672.9000207@ca.afilias.info>
	<4C1A5F07.9060700@selectacast.net> <4C1A6057.30206@ca.afilias.info>
	<4C1A89F0.5060906@selectacast.net>
	<4C1A90C8.4050501@ca.afilias.info>
	<4C1A986A.2010004@selectacast.net>
Message-ID: <4C1A9B58.5020601@ca.afilias.info>

Joseph S wrote:
> I'm having a problem with 2.0.4 but it isn't related to the problem I 
> was having with 2.0.3.  I had a table that I kept creating, adding to 
> the slony replication set, and then dropping and starting over.  During 
> this time I wasn't running slony because of the bugs in 2.0.3.  Now that 
> 2.0.4 rc2 is running it got stuck in a loop trying to create this table 
> and failing because it existed already.  I kept dropping the table from 
> my slave until that wasn't a problem anymore but now it is stuck in a 
> loop doing this:

When you say 'and then dropping and starting over'  exactly how are you 
dropping it?

The 'proper' way is to do a 'set drop table' via slonik and then drop 
the table.    If your doing a SQL DROP TABLE first, then you are 
probably hitting bug # 128 
http://bugs.slony.info/bugzilla/show_bug.cgi?id=128





> 
> 
> ERROR:  Slony-I: alterTableDropTriggers(): Table with id 41 not found
> CONTEXT:  SQL statement "SELECT  "_devel".alterTableDropTriggers( $1 )"
>         PL/pgSQL function "dropset_int" line 18 at PERFORM
> STATEMENT:  select "_devel".dropSet_int(999); insert into 
> "_devel".sl_event     (ev_origin, ev_seqno, ev_timestamp, ev_snapshot, 
> ev_type , ev_data1    ) values ('1', '5000048246', '2010-04-27 
> 18:44:50.666916', '720295:720295:', 'DROP_SET', '999'); insert into 
> "_devel".sl_confirm   (con_origin, con_received, con_seqno, 
> con_timestamp)    values (1, 2, '5000048246', now()); commit transaction;
> LOG:  unexpected EOF on client connection
> 
> How can I break it out of this loop?
> 
> Steve Singer wrote:
>> Joseph S wrote:
>>> I can't compile 2.0.4 rc2
>>>
>>> make[2]: *** [scan.o] Error 1
>>> make[2]: Leaving directory 
>>> `/local/download/src/slony1-2.0.4.rc2/src/slony_logshipper'
>>> make[1]: *** [all] Error 2
>>> make[1]: Leaving directory `/local/download/src/slony1-2.0.4.rc2/src'
>>> make: *** [all] Error 2
>>
>>
>> This is actually a bug in the distclean rule in slony_logshipper.  Our 
>> policy has been to require bison/flex for cvs builds but to include 
>> the generated files in the tarfiles.  The distclean rule seems to be 
>> deleting the scan.l file.
>>
>> If you install flex you can avoid that error.   We will include those 
>> files in the next tarfile (technically this means we should have a rc3 
>> but I'd like to see what else people find)
>>
>> Thanks
>>
>>>
>>> There is a scan.c under src/slonik/
>>>
>>> Steve Singer wrote:
>>>> Joseph S wrote:
>>>>> Should I blow away my old slave database and rereplicate from 
>>>>> scratch?  Or can I just upgrade slony and start it up?
>>>>>
>>>>> Steve Singer wrote:
>>>>>> I have tagged Slony-I 2.0.4 RC2
>>>>>>
>>>>>> This is like 2.0.4 RC1 plus the memory leak fixes discussed on the 
>>>>>> list.
>>>>>>
>>>>>> http://lists.slony.info/downloads/2.0/source/slony1-2.0.4.rc2.tar.bz2
>>>>>>
>>>>>> It would be nice if people could test it and report back to the list.
>>>>>>
>>>>>>
>>>>
>>>>
>>>> As long as your test database was in a resonable state the upgrade 
>>>> from a previous 2.0.x release should work fine.  Please report any 
>>>> issues you encounter
>>>>
>>>>
>>>>>>
>>>>>>
>>>>>>   
>>>>> _______________________________________________
>>>>> Slony1-general mailing list
>>>>> Slony1-general at lists.slony.info
>>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>>
>>>>
>>>
>>
>>
> 


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From jks at selectacast.net  Thu Jun 17 15:46:04 2010
From: jks at selectacast.net (Joseph S)
Date: Thu, 17 Jun 2010 18:46:04 -0400
Subject: [Slony1-general] 2.0.4 RC2
In-Reply-To: <4C1A9B58.5020601@ca.afilias.info>
References: <4C052672.9000207@ca.afilias.info>
	<4C1A5F07.9060700@selectacast.net> <4C1A6057.30206@ca.afilias.info>
	<4C1A89F0.5060906@selectacast.net>
	<4C1A90C8.4050501@ca.afilias.info>
	<4C1A986A.2010004@selectacast.net>
	<4C1A9B58.5020601@ca.afilias.info>
Message-ID: <4C1AA5AC.1080200@selectacast.net>

Steve Singer wrote:
> Joseph S wrote:
>> I'm having a problem with 2.0.4 but it isn't related to the problem I 
>> was having with 2.0.3.  I had a table that I kept creating, adding to 
>> the slony replication set, and then dropping and starting over.  
>> During this time I wasn't running slony because of the bugs in 2.0.3.  
>> Now that 2.0.4 rc2 is running it got stuck in a loop trying to create 
>> this table and failing because it existed already.  I kept dropping 
>> the table from my slave until that wasn't a problem anymore but now it 
>> is stuck in a loop doing this:
> 
> When you say 'and then dropping and starting over'  exactly how are you 
> dropping it?
> 
> The 'proper' way is to do a 'set drop table' via slonik and then drop 
> the table.    If your doing a SQL DROP TABLE first, then you are 
> probably hitting bug # 128 
> http://bugs.slony.info/bugzilla/show_bug.cgi?id=128
> 
Yup, that's exactly what I did.

From ssinger at ca.afilias.info  Fri Jun 18 05:15:05 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 18 Jun 2010 08:15:05 -0400
Subject: [Slony1-general] High memory usage in 'Slon' process
In-Reply-To: <478174F395A7E34BBD3891A50303D602DCAE15@ex3-del1.synapse.com>
References: <478174F395A7E34BBD3891A50303D602DCAE15@ex3-del1.synapse.com>
Message-ID: <4C1B6349.6040706@ca.afilias.info>

Vivek Gupta wrote:
> We have upgrade the Slony from 1.1.5 to 2.0.4rc2 and additionally the 
> platform has been migrated from 32-bit to 64-bit. To our strange ?slon? 
> process is taking ~250 MB of memory although its usage was in KBs 
> earlier. What could be the root cause either slony upgrade or the 
> platform change?

How many nodes are in your cluster and how many paths between the nodes 
are defined?

One large factor in how much memory slon will use is the number of 
connections that slon has to make to remote databases.  For each path 
between the node the slon is assinged to it will need 2 threads per path 
+ a libpq connection.

Another factor that might be influencing what you see it that I think in 
1.1.5 you would have had to manually add listeners to your cluster while 
in 2.0 (and 1.2 I think) you get listeners for each path.

On 32 bit linux I'm seeing a slon with paths  to 2 other nodes using 
just under 70megs of  virtual size while the same test with 64bit linux 
is using about 144megs.   (This is with 2.0.4 but 1.2.21 seems to have 
memory usage in the same range)

Memory usage (particularly with many paths) seems higher than I would 
like it to be.   Why we use so much memory should be investigated.







> 
>  
> 
> With regards,
> 
> Vivek Gupta
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From vivek at khera.org  Fri Jun 18 06:54:29 2010
From: vivek at khera.org (Vick Khera)
Date: Fri, 18 Jun 2010 09:54:29 -0400
Subject: [Slony1-general] 2.0.4 RC2
In-Reply-To: <4C1AA5AC.1080200@selectacast.net>
References: <4C052672.9000207@ca.afilias.info>
	<4C1A5F07.9060700@selectacast.net> <4C1A6057.30206@ca.afilias.info>
	<4C1A89F0.5060906@selectacast.net>
	<4C1A90C8.4050501@ca.afilias.info>
	<4C1A986A.2010004@selectacast.net>
	<4C1A9B58.5020601@ca.afilias.info>
	<4C1AA5AC.1080200@selectacast.net>
Message-ID: <AANLkTimSnPm__9zAf3WxtEpQWKQm9iAQ-MQBoszj73FD@mail.gmail.com>

On Thu, Jun 17, 2010 at 6:46 PM, Joseph S <jks at selectacast.net> wrote:
> Yup, that's exactly what I did.
>

So you were using the slony commands and had the slony schema
installed, but were not running slon daemons all this time?   You
should drop all slony stuff before you continue so you can start
fresh.  You have a lot of stuff clogged into the slony work queue.

From simon at 2ndQuadrant.com  Fri Jun 18 09:52:27 2010
From: simon at 2ndQuadrant.com (Simon Riggs)
Date: Fri, 18 Jun 2010 17:52:27 +0100
Subject: [Slony1-general] Reasons to upgrade to Slony 2.0
Message-ID: <1276879947.23257.85510.camel@ebony>


I'm struggling to find an official answer to a fairly simple question:

Why would I upgrade to Slony 2.0?

That doesn't seem to be mentioned anywhere, I'm pretty sure I remember
reading it somewhere.

I'm thinking by the time Slony 2.0 comes out, the new LISTEN/NOTIFY code
in Postgres 9.0 gives much reduced reason to upgrade.

(Yes, I know the current status of 2.0.x - that is not the question)

Thanks,

-- 
 Simon Riggs           www.2ndQuadrant.com
 PostgreSQL Development, 24x7 Support, Training and Services


From ssinger at ca.afilias.info  Fri Jun 18 10:01:05 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 18 Jun 2010 13:01:05 -0400
Subject: [Slony1-general] Reasons to upgrade to Slony 2.0
In-Reply-To: <1276879947.23257.85510.camel@ebony>
References: <1276879947.23257.85510.camel@ebony>
Message-ID: <4C1BA651.9080406@ca.afilias.info>

Simon Riggs wrote:
> I'm struggling to find an official answer to a fairly simple question:
> 
> Why would I upgrade to Slony 2.0?
> 
> That doesn't seem to be mentioned anywhere, I'm pretty sure I remember
> reading it somewhere.
> 
> I'm thinking by the time Slony 2.0 comes out, the new LISTEN/NOTIFY code
> in Postgres 9.0 gives much reduced reason to upgrade.
> 
> (Yes, I know the current status of 2.0.x - that is not the question)
> 
> Thanks,
> 


I gave a lightning talk on this at PGCon,  you can see the slides 
http://wiki.postgresql.org/wiki/Image:02_slony.pdf  (I think there 
should be a video of the lightning talks but I haven't seen them)

The big reasons to go to 2.0

-You can make DDL changes without execute script
-EXECUTE script doesn't automatically take an exclusive lock on every 
replicated table
-Slony no longer messes with the catalog, this means you can do things 
like take a pg_dump from a replica
-OMIT COPY and CLONE NODE allow you to populate your replicas through 
non slony means (ie pg_dump)





-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From vivek.gupta at globallogic.com  Fri Jun 18 10:03:01 2010
From: vivek.gupta at globallogic.com (Vivek Gupta)
Date: Fri, 18 Jun 2010 22:33:01 +0530
Subject: [Slony1-general] Replication time with 2.0.4rc2
Message-ID: <478174F395A7E34BBD3891A50303D602DCAEBF@ex3-del1.synapse.com>

We have upgrade the Slony from 1.1.5 to 2.0.4rc2 and additionally the
platform has been migrated from 32-bit to 64-bit. We have been observing
performance degradation in terms of the replication time. With the set
data earlier it was taking ~10 min to replicate now it's taking ~2 hours
for the same data set. Has anyone faced performance issues with
2.0.4rc2?

 

With regards,

Vivek Gupta

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100618/da1531db/attachment.htm 

From ssinger at ca.afilias.info  Fri Jun 18 11:51:42 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 18 Jun 2010 14:51:42 -0400
Subject: [Slony1-general] Replication time with 2.0.4rc2
In-Reply-To: <478174F395A7E34BBD3891A50303D602DCAEBF@ex3-del1.synapse.com>
References: <478174F395A7E34BBD3891A50303D602DCAEBF@ex3-del1.synapse.com>
Message-ID: <4C1BC03E.7020905@ca.afilias.info>

Vivek Gupta wrote:
> We have upgrade the Slony from 1.1.5 to 2.0.4rc2 and additionally the 
> platform has been migrated from 32-bit to 64-bit. We have been observing 
> performance degradation in terms of the replication time. With the set 
> data earlier it was taking ~10 min to replicate now it?s taking ~2 hours 
> for the same data set. Has anyone faced performance issues with 2.0.4rc2?

Hmm, any ideas why it is taking ~2 hours?

What is load like on your machines?
You mentioned higher memory usage for slon in your other post. Is this 
causing your machine to swap?

Have you also changed postgresql versions?  (I don't think you mentioned 
  which version of pg your running)  Are you using auto vacuum?

Do you have a lot of table bloat?  How many rows on average are in 
sl_log_1 and sl_log_2?  Does this number keep growing?




> 
>  
> 
> With regards,
> 
> Vivek Gupta
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From jaime at 2ndquadrant.com  Sat Jun 19 00:31:34 2010
From: jaime at 2ndquadrant.com (Jaime Casanova)
Date: Sat, 19 Jun 2010 02:31:34 -0500
Subject: [Slony1-general] [slony-general] dropping a table from a set
Message-ID: <AANLkTim-Yq1D9l21z2ztU_mDkW9sOXORhFWFjKVz4zIe@mail.gmail.com>

Hi,

Someone drop a FK constraint without using slonik (in slony1 1.2.20)
and inserted some rows in that table... some of those rows violate the
dropped constraint and obviously the replica can't insert the data
because in the replica the constraint still exist...

if i try to drop the constraint in the replica it says something about
a non related index (i guess this is because slony messed up
pg_trigger catalog for disabling triggers)...

i can't add the constraint in primary in order to properly drop it,
because there are rows violating the constraint... so i try to drop
the table from the replication set but because of the replica having
pending events that command is waiting in the queu...

so there is a way to solve this or i have to drop the replica and rebuild?

-- 
Jaime Casanova         www.2ndQuadrant.com
Soporte y capacitaci?n de PostgreSQL

From ssinger_pg at sympatico.ca  Sat Jun 19 06:03:10 2010
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Sat, 19 Jun 2010 09:03:10 -0400 (EDT)
Subject: [Slony1-general] [slony-general] dropping a table from a set
In-Reply-To: <AANLkTim-Yq1D9l21z2ztU_mDkW9sOXORhFWFjKVz4zIe@mail.gmail.com>
References: <AANLkTim-Yq1D9l21z2ztU_mDkW9sOXORhFWFjKVz4zIe@mail.gmail.com>
Message-ID: <BLU0-SMTP195ED5B0C60BDBC96A0C17ACC10@phx.gbl>

On Sat, 19 Jun 2010, Jaime Casanova wrote:

> Hi,
>
> Someone drop a FK constraint without using slonik (in slony1 1.2.20)
> and inserted some rows in that table... some of those rows violate the
> dropped constraint and obviously the replica can't insert the data
> because in the replica the constraint still exist...
>
> if i try to drop the constraint in the replica it says something about
> a non related index (i guess this is because slony messed up
> pg_trigger catalog for disabling triggers)...
>
> i can't add the constraint in primary in order to properly drop it,
> because there are rows violating the constraint... so i try to drop
> the table from the replication set but because of the replica having
> pending events that command is waiting in the queu...
>
> so there is a way to solve this or i have to drop the replica and rebuild?

Use EXECUTE SCRIPT with the ONLY ON option.

slonik will restore all of the tables on the slave to the normal state, run 
your script and put disable the triggers again.


You might also want to do an EXECUTE SCRIPT ONLY ON on the master (even if 
the script is something trivial like SELECT '1').   There are some types of 
DDL where if not done in an execute script your triggers will be left in a 
bad state until the next time execute script is done to reset things.



>
> -- 
> Jaime Casanova         www.2ndQuadrant.com
> Soporte y capacitaci?n de PostgreSQL
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>

From atsaloli.tech at gmail.com  Mon Jun 21 19:03:13 2010
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Mon, 21 Jun 2010 19:03:13 -0700
Subject: [Slony1-general] Does Slony 1 support PostgreSQL 8.4?
Message-ID: <AANLkTikf_uCTe3yeVqhx_k4LTYHjnB-JdAOc7poo1mQT@mail.gmail.com>

Hi.  We've been running PostgreSQL 8.4.x with Slony 2.0.2 but want to
move to Slony 1.2.21 due to the potential data loss issue with Slony 2.0.2.

However, Slony 1.2.21 README says that highest supported version of
PostgreSQL is 8.2.x.

Is that information up to date?  Or is 8.4 supported?

(I have been running Slony 1.2.20 / PostgreSQL 8.4.4 in my development
environment for a week using the RPM's from the PGDG84 YUM repo,
and it's been working fine, but I don't want to deploy an unsupported
configuration.)

Sincerely,
Aleksey

From bench at silentmedia.com  Mon Jun 21 21:34:08 2010
From: bench at silentmedia.com (Ben Chobot)
Date: Mon, 21 Jun 2010 21:34:08 -0700
Subject: [Slony1-general] Does Slony 1 support PostgreSQL 8.4?
In-Reply-To: <AANLkTikf_uCTe3yeVqhx_k4LTYHjnB-JdAOc7poo1mQT@mail.gmail.com>
References: <AANLkTikf_uCTe3yeVqhx_k4LTYHjnB-JdAOc7poo1mQT@mail.gmail.com>
Message-ID: <2CCDD398-D045-4974-857B-E9F8F4ADCA9F@silentmedia.com>

On Jun 21, 2010, at 7:03 PM, Aleksey Tsalolikhin wrote:

> However, Slony 1.2.21 README says that highest supported version of
> PostgreSQL is 8.2.x.
> 
> Is that information up to date?  Or is 8.4 supported?

I can't speak towards official support, but we use a whole lot of slony 1.2.20 on 8.4 and it's working great for us.

From scott.marlowe at gmail.com  Mon Jun 21 23:06:23 2010
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Tue, 22 Jun 2010 00:06:23 -0600
Subject: [Slony1-general] Does Slony 1 support PostgreSQL 8.4?
In-Reply-To: <AANLkTikf_uCTe3yeVqhx_k4LTYHjnB-JdAOc7poo1mQT@mail.gmail.com>
References: <AANLkTikf_uCTe3yeVqhx_k4LTYHjnB-JdAOc7poo1mQT@mail.gmail.com>
Message-ID: <AANLkTimPdqzf7Wr1UZB3wjwNbnrt9ganoQr9x3z89NlG@mail.gmail.com>

On Mon, Jun 21, 2010 at 8:03 PM, Aleksey Tsalolikhin
<atsaloli.tech at gmail.com> wrote:
> Hi. ?We've been running PostgreSQL 8.4.x with Slony 2.0.2 but want to
> move to Slony 1.2.21 due to the potential data loss issue with Slony 2.0.2.
>
> However, Slony 1.2.21 README says that highest supported version of
> PostgreSQL is 8.2.x.

My slony 1.2.21 README says nothing of the sort.

My INSTALL document, which has a datestamp of 2007-08-28 23:44 says it
has changes to support features expected in 8.3.  Which means this
file hasn't been updated since before 8.3 went official relase.

> Is that information up to date?

Computer says no.

> Or is 8.4 supported?

time to look for more up to date docs.  Go to the front page of slony,
click on the link on the right that says:
http://main.slony.info/downloads/1.2/source/slony1-1.2.21-docs.tar.bz2
and get a 404!  Woohoo.  Copy and paste that url, changing 21 to 20,
and viola, I get a download.

installation.html specifically mentions 8.3 and 8.4 and seem to imply
they're supports.

> (I have been running Slony 1.2.20 / PostgreSQL 8.4.4 in my development
> environment for a week using the RPM's from the PGDG84 YUM repo,
> and it's been working fine, but I don't want to deploy an unsupported
> configuration.)

I'm pretty sure it is considered supported.  It certainly works well
in my 8.3 cluster and has for two years straight, with bug fixes
against slony 1.2.x against all versions up to 8.4 being supported.

I wonder if it will be considered supported for 9.0.?

From ssinger at ca.afilias.info  Tue Jun 22 05:07:16 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 22 Jun 2010 08:07:16 -0400
Subject: [Slony1-general] Does Slony 1 support PostgreSQL 8.4?
In-Reply-To: <AANLkTikf_uCTe3yeVqhx_k4LTYHjnB-JdAOc7poo1mQT@mail.gmail.com>
References: <AANLkTikf_uCTe3yeVqhx_k4LTYHjnB-JdAOc7poo1mQT@mail.gmail.com>
Message-ID: <4C20A774.7040505@ca.afilias.info>

Aleksey Tsalolikhin wrote:
> Hi.  We've been running PostgreSQL 8.4.x with Slony 2.0.2 but want to
> move to Slony 1.2.21 due to the potential data loss issue with Slony 2.0.2.
> 
> However, Slony 1.2.21 README says that highest supported version of
> PostgreSQL is 8.2.x.
> 
> Is that information up to date?  Or is 8.4 supported?
> 

Slony 1.2.21 should be be fine with 8.4 and can be considered a 
supported version of postgresql.  I'm not aware of any current issues 
with slony and 8.4 but if you hit anything then you should bring it to 
our attention so we can look into it.  It should also work with 9.0 but 
we haven't done much testing of this combination (changes to get slony 
to compile against 9.0 are included in .21)

I've done a fair amount of testing against 2.0.4 and 9.0.  As long as 
your using 9.0 beta2 or above it seems to work okay (so far).  There was 
a Pg bug in beta1 that slony would sometimes hit but this has been fixed 
with beta2.  Strictly speaking 9.0 is still unsupported because it 
hasn't been released.


> (I have been running Slony 1.2.20 / PostgreSQL 8.4.4 in my development
> environment for a week using the RPM's from the PGDG84 YUM repo,
> and it's been working fine, but I don't want to deploy an unsupported
> configuration.)
> 
> Sincerely,
> Aleksey
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From devrim at gunduz.org  Tue Jun 22 05:07:58 2010
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Tue, 22 Jun 2010 15:07:58 +0300
Subject: [Slony1-general] Does Slony 1 support PostgreSQL 8.4?
In-Reply-To: <4C20A774.7040505@ca.afilias.info>
References: <AANLkTikf_uCTe3yeVqhx_k4LTYHjnB-JdAOc7poo1mQT@mail.gmail.com>
	<4C20A774.7040505@ca.afilias.info>
Message-ID: <1277208478.6252.9.camel@hp-laptop2.gunduz.org>

On Tue, 2010-06-22 at 08:07 -0400, Steve Singer wrote:
> Slony 1.2.21 should be be fine with 8.4 and can be considered a 
> supported version of postgresql. 

We are using this combination on our prod servers under load, and it
works w/o any issues.
-- 
Devrim G?ND?Z
PostgreSQL Dan??man?/Consultant, Red Hat Certified Engineer
PostgreSQL RPM Repository: http://yum.pgrpms.org
Community: devrim~PostgreSQL.org, devrim.gunduz~linux.org.tr
http://www.gunduz.org  Twitter: http://twitter.com/devrimgunduz
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 198 bytes
Desc: This is a digitally signed message part
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20100622/e42e5887/attachment.pgp 

From jks at selectacast.net  Tue Jun 22 09:49:11 2010
From: jks at selectacast.net (Joseph S)
Date: Tue, 22 Jun 2010 12:49:11 -0400
Subject: [Slony1-general] 2.0.4 RC2
In-Reply-To: <4C1AA5AC.1080200@selectacast.net>
References: <4C052672.9000207@ca.afilias.info>	<4C1A5F07.9060700@selectacast.net>
	<4C1A6057.30206@ca.afilias.info>	<4C1A89F0.5060906@selectacast.net>	<4C1A90C8.4050501@ca.afilias.info>	<4C1A986A.2010004@selectacast.net>	<4C1A9B58.5020601@ca.afilias.info>
	<4C1AA5AC.1080200@selectacast.net>
Message-ID: <4C20E987.4060800@selectacast.net>

I dropped and recreated the slony installation to get around the 
problems I had created, and everything has been working normally for a 
few days.  +1 on 2.0.4 RC2

Joseph S wrote:
> Steve Singer wrote:
>   
>> Joseph S wrote:
>>     
>>> I'm having a problem with 2.0.4 but it isn't related to the problem I 
>>> was having with 2.0.3.  I had a table that I kept creating, adding to 
>>> the slony replication set, and then dropping and starting over.  
>>> During this time I wasn't running slony because of the bugs in 2.0.3.  
>>> Now that 2.0.4 rc2 is running it got stuck in a loop trying to create 
>>> this table and failing because it existed already.  I kept dropping 
>>> the table from my slave until that wasn't a problem anymore but now it 
>>> is stuck in a loop doing this:
>>>       
>> When you say 'and then dropping and starting over'  exactly how are you 
>> dropping it?
>>
>> The 'proper' way is to do a 'set drop table' via slonik and then drop 
>> the table.    If your doing a SQL DROP TABLE first, then you are 
>> probably hitting bug # 128 
>> http://bugs.slony.info/bugzilla/show_bug.cgi?id=128
>>
>>     
> Yup, that's exactly what I did.
>   

From atsaloli.tech at gmail.com  Tue Jun 22 11:16:39 2010
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Tue, 22 Jun 2010 11:16:39 -0700
Subject: [Slony1-general] Does Slony 1 support PostgreSQL 8.4?
In-Reply-To: <1277208478.6252.9.camel@hp-laptop2.gunduz.org>
References: <AANLkTikf_uCTe3yeVqhx_k4LTYHjnB-JdAOc7poo1mQT@mail.gmail.com>
	<4C20A774.7040505@ca.afilias.info>
	<1277208478.6252.9.camel@hp-laptop2.gunduz.org>
Message-ID: <AANLkTiniKSCWXFTBKdoRdFjjH3W30BnvACCmx4gpJq74@mail.gmail.com>

2010/6/22 Devrim G?ND?Z <devrim at gunduz.org>:
> On Tue, 2010-06-22 at 08:07 -0400, Steve Singer wrote:
>> Slony 1.2.21 should be be fine with 8.4 and can be considered a
>> supported version of postgresql.
>
> We are using this combination on our prod servers under load, and it
> works w/o any issues.

Dear all,

  Thanks, that's good to hear!

  Dear Devrim,

  I see "devrim" in the RCS id for the INSTALL doc, may I offer a
patch to bring the supported versions list up to date?

Sincerely,
Aleksey

*** INSTALL     2007-08-28 22:44:57.000000000 -0700
--- INSTALL.new 2010-06-22 11:12:26.000000000 -0700
***************
*** 4,12 ****
  $Id: INSTALL,v 1.12.2.2 2007-08-29 05:44:57 devrim Exp $

  Slony-I currently supports PostgreSQL 7.4.0 (and higher), 8.0.x,
! 8.1.x, and 8.2.x.  There have also been changes made to support
! features expected in 8.3.  The "UPDATE FUNCTIONS" command has some
! problems on 8.1.0 thru 8.1.3; see the FAQ for how to cope with this.

  Note that earlier versions supported versions in the 7.3.x series; as
  of Slony-I 1.2.0, 7.3 support has been dropped.
--- 4,13 ----
  $Id: INSTALL,v 1.12.2.2 2007-08-29 05:44:57 devrim Exp $

  Slony-I currently supports PostgreSQL 7.4.0 (and higher), 8.0.x,
! 8.1.x, 8.2.x, 8.3.x and 8.4.x.
!
! The "UPDATE FUNCTIONS" command has some problems on 8.1.0 thru 8.1.3;
! see the FAQ for how to cope with this.

  Note that earlier versions supported versions in the 7.3.x series; as
  of Slony-I 1.2.0, 7.3 support has been dropped.

From atsaloli.tech at gmail.com  Tue Jun 22 11:40:00 2010
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Tue, 22 Jun 2010 11:40:00 -0700
Subject: [Slony1-general] trouble building slony1 1.2.21 for EnterpriseDB
	Postgres 8.4.2
Message-ID: <AANLkTimZp9YJ4DE3RVqL4ARrUANNBuTEz1peOm7i5cDA@mail.gmail.com>

Hi.  We're running Postgres 8.4.2 (installed with EnterpriseDB
installation wizard).

I am trying to build slony 1.2.21, and it bombs out saying my Postgres
version is less than 7.3.

Any suggestions?

./configure --with-pgconfigdir=/opt/PostgreSQL/8.4/bin/
--prefix=/opt/slony1-1.2.21
...
checking for pg_config... /opt/PostgreSQL/8.4/bin//pg_config
pg_config says pg_bindir is /opt/PostgreSQL/8.4/bin/
pg_config says pg_libdir is /opt/PostgreSQL/8.4/lib/
pg_config says pg_includedir is /opt/PostgreSQL/8.4/include/
pg_config says pg_pkglibdir is /opt/PostgreSQL/8.4/lib/postgresql/
pg_config says pg_includeserverdir is
/opt/PostgreSQL/8.4/include/postgresql/server/
checking for correct version of PostgreSQL... 8.4
pg_config says pg_sharedir is /opt/PostgreSQL/8.4/share/postgresql/
checking for PQunescapeBytea in -lpq... no
configure: error: Your version of libpq doesn't have PQunescapeBytea
     this means that your version of PostgreSQL is lower than 7.3
     and thus not supported by Slony-I.


I've run the "./configure" through an strace and it's picking up
/opt/PostgreSQL/8.4/lib/libpq.so

Thanks,
-at

From cbbrowne at ca.afilias.info  Tue Jun 22 14:24:09 2010
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue, 22 Jun 2010 17:24:09 -0400
Subject: [Slony1-general] trouble building slony1 1.2.21 for
	EnterpriseDB Postgres 8.4.2
In-Reply-To: <AANLkTimZp9YJ4DE3RVqL4ARrUANNBuTEz1peOm7i5cDA@mail.gmail.com>
	(Aleksey Tsalolikhin's message of "Tue,
	22 Jun 2010 11:40:00 -0700")
References: <AANLkTimZp9YJ4DE3RVqL4ARrUANNBuTEz1peOm7i5cDA@mail.gmail.com>
Message-ID: <871vbyhibq.fsf@cbbrowne-laptop.afilias-int.info>

Aleksey Tsalolikhin <atsaloli.tech at gmail.com> writes:
> Hi.  We're running Postgres 8.4.2 (installed with EnterpriseDB
> installation wizard).
>
> I am trying to build slony 1.2.21, and it bombs out saying my Postgres
> version is less than 7.3.
>
> Any suggestions?
>
> ./configure --with-pgconfigdir=/opt/PostgreSQL/8.4/bin/
> --prefix=/opt/slony1-1.2.21
> ...
> checking for pg_config... /opt/PostgreSQL/8.4/bin//pg_config
> pg_config says pg_bindir is /opt/PostgreSQL/8.4/bin/
> pg_config says pg_libdir is /opt/PostgreSQL/8.4/lib/
> pg_config says pg_includedir is /opt/PostgreSQL/8.4/include/
> pg_config says pg_pkglibdir is /opt/PostgreSQL/8.4/lib/postgresql/
> pg_config says pg_includeserverdir is
> /opt/PostgreSQL/8.4/include/postgresql/server/
> checking for correct version of PostgreSQL... 8.4
> pg_config says pg_sharedir is /opt/PostgreSQL/8.4/share/postgresql/
> checking for PQunescapeBytea in -lpq... no
> configure: error: Your version of libpq doesn't have PQunescapeBytea
>      this means that your version of PostgreSQL is lower than 7.3
>      and thus not supported by Slony-I.
>
>
> I've run the "./configure" through an strace and it's picking up
> /opt/PostgreSQL/8.4/lib/libpq.so

Well, configure didn't find PQunescapeBytea() when checking out libpq.

It's doubtless NOT a problem of running PostgreSQL 7.2, and I think
I've changed Configure in later releases to NOT give that diagnosis.
It's probably that it couldn't find libpq to link against, or some
such thing.

There's probably some further clue as to why in config.log, which is
generated by the configure script.  Look near the bottom for an error
message.  Alas, configure is verbose enough that the relevant error is
probably 100 lines before the end of the log file :-(.
-- 
"cbbrowne","@","ca.afilias.info"
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)

From atsaloli.tech at gmail.com  Tue Jun 22 15:21:07 2010
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Tue, 22 Jun 2010 15:21:07 -0700
Subject: [Slony1-general] trouble building slony1 1.2.21 for
	EnterpriseDB Postgres 8.4.2
In-Reply-To: <871vbyhibq.fsf@cbbrowne-laptop.afilias-int.info>
References: <AANLkTimZp9YJ4DE3RVqL4ARrUANNBuTEz1peOm7i5cDA@mail.gmail.com>
	<871vbyhibq.fsf@cbbrowne-laptop.afilias-int.info>
Message-ID: <AANLkTilpp1uqxR8WQTDUiFH6pD6xZ4R9XStBWvWgfp9d@mail.gmail.com>

Thanks, Christopher.  I did look in config.log and found this:

configure: caught signal 2
configure: exit 1

There is no mention of "pq" or "PQ" in config.log

I was not aware there was a release of Slony later than 1.2.21.  Are
you talking about a next version (development version) ?

Thanks,
Aleksey

From atsaloli.tech at gmail.com  Tue Jun 22 15:23:45 2010
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Tue, 22 Jun 2010 15:23:45 -0700
Subject: [Slony1-general] trouble building slony1 1.2.21 for
	EnterpriseDB Postgres 8.4.2
In-Reply-To: <AANLkTilpp1uqxR8WQTDUiFH6pD6xZ4R9XStBWvWgfp9d@mail.gmail.com>
References: <AANLkTimZp9YJ4DE3RVqL4ARrUANNBuTEz1peOm7i5cDA@mail.gmail.com>
	<871vbyhibq.fsf@cbbrowne-laptop.afilias-int.info>
	<AANLkTilpp1uqxR8WQTDUiFH6pD6xZ4R9XStBWvWgfp9d@mail.gmail.com>
Message-ID: <AANLkTinsVzcFCwGvJlMd-zuOXG_HivaIKGLyW3S6ZPQI@mail.gmail.com>

P.S.  Chris, you wrote:  It's probably that it couldn't find libpq to
link against, or some
such thing.

I did run the configure under strace and I see it opens libpq.so.

Just to double check, I tried setting gcc's LIBRARY_PATH variable:

LIBRARY_PATH=/opt/PostgreSQL/8.4/lib  ./configure
--with-pgconfigdir=/opt/PostgreSQL/8.4/bin/
--prefix=/opt/slony1-1.2.21
...
checking for PQunescapeBytea in -lpq... no
configure: error: Your version of libpq doesn't have PQunescapeBytea
     this means that your version of PostgreSQL is lower than 7.3
     and thus not supported by Slony-I.

Best,
-at

From ssinger at ca.afilias.info  Wed Jun 23 08:20:07 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 23 Jun 2010 11:20:07 -0400
Subject: [Slony1-general] trouble building slony1 1.2.21
 for	EnterpriseDB Postgres 8.4.2
In-Reply-To: <AANLkTinsVzcFCwGvJlMd-zuOXG_HivaIKGLyW3S6ZPQI@mail.gmail.com>
References: <AANLkTimZp9YJ4DE3RVqL4ARrUANNBuTEz1peOm7i5cDA@mail.gmail.com>	<871vbyhibq.fsf@cbbrowne-laptop.afilias-int.info>	<AANLkTilpp1uqxR8WQTDUiFH6pD6xZ4R9XStBWvWgfp9d@mail.gmail.com>
	<AANLkTinsVzcFCwGvJlMd-zuOXG_HivaIKGLyW3S6ZPQI@mail.gmail.com>
Message-ID: <4C222627.2000804@ca.afilias.info>

Aleksey Tsalolikhin wrote:

Find the exact error your getting in config.log

If I Download the EDB installer on and try to build slony against it I 
get the following:

configure:5192: gcc -o conftest -g -O2   -L/opt/PostgreSQL/8.4/lib/ 
conftest.c -lpq   >&5
/usr/bin/ld: warning: libssl.so.4, needed by 
/opt/PostgreSQL/8.4/lib//libpq.so, not found (try using -rpath or 
-rpath-link)
/usr/bin/ld: warning: libcrypto.so.4, needed by 
/opt/PostgreSQL/8.4/lib//libpq.so, not found (try using -rpath or 
-rpath-link)
/opt/PostgreSQL/8.4/lib//libpq.so: undefined reference to `TLSv1_method'
/opt/PostgreSQL/8.4/lib//libpq.so: undefined reference to `SSL_set_ex_data'
/opt/PostgreSQL/8.4/lib//libpq.so: undefined reference to `SSL_connect'

which is probably what you are getting.  It looks like EDB libpq was 
built against libssl which we are not linking to?



> P.S.  Chris, you wrote:  It's probably that it couldn't find libpq to
> link against, or some
> such thing.
> 
> I did run the configure under strace and I see it opens libpq.so.
> 
> Just to double check, I tried setting gcc's LIBRARY_PATH variable:
> 
> LIBRARY_PATH=/opt/PostgreSQL/8.4/lib  ./configure
> --with-pgconfigdir=/opt/PostgreSQL/8.4/bin/
> --prefix=/opt/slony1-1.2.21
> ...
> checking for PQunescapeBytea in -lpq... no
> configure: error: Your version of libpq doesn't have PQunescapeBytea
>      this means that your version of PostgreSQL is lower than 7.3
>      and thus not supported by Slony-I.
> 
> Best,
> -at
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From ssinger at ca.afilias.info  Wed Jun 23 09:55:23 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 23 Jun 2010 12:55:23 -0400
Subject: [Slony1-general] trouble building slony1 1.2.21
 for	EnterpriseDB Postgres 8.4.2
In-Reply-To: <4C222627.2000804@ca.afilias.info>
References: <AANLkTimZp9YJ4DE3RVqL4ARrUANNBuTEz1peOm7i5cDA@mail.gmail.com>	<871vbyhibq.fsf@cbbrowne-laptop.afilias-int.info>	<AANLkTilpp1uqxR8WQTDUiFH6pD6xZ4R9XStBWvWgfp9d@mail.gmail.com>	<AANLkTinsVzcFCwGvJlMd-zuOXG_HivaIKGLyW3S6ZPQI@mail.gmail.com>
	<4C222627.2000804@ca.afilias.info>
Message-ID: <4C223C7B.1060403@ca.afilias.info>

Steve Singer wrote:
> Aleksey Tsalolikhin wrote:
> 
> Find the exact error your getting in config.log
> 
> If I Download the EDB installer on and try to build slony against it I 
> get the following:
> 
> configure:5192: gcc -o conftest -g -O2   -L/opt/PostgreSQL/8.4/lib/ 
> conftest.c -lpq   >&5
> /usr/bin/ld: warning: libssl.so.4, needed by 
> /opt/PostgreSQL/8.4/lib//libpq.so, not found (try using -rpath or 
> -rpath-link)
> /usr/bin/ld: warning: libcrypto.so.4, needed by 
> /opt/PostgreSQL/8.4/lib//libpq.so, not found (try using -rpath or 
> -rpath-link)
> /opt/PostgreSQL/8.4/lib//libpq.so: undefined reference to `TLSv1_method'
> /opt/PostgreSQL/8.4/lib//libpq.so: undefined reference to `SSL_set_ex_data'
> /opt/PostgreSQL/8.4/lib//libpq.so: undefined reference to `SSL_connect'
> 
> which is probably what you are getting.  It looks like EDB libpq was 
> built against libssl which we are not linking to?
> 

Aleksey,

If this is the error that you are getting then to compile slony you should

1) Make sure that the ssl development package is installed on debian 
based systems it is named 'libssl-dev' I'm not sure what it is named on 
RedHat based systems (maybe the same?)

2) do  add "LIBS=-lssl" to your environment before calling configure. 
It should then link in the ssl library.





> 
> 
>> P.S.  Chris, you wrote:  It's probably that it couldn't find libpq to
>> link against, or some
>> such thing.
>>
>> I did run the configure under strace and I see it opens libpq.so.
>>
>> Just to double check, I tried setting gcc's LIBRARY_PATH variable:
>>
>> LIBRARY_PATH=/opt/PostgreSQL/8.4/lib  ./configure
>> --with-pgconfigdir=/opt/PostgreSQL/8.4/bin/
>> --prefix=/opt/slony1-1.2.21
>> ...
>> checking for PQunescapeBytea in -lpq... no
>> configure: error: Your version of libpq doesn't have PQunescapeBytea
>>      this means that your version of PostgreSQL is lower than 7.3
>>      and thus not supported by Slony-I.
>>
>> Best,
>> -at
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From ssinger at ca.afilias.info  Thu Jun 24 14:28:21 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 24 Jun 2010 17:28:21 -0400
Subject: [Slony1-general] Slony-I 2.0.4 released
Message-ID: <4C23CDF5.5040304@ca.afilias.info>

Slony-I version 2.0.4 has been released.

This version of Slony includes fixes for the memory corruption issues 
introduced in 2.0.3   2.0.3 users are encouraged to upgrade.

You can download the latest Slony release from http://www.slony.info

This release includes:

* Fixes to memory corruption issues with large rows (introduced in 2.0.3)
* Fixed syntax script error  (Bug # 108)
* Various documentation updates
* Supports newer versions of flex
* Fixes for CLONE NODE FINISH (Bug # 119)






-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From ssinger at ca.afilias.info  Thu Jun 24 14:28:21 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 24 Jun 2010 17:28:21 -0400
Subject: [Slony1-general] [ANNOUNCE] Slony-I 2.0.4 released - Sender
	blacklisted [Black_list]
Message-ID: <4C23CDF5.5040304@ca.afilias.info>

Slony-I version 2.0.4 has been released.

This version of Slony includes fixes for the memory corruption issues 
introduced in 2.0.3   2.0.3 users are encouraged to upgrade.

You can download the latest Slony release from http://www.slony.info

This release includes:

* Fixes to memory corruption issues with large rows (introduced in 2.0.3)
* Fixed syntax script error  (Bug # 108)
* Various documentation updates
* Supports newer versions of flex
* Fixes for CLONE NODE FINISH (Bug # 119)






-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

---------------------------(end of broadcast)---------------------------
-To unsubscribe from this list, send an email to:

               pgsql-announce-unsubscribe at postgresql.org

From Sachin.Ku at globallogic.com  Mon Jun 28 00:36:14 2010
From: Sachin.Ku at globallogic.com (Sachin Kumar)
Date: Mon, 28 Jun 2010 13:06:14 +0530
Subject: [Slony1-general] Performance issues with slony-2.0.4.rc2
Message-ID: <21026437C2BC0D45BE196CB0A35EFEFD0526FD1B@ex2-del1.synapse.com>

Hi,

 

We are using postgresql-8.4.0 on 64-bit Linux machine (open-SUSE 11.x).
It's a master/slave deployment & slony-2.0.4.rc2 is used for DB
replication on the slave box.

 

At times we have observed that postgres stops responding, even couldn't
fetch the number of entries in a particular table. One such instance
happens when we execute the

following steps:

-         Add few lakh entries (~ 20) to table X on the master DB.

-         After addition, slony starts replication on the slave DB. It
takes several minutes (~25 mins) for replication to finish.

-         During this time (while replication is in progress), postgres
stops responding, i.e. we couldn't even fetch the number of entries in
any table (X, Y, etc).

 

Can you please let us know:

-         Why it takes so long for replication? How it can be improved?

-         What could be reason for postgres to stop responding?

 

 

Thanks in advance!!

 

Regards,

Sachin

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100628/bc10dc61/attachment.htm 

From ian.lea at gmail.com  Mon Jun 28 06:39:06 2010
From: ian.lea at gmail.com (Ian Lea)
Date: Mon, 28 Jun 2010 14:39:06 +0100
Subject: [Slony1-general] Performance issues with slony-2.0.4.rc2
In-Reply-To: <21026437C2BC0D45BE196CB0A35EFEFD0526FD1B@ex2-del1.synapse.com>
References: <21026437C2BC0D45BE196CB0A35EFEFD0526FD1B@ex2-del1.synapse.com>
Message-ID: <AANLkTimBcLKmBZrfcqmppRmaejUMmDM8wkU2x8hLbE47@mail.gmail.com>

It seems likely that something other than slony is causing your
problems, but hard to know what.  Maybe the network or database
locking.  Some suggestions:

Use the released version of 2.0.4.  I don't know if anything changed
or not, but always best to use released versions where possible.

Before replicating anything, double check that postgres is working as
rapidly as you would expect, both master and slave, accessed from both
servers.

Double check the network comms between master and slave databases.


Then switch replication on and see what happens.



--
Ian.


On Mon, Jun 28, 2010 at 8:36 AM, Sachin Kumar <Sachin.Ku at globallogic.com> wrote:
> Hi,
>
>
>
> We are using postgresql-8.4.0 on 64-bit Linux machine (open-SUSE 11.x). It?s
> a master/slave deployment & slony-2.0.4.rc2 is used for DB replication on
> the slave box.
>
>
>
> At times we have observed that postgres stops responding, even couldn?t
> fetch the number of entries in a particular table. One such instance happens
> when we execute the
>
> following steps:
>
> -???????? Add few lakh entries (~ 20) to table X on the master DB.
>
> -???????? After addition, slony starts replication on the slave DB. It takes
> several minutes (~25 mins) for replication to finish.
>
> -???????? During this time (while replication is in progress), postgres
> stops responding, i.e. we couldn?t even fetch the number of entries in any
> table (X, Y, etc).
>
>
>
> Can you please let us know:
>
> -???????? Why it takes so long for replication? How it can be improved?
>
> -???????? What could be reason for postgres to stop responding?
>
>
>
>
>
> Thanks in advance!!
>
>
>
> Regards,
>
> Sachin
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>

From ssinger at ca.afilias.info  Mon Jun 28 06:41:06 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 28 Jun 2010 09:41:06 -0400
Subject: [Slony1-general] Performance issues with slony-2.0.4.rc2
In-Reply-To: <AANLkTimBcLKmBZrfcqmppRmaejUMmDM8wkU2x8hLbE47@mail.gmail.com>
References: <21026437C2BC0D45BE196CB0A35EFEFD0526FD1B@ex2-del1.synapse.com>
	<AANLkTimBcLKmBZrfcqmppRmaejUMmDM8wkU2x8hLbE47@mail.gmail.com>
Message-ID: <4C28A672.2080701@ca.afilias.info>

Ian Lea wrote:
> 
> Use the released version of 2.0.4.  I don't know if anything changed
> or not, but always best to use released versions where possible.

The only changes between 2.0.4rc2 and 2.0.4 where documentation and 
packaging related (no code changes).


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From ian.lea at gmail.com  Mon Jun 28 06:47:09 2010
From: ian.lea at gmail.com (Ian Lea)
Date: Mon, 28 Jun 2010 14:47:09 +0100
Subject: [Slony1-general] Performance issues with slony-2.0.4.rc2
In-Reply-To: <4C28A672.2080701@ca.afilias.info>
References: <21026437C2BC0D45BE196CB0A35EFEFD0526FD1B@ex2-del1.synapse.com> 
	<AANLkTimBcLKmBZrfcqmppRmaejUMmDM8wkU2x8hLbE47@mail.gmail.com> 
	<4C28A672.2080701@ca.afilias.info>
Message-ID: <AANLkTikMDhR9h1PePNWQbfYtpf_hQU-5-4CVH_ROuHY6@mail.gmail.com>

>> Use the released version of 2.0.4. ?I don't know if anything changed
>> or not, but always best to use released versions where possible.
>
> The only changes between 2.0.4rc2 and 2.0.4 where documentation and
> packaging related (no code changes).

OK, thanks for the info.

--
Ian.

From vivek at khera.org  Mon Jun 28 08:09:30 2010
From: vivek at khera.org (Vick Khera)
Date: Mon, 28 Jun 2010 11:09:30 -0400
Subject: [Slony1-general] Performance issues with slony-2.0.4.rc2
In-Reply-To: <21026437C2BC0D45BE196CB0A35EFEFD0526FD1B@ex2-del1.synapse.com>
References: <21026437C2BC0D45BE196CB0A35EFEFD0526FD1B@ex2-del1.synapse.com>
Message-ID: <AANLkTinm6VSoObiwuZUQv-4VQ1wysmdIh4KNyVuinQxH@mail.gmail.com>

On Mon, Jun 28, 2010 at 3:36 AM, Sachin Kumar <Sachin.Ku at globallogic.com> wrote:
> -???????? During this time (while replication is in progress), postgres
> stops responding, i.e. we couldn?t even fetch the number of entries in any
> table (X, Y, etc).

My guess is that your hardware is underpowered, and simply cannot
handle the load of doing all the replication work and the normal
queries.

From dimitarn at abv.bg  Wed Jun 30 02:00:13 2010
From: dimitarn at abv.bg (dimitarn)
Date: Wed, 30 Jun 2010 02:00:13 -0700 (PDT)
Subject: [Slony1-general] Slony + PGPool for HA/Failover only
In-Reply-To: <20091118134014.GA23083@shinkuro.com>
References: <20091118001529.GB81625@mr-paradox.net>
	<20091118134014.GA23083@shinkuro.com>
Message-ID: <29032292.post@talk.nabble.com>


Hi

I tried pgpool2 + slony-1 replication in master/slave and load_balance=false
and replicate_select = false mode. It works fine. The only big problem is
that i thought pgpool will send all my write queries to the master and all
my read queries to the slave. But i am not sure that it does this. Any help?



Andrew Sullivan-8 wrote:
> 
> On Tue, Nov 17, 2009 at 04:15:29PM -0800, David Kerr wrote:
>> I like the idea of load balancing, but how does that work, exactly, since
>> Slony 
>> is asynchronous? 
> 
> You handle read-only and read-write queries differently.  Read-only
> queries get a consistent but not necessarily current view of the
> data.  So if your pattern is "update database, read results" with web
> pages, it just won't work -- you need the latest version of the data.
> For environments where imperfect currency is ok (think most web pages,
> for instance), it'll work.  But be careful: I never got pgpool to
> work well for me.
> 
>> Second for an HA solution, my real concern, if the load balancing isn't
>> viable
>> (so to speak) can you configure PGPool to point to one node and then fail
>> over
>> to another node (automatically) ? I understand that's a PGPool specific
>> question
>> but since it seems like a common config with Slony I thought i'd chance
>> it 
>> and ask here.
> 
> The question here is how much data loss you're willing to take.
> Alternatively, how much read-write downtime can you stand?
> 
> If you MUST have an up-to-date view of the data, but if the read-write
> node fails you can just live with reads, then you're golden.  Set up
> two pools.  Pool 1 is for read-write and read, and handles all the
> data under normal circumstances.  Pool 2 is a standby, and if the
> database on pool 1 is lost, it takes over.  Write transactions on this
> pool always fail, but it will allow you to do read-only for as long as
> you still have a good database system there.  The data is current as
> of the last applied snapshot (which need not be the last write action
> in the data origin, but it might be good enough).
> 
> If you can stand to lose some transactions, then you can do full
> automatic failover.  But careful!  That data is lost more or less
> forever, so you need to be prepared for that.
> 
> A
> 
> -- 
> Andrew Sullivan
> ajs at crankycanuck.ca
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://old.nabble.com/Slony-%2B-PGPool-for-HA-Failover-only-tp26400286p29032292.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.


