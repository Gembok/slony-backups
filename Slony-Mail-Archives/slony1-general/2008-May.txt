From sangsuan.gam at gmail.com  Thu May  1 02:42:22 2008
From: sangsuan.gam at gmail.com (sang-suan gam)
Date: Thu May  1 02:42:47 2008
Subject: [Slony1-general] try: subscrptions lost after move set ?
In-Reply-To: <72dbd3150804301916r201cef6cna4610516a7b97ffc@mail.gmail.com>
References: <72dbd3150804301916r201cef6cna4610516a7b97ffc@mail.gmail.com>
Message-ID: <504f5c2c0805010242oab4d6exf73db1151682007b@mail.gmail.com>

Thanks David,

i'll check the versions tomorrow (it's a holiday here today).

the description mentioned in the link you mentioned sounds
a lot like the problem I am seeing.

Thanks again,
sam

On Thu, May 1, 2008 at 10:16 AM, David Rees <drees76@gmail.com> wrote:
> On Wed, Apr 30, 2008 at 2:25 AM, ssgam <sangsuan.gam@gmail.com> wrote:
>  >  after running the 2 scripts above, the origin was moved from N1 to N2.
>  >  i was able to update a row in a table in the set on N2, and the change
>  >  was reflected on N1.
>
>  What version of Slony? According this message[1] there is a bug when
>  failing over in 1.2.13 - I wonder if this is the bug you are hitting.
>  Can you try the latest from CVS?
>
>  -Dave
>
>  [1] http://www.nabble.com/Slony-version-1.2.14--to16972969.html
>
From giuseppe.laccone at gmail.com  Thu May  1 11:01:03 2008
From: giuseppe.laccone at gmail.com (Giuseppe Laccone)
Date: Thu May  1 11:01:12 2008
Subject: [Slony1-general] configure:7134: error: Headers for libpqserver are
	not found in the includeserverdir
Message-ID: <a34d0ed40805011101o700c0846v65d770d9796409dd@mail.gmail.com>

hi all,
I am trying to compile slony1 with PG8.1.11 for win32 with mingwin32
and msys as suggested in the
documentation:

./configure --with-pgbindir=/usr/local/pgsql/bin
--with-pgincludedir=/usr/local/pgsql/include
--with-pgincludeserverdir=/usr/local/pgsql/include/server
--with-pgconfigdir=/usr/local/pgsql/bin

Unfortunately I  receive always the same error even is I specify
pgincludeserverdir:

configure:7113: result: no
configure:7134: error: Headers for libpqserver are not found in the
includeserverdir.
This is the path to postgres.h. Please specify the includeserverdir
with    --with-pgincludeserverdir=<dir>

I didn't execute the make install-all-headers by the compilation of
PG81 because it is just necessary for
versions < 7.4.

any suggestion?

Thanks a lot in advance!!!


This is the config.log

It was created by postgresql-slony1-engine configure 1.1.9, which was
generated by GNU Autoconf 2.61.  Invocation command line was

  $ ./configure --with-pgbindir=/usr/local/pgsql/bin
--with-pgincludedir=/usr/local/pgsql/include

--with-pgincludeserverdir=/usr/local/pgsql/include/server
--with-pgconfigdir=/usr/local/pgsql/bin

## --------- ##

## Platform. ##

## --------- ##

hostname = snoopy

uname -m = i686

uname -r = 1.0.10(0.46/3/2)

uname -s = MINGW32_NT-5.1

uname -v = 2004-03-15 07:17

/usr/bin/uname -p = unknown

/bin/uname -X     = unknown

/bin/arch              = unknown

/usr/bin/arch -k       = unknown

/usr/convex/getsysinfo = unknown

/usr/bin/hostinfo      = unknown

/bin/machine           = unknown

/usr/bin/oslevel       = unknown

/bin/universe          = unknown



PATH: .

PATH: /usr/local/bin

PATH: /mingw/bin

PATH: /bin

PATH: /c/WINDOWS/system32

PATH: /c/WINDOWS

PATH: /c/WINDOWS/System32/Wbem

PATH: /c/Programmi/ATI Technologies/ATI Control Panel

## ----------- ##

## Core tests. ##

## ----------- ##


configure:1804: checking build system type

configure:1822: result: i686-pc-mingw32

configure:1844: checking host system type

configure:1859: result: i686-pc-mingw32

configure:1883: checking which template to use

configure:1939: result: win

configure:1976: using CFLAGS=

configure:2026: checking for gcc

configure:2042: found /mingw/bin/gcc

configure:2053: result: gcc

configure:2291: checking for C compiler version

configure:2298: gcc --version >&5

gcc.exe (GCC) 3.4.5 (mingw special)

Copyright (C) 2004 Free Software Foundation, Inc.

This is free software; see the source for copying conditions.  There is NO

warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.



configure:2301: $? = 0

configure:2308: gcc -v >&5

Reading specs from c:/mingw/bin/../lib/gcc/mingw32/3.4.5/specs

Configured with: ../gcc-3.4.5/configure --with-gcc --with-gnu-ld
--with-gnu-as --host=mingw32

--target=mingw32 --prefix=/mingw --enable-threads --disable-nls
--enable-languages=c,c++,f77,ada,objc,java

--disable-win32-registry --disable-shared --enable-sjlj-exceptions
--enable-libgcj --disable-java-awt

--without-x --enable-java-gc=boehm --disable-libgcj-debug
--enable-interpreter --enable-hash-synchronization

--enable-libstdcxx-debug

Thread model: win32

gcc version 3.4.5 (mingw special)

configure:2311: $? = 0

configure:2318: gcc -V >&5

gcc.exe: `-V' option must have argument

configure:2321: $? = 1

configure:2344: checking for C compiler default output file name

configure:2371: gcc    conftest.c  >&5

configure:2374: $? = 0

configure:2412: result: a.exe

configure:2429: checking whether the C compiler works

configure:2439: ./a.exe

configure:2442: $? = 0

configure:2459: result: yes

configure:2466: checking whether we are cross compiling

configure:2468: result: no

configure:2471: checking for suffix of executables

configure:2478: gcc -o conftest.exe    conftest.c  >&5

configure:2481: $? = 0

configure:2505: result: .exe

configure:2511: checking for suffix of object files

configure:2537: gcc -c   conftest.c >&5

configure:2540: $? = 0

configure:2563: result: o

configure:2567: checking whether we are using the GNU C compiler

configure:2596: gcc -c   conftest.c >&5

configure:2602: $? = 0

configure:2619: result: yes

configure:2624: checking whether gcc accepts -g

configure:2654: gcc -c -g  conftest.c >&5

configure:2660: $? = 0

configure:2759: result: yes

configure:2776: checking for gcc option to accept ISO C89

configure:2850: gcc  -c -g -O2  conftest.c >&5

configure:2856: $? = 0

configure:2879: result: none needed

configure:2908: checking for ld used by GCC

configure:2971: result: c:/mingw/mingw32/bin/ld.exe

configure:2980: checking if the linker (c:/mingw/mingw32/bin/ld.exe) is GNU ld

GNU ld version 2.17.50 20060824

configure:2992: result: yes

configure:2999: checking for perl

configure:3032: result: no

configure:3039: checking for tar

configure:3057: found /bin/tar

configure:3069: result: /bin/tar

configure:3081: checking for flex

configure:3111: result: no

configure:3081: checking for ,

configure:3111: result: no

configure:3081: checking for lex

configure:3111: result: no

configure:3123: checking for bison

configure:3153: result: no

configure:3123: checking for ,

configure:3153: result: no

configure:3123: checking for yacc

configure:3153: result: no

configure:3165: checking for sed

configure:3181: found /bin/sed

configure:3192: result: sed

configure:3412: checking for the pthreads library -lpthreads

configure:3455: gcc -o conftest.exe -g -O2    conftest.c -lpthreads  >&5

conftest.c:8:21: pthread.h: No such file or directory

conftest.c: In function `main':

conftest.c:12: error: `pthread_t' undeclared (first use in this function)

conftest.c:12: error: (Each undeclared identifier is reported only once

conftest.c:12: error: for each function it appears in.)

conftest.c:12: error: syntax error before "th"

conftest.c:12: error: `th' undeclared (first use in this function)

configure:3461: $? = 1

configure: failed program was:

| /* confdefs.h.  */

| #define PACKAGE_NAME "postgresql-slony1-engine"

| #define PACKAGE_TARNAME "postgresql-slony1-engine"

| #define PACKAGE_VERSION "1.1.9"

| #define PACKAGE_STRING "postgresql-slony1-engine 1.1.9"

| #define PACKAGE_BUGREPORT ""

| /* end confdefs.h.  */

| #include <pthread.h>

| int

| main ()

| {

| pthread_t th; pthread_join(th, 0);

|                      pthread_attr_init(0); pthread_cleanup_push(0, 0);

|                      pthread_create(0,0,0,0); pthread_cleanup_pop(0);

|   ;

|   return 0;

| }

configure:3481: result: no

configure:3357: checking whether pthreads work without any flags

configure:3455: gcc -o conftest.exe -g -O2    conftest.c   >&5

conftest.c:8:21: pthread.h: No such file or directory

conftest.c: In function `main':

conftest.c:12: error: `pthread_t' undeclared (first use in this function)

conftest.c:12: error: (Each undeclared identifier is reported only once

conftest.c:12: error: for each function it appears in.)

conftest.c:12: error: syntax error before "th"

conftest.c:12: error: `th' undeclared (first use in this function)

configure:3461: $? = 1

configure: failed program was:

| /* confdefs.h.  */

| #define PACKAGE_NAME "postgresql-slony1-engine"

| #define PACKAGE_TARNAME "postgresql-slony1-engine"

| #define PACKAGE_VERSION "1.1.9"

| #define PACKAGE_STRING "postgresql-slony1-engine 1.1.9"

| #define PACKAGE_BUGREPORT ""

| /* end confdefs.h.  */



......
......


configure:6859: gcc -E  -I/usr/local/pgsql/include conftest.c

configure:6865: $? = 0

configure:6879: result: yes

configure:6907: checking for libpq-fe.h

configure:6914: result: yes

configure:6951: checking postgres.h usability

configure:6968: gcc -c -g -O2  -I/usr/local/pgsql/include
-I/usr/local/pgsql/include/server conftest.c >&5

In file included from C:/msys/1.0/local/pgsql/include/server/c.h:53,

                 from C:/msys/1.0/local/pgsql/include/server/postgres.h:48,

                 from conftest.c:71:

C:/msys/1.0/local/pgsql/include/server/pg_config.h:584:1: warning:
"PACKAGE_BUGREPORT" redefined

conftest.c:6:1: warning: this is the location of the previous definition

C:/msys/1.0/local/pgsql/include/server/pg_config.h:587:1: warning:
"PACKAGE_NAME" redefined

conftest.c:2:1: warning: this is the location of the previous definition

C:/msys/1.0/local/pgsql/include/server/pg_config.h:590:1: warning:
"PACKAGE_STRING" redefined

conftest.c:5:1: warning: this is the location of the previous definition

C:/msys/1.0/local/pgsql/include/server/pg_config.h:593:1: warning:
"PACKAGE_TARNAME" redefined

conftest.c:3:1: warning: this is the location of the previous definition

C:/msys/1.0/local/pgsql/include/server/pg_config.h:596:1: warning:
"PACKAGE_VERSION" redefined

conftest.c:4:1: warning: this is the location of the previous definition

In file included from C:/msys/1.0/local/pgsql/include/server/c.h:822,

                 from C:/msys/1.0/local/pgsql/include/server/postgres.h:48,

                 from conftest.c:71:

C:/msys/1.0/local/pgsql/include/server/port.h:16:17: pwd.h: No such
file or directory

C:/msys/1.0/local/pgsql/include/server/port.h:17:19: netdb.h: No such
file or directory

C:/msys/1.0/local/pgsql/include/server/port.h:308:24: netinet/in.h: No
such file or directory

C:/msys/1.0/local/pgsql/include/server/port.h:309:23: arpa/inet.h: No
such file or directory

In file included from C:/msys/1.0/local/pgsql/include/server/postgres.h:49,

                 from conftest.c:71:

C:/msys/1.0/local/pgsql/include/server/utils/elog.h:40:1: warning:
"ERROR" redefined

In file included from
c:/mingw/bin/../lib/gcc/mingw32/3.4.5/../../../../include/windows.h:52,

                 from C:/msys/1.0/local/pgsql/include/server/pg_config_os.h:7,

                 from C:/msys/1.0/local/pgsql/include/server/c.h:85,

                 from C:/msys/1.0/local/pgsql/include/server/postgres.h:48,

                 from conftest.c:71:

c:/mingw/bin/../lib/gcc/mingw32/3.4.5/../../../../include/wingdi.h:313:1:
warning: this is the location of

the previous definition

configure:6974: $? = 1

configure: failed program was:

| /* confdefs.h.  */

| #define PACKAGE_NAME "postgresql-slony1-engine"

| #define PACKAGE_TARNAME "postgresql-slony1-engine"

| #define PACKAGE_VERSION "1.1.9"

| #define PACKAGE_STRING "postgresql-slony1-engine 1.1.9"

| #define PACKAGE_BUGREPORT ""

| #define STDC_HEADERS 1

| #define HAVE_SYS_TYPES_H 1

| #define HAVE_SYS_STAT_H 1

| #define HAVE_STDLIB_H 1

| #define HAVE_STRING_H 1

| #define HAVE_MEMORY_H 1

| #define HAVE_STRINGS_H 1

| #define HAVE_INTTYPES_H 1

| #define HAVE_STDINT_H 1

| #define HAVE_UNISTD_H 1

| #define HAVE_FCNTL_H 1

| #define HAVE_LIMITS_H 1

| #define HAVE_STDDEF_H 1

| #define HAVE_SYS_TIME_H 1

| #define HAVE_INTTYPES_H 1

| #define HAVE_GETTIMEOFDAY 1

| #define HAVE_DUP2 1

| #define HAVE_MEMSET 1

| #define HAVE_STRDUP 1

| #define HAVE_STRERROR 1

| #define HAVE_STRTOL 1

| #define HAVE_STRTOUL 1

| #define HAVE_INT32_T 1

| #define HAVE_UINT32_T 1

| #define HAVE_INT64_T 1

| #define HAVE_UINT64_T 1

| #define HAVE_SSIZE_T 1

| #define PG_VERSION_OK 1

| #define PG_VERSION_VERIFIED 1

| #define PG_LIBPQ_VERIFIED 1

| /* end confdefs.h.  */

| #include <stdio.h>

| #ifdef HAVE_SYS_TYPES_H

| # include <sys/types.h>

| #endif

| #ifdef HAVE_SYS_STAT_H

| # include <sys/stat.h>

| #endif

| #ifdef STDC_HEADERS

| # include <stdlib.h>

| # include <stddef.h>

| #else

| # ifdef HAVE_STDLIB_H

| #  include <stdlib.h>

| # endif

| #endif

| #ifdef HAVE_STRING_H

| # if !defined STDC_HEADERS && defined HAVE_MEMORY_H

| #  include <memory.h>

| # endif

| # include <string.h>

| #endif

| #ifdef HAVE_STRINGS_H

| # include <strings.h>

| #endif

| #ifdef HAVE_INTTYPES_H

| # include <inttypes.h>

| #endif

| #ifdef HAVE_STDINT_H

| # include <stdint.h>

| #endif

| #ifdef HAVE_UNISTD_H

| # include <unistd.h>

| #endif

| #include <postgres.h>

configure:6988: result: no

configure:6992: checking postgres.h presence

configure:7007: gcc -E  -I/usr/local/pgsql/include
-I/usr/local/pgsql/include/server conftest.c

In file included from C:/msys/1.0/local/pgsql/include/server/c.h:53,

                 from C:/msys/1.0/local/pgsql/include/server/postgres.h:48,

                 from conftest.c:38:

C:/msys/1.0/local/pgsql/include/server/pg_config.h:584:1: warning:
"PACKAGE_BUGREPORT" redefined

conftest.c:6:1: warning: this is the location of the previous definition

C:/msys/1.0/local/pgsql/include/server/pg_config.h:587:1: warning:
"PACKAGE_NAME" redefined

conftest.c:2:1: warning: this is the location of the previous definition

C:/msys/1.0/local/pgsql/include/server/pg_config.h:590:1: warning:
"PACKAGE_STRING" redefined

conftest.c:5:1: warning: this is the location of the previous definition

C:/msys/1.0/local/pgsql/include/server/pg_config.h:593:1: warning:
"PACKAGE_TARNAME" redefined

conftest.c:3:1: warning: this is the location of the previous definition

C:/msys/1.0/local/pgsql/include/server/pg_config.h:596:1: warning:
"PACKAGE_VERSION" redefined

conftest.c:4:1: warning: this is the location of the previous definition

In file included from C:/msys/1.0/local/pgsql/include/server/c.h:822,

                 from C:/msys/1.0/local/pgsql/include/server/postgres.h:48,

                 from conftest.c:38:

C:/msys/1.0/local/pgsql/include/server/port.h:16:17: pwd.h: No such
file or directory

C:/msys/1.0/local/pgsql/include/server/port.h:17:19: netdb.h: No such
file or directory

C:/msys/1.0/local/pgsql/include/server/port.h:308:24: netinet/in.h: No
such file or directory

C:/msys/1.0/local/pgsql/include/server/port.h:309:23: arpa/inet.h: No
such file or directory

In file included from C:/msys/1.0/local/pgsql/include/server/postgres.h:49,

                 from conftest.c:38:

C:/msys/1.0/local/pgsql/include/server/utils/elog.h:40:1: warning:
"ERROR" redefined

In file included from
c:/mingw/bin/../lib/gcc/mingw32/3.4.5/../../../../include/windows.h:52,

                 from C:/msys/1.0/local/pgsql/include/server/pg_config_os.h:7,

                 from C:/msys/1.0/local/pgsql/include/server/c.h:85,

                 from C:/msys/1.0/local/pgsql/include/server/postgres.h:48,

                 from conftest.c:38:

c:/mingw/bin/../lib/gcc/mingw32/3.4.5/../../../../include/wingdi.h:313:1:
warning: this is the location of

the previous definition

configure:7013: $? = 1

configure: failed program was:

| /* confdefs.h.  */

| #define PACKAGE_NAME "postgresql-slony1-engine"

| #define PACKAGE_TARNAME "postgresql-slony1-engine"

| #define PACKAGE_VERSION "1.1.9"

| #define PACKAGE_STRING "postgresql-slony1-engine 1.1.9"

| #define PACKAGE_BUGREPORT ""

| #define STDC_HEADERS 1

| #define HAVE_SYS_TYPES_H 1

| #define HAVE_SYS_STAT_H 1

| #define HAVE_STDLIB_H 1

| #define HAVE_STRING_H 1

| #define HAVE_MEMORY_H 1

| #define HAVE_STRINGS_H 1

| #define HAVE_INTTYPES_H 1

| #define HAVE_STDINT_H 1

| #define HAVE_UNISTD_H 1

| #define HAVE_FCNTL_H 1

| #define HAVE_LIMITS_H 1

| #define HAVE_STDDEF_H 1

| #define HAVE_SYS_TIME_H 1

| #define HAVE_INTTYPES_H 1

| #define HAVE_GETTIMEOFDAY 1

| #define HAVE_DUP2 1

| #define HAVE_MEMSET 1

| #define HAVE_STRDUP 1

| #define HAVE_STRERROR 1

| #define HAVE_STRTOL 1

| #define HAVE_STRTOUL 1

| #define HAVE_INT32_T 1

| #define HAVE_UINT32_T 1

| #define HAVE_INT64_T 1

| #define HAVE_UINT64_T 1

| #define HAVE_SSIZE_T 1

| #define PG_VERSION_OK 1

| #define PG_VERSION_VERIFIED 1

| #define PG_LIBPQ_VERIFIED 1

| /* end confdefs.h.  */

| #include <postgres.h>

configure:7027: result: no

configure:7055: checking for postgres.h

configure:7062: result: no

configure:7071: checking for utils/typcache.h

configure:7092: gcc -c -g -O2  -I/usr/local/pgsql/include
-I/usr/local/pgsql/include/server conftest.c >&5

In file included from C:/msys/1.0/local/pgsql/include/server/c.h:53,

                 from C:/msys/1.0/local/pgsql/include/server/postgres.h:48,

                 from conftest.c:38:

C:/msys/1.0/local/pgsql/include/server/pg_config.h:584:1: warning:
"PACKAGE_BUGREPORT" redefined

conftest.c:6:1: warning: this is the location of the previous definition

C:/msys/1.0/local/pgsql/include/server/pg_config.h:587:1: warning:
"PACKAGE_NAME" redefined

conftest.c:2:1: warning: this is the location of the previous definition

C:/msys/1.0/local/pgsql/include/server/pg_config.h:590:1: warning:
"PACKAGE_STRING" redefined

conftest.c:5:1: warning: this is the location of the previous definition

C:/msys/1.0/local/pgsql/include/server/pg_config.h:593:1: warning:
"PACKAGE_TARNAME" redefined

conftest.c:3:1: warning: this is the location of the previous definition

C:/msys/1.0/local/pgsql/include/server/pg_config.h:596:1: warning:
"PACKAGE_VERSION" redefined

conftest.c:4:1: warning: this is the location of the previous definition

In file included from C:/msys/1.0/local/pgsql/include/server/c.h:822,

                 from C:/msys/1.0/local/pgsql/include/server/postgres.h:48,

                 from conftest.c:38:

C:/msys/1.0/local/pgsql/include/server/port.h:16:17: pwd.h: No such
file or directory

C:/msys/1.0/local/pgsql/include/server/port.h:17:19: netdb.h: No such
file or directory

C:/msys/1.0/local/pgsql/include/server/port.h:308:24: netinet/in.h: No
such file or directory

C:/msys/1.0/local/pgsql/include/server/port.h:309:23: arpa/inet.h: No
such file or directory

In file included from C:/msys/1.0/local/pgsql/include/server/postgres.h:49,

                 from conftest.c:38:

C:/msys/1.0/local/pgsql/include/server/utils/elog.h:40:1: warning:
"ERROR" redefined

In file included from
c:/mingw/bin/../lib/gcc/mingw32/3.4.5/../../../../include/windows.h:52,

                 from C:/msys/1.0/local/pgsql/include/server/pg_config_os.h:7,

                 from C:/msys/1.0/local/pgsql/include/server/c.h:85,

                 from C:/msys/1.0/local/pgsql/include/server/postgres.h:48,

                 from conftest.c:38:

c:/mingw/bin/../lib/gcc/mingw32/3.4.5/../../../../include/wingdi.h:313:1:
warning: this is the location of

the previous definition

configure:7098: $? = 1

configure: failed program was:

| /* confdefs.h.  */

| #define PACKAGE_NAME "postgresql-slony1-engine"

| #define PACKAGE_TARNAME "postgresql-slony1-engine"

| #define PACKAGE_VERSION "1.1.9"

| #define PACKAGE_STRING "postgresql-slony1-engine 1.1.9"

| #define PACKAGE_BUGREPORT ""

| #define STDC_HEADERS 1

| #define HAVE_SYS_TYPES_H 1

| #define HAVE_SYS_STAT_H 1

| #define HAVE_STDLIB_H 1

| #define HAVE_STRING_H 1

| #define HAVE_MEMORY_H 1

| #define HAVE_STRINGS_H 1

| #define HAVE_INTTYPES_H 1

| #define HAVE_STDINT_H 1

| #define HAVE_UNISTD_H 1

| #define HAVE_FCNTL_H 1

| #define HAVE_LIMITS_H 1

| #define HAVE_STDDEF_H 1

| #define HAVE_SYS_TIME_H 1

| #define HAVE_INTTYPES_H 1

| #define HAVE_GETTIMEOFDAY 1

| #define HAVE_DUP2 1

| #define HAVE_MEMSET 1

| #define HAVE_STRDUP 1

| #define HAVE_STRERROR 1

| #define HAVE_STRTOL 1

| #define HAVE_STRTOUL 1

| #define HAVE_INT32_T 1

| #define HAVE_UINT32_T 1

| #define HAVE_INT64_T 1

| #define HAVE_UINT64_T 1

| #define HAVE_SSIZE_T 1

| #define PG_VERSION_OK 1

| #define PG_VERSION_VERIFIED 1

| #define PG_LIBPQ_VERIFIED 1

| /* end confdefs.h.  */

| #include "postgres.h"

|

| #include <utils/typcache.h>

configure:7113: result: no

configure:7134: error: Headers for libpqserver are not found in the
includeserverdir.

    This is the path to postgres.h. Please specify the includeserverdir with

    --with-pgincludeserverdir=<dir>





## ---------------- ##

## Cache variables. ##

## ---------------- ##



ac_cv_build=i686-pc-mingw32

ac_cv_c_compiler_gnu=yes

ac_cv_env_CC_set=

ac_cv_env_CC_value=

ac_cv_env_CFLAGS_set=

ac_cv_env_CFLAGS_value=

ac_cv_env_CPPFLAGS_set=

ac_cv_env_CPPFLAGS_value=

ac_cv_env_CPP_set=

ac_cv_env_CPP_value=

ac_cv_env_DOCBOOKSTYLE_set=

ac_cv_env_DOCBOOKSTYLE_value=

ac_cv_env_LDFLAGS_set=

ac_cv_env_LDFLAGS_value=

ac_cv_env_LIBS_set=

ac_cv_env_LIBS_value=

ac_cv_env_build_alias_set=

ac_cv_env_build_alias_value=

ac_cv_env_host_alias_set=

ac_cv_env_host_alias_value=

ac_cv_env_target_alias_set=

ac_cv_env_target_alias_value=

ac_cv_exeext=.exe

ac_cv_func_alarm=no

ac_cv_func_dup2=yes

ac_cv_func_gettimeofday=yes

ac_cv_func_memset=yes

ac_cv_func_select=no

ac_cv_func_strdup=yes

ac_cv_func_strerror=yes

ac_cv_func_strtol=yes

ac_cv_func_strtoul=yes

ac_cv_header_fcntl_h=yes

ac_cv_header_inttypes_h=yes

ac_cv_header_libpq_fe_h=yes

ac_cv_header_limits_h=yes

ac_cv_header_memory_h=yes

ac_cv_header_postgres_h=no

ac_cv_header_stdc=yes

ac_cv_header_stddef_h=yes

ac_cv_header_stdint_h=yes

ac_cv_header_stdlib_h=yes

ac_cv_header_string_h=yes

ac_cv_header_strings_h=yes

ac_cv_header_sys_socket_h=no

ac_cv_header_sys_stat_h=yes

ac_cv_header_sys_time_h=yes

ac_cv_header_sys_types_h=yes

ac_cv_header_unistd_h=yes

ac_cv_header_utils_typcache_h=no

ac_cv_host=i686-pc-mingw32

ac_cv_lib_pq_PQunescapeBytea=yes

ac_cv_objext=o

ac_cv_path_EGREP='/bin/grep -E'

ac_cv_path_GREP=/bin/grep

ac_cv_path_LD=c:/mingw/mingw32/bin/ld.exe

ac_cv_path_TAR=/bin/tar

ac_cv_prog_CPP='gcc -E'

ac_cv_prog_SED=sed

ac_cv_prog_ac_ct_CC=gcc

ac_cv_prog_acx_pthread_config=no

ac_cv_prog_cc_c89=

ac_cv_prog_cc_g=yes

ac_cv_prog_gnu_ld=yes

ac_cv_type_int32_t=yes

ac_cv_type_int64_t=yes

ac_cv_type_ssize_t=yes

ac_cv_type_u_int32_t=no

ac_cv_type_u_int64_t=no

ac_cv_type_uint32_t=yes

ac_cv_type_uint64_t=yes

slonac_cv_func_posix_signals=no



## ----------------- ##

## Output variables. ##

## ----------------- ##



1.1.9='dummy.1.9'

CC='gcc'

CFLAGS='-g -O2'

COLLATEINDEX=''

CONVERT=''

CPP='gcc -E'

CPPFLAGS=' -I/usr/local/pgsql/include -I/usr/local/pgsql/include/server'

DEFS=''

DJPEG=''

DOCBOOKSTYLE=''

DOCDIR=''

ECHO_C=''

ECHO_N='-n'

ECHO_T=''

EGREP='/bin/grep -E'

EXEEXT='.exe'

GCC='yes'

GREP='/bin/grep'

GROFF=''

HAVE_NETSNMP=''

HAVE_POSIX_SIGNALS=''

HOST_OS=''

JADE=''

LD='c:/mingw/mingw32/bin/ld.exe'

LDFLAGS=' -LC:/msys/1.0/local/pgsql/lib/'

LEX=''

LEXFLAGS=''

LIBOBJS=''

LIBS=''

LTLIBOBJS=''

NETSNMP_AGENTLIBS=''

NETSNMP_CFLAGS=''

NSGMLS=''

OBJEXT='o'

PACKAGE_BUGREPORT=''

PACKAGE_NAME='postgresql-slony1-engine'

PACKAGE_STRING='postgresql-slony1-engine 1.1.9'

PACKAGE_TARNAME='postgresql-slony1-engine'

PACKAGE_VERSION='1.1.9'

PATH_SEPARATOR=':'

PERL=''

PGAUTODOC=''

PGBINDIR=''

PGINCLUDEDIR=''

PGINCLUDESERVERDIR=''

PGLIBDIR=''

PGPKGLIBDIR=''

PGSHAREDIR=''

PNMTOPS=''

PORTNAME=''

PS2PDF=''

PTHREAD_CC='gcc'

PTHREAD_CFLAGS=''

PTHREAD_LIBS=''

SED='sed'

SGMLSPL=''

SHELL='/bin/sh'

SLONBINDIR=''

SLONYPATH='/slony19'

TAR='/bin/tar'

TOOLSBIN=''

YACC=''

YFLAGS=''

ac_ct_CC='gcc'

acx_pthread_config='no'

bindir='${exec_prefix}/bin'

build='i686-pc-mingw32'

build_alias=''

build_cpu='i686'

build_os='mingw32'

build_vendor='pc'

datadir='${datarootdir}'

datarootdir='${prefix}/share'

docdir='${datarootdir}/doc/${PACKAGE_TARNAME}'

dvidir='${docdir}'

enable_debug='no'

exec_prefix='NONE'

have_docbook=''

host='i686-pc-mingw32'

host_alias=''

host_cpu='i686'

host_os='mingw32'

host_vendor='pc'

htmldir='${docdir}'

includedir='${prefix}/include'

infodir='${datarootdir}/info'

libdir='${exec_prefix}/lib'

libexecdir='${exec_prefix}/libexec'

localedir='${datarootdir}/locale'

localstatedir='${prefix}/var'

mandir='${datarootdir}/man'

oldincludedir='/usr/include'

pdfdir='${docdir}'

prefix='NONE'

program_transform_name='s,x,x,'

psdir='${docdir}'

sbindir='${exec_prefix}/sbin'

sharedstatedir='${prefix}/com'

sysconfdir='${prefix}/etc'

target_alias=''

with_docs=''

with_gnu_ld='yes'



## ----------- ##

## confdefs.h. ##

## ----------- ##



#define PACKAGE_NAME "postgresql-slony1-engine"

#define PACKAGE_TARNAME "postgresql-slony1-engine"

#define PACKAGE_VERSION "1.1.9"

#define PACKAGE_STRING "postgresql-slony1-engine 1.1.9"

#define PACKAGE_BUGREPORT ""

#define STDC_HEADERS 1

#define HAVE_SYS_TYPES_H 1

#define HAVE_SYS_STAT_H 1

#define HAVE_STDLIB_H 1

#define HAVE_STRING_H 1

#define HAVE_MEMORY_H 1

#define HAVE_STRINGS_H 1

#define HAVE_INTTYPES_H 1

#define HAVE_STDINT_H 1

#define HAVE_UNISTD_H 1

#define HAVE_FCNTL_H 1

#define HAVE_LIMITS_H 1

#define HAVE_STDDEF_H 1

#define HAVE_SYS_TIME_H 1

#define HAVE_INTTYPES_H 1

#define HAVE_GETTIMEOFDAY 1

#define HAVE_DUP2 1

#define HAVE_MEMSET 1

#define HAVE_STRDUP 1

#define HAVE_STRERROR 1

#define HAVE_STRTOL 1

#define HAVE_STRTOUL 1

#define HAVE_INT32_T 1

#define HAVE_UINT32_T 1

#define HAVE_INT64_T 1

#define HAVE_UINT64_T 1

#define HAVE_SSIZE_T 1

#define PG_VERSION_OK 1

#define PG_VERSION_VERIFIED 1

#define PG_LIBPQ_VERIFIED 1



configure: exit 1
From tmblue at gmail.com  Thu May  1 12:17:26 2008
From: tmblue at gmail.com (Tory M Blue)
Date: Thu May  1 12:17:35 2008
Subject: [Slony1-general] 1.2.14rc still does not appear to handle
	switchover cleanly
Message-ID: <8a547c840805011217y45deaf70ob58d98ad6dc75369@mail.gmail.com>

As posted by Mikhail Kolesnik in a discussion with St?phane A. Schildknecht

An issue was possibly introduced into 1.2.12 that caused failover
problems (although it appears from the conversation that folks thought
it was in the 1.2.13 branch and I believe this major issue appeared
before 1.2.13.

So I've done exhaustive testing and 1.2.11 will allow failover with no
issues, I have a 4 node scheme

1 masterhost
1 slavehost
2 qslavehosts (query only)

1.2.11 and postgresql 8.2.5

Other than a possible single instance of:
ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep

Which is short lived, failover works back and forth between master and
slave, all day long. I switchover, wait for replication, switchback,
wait for replication etc.etc.etc.etc. No issues.

Now with 1.2.12 and 1.2.14rc (have not tested 1.2.13 yet (but since
it's apparent in 1.2.12 and in 1.2.14rc even with the "patch/possible
fix", I'm guessing the issue is very much in 1.2.13 and there is a
large issue as failover and switchover are key elements in this
application.

The symptoms in 1.2.12 and 1.2.14rc are that the qslaves freak the
heck out. We can get fail over to work, but we MUST drop the affected
qslave host and re add, and when one is doing weekly indexes and has
to end up rebuilding each time, that's an issue.

The qservers get into this state (ps -ef) during and after a failover.
One can stop slon the failover will take place and once you restart
slon the node is instantly in a bas state (2008-05-01 11:54:42 PDT
DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep)

Qslavehost
postgres  6467  1161  0 11:47 ?        00:00:00 postgres: postgres
clsdb 10.40.5.243(54273) idle
postgres  6468  6442  0 11:48 pts/0    00:00:00 slon -f
/data/pgsql/slon.conf cls dbname=clsdb user=postgres
postgres  6545  6468  0 11:51 pts/0    00:00:00 slon -f
/data/pgsql/slon.conf cls dbname=clsdb user=postgres
postgres  6549  1161  0 11:51 ?        00:00:00 postgres: postgres
clsdb 10.40.5.250(54310) idle
postgres  6552  1161  0 11:51 ?        00:00:00 postgres: postgres
clsdb 10.40.5.250(54311) idle in transaction
postgres  6558  1161  0 11:51 ?        00:00:00 postgres: postgres
clsdb 10.40.5.250(54312) LOCK TABLE waiting  <--- this is wrong
postgres  6560  1161  0 11:51 ?        00:00:00 postgres: postgres
clsdb 10.40.5.250(54313) idle
postgres  6561  1161  0 11:51 ?        00:00:00 postgres: postgres
clsdb 10.40.5.250(54315) idle
postgres  6563  1161  0 11:51 ?        00:00:00 postgres: postgres
clsdb 10.40.5.250(54316) idle

The logs show:


2008-05-01 11:54:20 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 54
2008-05-01 11:54:20 PDT DEBUG2 remoteListenThread_1: LISTEN
2008-05-01 11:54:22 PDT DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
not received yet - sleep
2008-05-01 11:54:25 PDT DEBUG2 remoteListenThread_1: queue event 1,153 SYNC
2008-05-01 11:54:25 PDT DEBUG2 remoteListenThread_1: UNLISTEN
2008-05-01 11:54:27 PDT DEBUG2 remoteListenThread_4: LISTEN
2008-05-01 11:54:30 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 55
2008-05-01 11:54:30 PDT DEBUG2 localListenThread: Received event 3,54 SYNC
2008-05-01 11:54:30 PDT DEBUG2 localListenThread: Received event 3,55 SYNC
2008-05-01 11:54:32 PDT DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
not received yet - sleep
2008-05-01 11:54:34 PDT DEBUG2 remoteListenThread_2: queue event 2,72 SYNC
2008-05-01 11:54:34 PDT DEBUG2 remoteListenThread_2: UNLISTEN
2008-05-01 11:54:37 PDT DEBUG2 remoteListenThread_4: LISTEN
2008-05-01 11:54:39 PDT DEBUG2 remoteListenThread_2: queue event 2,73 SYNC
2008-05-01 11:54:39 PDT DEBUG2 remoteListenThread_2: UNLISTEN
2008-05-01 11:54:40 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 56
2008-05-01 11:54:40 PDT DEBUG2 remoteListenThread_1: queue event 1,154 SYNC
2008-05-01 11:54:42 PDT DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
not received yet - sleep

And i also note that I've seen this on occasions.

2008-05-01 11:52:06 PDT DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
not received yet - sleep
2008-05-01 11:52:07 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 34
2008-05-01 11:52:07 PDT DEBUG4 version for "dbname=clsdb
host=devidb04.domain.com user=postgres password=SECURED" is 80205
2008-05-01 11:52:07 PDT ERROR  remoteListenThread_3:
db_getLocalNodeId() returned 2 - wrong database?
2008-05-01 11:52:08 PDT DEBUG2 remoteListenThread_1: queue event 1,122 SYNC
2008-05-01 11:52:08 PDT DEBUG2 remoteListenThread_1: UNLISTEN
2008-05-01 11:52:10 PDT DEBUG2 remoteListenThread_2: LISTEN
2008-05-01 11:52:11 PDT DEBUG2 remoteListenThread_2: queue event 3,35 SYNC
2008-05-01 11:52:11 PDT DEBUG2 remoteListenThread_2: UNLISTEN
2008-05-01 11:52:11 PDT DEBUG2 remoteWorkerThread_3: Received event 3,35 SYNC
2008-05-01 11:52:11 PDT DEBUG2 calc sync size - last time: 1 last
length: 11025 ideal: 5 proposed size: 3
2008-05-01 11:52:11 PDT DEBUG2 remoteWorkerThread_3: SYNC 35 processing
2008-05-01 11:52:11 PDT DEBUG2 remoteWorkerThread_3: no sets need
syncing for this event
2008-05-01 11:52:12 PDT DEBUG2 localListenThread: Received event 4,34 SYNC
2008-05-01 11:52:13 PDT DEBUG2 remoteListenThread_1: LISTEN
2008-05-01 11:52:16 PDT DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
not received yet - sleep
2008-05-01 11:52:16 PDT DEBUG2 remoteListenThread_2: queue event 2,47 SYNC
2008-05-01 11:52:16 PDT DEBUG2 remoteListenThread_2: UNLISTEN
2008-05-01 11:52:16 PDT DEBUG2 remoteListenThread_1: LISTEN
2008-05-01 11:52:17 PDT DEBUG2 remoteListenThread_1: queue event 1,123
STORE_PATH
2008-05-01 11:52:17 PDT DEBUG2 remoteListenThread_1: UNLISTEN
2008-05-01 11:52:17 PDT DEBUG2 remoteListenThread_1: queue event 1,124
STORE_LISTEN
2008-05-01 11:52:17 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 35
2008-05-01 11:52:17 PDT DEBUG4 version for "dbname=clsdb
host=devidb04.domain.com user=postgres password=SECURED" is 80205
2008-05-01 11:52:17 PDT ERROR  remoteListenThread_3:
db_getLocalNodeId() returned 2 - wrong database?

Sometimes in 1.2.12 and 1.2.14rc the failover works, but your not
going to get more than one successful failover before you have to drop
and add a node. Also this situation causes switchover to hang, until
you kill slon on the affected qslave.

I'm more than happy to work thru this as I really want to push out
8.3.1 and would love to have a functioning 1.2.14 slon release, but
something bad happened between 1.2.11 and current.. Either something
new that I have not added to my setup scripts or it's the code.

I'll work with someone on this!!!

Thanks
Tory
From dba at richyen.com  Thu May  1 14:52:32 2008
From: dba at richyen.com (Richard Yen)
Date: Thu May  1 14:52:46 2008
Subject: [Slony1-general] some questions about slony_logshipper
Message-ID: <3CFA3A13-2C53-45CC-A21B-22EF0742E3E0@richyen.com>

Hi,

Wondering if I could get an explanation for the design decision to lex  
and parse logfiles before applying them to the remote host?  Seems  
like overkill?  I'm probably thinking to simplistically, but would it  
suffice to simply read in the at_counter and pipe everything in if the  
number is correct?

Also, wondering who I need to contact re. some changes to the code I'd  
like to suggest?  I had posted to this list last week re. some changes  
I made to parser.y and scan.l, and wanted to verify that they are  
indeed changes that need to be committed to the repository.

Thanks!
--Richard
From charles.duffy at gmail.com  Thu May  1 23:35:07 2008
From: charles.duffy at gmail.com (Charles Duffy)
Date: Thu May  1 23:35:28 2008
Subject: [Slony1-general] 1.2.14rc still does not appear to handle
	switchover cleanly
In-Reply-To: <8a547c840805011217y45deaf70ob58d98ad6dc75369@mail.gmail.com>
References: <8a547c840805011217y45deaf70ob58d98ad6dc75369@mail.gmail.com>
Message-ID: <dfdaea8f0805012335w5162ba4eueab9ff11838c4105@mail.gmail.com>

>  Now with 1.2.12 and 1.2.14rc (have not tested 1.2.13 yet (but since
>  it's apparent in 1.2.12 and in 1.2.14rc even with the "patch/possible
>  fix", I'm guessing the issue is very much in 1.2.13 and there is a
>  large issue as failover and switchover are key elements in this
>  application.

Just to add my test report on 1.2.14RC - I agree that it still exhibits
the "move set" problem.

I tested it out with a 3-node PostgreSQL 8.3.1 cluster:

      node3 <-- node1 --> node2

There's one replication set. It contains all the tables used in a pgbench
installation (the test app). The initial state was: origin on node1,
with node2 and
node3 both subscribing directly to node1.

It works fine in this initial state:

node1=# insert into accounts values (100000000, 100000, 100000, 'hello sailor');
INSERT 0 1
node1=# \c node2
You are now connected to database "node2".
node2=# select * from accounts where aid = 100000000;
    aid    |  bid   | abalance |                                        filler
-----------+--------+----------+--------------------------------------------------------------------------------------
 100000000 | 100000 |   100000 | hello sailor
(1 row)

node2=# \c node3
You are now connected to database "node3".
node3=# select * from accounts where aid = 100000000;
    aid    |  bid   | abalance |                                        filler
-----------+--------+----------+--------------------------------------------------------------------------------------
 100000000 | 100000 |   100000 | hello sailor
(1 row)

node3=# \q

So, we're getting data. It worked with a large pgbench run as well.

The move set, however, causes problems:

[ccd@hpsystem slontest]$ slonik_move_set 1 1 2 | slonik
<stdin>:5: Locking down set 1 on node 1
<stdin>:7: Locked down - moving it
<stdin>:9: Replication set 1 moved from node 1 to 2.  Remember to
<stdin>:10: update your configuration file, if necessary, to note the
new location
<stdin>:11: for the set.

[ccd@hpsystem slontest]$ psql -U postgres node1
Password for user postgres:
Welcome to psql 8.3.1, the PostgreSQL interactive terminal.

node1=# select * from _replication.sl_subscribe ;
 sub_set | sub_provider | sub_receiver | sub_forward | sub_active
---------+--------------+--------------+-------------+------------
       1 |            1 |            3 | t           | t
       1 |            2 |            1 | t           | t
(2 rows)

node1=# \c node2
You are now connected to database "node2".
node2=# select * from _replication.sl_subscribe ;
 sub_set | sub_provider | sub_receiver | sub_forward | sub_active
---------+--------------+--------------+-------------+------------
       1 |            1 |            3 | t           | t
       1 |            2 |            1 | t           | t
(2 rows)

node2=# \c node3
You are now connected to database "node3".
node3=# select * from _replication.sl_subscribe ;
 sub_set | sub_provider | sub_receiver | sub_forward | sub_active
---------+--------------+--------------+-------------+------------
       1 |            1 |            2 | t           | t
       1 |            1 |            3 | t           | t


Node3 never reflects the new state of the cluster. Restarting slon
daemons was ineffective.
Looking at locks, the same symptomatic lock is present (as in all
other reports of this issue):


node3=# select * from pg_locks;
   locktype    | database | relation | page | tuple | virtualxid |
transactionid | classid | objid | objsubid | virtualtransaction | pid
|        mode         | granted
---------------+----------+----------+------+-------+------------+---------------+---------+-------+----------+--------------------+------+---------------------+---------
 virtualxid    |          |          |      |       | 16/121     |
          |         |       |          | 16/121             | 3480 |
ExclusiveLock       | t
 relation      |    16387 |    20718 |      |       |            |
          |         |       |          | 16/121             | 3480 |
AccessExclusiveLock | f
 relation      |    16387 |    20663 |      |       |            |
          |         |       |          | 21/108             | 3489 |
AccessShareLock     | t
 virtualxid    |          |          |      |       | 9/130      |
          |         |       |          | 9/130              | 3426 |
ExclusiveLock       | t
 relation      |    16386 |    20311 |      |       |            |
          |         |       |          | 9/130              | 3426 |
RowExclusiveLock    | t
 relation      |    16386 |    20303 |      |       |            |
          |         |       |          | 9/130              | 3426 |
RowExclusiveLock    | t
 transactionid |          |          |      |       |            |
    36277 |         |       |          | 9/130              | 3426 |
ExclusiveLock       | t
 virtualxid    |          |          |      |       | 4/396      |
          |         |       |          | 4/396              | 3382 |
ExclusiveLock       | t
 relation      |    16387 |    10969 |      |       |            |
          |         |       |          | 23/134             | 5232 |
AccessShareLock     | t
 virtualxid    |          |          |      |       | 21/108     |
          |         |       |          | 21/108             | 3489 |
ExclusiveLock       | t
 relation      |    16386 |     2614 |      |       |            |
          |         |       |          | 9/130              | 3426 |
ExclusiveLock       | t
 relation      |    16387 |    20669 |      |       |            |
          |         |       |          | 21/108             | 3489 |
AccessShareLock     | t
 relation      |    16386 |     2614 |      |       |            |
          |         |       |          | 4/396              | 3382 |
ExclusiveLock       | f
 virtualxid    |          |          |      |       | 23/134     |
          |         |       |          | 23/134             | 5232 |
ExclusiveLock       | t
 relation      |    16387 |    20718 |      |       |            |
          |         |       |          | 21/108             | 3489 |
AccessExclusiveLock | t
(15 rows)

node3=# select * from pg_locks where granted = 'f';
 locktype | database | relation | page | tuple | virtualxid |
transactionid | classid | objid | objsubid | virtualtransaction | pid
|        mode         | granted
----------+----------+----------+------+-------+------------+---------------+---------+-------+----------+--------------------+------+---------------------+---------
 relation |    16387 |    20718 |      |       |            |
     |         |       |          | 16/121             | 3480 |
AccessExclusiveLock | f
(1 row)

node3=# select relname,relnamespace from pg_class where oid = 20718;
    relname     | relnamespace
----------------+--------------
 sl_config_lock |        20491
(1 row)


looking at logfiles:

Node2's slon daemon logfile contains:
=================================
2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: queue event 1,67 MOVE_SET
2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: UNLISTEN
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event 1,66 SYNC
2008-05-02 14:34:22 EST DEBUG2 calc sync size - last time: 1 last
length: 7719 ideal: 7 proposed size: 3
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 processing
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: syncing set 1
with 4 table(s) from provider 1
2008-05-02 14:34:22 EST DEBUG2  ssy_action_list length: 0
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: current local
log_status is 0
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1_1: current remote
log_status = 0
2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.001 seconds
delay for first row
2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.001 seconds
until close cursor
2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: inserts=0
updates=0 deletes=0
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: new sl_rowid_seq
value: 1000000000011000
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 done in
0.003 seconds
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event
1,67 MOVE_SET
2008-05-02 14:34:22 EST CONFIG moveSet: set_id=1 old_origin=1 new_origin=2
2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
+ worker signaled)
2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=2 (0 threads
+ worker signaled)
2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=1 li_receiver=2
li_provider=1
2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
+ worker signaled)
2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=3 li_receiver=2
li_provider=3
2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads
+ worker signaled)
2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=1 li_receiver=2
li_provider=3
2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads
+ worker signaled)
2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=3 li_receiver=2
li_provider=1
2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
+ worker signaled)
2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
+ worker signaled)
2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads
+ worker signaled)
2008-05-02 14:34:22 EST DEBUG1 remoteWorkerThread_1: helper thread for
provider 1 terminated
2008-05-02 14:34:22 EST DEBUG1 remoteWorkerThread_1: disconnecting
from data provider 1
2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_3: start listening
for event origin 1
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_3: forward confirm
1,66 received by 3
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_3: forward confirm
2,34 received by 3
2008-05-02 14:34:23 EST DEBUG2 remoteWorkerThread_1: forward confirm
2,34 received by 1
2008-05-02 14:34:23 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 36
2008-05-02 14:34:25 EST DEBUG2 localListenThread: Received event 2,34 SYNC
2008-05-02 14:34:25 EST DEBUG2 localListenThread: Received event 2,35 ACCEPT_SET
2008-05-02 14:34:25 EST DEBUG2 localListenThread: ACCEPT_SET

Node3's slon daemon logfile contains:
=================================
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_2: Received event
2,35 ACCEPT_SET
2008-05-02 14:34:22 EST DEBUG2 start processing ACCEPT_SET
2008-05-02 14:34:22 EST DEBUG2 ACCEPT: set=1
2008-05-02 14:34:22 EST DEBUG2 ACCEPT: old origin=1
2008-05-02 14:34:22 EST DEBUG2 ACCEPT: new origin=2
2008-05-02 14:34:22 EST DEBUG2 ACCEPT: move set seq=67
2008-05-02 14:34:22 EST DEBUG2 got parms ACCEPT_SET
2008-05-02 14:34:22 EST DEBUG2 ACCEPT_SET - node not origin
2008-05-02 14:34:22 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
not received yet - sleep
2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: queue event 1,66 SYNC
2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: queue event 1,67 MOVE_SET
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event 1,66 SYNC
2008-05-02 14:34:22 EST DEBUG2 calc sync size - last time: 1 last
length: 111 ideal: 540 proposed size: 3
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 processing
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: syncing set 1
with 4 table(s) from provider 1
2008-05-02 14:34:22 EST DEBUG2  ssy_action_list length: 0
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: current local
log_status is 0
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1_1: current remote
log_status = 0
2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.000 seconds
delay for first row
2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.000 seconds
until close cursor
2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: inserts=0
updates=0 deletes=0
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: new sl_rowid_seq
value: 1000000000011000
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 done in
0.002 seconds
2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event
1,67 MOVE_SET
2008-05-02 14:34:23 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 35
2008-05-02 14:34:24 EST DEBUG2 localListenThread: Received event 3,35 SYNC
2008-05-02 14:34:26 EST DEBUG2 remoteListenThread_2: queue event 2,36 SYNC
2008-05-02 14:34:30 EST DEBUG2 remoteListenThread_1: queue event 1,68 SYNC
2008-05-02 14:34:32 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
not received yet - sleep
2008-05-02 14:34:33 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 36
2008-05-02 14:34:34 EST DEBUG2 localListenThread: Received event 3,36 SYNC
2008-05-02 14:34:34 EST DEBUG2 remoteListenThread_1: queue event 1,69 SYNC
2008-05-02 14:34:34 EST DEBUG2 remoteListenThread_1: queue event 2,37 SYNC
2008-05-02 14:34:38 EST DEBUG2 remoteListenThread_1: LISTEN
2008-05-02 14:34:42 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
not received yet - sleep
2008-05-02 14:34:44 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 37
2008-05-02 14:34:44 EST DEBUG2 remoteListenThread_1: queue event 1,70 SYNC
2008-05-02 14:34:44 EST DEBUG2 remoteListenThread_1: UNLISTEN
2008-05-02 14:34:44 EST DEBUG2 remoteListenThread_2: queue event 2,38 SYNC
2008-05-02 14:34:44 EST DEBUG2 localListenThread: Received event 3,37 SYNC
2008-05-02 14:34:48 EST DEBUG2 remoteListenThread_1: LISTEN
2008-05-02 14:34:52 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
not received yet - sleep

The "ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep"
continue....

>  Sometimes in 1.2.12 and 1.2.14rc the failover works,

Yes, I've also experienced this - _occasionally_ the first 'move set'
will work, but moving again always kills it.

>
>  I'm more than happy to work thru this as I really want to push out
>  8.3.1 and would love to have a functioning 1.2.14 slon release, but
>  something bad happened between 1.2.11 and current.. Either something
>  new that I have not added to my setup scripts or it's the code.
>
>  I'll work with someone on this!!!

I'm going to investigate this myself in more detail when I get some
time - if I come up with anything useful, I'll be posting...

I know this post is basically just  yet another description of the
issue, but I thought it might be valuable nonetheless. Anyone see
anything wrong in my setup that might be causing this, instead of the
bug?

Thanks

Charles Duffy
From y-mori at sraoss.co.jp  Thu May  1 23:49:55 2008
From: y-mori at sraoss.co.jp (Yoshiharu Mori)
Date: Thu May  1 23:50:19 2008
Subject: [Slony1-general] 1.2.14rc still does not appear to handle
	switchover cleanly
In-Reply-To: <dfdaea8f0805012335w5162ba4eueab9ff11838c4105@mail.gmail.com>
References: <8a547c840805011217y45deaf70ob58d98ad6dc75369@mail.gmail.com>
	<dfdaea8f0805012335w5162ba4eueab9ff11838c4105@mail.gmail.com>
Message-ID: <20080502154955.91956aa2.y-mori@sraoss.co.jp>

Hi 

> >  Now with 1.2.12 and 1.2.14rc (have not tested 1.2.13 yet (but since
> >  it's apparent in 1.2.12 and in 1.2.14rc even with the "patch/possible
> >  fix", I'm guessing the issue is very much in 1.2.13 and there is a
> >  large issue as failover and switchover are key elements in this
> >  application.
> 
> Just to add my test report on 1.2.14RC - I agree that it still exhibits
> the "move set" problem.

Do you attach the patch on the following pages ?
http://lists.slony.info/pipermail/slony1-general/2008-March/007690.html

> 
> I tested it out with a 3-node PostgreSQL 8.3.1 cluster:
> 
>       node3 <-- node1 --> node2
> 
> There's one replication set. It contains all the tables used in a pgbench
> installation (the test app). The initial state was: origin on node1,
> with node2 and
> node3 both subscribing directly to node1.
> 
> It works fine in this initial state:
> 
> node1=# insert into accounts values (100000000, 100000, 100000, 'hello sailor');
> INSERT 0 1
> node1=# \c node2
> You are now connected to database "node2".
> node2=# select * from accounts where aid = 100000000;
>     aid    |  bid   | abalance |                                        filler
> -----------+--------+----------+--------------------------------------------------------------------------------------
>  100000000 | 100000 |   100000 | hello sailor
> (1 row)
> 
> node2=# \c node3
> You are now connected to database "node3".
> node3=# select * from accounts where aid = 100000000;
>     aid    |  bid   | abalance |                                        filler
> -----------+--------+----------+--------------------------------------------------------------------------------------
>  100000000 | 100000 |   100000 | hello sailor
> (1 row)
> 
> node3=# \q
> 
> So, we're getting data. It worked with a large pgbench run as well.
> 
> The move set, however, causes problems:
> 
> [ccd@hpsystem slontest]$ slonik_move_set 1 1 2 | slonik
> <stdin>:5: Locking down set 1 on node 1
> <stdin>:7: Locked down - moving it
> <stdin>:9: Replication set 1 moved from node 1 to 2.  Remember to
> <stdin>:10: update your configuration file, if necessary, to note the
> new location
> <stdin>:11: for the set.
> 
> [ccd@hpsystem slontest]$ psql -U postgres node1
> Password for user postgres:
> Welcome to psql 8.3.1, the PostgreSQL interactive terminal.
> 
> node1=# select * from _replication.sl_subscribe ;
>  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
> ---------+--------------+--------------+-------------+------------
>        1 |            1 |            3 | t           | t
>        1 |            2 |            1 | t           | t
> (2 rows)
> 
> node1=# \c node2
> You are now connected to database "node2".
> node2=# select * from _replication.sl_subscribe ;
>  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
> ---------+--------------+--------------+-------------+------------
>        1 |            1 |            3 | t           | t
>        1 |            2 |            1 | t           | t
> (2 rows)
> 
> node2=# \c node3
> You are now connected to database "node3".
> node3=# select * from _replication.sl_subscribe ;
>  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
> ---------+--------------+--------------+-------------+------------
>        1 |            1 |            2 | t           | t
>        1 |            1 |            3 | t           | t
> 
> 
> Node3 never reflects the new state of the cluster. Restarting slon
> daemons was ineffective.
> Looking at locks, the same symptomatic lock is present (as in all
> other reports of this issue):
> 
> 
> node3=# select * from pg_locks;
>    locktype    | database | relation | page | tuple | virtualxid |
> transactionid | classid | objid | objsubid | virtualtransaction | pid
> |        mode         | granted
> ---------------+----------+----------+------+-------+------------+---------------+---------+-------+----------+--------------------+------+---------------------+---------
>  virtualxid    |          |          |      |       | 16/121     |
>           |         |       |          | 16/121             | 3480 |
> ExclusiveLock       | t
>  relation      |    16387 |    20718 |      |       |            |
>           |         |       |          | 16/121             | 3480 |
> AccessExclusiveLock | f
>  relation      |    16387 |    20663 |      |       |            |
>           |         |       |          | 21/108             | 3489 |
> AccessShareLock     | t
>  virtualxid    |          |          |      |       | 9/130      |
>           |         |       |          | 9/130              | 3426 |
> ExclusiveLock       | t
>  relation      |    16386 |    20311 |      |       |            |
>           |         |       |          | 9/130              | 3426 |
> RowExclusiveLock    | t
>  relation      |    16386 |    20303 |      |       |            |
>           |         |       |          | 9/130              | 3426 |
> RowExclusiveLock    | t
>  transactionid |          |          |      |       |            |
>     36277 |         |       |          | 9/130              | 3426 |
> ExclusiveLock       | t
>  virtualxid    |          |          |      |       | 4/396      |
>           |         |       |          | 4/396              | 3382 |
> ExclusiveLock       | t
>  relation      |    16387 |    10969 |      |       |            |
>           |         |       |          | 23/134             | 5232 |
> AccessShareLock     | t
>  virtualxid    |          |          |      |       | 21/108     |
>           |         |       |          | 21/108             | 3489 |
> ExclusiveLock       | t
>  relation      |    16386 |     2614 |      |       |            |
>           |         |       |          | 9/130              | 3426 |
> ExclusiveLock       | t
>  relation      |    16387 |    20669 |      |       |            |
>           |         |       |          | 21/108             | 3489 |
> AccessShareLock     | t
>  relation      |    16386 |     2614 |      |       |            |
>           |         |       |          | 4/396              | 3382 |
> ExclusiveLock       | f
>  virtualxid    |          |          |      |       | 23/134     |
>           |         |       |          | 23/134             | 5232 |
> ExclusiveLock       | t
>  relation      |    16387 |    20718 |      |       |            |
>           |         |       |          | 21/108             | 3489 |
> AccessExclusiveLock | t
> (15 rows)
> 
> node3=# select * from pg_locks where granted = 'f';
>  locktype | database | relation | page | tuple | virtualxid |
> transactionid | classid | objid | objsubid | virtualtransaction | pid
> |        mode         | granted
> ----------+----------+----------+------+-------+------------+---------------+---------+-------+----------+--------------------+------+---------------------+---------
>  relation |    16387 |    20718 |      |       |            |
>      |         |       |          | 16/121             | 3480 |
> AccessExclusiveLock | f
> (1 row)
> 
> node3=# select relname,relnamespace from pg_class where oid = 20718;
>     relname     | relnamespace
> ----------------+--------------
>  sl_config_lock |        20491
> (1 row)
> 
> 
> looking at logfiles:
> 
> Node2's slon daemon logfile contains:
> =================================
> 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: queue event 1,67 MOVE_SET
> 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: UNLISTEN
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event 1,66 SYNC
> 2008-05-02 14:34:22 EST DEBUG2 calc sync size - last time: 1 last
> length: 7719 ideal: 7 proposed size: 3
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 processing
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: syncing set 1
> with 4 table(s) from provider 1
> 2008-05-02 14:34:22 EST DEBUG2  ssy_action_list length: 0
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: current local
> log_status is 0
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1_1: current remote
> log_status = 0
> 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.001 seconds
> delay for first row
> 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.001 seconds
> until close cursor
> 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: inserts=0
> updates=0 deletes=0
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: new sl_rowid_seq
> value: 1000000000011000
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 done in
> 0.003 seconds
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event
> 1,67 MOVE_SET
> 2008-05-02 14:34:22 EST CONFIG moveSet: set_id=1 old_origin=1 new_origin=2
> 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
> + worker signaled)
> 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=2 (0 threads
> + worker signaled)
> 2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=1 li_receiver=2
> li_provider=1
> 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
> + worker signaled)
> 2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=3 li_receiver=2
> li_provider=3
> 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads
> + worker signaled)
> 2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=1 li_receiver=2
> li_provider=3
> 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads
> + worker signaled)
> 2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=3 li_receiver=2
> li_provider=1
> 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
> + worker signaled)
> 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
> + worker signaled)
> 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads
> + worker signaled)
> 2008-05-02 14:34:22 EST DEBUG1 remoteWorkerThread_1: helper thread for
> provider 1 terminated
> 2008-05-02 14:34:22 EST DEBUG1 remoteWorkerThread_1: disconnecting
> from data provider 1
> 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_3: start listening
> for event origin 1
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_3: forward confirm
> 1,66 received by 3
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_3: forward confirm
> 2,34 received by 3
> 2008-05-02 14:34:23 EST DEBUG2 remoteWorkerThread_1: forward confirm
> 2,34 received by 1
> 2008-05-02 14:34:23 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 36
> 2008-05-02 14:34:25 EST DEBUG2 localListenThread: Received event 2,34 SYNC
> 2008-05-02 14:34:25 EST DEBUG2 localListenThread: Received event 2,35 ACCEPT_SET
> 2008-05-02 14:34:25 EST DEBUG2 localListenThread: ACCEPT_SET
> 
> Node3's slon daemon logfile contains:
> =================================
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_2: Received event
> 2,35 ACCEPT_SET
> 2008-05-02 14:34:22 EST DEBUG2 start processing ACCEPT_SET
> 2008-05-02 14:34:22 EST DEBUG2 ACCEPT: set=1
> 2008-05-02 14:34:22 EST DEBUG2 ACCEPT: old origin=1
> 2008-05-02 14:34:22 EST DEBUG2 ACCEPT: new origin=2
> 2008-05-02 14:34:22 EST DEBUG2 ACCEPT: move set seq=67
> 2008-05-02 14:34:22 EST DEBUG2 got parms ACCEPT_SET
> 2008-05-02 14:34:22 EST DEBUG2 ACCEPT_SET - node not origin
> 2008-05-02 14:34:22 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
> not received yet - sleep
> 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: queue event 1,66 SYNC
> 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: queue event 1,67 MOVE_SET
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event 1,66 SYNC
> 2008-05-02 14:34:22 EST DEBUG2 calc sync size - last time: 1 last
> length: 111 ideal: 540 proposed size: 3
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 processing
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: syncing set 1
> with 4 table(s) from provider 1
> 2008-05-02 14:34:22 EST DEBUG2  ssy_action_list length: 0
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: current local
> log_status is 0
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1_1: current remote
> log_status = 0
> 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.000 seconds
> delay for first row
> 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.000 seconds
> until close cursor
> 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: inserts=0
> updates=0 deletes=0
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: new sl_rowid_seq
> value: 1000000000011000
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 done in
> 0.002 seconds
> 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event
> 1,67 MOVE_SET
> 2008-05-02 14:34:23 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 35
> 2008-05-02 14:34:24 EST DEBUG2 localListenThread: Received event 3,35 SYNC
> 2008-05-02 14:34:26 EST DEBUG2 remoteListenThread_2: queue event 2,36 SYNC
> 2008-05-02 14:34:30 EST DEBUG2 remoteListenThread_1: queue event 1,68 SYNC
> 2008-05-02 14:34:32 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
> not received yet - sleep
> 2008-05-02 14:34:33 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 36
> 2008-05-02 14:34:34 EST DEBUG2 localListenThread: Received event 3,36 SYNC
> 2008-05-02 14:34:34 EST DEBUG2 remoteListenThread_1: queue event 1,69 SYNC
> 2008-05-02 14:34:34 EST DEBUG2 remoteListenThread_1: queue event 2,37 SYNC
> 2008-05-02 14:34:38 EST DEBUG2 remoteListenThread_1: LISTEN
> 2008-05-02 14:34:42 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
> not received yet - sleep
> 2008-05-02 14:34:44 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 37
> 2008-05-02 14:34:44 EST DEBUG2 remoteListenThread_1: queue event 1,70 SYNC
> 2008-05-02 14:34:44 EST DEBUG2 remoteListenThread_1: UNLISTEN
> 2008-05-02 14:34:44 EST DEBUG2 remoteListenThread_2: queue event 2,38 SYNC
> 2008-05-02 14:34:44 EST DEBUG2 localListenThread: Received event 3,37 SYNC
> 2008-05-02 14:34:48 EST DEBUG2 remoteListenThread_1: LISTEN
> 2008-05-02 14:34:52 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
> not received yet - sleep
> 
> The "ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep"
> continue....
> 
> >  Sometimes in 1.2.12 and 1.2.14rc the failover works,
> 
> Yes, I've also experienced this - _occasionally_ the first 'move set'
> will work, but moving again always kills it.
> 
> >
> >  I'm more than happy to work thru this as I really want to push out
> >  8.3.1 and would love to have a functioning 1.2.14 slon release, but
> >  something bad happened between 1.2.11 and current.. Either something
> >  new that I have not added to my setup scripts or it's the code.
> >
> >  I'll work with someone on this!!!
> 
> I'm going to investigate this myself in more detail when I get some
> time - if I come up with anything useful, I'll be posting...
> 
> I know this post is basically just  yet another description of the
> issue, but I thought it might be valuable nonetheless. Anyone see
> anything wrong in my setup that might be causing this, instead of the
> bug?
> 
> Thanks
> 
> Charles Duffy
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 


-- 
SRA OSS, Inc. Japan
Yoshiharu Mori <y-mori@sraoss.co.jp>
http://www.sraoss.co.jp/
From charles.duffy at gmail.com  Thu May  1 23:57:15 2008
From: charles.duffy at gmail.com (Charles Duffy)
Date: Thu May  1 23:57:35 2008
Subject: [Slony1-general] 1.2.14rc still does not appear to handle
	switchover cleanly
In-Reply-To: <20080502154955.91956aa2.y-mori@sraoss.co.jp>
References: <8a547c840805011217y45deaf70ob58d98ad6dc75369@mail.gmail.com>
	<dfdaea8f0805012335w5162ba4eueab9ff11838c4105@mail.gmail.com>
	<20080502154955.91956aa2.y-mori@sraoss.co.jp>
Message-ID: <dfdaea8f0805012357u4eaa93cdk318aca580f8c4b35@mail.gmail.com>

On Fri, May 2, 2008 at 4:49 PM, Yoshiharu Mori <y-mori@sraoss.co.jp> wrote:
> Hi
>
>
>  > >  Now with 1.2.12 and 1.2.14rc (have not tested 1.2.13 yet (but since
>  > >  it's apparent in 1.2.12 and in 1.2.14rc even with the "patch/possible
>  > >  fix", I'm guessing the issue is very much in 1.2.13 and there is a
>  > >  large issue as failover and switchover are key elements in this
>  > >  application.
>  >
>  > Just to add my test report on 1.2.14RC - I agree that it still exhibits
>  > the "move set" problem.
>
>  Do you attach the patch on the following pages ?
>  http://lists.slony.info/pipermail/slony1-general/2008-March/007690.html
>

No, silly me. I'll try it now...

Charles Duffy
From glynastill at yahoo.co.uk  Fri May  2 02:07:18 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Fri May  2 02:07:46 2008
Subject: [Slony1-general] 1.2.14rc still does not appear to handle
	switchover cleanly
Message-ID: <82646.48814.qm@web25802.mail.ukl.yahoo.com>

Do we need to apply this patch? We have 3 nodes, 1 origin with 2 subscribers to it.

I'd have expected such a patch would have been applied to the 1.2.14rc tarball? I guess I don''t understand something about the release procedures etc.

----- Original Message ----
> From: Yoshiharu Mori <y-mori@sraoss.co.jp>
> To: Charles Duffy <charles.duffy@gmail.com>
> Cc: slony1-general@lists.slony.info
> Sent: Friday, 2 May, 2008 7:49:55 AM
> Subject: Re: [Slony1-general] 1.2.14rc still does not appear to handle switchover cleanly
> 
> Hi 
> 
> > >  Now with 1.2.12 and 1.2.14rc (have not tested 1.2.13 yet (but since
> > >  it's apparent in 1.2.12 and in 1.2.14rc even with the "patch/possible
> > >  fix", I'm guessing the issue is very much in 1.2.13 and there is a
> > >  large issue as failover and switchover are key elements in this
> > >  application.
> > 
> > Just to add my test report on 1.2.14RC - I agree that it still exhibits
> > the "move set" problem.
> 
> Do you attach the patch on the following pages ?
> http://lists.slony.info/pipermail/slony1-general/2008-March/007690.html
> 
> > 
> > I tested it out with a 3-node PostgreSQL 8..3.1 cluster:
> > 
> >       node3 <-- node1 --> node2
> > 
> > There's one replication set. It contains all the tables used in a pgbench
> > installation (the test app). The initial state was: origin on node1,
> > with node2 and
> > node3 both subscribing directly to node1.
> > 
> > It works fine in this initial state:
> > 
> > node1=# insert into accounts values (100000000, 100000, 100000, 'hello 
> sailor');
> > INSERT 0 1
> > node1=# \c node2
> > You are now connected to database "node2".
> > node2=# select * from accounts where aid = 100000000;
> >     aid    |  bid   | abalance |                                        filler
> > 
> -----------+--------+----------+--------------------------------------------------------------------------------------
> >  100000000 | 100000 |   100000 | hello sailor
> > (1 row)
> > 
> > node2=# \c node3
> > You are now connected to database "node3".
> > node3=# select * from accounts where aid = 100000000;
> >     aid    |  bid   | abalance |                                        filler
> > 
> -----------+--------+----------+--------------------------------------------------------------------------------------
> >  100000000 | 100000 |   100000 | hello sailor
> > (1 row)
> > 
> > node3=# \q
> > 
> > So, we're getting data. It worked with a large pgbench run as well.
> > 
> > The move set, however, causes problems:
> > 
> > [ccd@hpsystem slontest]$ slonik_move_set 1 1 2 | slonik
> > :5: Locking down set 1 on node 1
> > :7: Locked down - moving it
> > :9: Replication set 1 moved from node 1 to 2.  Remember to
> > :10: update your configuration file, if necessary, to note the
> > new location
> > :11: for the set.
> > 
> > [ccd@hpsystem slontest]$ psql -U postgres node1
> > Password for user postgres:
> > Welcome to psql 8.3.1, the PostgreSQL interactive terminal.
> > 
> > node1=# select * from _replication.sl_subscribe ;
> >  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
> > ---------+--------------+--------------+-------------+------------
> >        1 |            1 |            3 | t           | t
> >        1 |            2 |            1 | t           | t
> > (2 rows)
> > 
> > node1=# \c node2
> > You are now connected to database "node2".
> > node2=# select * from _replication.sl_subscribe ;
> >  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
> > ---------+--------------+--------------+-------------+------------
> >        1 |            1 |            3 | t           | t
> >        1 |            2 |            1 | t           | t
> > (2 rows)
> > 
> > node2=# \c node3
> > You are now connected to database "node3".
> > node3=# select * from _replication.sl_subscribe ;
> >  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
> > ---------+--------------+--------------+-------------+------------
> >        1 |            1 |            2 | t           | t
> >        1 |            1 |            3 | t           | t
> > 
> > 
> > Node3 never reflects the new state of the cluster. Restarting slon
> > daemons was ineffective.
> > Looking at locks, the same symptomatic lock is present (as in all
> > other reports of this issue):
> > 
> > 
> > node3=# select * from pg_locks;
> >    locktype    | database | relation | page | tuple | virtualxid |
> > transactionid | classid | objid | objsubid | virtualtransaction | pid
> > |        mode         | granted
> > 
> ---------------+----------+----------+------+-------+------------+---------------+---------+-------+----------+--------------------+------+---------------------+---------
> >  virtualxid    |          |          |      |       | 16/121     |
> >           |         |       |          | 16/121             | 3480 |
> > ExclusiveLock       | t
> >  relation      |    16387 |    20718 |      |       |            |
> >           |         |       |          | 16/121             | 3480 |
> > AccessExclusiveLock | f
> >  relation      |    16387 |    20663 |      |       |            |
> >           |         |       |          | 21/108             | 3489 |
> > AccessShareLock     | t
> >  virtualxid    |          |          |      |       | 9/130      |
> >           |         |       |          | 9/130              | 3426 |
> > ExclusiveLock       | t
> >  relation      |    16386 |    20311 |      |       |            |
> >           |         |       |          | 9/130              | 3426 |
> > RowExclusiveLock    | t
> >  relation      |    16386 |    20303 |      |       |            |
> >           |         |       |          | 9/130              | 3426 |
> > RowExclusiveLock    | t
> >  transactionid |          |          |      |       |            |
> >     36277 |         |       |          | 9/130              | 3426 |
> > ExclusiveLock       | t
> >  virtualxid    |          |          |      |       | 4/396      |
> >           |         |       |          | 4/396              | 3382 |
> > ExclusiveLock       | t
> >  relation      |    16387 |    10969 |      |       |            |
> >           |         |       |          | 23/134             | 5232 |
> > AccessShareLock     | t
> >  virtualxid    |          |          |      |       | 21/108     |
> >           |         |       |          | 21/108             | 3489 |
> > ExclusiveLock       | t
> >  relation      |    16386 |     2614 |      |       |            |
> >           |         |       |          | 9/130              | 3426 |
> > ExclusiveLock       | t
> >  relation      |    16387 |    20669 |      |       |            |
> >           |         |       |          | 21/108             | 3489 |
> > AccessShareLock     | t
> >  relation      |    16386 |     2614 |      |       |            |
> >           |         |       |          | 4/396              | 3382 |
> > ExclusiveLock       | f
> >  virtualxid    |          |          |      |       | 23/134     |
> >           |         |       |          | 23/134             | 5232 |
> > ExclusiveLock       | t
> >  relation      |    16387 |    20718 |      |       |            |
> >           |         |       |          | 21/108             | 3489 |
> > AccessExclusiveLock | t
> > (15 rows)
> > 
> > node3=# select * from pg_locks where granted = 'f';
> >  locktype | database | relation | page | tuple | virtualxid |
> > transactionid | classid | objid | objsubid | virtualtransaction | pid
> > |        mode         | granted
> > 
> ----------+----------+----------+------+-------+------------+---------------+---------+-------+----------+--------------------+------+---------------------+---------
> >  relation |    16387 |    20718 |      |       |            |
> >      |         |       |          | 16/121             | 3480 |
> > AccessExclusiveLock | f
> > (1 row)
> > 
> > node3=# select relname,relnamespace from pg_class where oid = 20718;
> >     relname     | relnamespace
> > ----------------+--------------
> >  sl_config_lock |        20491
> > (1 row)
> > 
> > 
> > looking at logfiles:
> > 
> > Node2's slon daemon logfile contains:
> > =================================
> > 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: queue event 1,67 MOVE_SET
> > 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: UNLISTEN
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event 1,66 SYNC
> > 2008-05-02 14:34:22 EST DEBUG2 calc sync size - last time: 1 last
> > length: 7719 ideal: 7 proposed size: 3
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 processing
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: syncing set 1
> > with 4 table(s) from provider 1
> > 2008-05-02 14:34:22 EST DEBUG2  ssy_action_list length: 0
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: current local
> > log_status is 0
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1_1: current remote
> > log_status = 0
> > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.001 seconds
> > delay for first row
> > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.001 seconds
> > until close cursor
> > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: inserts=0
> > updates=0 deletes=0
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: new sl_rowid_seq
> > value: 1000000000011000
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 done in
> > 0.003 seconds
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event
> > 1,67 MOVE_SET
> > 2008-05-02 14:34:22 EST CONFIG moveSet: set_id=1 old_origin=1 new_origin=2
> > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
> > + worker signaled)
> > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=2 (0 threads
> > + worker signaled)
> > 2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=1 li_receiver=2
> > li_provider=1
> > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
> > + worker signaled)
> > 2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=3 li_receiver=2
> > li_provider=3
> > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads
> > + worker signaled)
> > 2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=1 li_receiver=2
> > li_provider=3
> > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads
> > + worker signaled)
> > 2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=3 li_receiver=2
> > li_provider=1
> > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
> > + worker signaled)
> > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
> > + worker signaled)
> > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads
> > + worker signaled)
> > 2008-05-02 14:34:22 EST DEBUG1 remoteWorkerThread_1: helper thread for
> > provider 1 terminated
> > 2008-05-02 14:34:22 EST DEBUG1 remoteWorkerThread_1: disconnecting
> > from data provider 1
> > 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_3: start listening
> > for event origin 1
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_3: forward confirm
> > 1,66 received by 3
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_3: forward confirm
> > 2,34 received by 3
> > 2008-05-02 14:34:23 EST DEBUG2 remoteWorkerThread_1: forward confirm
> > 2,34 received by 1
> > 2008-05-02 14:34:23 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 36
> > 2008-05-02 14:34:25 EST DEBUG2 localListenThread: Received event 2,34 SYNC
> > 2008-05-02 14:34:25 EST DEBUG2 localListenThread: Received event 2,35 
> ACCEPT_SET
> > 2008-05-02 14:34:25 EST DEBUG2 localListenThread: ACCEPT_SET
> > 
> > Node3's slon daemon logfile contains:
> > =================================
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_2: Received event
> > 2,35 ACCEPT_SET
> > 2008-05-02 14:34:22 EST DEBUG2 start processing ACCEPT_SET
> > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT: set=1
> > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT: old origin=1
> > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT: new origin=2
> > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT: move set seq=67
> > 2008-05-02 14:34:22 EST DEBUG2 got parms ACCEPT_SET
> > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT_SET - node not origin
> > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
> > not received yet - sleep
> > 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: queue event 1,66 SYNC
> > 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: queue event 1,67 MOVE_SET
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event 1,66 SYNC
> > 2008-05-02 14:34:22 EST DEBUG2 calc sync size - last time: 1 last
> > length: 111 ideal: 540 proposed size: 3
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 processing
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: syncing set 1
> > with 4 table(s) from provider 1
> > 2008-05-02 14:34:22 EST DEBUG2  ssy_action_list length: 0
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: current local
> > log_status is 0
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1_1: current remote
> > log_status = 0
> > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.000 seconds
> > delay for first row
> > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.000 seconds
> > until close cursor
> > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: inserts=0
> > updates=0 deletes=0
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: new sl_rowid_seq
> > value: 1000000000011000
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 done in
> > 0.002 seconds
> > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event
> > 1,67 MOVE_SET
> > 2008-05-02 14:34:23 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 35
> > 2008-05-02 14:34:24 EST DEBUG2 localListenThread: Received event 3,35 SYNC
> > 2008-05-02 14:34:26 EST DEBUG2 remoteListenThread_2: queue event 2,36 SYNC
> > 2008-05-02 14:34:30 EST DEBUG2 remoteListenThread_1: queue event 1,68 SYNC
> > 2008-05-02 14:34:32 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
> > not received yet - sleep
> > 2008-05-02 14:34:33 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 36
> > 2008-05-02 14:34:34 EST DEBUG2 localListenThread: Received event 3,36 SYNC
> > 2008-05-02 14:34:34 EST DEBUG2 remoteListenThread_1: queue event 1,69 SYNC
> > 2008-05-02 14:34:34 EST DEBUG2 remoteListenThread_1: queue event 2,37 SYNC
> > 2008-05-02 14:34:38 EST DEBUG2 remoteListenThread_1: LISTEN
> > 2008-05-02 14:34:42 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
> > not received yet - sleep
> > 2008-05-02 14:34:44 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 37
> > 2008-05-02 14:34:44 EST DEBUG2 remoteListenThread_1: queue event 1,70 SYNC
> > 2008-05-02 14:34:44 EST DEBUG2 remoteListenThread_1: UNLISTEN
> > 2008-05-02 14:34:44 EST DEBUG2 remoteListenThread_2: queue event 2,38 SYNC
> > 2008-05-02 14:34:44 EST DEBUG2 localListenThread: Received event 3,37 SYNC
> > 2008-05-02 14:34:48 EST DEBUG2 remoteListenThread_1: LISTEN
> > 2008-05-02 14:34:52 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
> > not received yet - sleep
> > 
> > The "ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep"
> > continue....
> > 
> > >  Sometimes in 1.2.12 and 1.2.14rc the failover works,
> > 
> > Yes, I've also experienced this - _occasionally_ the first 'move set'
> > will work, but moving again always kills it.
> > 
> > >
> > >  I'm more than happy to work thru this as I really want to push out
> > >  8.3.1 and would love to have a functioning 1.2.14 slon release, but
> > >  something bad happened between 1.2.11 and current.. Either something
> > >  new that I have not added to my setup scripts or it's the code.
> > >
> > >  I'll work with someone on this!!!
> > 
> > I'm going to investigate this myself in more detail when I get some
> > time - if I come up with anything useful, I'll be posting...
> > 
> > I know this post is basically just  yet another description of the
> > issue, but I thought it might be valuable nonetheless. Anyone see
> > anything wrong in my setup that might be causing this, instead of the
> > bug?
> > 
> > Thanks
> > 
> > Charles Duffy
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general@lists.slony.info
> > http://lists.slony.info/mailman/listinfo/slony1-general
> > 
> 
> 
> -- 
> SRA OSS, Inc. Japan
> Yoshiharu Mori 
> http://www.sraoss.co.jp/
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 




      __________________________________________________________
Sent from Yahoo! Mail.
A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html

From charles.duffy at gmail.com  Fri May  2 02:20:02 2008
From: charles.duffy at gmail.com (Charles Duffy)
Date: Fri May  2 02:20:26 2008
Subject: [Slony1-general] 1.2.14rc still does not appear to handle
	switchover cleanly
In-Reply-To: <82646.48814.qm@web25802.mail.ukl.yahoo.com>
References: <82646.48814.qm@web25802.mail.ukl.yahoo.com>
Message-ID: <dfdaea8f0805020220t11356626vbcf6538c90c4484e@mail.gmail.com>

On Fri, May 2, 2008 at 7:07 PM, Glyn Astill <glynastill@yahoo.co.uk> wrote:
> Do we need to apply this patch? We have 3 nodes, 1 origin with 2 subscribers to it.
>

Well, I applied the patch Mori pointed me to, repeated the test I
described in my previous post (many times), and now it no longer
exhibits the problem. Everything seems to work properly so far in my
testing with the patched 1.2.14RC. The unpatched version from the
tarball was certainly problematic.

So, your 3-node cluster works OK with the unpatched 1.2.14RC ?


>  I'd have expected such a patch would have been applied to the 1.2.14rc tarball?

I would have thought so. Just an oversight, maybe.

Charles Duffy
From y-mori at sraoss.co.jp  Fri May  2 02:26:57 2008
From: y-mori at sraoss.co.jp (Yoshiharu Mori)
Date: Fri May  2 02:27:23 2008
Subject: [Slony1-general] 1.2.14rc still does not appear to handle
	switchover cleanly
In-Reply-To: <82646.48814.qm@web25802.mail.ukl.yahoo.com>
References: <82646.48814.qm@web25802.mail.ukl.yahoo.com>
Message-ID: <20080502182657.2f0cd170.y-mori@sraoss.co.jp>


> Do we need to apply this patch? We have 3 nodes, 1 origin with 2 subscribers to it.

Yes.

> 
> I'd have expected such a patch would have been applied to the 1.2.14rc tarball? I guess I don''t understand something about the release procedures etc.

I think so too.. the original patch which I sent was not applied correctly,
I hope the release 1.2.14 or 1.2.14rc2. 

> 
> ----- Original Message ----
> > From: Yoshiharu Mori <y-mori@sraoss.co.jp>
> > To: Charles Duffy <charles.duffy@gmail.com>
> > Cc: slony1-general@lists.slony.info
> > Sent: Friday, 2 May, 2008 7:49:55 AM
> > Subject: Re: [Slony1-general] 1.2.14rc still does not appear to handle switchover cleanly
> > 
> > Hi 
> > 
> > > >  Now with 1.2.12 and 1.2.14rc (have not tested 1.2.13 yet (but since
> > > >  it's apparent in 1.2.12 and in 1.2.14rc even with the "patch/possible
> > > >  fix", I'm guessing the issue is very much in 1.2.13 and there is a
> > > >  large issue as failover and switchover are key elements in this
> > > >  application.
> > > 
> > > Just to add my test report on 1.2.14RC - I agree that it still exhibits
> > > the "move set" problem.
> > 
> > Do you attach the patch on the following pages ?
> > http://lists.slony.info/pipermail/slony1-general/2008-March/007690.html
> > 
> > > 
> > > I tested it out with a 3-node PostgreSQL 8..3.1 cluster:
> > > 
> > >       node3 <-- node1 --> node2
> > > 
> > > There's one replication set. It contains all the tables used in a pgbench
> > > installation (the test app). The initial state was: origin on node1,
> > > with node2 and
> > > node3 both subscribing directly to node1.
> > > 
> > > It works fine in this initial state:
> > > 
> > > node1=# insert into accounts values (100000000, 100000, 100000, 'hello 
> > sailor');
> > > INSERT 0 1
> > > node1=# \c node2
> > > You are now connected to database "node2".
> > > node2=# select * from accounts where aid = 100000000;
> > >     aid    |  bid   | abalance |                                        filler
> > > 
> > -----------+--------+----------+--------------------------------------------------------------------------------------
> > >  100000000 | 100000 |   100000 | hello sailor
> > > (1 row)
> > > 
> > > node2=# \c node3
> > > You are now connected to database "node3".
> > > node3=# select * from accounts where aid = 100000000;
> > >     aid    |  bid   | abalance |                                        filler
> > > 
> > -----------+--------+----------+--------------------------------------------------------------------------------------
> > >  100000000 | 100000 |   100000 | hello sailor
> > > (1 row)
> > > 
> > > node3=# \q
> > > 
> > > So, we're getting data. It worked with a large pgbench run as well.
> > > 
> > > The move set, however, causes problems:
> > > 
> > > [ccd@hpsystem slontest]$ slonik_move_set 1 1 2 | slonik
> > > :5: Locking down set 1 on node 1
> > > :7: Locked down - moving it
> > > :9: Replication set 1 moved from node 1 to 2.  Remember to
> > > :10: update your configuration file, if necessary, to note the
> > > new location
> > > :11: for the set.
> > > 
> > > [ccd@hpsystem slontest]$ psql -U postgres node1
> > > Password for user postgres:
> > > Welcome to psql 8.3.1, the PostgreSQL interactive terminal.
> > > 
> > > node1=# select * from _replication.sl_subscribe ;
> > >  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
> > > ---------+--------------+--------------+-------------+------------
> > >        1 |            1 |            3 | t           | t
> > >        1 |            2 |            1 | t           | t
> > > (2 rows)
> > > 
> > > node1=# \c node2
> > > You are now connected to database "node2".
> > > node2=# select * from _replication.sl_subscribe ;
> > >  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
> > > ---------+--------------+--------------+-------------+------------
> > >        1 |            1 |            3 | t           | t
> > >        1 |            2 |            1 | t           | t
> > > (2 rows)
> > > 
> > > node2=# \c node3
> > > You are now connected to database "node3".
> > > node3=# select * from _replication.sl_subscribe ;
> > >  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
> > > ---------+--------------+--------------+-------------+------------
> > >        1 |            1 |            2 | t           | t
> > >        1 |            1 |            3 | t           | t
> > > 
> > > 
> > > Node3 never reflects the new state of the cluster. Restarting slon
> > > daemons was ineffective.
> > > Looking at locks, the same symptomatic lock is present (as in all
> > > other reports of this issue):
> > > 
> > > 
> > > node3=# select * from pg_locks;
> > >    locktype    | database | relation | page | tuple | virtualxid |
> > > transactionid | classid | objid | objsubid | virtualtransaction | pid
> > > |        mode         | granted
> > > 
> > ---------------+----------+----------+------+-------+------------+---------------+---------+-------+----------+--------------------+------+---------------------+---------
> > >  virtualxid    |          |          |      |       | 16/121     |
> > >           |         |       |          | 16/121             | 3480 |
> > > ExclusiveLock       | t
> > >  relation      |    16387 |    20718 |      |       |            |
> > >           |         |       |          | 16/121             | 3480 |
> > > AccessExclusiveLock | f
> > >  relation      |    16387 |    20663 |      |       |            |
> > >           |         |       |          | 21/108             | 3489 |
> > > AccessShareLock     | t
> > >  virtualxid    |          |          |      |       | 9/130      |
> > >           |         |       |          | 9/130              | 3426 |
> > > ExclusiveLock       | t
> > >  relation      |    16386 |    20311 |      |       |            |
> > >           |         |       |          | 9/130              | 3426 |
> > > RowExclusiveLock    | t
> > >  relation      |    16386 |    20303 |      |       |            |
> > >           |         |       |          | 9/130              | 3426 |
> > > RowExclusiveLock    | t
> > >  transactionid |          |          |      |       |            |
> > >     36277 |         |       |          | 9/130              | 3426 |
> > > ExclusiveLock       | t
> > >  virtualxid    |          |          |      |       | 4/396      |
> > >           |         |       |          | 4/396              | 3382 |
> > > ExclusiveLock       | t
> > >  relation      |    16387 |    10969 |      |       |            |
> > >           |         |       |          | 23/134             | 5232 |
> > > AccessShareLock     | t
> > >  virtualxid    |          |          |      |       | 21/108     |
> > >           |         |       |          | 21/108             | 3489 |
> > > ExclusiveLock       | t
> > >  relation      |    16386 |     2614 |      |       |            |
> > >           |         |       |          | 9/130              | 3426 |
> > > ExclusiveLock       | t
> > >  relation      |    16387 |    20669 |      |       |            |
> > >           |         |       |          | 21/108             | 3489 |
> > > AccessShareLock     | t
> > >  relation      |    16386 |     2614 |      |       |            |
> > >           |         |       |          | 4/396              | 3382 |
> > > ExclusiveLock       | f
> > >  virtualxid    |          |          |      |       | 23/134     |
> > >           |         |       |          | 23/134             | 5232 |
> > > ExclusiveLock       | t
> > >  relation      |    16387 |    20718 |      |       |            |
> > >           |         |       |          | 21/108             | 3489 |
> > > AccessExclusiveLock | t
> > > (15 rows)
> > > 
> > > node3=# select * from pg_locks where granted = 'f';
> > >  locktype | database | relation | page | tuple | virtualxid |
> > > transactionid | classid | objid | objsubid | virtualtransaction | pid
> > > |        mode         | granted
> > > 
> > ----------+----------+----------+------+-------+------------+---------------+---------+-------+----------+--------------------+------+---------------------+---------
> > >  relation |    16387 |    20718 |      |       |            |
> > >      |         |       |          | 16/121             | 3480 |
> > > AccessExclusiveLock | f
> > > (1 row)
> > > 
> > > node3=# select relname,relnamespace from pg_class where oid = 20718;
> > >     relname     | relnamespace
> > > ----------------+--------------
> > >  sl_config_lock |        20491
> > > (1 row)
> > > 
> > > 
> > > looking at logfiles:
> > > 
> > > Node2's slon daemon logfile contains:
> > > =================================
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: queue event 1,67 MOVE_SET
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: UNLISTEN
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event 1,66 SYNC
> > > 2008-05-02 14:34:22 EST DEBUG2 calc sync size - last time: 1 last
> > > length: 7719 ideal: 7 proposed size: 3
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 processing
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: syncing set 1
> > > with 4 table(s) from provider 1
> > > 2008-05-02 14:34:22 EST DEBUG2  ssy_action_list length: 0
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: current local
> > > log_status is 0
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1_1: current remote
> > > log_status = 0
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.001 seconds
> > > delay for first row
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.001 seconds
> > > until close cursor
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: inserts=0
> > > updates=0 deletes=0
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: new sl_rowid_seq
> > > value: 1000000000011000
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 done in
> > > 0.003 seconds
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event
> > > 1,67 MOVE_SET
> > > 2008-05-02 14:34:22 EST CONFIG moveSet: set_id=1 old_origin=1 new_origin=2
> > > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
> > > + worker signaled)
> > > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=2 (0 threads
> > > + worker signaled)
> > > 2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=1 li_receiver=2
> > > li_provider=1
> > > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
> > > + worker signaled)
> > > 2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=3 li_receiver=2
> > > li_provider=3
> > > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads
> > > + worker signaled)
> > > 2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=1 li_receiver=2
> > > li_provider=3
> > > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads
> > > + worker signaled)
> > > 2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=3 li_receiver=2
> > > li_provider=1
> > > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
> > > + worker signaled)
> > > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
> > > + worker signaled)
> > > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads
> > > + worker signaled)
> > > 2008-05-02 14:34:22 EST DEBUG1 remoteWorkerThread_1: helper thread for
> > > provider 1 terminated
> > > 2008-05-02 14:34:22 EST DEBUG1 remoteWorkerThread_1: disconnecting
> > > from data provider 1
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_3: start listening
> > > for event origin 1
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_3: forward confirm
> > > 1,66 received by 3
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_3: forward confirm
> > > 2,34 received by 3
> > > 2008-05-02 14:34:23 EST DEBUG2 remoteWorkerThread_1: forward confirm
> > > 2,34 received by 1
> > > 2008-05-02 14:34:23 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 36
> > > 2008-05-02 14:34:25 EST DEBUG2 localListenThread: Received event 2,34 SYNC
> > > 2008-05-02 14:34:25 EST DEBUG2 localListenThread: Received event 2,35 
> > ACCEPT_SET
> > > 2008-05-02 14:34:25 EST DEBUG2 localListenThread: ACCEPT_SET
> > > 
> > > Node3's slon daemon logfile contains:
> > > =================================
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_2: Received event
> > > 2,35 ACCEPT_SET
> > > 2008-05-02 14:34:22 EST DEBUG2 start processing ACCEPT_SET
> > > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT: set=1
> > > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT: old origin=1
> > > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT: new origin=2
> > > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT: move set seq=67
> > > 2008-05-02 14:34:22 EST DEBUG2 got parms ACCEPT_SET
> > > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT_SET - node not origin
> > > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
> > > not received yet - sleep
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: queue event 1,66 SYNC
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: queue event 1,67 MOVE_SET
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event 1,66 SYNC
> > > 2008-05-02 14:34:22 EST DEBUG2 calc sync size - last time: 1 last
> > > length: 111 ideal: 540 proposed size: 3
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 processing
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: syncing set 1
> > > with 4 table(s) from provider 1
> > > 2008-05-02 14:34:22 EST DEBUG2  ssy_action_list length: 0
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: current local
> > > log_status is 0
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1_1: current remote
> > > log_status = 0
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.000 seconds
> > > delay for first row
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.000 seconds
> > > until close cursor
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: inserts=0
> > > updates=0 deletes=0
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: new sl_rowid_seq
> > > value: 1000000000011000
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 done in
> > > 0.002 seconds
> > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event
> > > 1,67 MOVE_SET
> > > 2008-05-02 14:34:23 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 35
> > > 2008-05-02 14:34:24 EST DEBUG2 localListenThread: Received event 3,35 SYNC
> > > 2008-05-02 14:34:26 EST DEBUG2 remoteListenThread_2: queue event 2,36 SYNC
> > > 2008-05-02 14:34:30 EST DEBUG2 remoteListenThread_1: queue event 1,68 SYNC
> > > 2008-05-02 14:34:32 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
> > > not received yet - sleep
> > > 2008-05-02 14:34:33 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 36
> > > 2008-05-02 14:34:34 EST DEBUG2 localListenThread: Received event 3,36 SYNC
> > > 2008-05-02 14:34:34 EST DEBUG2 remoteListenThread_1: queue event 1,69 SYNC
> > > 2008-05-02 14:34:34 EST DEBUG2 remoteListenThread_1: queue event 2,37 SYNC
> > > 2008-05-02 14:34:38 EST DEBUG2 remoteListenThread_1: LISTEN
> > > 2008-05-02 14:34:42 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
> > > not received yet - sleep
> > > 2008-05-02 14:34:44 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 37
> > > 2008-05-02 14:34:44 EST DEBUG2 remoteListenThread_1: queue event 1,70 SYNC
> > > 2008-05-02 14:34:44 EST DEBUG2 remoteListenThread_1: UNLISTEN
> > > 2008-05-02 14:34:44 EST DEBUG2 remoteListenThread_2: queue event 2,38 SYNC
> > > 2008-05-02 14:34:44 EST DEBUG2 localListenThread: Received event 3,37 SYNC
> > > 2008-05-02 14:34:48 EST DEBUG2 remoteListenThread_1: LISTEN
> > > 2008-05-02 14:34:52 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
> > > not received yet - sleep
> > > 
> > > The "ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep"
> > > continue....
> > > 
> > > >  Sometimes in 1.2.12 and 1.2.14rc the failover works,
> > > 
> > > Yes, I've also experienced this - _occasionally_ the first 'move set'
> > > will work, but moving again always kills it.
> > > 
> > > >
> > > >  I'm more than happy to work thru this as I really want to push out
> > > >  8.3.1 and would love to have a functioning 1.2.14 slon release, but
> > > >  something bad happened between 1.2.11 and current.. Either something
> > > >  new that I have not added to my setup scripts or it's the code.
> > > >
> > > >  I'll work with someone on this!!!
> > > 
> > > I'm going to investigate this myself in more detail when I get some
> > > time - if I come up with anything useful, I'll be posting...
> > > 
> > > I know this post is basically just  yet another description of the
> > > issue, but I thought it might be valuable nonetheless. Anyone see
> > > anything wrong in my setup that might be causing this, instead of the
> > > bug?
> > > 
> > > Thanks
> > > 
> > > Charles Duffy
> > > _______________________________________________
> > > Slony1-general mailing list
> > > Slony1-general@lists.slony.info
> > > http://lists.slony.info/mailman/listinfo/slony1-general
> > > 
> > 
> > 
> > -- 
> > SRA OSS, Inc. Japan
> > Yoshiharu Mori 
> > http://www.sraoss.co.jp/
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general@lists.slony.info
> > http://lists.slony.info/mailman/listinfo/slony1-general
> > 
> 
> 
> 
> 
>       __________________________________________________________
> Sent from Yahoo! Mail.
> A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html
> 


-- 
SRA OSS, Inc. ????
Yoshiharu Mori <y-mori@sraoss.co.jp>
http://www.sraoss.co.jp/
From glynastill at yahoo.co.uk  Fri May  2 03:46:56 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Fri May  2 03:47:23 2008
Subject: [Slony1-general] 1.2.14rc still does not appear to handle
	switchover cleanly
Message-ID: <259267.19934.qm@web25815.mail.ukl.yahoo.com>

Thanks, Yoshiharu.

I have applied the patch, moved set and moved back OK, however I'd not even attempted to do this yet since upgrading to 1.2.14rc.

----- Original Message ----
> From: Yoshiharu Mori <y-mori@sraoss.co.jp>
> To: Glyn Astill <glynastill@yahoo.co.uk>
> Cc: slony1-general@lists.slony.info
> Sent: Friday, 2 May, 2008 10:26:57 AM
> Subject: Re: [Slony1-general] 1.2.14rc still does not appear to handle switchover cleanly
> 
> 
> > Do we need to apply this patch? We have 3 nodes, 1 origin with 2 subscribers 
> to it.
> 
> Yes.
> 
> > 
> > I'd have expected such a patch would have been applied to the 1.2.14rc 
> tarball? I guess I don''t understand something about the release procedures etc.
> 
> I think so too.. the original patch which I sent was not applied correctly,
> I hope the release 1.2.14 or 1.2.14rc2. 
> 
> > 
> > ----- Original Message ----
> > > From: Yoshiharu Mori 
> > > To: Charles Duffy 
> > > Cc: slony1-general@lists.slony.info
> > > Sent: Friday, 2 May, 2008 7:49:55 AM
> > > Subject: Re: [Slony1-general] 1.2.14rc still does not appear to handle 
> switchover cleanly
> > > 
> > > Hi 
> > > 
> > > > >  Now with 1.2.12 and 1.2.14rc (have not tested 1.2.13 yet (but since
> > > > >  it's apparent in 1.2.12 and in 1.2.14rc even with the "patch/possible
> > > > >  fix", I'm guessing the issue is very much in 1.2.13 and there is a
> > > > >  large issue as failover and switchover are key elements in this
> > > > >  application.
> > > > 
> > > > Just to add my test report on 1.2.14RC - I agree that it still exhibits
> > > > the "move set" problem.
> > > 
> > > Do you attach the patch on the following pages ?
> > > http://lists.slony.info/pipermail/slony1-general/2008-March/007690.html
> > > 
> > > > 
> > > > I tested it out with a 3-node PostgreSQL 8..3.1 cluster:
> > > > 
> > > >       node3 <-- node1 --> node2
> > > > 
> > > > There's one replication set. It contains all the tables used in a pgbench
> > > > installation (the test app). The initial state was: origin on node1,
> > > > with node2 and
> > > > node3 both subscribing directly to node1.
> > > > 
> > > > It works fine in this initial state:
> > > > 
> > > > node1=# insert into accounts values (100000000, 100000, 100000, 'hello 
> > > sailor');
> > > > INSERT 0 1
> > > > node1=# \c node2
> > > > You are now connected to database "node2".
> > > > node2=# select * from accounts where aid = 100000000;
> > > >     aid    |  bid   | abalance |                                        
> filler
> > > > 
> > > 
> -----------+--------+----------+--------------------------------------------------------------------------------------
> > > >  100000000 | 100000 |   100000 | hello sailor
> > > > (1 row)
> > > > 
> > > > node2=# \c node3
> > > > You are now connected to database "node3".
> > > > node3=# select * from accounts where aid = 100000000;
> > > >     aid    |  bid   | abalance |                                        
> filler
> > > > 
> > > 
> -----------+--------+----------+--------------------------------------------------------------------------------------
> > > >  100000000 | 100000 |   100000 | hello sailor
> > > > (1 row)
> > > > 
> > > > node3=# \q
> > > > 
> > > > So, we're getting data. It worked with a large pgbench run as well.
> > > > 
> > > > The move set, however, causes problems:
> > > > 
> > > > [ccd@hpsystem slontest]$ slonik_move_set 1 1 2 | slonik
> > > > :5: Locking down set 1 on node 1
> > > > :7: Locked down - moving it
> > > > :9: Replication set 1 moved from node 1 to 2.  Remember to
> > > > :10: update your configuration file, if necessary, to note the
> > > > new location
> > > > :11: for the set.
> > > > 
> > > > [ccd@hpsystem slontest]$ psql -U postgres node1
> > > > Password for user postgres:
> > > > Welcome to psql 8.3.1, the PostgreSQL interactive terminal.
> > > > 
> > > > node1=# select * from _replication.sl_subscribe ;
> > > >  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
> > > > ---------+--------------+--------------+-------------+------------
> > > >        1 |            1 |            3 | t           | t
> > > >        1 |            2 |            1 | t           | t
> > > > (2 rows)
> > > > 
> > > > node1=# \c node2
> > > > You are now connected to database "node2".
> > > > node2=# select * from _replication.sl_subscribe ;
> > > >  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
> > > > ---------+--------------+--------------+-------------+------------
> > > >        1 |            1 |            3 | t           | t
> > > >        1 |            2 |            1 | t           | t
> > > > (2 rows)
> > > > 
> > > > node2=# \c node3
> > > > You are now connected to database "node3".
> > > > node3=# select * from _replication.sl_subscribe ;
> > > >  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
> > > > ---------+--------------+--------------+-------------+------------
> > > >        1 |            1 |            2 | t           | t
> > > >        1 |            1 |            3 | t           | t
> > > > 
> > > > 
> > > > Node3 never reflects the new state of the cluster. Restarting slon
> > > > daemons was ineffective.
> > > > Looking at locks, the same symptomatic lock is present (as in all
> > > > other reports of this issue):
> > > > 
> > > > 
> > > > node3=# select * from pg_locks;
> > > >    locktype    | database | relation | page | tuple | virtualxid |
> > > > transactionid | classid | objid | objsubid | virtualtransaction | pid
> > > > |        mode         | granted
> > > > 
> > > 
> ---------------+----------+----------+------+-------+------------+---------------+---------+-------+----------+--------------------+------+---------------------+---------
> > > >  virtualxid    |          |          |      |       | 16/121     |
> > > >           |         |       |          | 16/121             | 3480 |
> > > > ExclusiveLock       | t
> > > >  relation      |    16387 |    20718 |      |       |            |
> > > >           |         |       |          | 16/121             | 3480 |
> > > > AccessExclusiveLock | f
> > > >  relation      |    16387 |    20663 |      |       |            |
> > > >           |         |       |          | 21/108             | 3489 |
> > > > AccessShareLock     | t
> > > >  virtualxid    |          |          |      |       | 9/130      |
> > > >           |         |       |          | 9/130              | 3426 |
> > > > ExclusiveLock       | t
> > > >  relation      |    16386 |    20311 |      |       |            |
> > > >           |         |       |          | 9/130              | 3426 |
> > > > RowExclusiveLock    | t
> > > >  relation      |    16386 |    20303 |      |       |            |
> > > >           |         |       |          | 9/130              | 3426 |
> > > > RowExclusiveLock    | t
> > > >  transactionid |          |          |      |       |            |
> > > >     36277 |         |       |          | 9/130              | 3426 |
> > > > ExclusiveLock       | t
> > > >  virtualxid    |          |          |      |       | 4/396      |
> > > >           |         |       |          | 4/396              | 3382 |
> > > > ExclusiveLock       | t
> > > >  relation      |    16387 |    10969 |      |       |            |
> > > >           |         |       |          | 23/134             | 5232 |
> > > > AccessShareLock     | t
> > > >  virtualxid    |          |          |      |       | 21/108     |
> > > >           |         |       |          | 21/108             | 3489 |
> > > > ExclusiveLock       | t
> > > >  relation      |    16386 |     2614 |      |       |            |
> > > >           |         |       |          | 9/130              | 3426 |
> > > > ExclusiveLock       | t
> > > >  relation      |    16387 |    20669 |      |       |            |
> > > >           |         |       |          | 21/108             | 3489 |
> > > > AccessShareLock     | t
> > > >  relation      |    16386 |     2614 |      |       |            |
> > > >           |         |       |          | 4/396              | 3382 |
> > > > ExclusiveLock       | f
> > > >  virtualxid    |          |          |      |       | 23/134     |
> > > >           |         |       |          | 23/134             | 5232 |
> > > > ExclusiveLock       | t
> > > >  relation      |    16387 |    20718 |      |       |            |
> > > >           |         |       |          | 21/108             | 3489 |
> > > > AccessExclusiveLock | t
> > > > (15 rows)
> > > > 
> > > > node3=# select * from pg_locks where granted = 'f';
> > > >  locktype | database | relation | page | tuple | virtualxid |
> > > > transactionid | classid | objid | objsubid | virtualtransaction | pid
> > > > |        mode         | granted
> > > > 
> > > 
> ----------+----------+----------+------+-------+------------+---------------+---------+-------+----------+--------------------+------+---------------------+---------
> > > >  relation |    16387 |    20718 |      |       |            |
> > > >      |         |       |          | 16/121             | 3480 |
> > > > AccessExclusiveLock | f
> > > > (1 row)
> > > > 
> > > > node3=# select relname,relnamespace from pg_class where oid = 20718;
> > > >     relname     | relnamespace
> > > > ----------------+--------------
> > > >  sl_config_lock |        20491
> > > > (1 row)
> > > > 
> > > > 
> > > > looking at logfiles:
> > > > 
> > > > Node2's slon daemon logfile contains:
> > > > =================================
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: queue event 1,67 
> MOVE_SET
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: UNLISTEN
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event 1,66 
> SYNC
> > > > 2008-05-02 14:34:22 EST DEBUG2 calc sync size - last time: 1 last
> > > > length: 7719 ideal: 7 proposed size: 3
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 processing
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: syncing set 1
> > > > with 4 table(s) from provider 1
> > > > 2008-05-02 14:34:22 EST DEBUG2  ssy_action_list length: 0
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: current local
> > > > log_status is 0
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1_1: current remote
> > > > log_status = 0
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.001 seconds
> > > > delay for first row
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.001 seconds
> > > > until close cursor
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: inserts=0
> > > > updates=0 deletes=0
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: new sl_rowid_seq
> > > > value: 1000000000011000
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 done in
> > > > 0.003 seconds
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event
> > > > 1,67 MOVE_SET
> > > > 2008-05-02 14:34:22 EST CONFIG moveSet: set_id=1 old_origin=1 new_origin=2
> > > > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
> > > > + worker signaled)
> > > > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=2 (0 threads
> > > > + worker signaled)
> > > > 2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=1 li_receiver=2
> > > > li_provider=1
> > > > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
> > > > + worker signaled)
> > > > 2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=3 li_receiver=2
> > > > li_provider=3
> > > > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads
> > > > + worker signaled)
> > > > 2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=1 li_receiver=2
> > > > li_provider=3
> > > > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads
> > > > + worker signaled)
> > > > 2008-05-02 14:34:22 EST CONFIG storeListen: li_origin=3 li_receiver=2
> > > > li_provider=1
> > > > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
> > > > + worker signaled)
> > > > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads
> > > > + worker signaled)
> > > > 2008-05-02 14:34:22 EST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads
> > > > + worker signaled)
> > > > 2008-05-02 14:34:22 EST DEBUG1 remoteWorkerThread_1: helper thread for
> > > > provider 1 terminated
> > > > 2008-05-02 14:34:22 EST DEBUG1 remoteWorkerThread_1: disconnecting
> > > > from data provider 1
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_3: start listening
> > > > for event origin 1
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_3: forward confirm
> > > > 1,66 received by 3
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_3: forward confirm
> > > > 2,34 received by 3
> > > > 2008-05-02 14:34:23 EST DEBUG2 remoteWorkerThread_1: forward confirm
> > > > 2,34 received by 1
> > > > 2008-05-02 14:34:23 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 36
> > > > 2008-05-02 14:34:25 EST DEBUG2 localListenThread: Received event 2,34 SYNC
> > > > 2008-05-02 14:34:25 EST DEBUG2 localListenThread: Received event 2,35 
> > > ACCEPT_SET
> > > > 2008-05-02 14:34:25 EST DEBUG2 localListenThread: ACCEPT_SET
> > > > 
> > > > Node3's slon daemon logfile contains:
> > > > =================================
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_2: Received event
> > > > 2,35 ACCEPT_SET
> > > > 2008-05-02 14:34:22 EST DEBUG2 start processing ACCEPT_SET
> > > > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT: set=1
> > > > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT: old origin=1
> > > > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT: new origin=2
> > > > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT: move set seq=67
> > > > 2008-05-02 14:34:22 EST DEBUG2 got parms ACCEPT_SET
> > > > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT_SET - node not origin
> > > > 2008-05-02 14:34:22 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
> > > > not received yet - sleep
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: queue event 1,66 SYNC
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteListenThread_1: queue event 1,67 
> MOVE_SET
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event 1,66 
> SYNC
> > > > 2008-05-02 14:34:22 EST DEBUG2 calc sync size - last time: 1 last
> > > > length: 111 ideal: 540 proposed size: 3
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 processing
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: syncing set 1
> > > > with 4 table(s) from provider 1
> > > > 2008-05-02 14:34:22 EST DEBUG2  ssy_action_list length: 0
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: current local
> > > > log_status is 0
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1_1: current remote
> > > > log_status = 0
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.000 seconds
> > > > delay for first row
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: 0.000 seconds
> > > > until close cursor
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteHelperThread_1_1: inserts=0
> > > > updates=0 deletes=0
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: new sl_rowid_seq
> > > > value: 1000000000011000
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: SYNC 66 done in
> > > > 0.002 seconds
> > > > 2008-05-02 14:34:22 EST DEBUG2 remoteWorkerThread_1: Received event
> > > > 1,67 MOVE_SET
> > > > 2008-05-02 14:34:23 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 35
> > > > 2008-05-02 14:34:24 EST DEBUG2 localListenThread: Received event 3,35 SYNC
> > > > 2008-05-02 14:34:26 EST DEBUG2 remoteListenThread_2: queue event 2,36 SYNC
> > > > 2008-05-02 14:34:30 EST DEBUG2 remoteListenThread_1: queue event 1,68 SYNC
> > > > 2008-05-02 14:34:32 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
> > > > not received yet - sleep
> > > > 2008-05-02 14:34:33 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 36
> > > > 2008-05-02 14:34:34 EST DEBUG2 localListenThread: Received event 3,36 SYNC
> > > > 2008-05-02 14:34:34 EST DEBUG2 remoteListenThread_1: queue event 1,69 SYNC
> > > > 2008-05-02 14:34:34 EST DEBUG2 remoteListenThread_1: queue event 2,37 SYNC
> > > > 2008-05-02 14:34:38 EST DEBUG2 remoteListenThread_1: LISTEN
> > > > 2008-05-02 14:34:42 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
> > > > not received yet - sleep
> > > > 2008-05-02 14:34:44 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 37
> > > > 2008-05-02 14:34:44 EST DEBUG2 remoteListenThread_1: queue event 1,70 SYNC
> > > > 2008-05-02 14:34:44 EST DEBUG2 remoteListenThread_1: UNLISTEN
> > > > 2008-05-02 14:34:44 EST DEBUG2 remoteListenThread_2: queue event 2,38 SYNC
> > > > 2008-05-02 14:34:44 EST DEBUG2 localListenThread: Received event 3,37 SYNC
> > > > 2008-05-02 14:34:48 EST DEBUG2 remoteListenThread_1: LISTEN
> > > > 2008-05-02 14:34:52 EST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET
> > > > not received yet - sleep
> > > > 
> > > > The "ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep"
> > > > continue....
> > > > 
> > > > >  Sometimes in 1.2.12 and 1.2.14rc the failover works,
> > > > 
> > > > Yes, I've also experienced this - _occasionally_ the first 'move set'
> > > > will work, but moving again always kills it.
> > > > 
> > > > >
> > > > >  I'm more than happy to work thru this as I really want to push out
> > > > >  8.3.1 and would love to have a functioning 1.2.14 slon release, but
> > > > >  something bad happened between 1.2.11 and current.. Either something
> > > > >  new that I have not added to my setup scripts or it's the code.
> > > > >
> > > > >  I'll work with someone on this!!!
> > > > 
> > > > I'm going to investigate this myself in more detail when I get some
> > > > time - if I come up with anything useful, I'll be posting...
> > > > 
> > > > I know this post is basically just  yet another description of the
> > > > issue, but I thought it might be valuable nonetheless. Anyone see
> > > > anything wrong in my setup that might be causing this, instead of the
> > > > bug?
> > > > 
> > > > Thanks
> > > > 
> > > > Charles Duffy
> > > > _______________________________________________
> > > > Slony1-general mailing list
> > > > Slony1-general@lists.slony.info
> > > > http://lists.slony.info/mailman/listinfo/slony1-general
> > > > 
> > > 
> > > 
> > > -- 
> > > SRA OSS, Inc. Japan
> > > Yoshiharu Mori 
> > > http://www.sraoss.co.jp/
> > > _______________________________________________
> > > Slony1-general mailing list
> > > Slony1-general@lists.slony.info
> > > http://lists.slony.info/mailman/listinfo/slony1-general
> > > 
> > 
> > 
> > 
> > 
> >       __________________________________________________________
> > Sent from Yahoo! Mail.
> > A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html
> > 
> 
> 
> -- 
> SRA OSS, Inc. ????
> Yoshiharu Mori 
> http://www.sraoss.co.jp/
> 




      __________________________________________________________
Sent from Yahoo! Mail..
A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html

From charles.duffy at gmail.com  Fri May  2 05:58:44 2008
From: charles.duffy at gmail.com (Charles Duffy)
Date: Fri May  2 05:58:47 2008
Subject: [Slony1-general] 1.2.14rc still does not appear to handle
	switchover cleanly
In-Reply-To: <20080502182657.2f0cd170.y-mori@sraoss.co.jp>
References: <82646.48814.qm@web25802.mail.ukl.yahoo.com>
	<20080502182657.2f0cd170.y-mori@sraoss.co.jp>
Message-ID: <dfdaea8f0805020558x5a143ff6l31f07cd7fa8ac97e@mail.gmail.com>

>  > I'd have expected such a patch would have been applied to the 1.2.14rc tarball? I guess I don''t understand something about the release procedures etc.
>
>  I think so too.. the original patch which I sent was not applied correctly,
>  I hope the release 1.2.14 or 1.2.14rc2.
>

Oh, just noticed that it was committed:

http://lists.slony.info/pipermail/slony1-commit/2008-March/002214.html

Didn't make it into the tarball...

Charles Duffy
From tmblue at gmail.com  Fri May  2 08:46:40 2008
From: tmblue at gmail.com (Tory M Blue)
Date: Fri May  2 08:46:46 2008
Subject: [Slony1-general] 1.2.14rc still does not appear to handle
	switchover cleanly
In-Reply-To: <dfdaea8f0805020558x5a143ff6l31f07cd7fa8ac97e@mail.gmail.com>
References: <82646.48814.qm@web25802.mail.ukl.yahoo.com>
	<20080502182657.2f0cd170.y-mori@sraoss.co.jp>
	<dfdaea8f0805020558x5a143ff6l31f07cd7fa8ac97e@mail.gmail.com>
Message-ID: <8a547c840805020846k12ce63ebgbc9e3c70ee52ca8f@mail.gmail.com>

On Fri, May 2, 2008 at 5:58 AM, Charles Duffy <charles.duffy@gmail.com> wrote:
> >  > I'd have expected such a patch would have been applied to the 1.2.14rc tarball? I guess I don''t understand something about the release procedures etc.
>  >
>  >  I think so too.. the original patch which I sent was not applied correctly,
>  >  I hope the release 1.2.14 or 1.2.14rc2.
>  >
>
>  Oh, just noticed that it was committed:
>
>  http://lists.slony.info/pipermail/slony1-commit/2008-March/002214.html
>
>  Didn't make it into the tarball...

Okay going to patch the 1.2.14rc release and do some further testing
thanks
Tory
From tmblue at gmail.com  Fri May  2 10:02:45 2008
From: tmblue at gmail.com (Tory M Blue)
Date: Fri May  2 10:02:53 2008
Subject: [Slony1-general] 1.2.14rc still does not appear to handle
	switchover cleanly
In-Reply-To: <8a547c840805020846k12ce63ebgbc9e3c70ee52ca8f@mail.gmail.com>
References: <82646.48814.qm@web25802.mail.ukl.yahoo.com>
	<20080502182657.2f0cd170.y-mori@sraoss.co.jp>
	<dfdaea8f0805020558x5a143ff6l31f07cd7fa8ac97e@mail.gmail.com>
	<8a547c840805020846k12ce63ebgbc9e3c70ee52ca8f@mail.gmail.com>
Message-ID: <8a547c840805021002m75721050w3fbf7c6d0c5a3e35@mail.gmail.com>

On Fri, May 2, 2008 at 8:46 AM, Tory M Blue <tmblue@gmail.com> wrote:
> On Fri, May 2, 2008 at 5:58 AM, Charles Duffy <charles.duffy@gmail.com> wrote:
>  > >  > I'd have expected such a patch would have been applied to the 1.2.14rc tarball? I guess I don''t understand something about the release procedures etc.
>  >  >
>  >  >  I think so too.. the original patch which I sent was not applied correctly,
>  >  >  I hope the release 1.2.14 or 1.2.14rc2.
>  >  >
>  >
>  >  Oh, just noticed that it was committed:
>  >
>  >  http://lists.slony.info/pipermail/slony1-commit/2008-March/002214.html
>  >
>  >  Didn't make it into the tarball...
>
>  Okay going to patch the 1.2.14rc release and do some further testing
>  thanks
>  Tory


So appears this does in fact fix the fail over issue for the most
part.Although I see this on one of the slave nodes after initial
switchover. I do not see a hanging slave node causing issues with
failover nor did I have to rebuild, but I did see this error, each
time I switchover. Have done this test 4 times now (switchover,
switchback, verifying replication between each switch).

any idea? It appears to be the last node each time..

Qslavehost2 (node 4)

-Tory

2008-05-02 09:34:42 PDT DEBUG4 version for "dbname=clsdb
host=devidb04.domain.com user=postgres password=SECURED" is 80205
2008-05-02 09:34:42 PDT ERROR  remoteListenThread_3:
db_getLocalNodeId() returned 2 - wrong database?
2008-05-02 09:34:42 PDT DEBUG2 remoteListenThread_1: LISTEN
2008-05-02 09:34:42 PDT DEBUG2 remoteWorkerThread_1: forward confirm
2,103 received by 1
2008-05-02 09:34:44 PDT DEBUG2 remoteListenThread_1: queue event 1,180 SYNC
2008-05-02 09:34:44 PDT DEBUG2 remoteListenThread_1: UNLISTEN
2008-05-02 09:34:44 PDT DEBUG2 remoteWorkerThread_1: Received event 1,180 SYNC
2008-05-02 09:34:44 PDT DEBUG2 calc sync size - last time: 1 last
length: 3010 ideal: 19 proposed size: 3
2008-05-02 09:34:44 PDT DEBUG2 remoteWorkerThread_1: SYNC 180 processing
2008-05-02 09:34:44 PDT DEBUG2 remoteWorkerThread_1: no sets need
syncing for this event
2008-05-02 09:34:44 PDT DEBUG2 remoteWorkerThread_1: forward confirm
2,103 received by 3
2008-05-02 09:34:44 PDT DEBUG2 remoteWorkerThread_1: forward confirm
1,180 received by 3
2008-05-02 09:34:46 PDT DEBUG2 remoteListenThread_2: LISTEN
2008-05-02 09:34:46 PDT DEBUG2 remoteWorkerThread_2: forward confirm
1,180 received by 2
2008-05-02 09:34:46 PDT DEBUG2 localListenThread: Received event 4,92 SYNC
2008-05-02 09:34:46 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 93
2008-05-02 09:34:48 PDT DEBUG2 remoteWorkerThread_2: forward confirm
4,93 received by 2
2008-05-02 09:34:49 PDT DEBUG2 remoteListenThread_1: queue event 3,94 SYNC
2008-05-02 09:34:49 PDT DEBUG2 remoteListenThread_1: UNLISTEN
2008-05-02 09:34:49 PDT DEBUG2 remoteWorkerThread_3: Received event 3,94 SYNC
2008-05-02 09:34:49 PDT DEBUG2 calc sync size - last time: 1 last
length: 5015 ideal: 11 proposed size: 3
2008-05-02 09:34:49 PDT DEBUG2 remoteWorkerThread_3: SYNC 94 processing
2008-05-02 09:34:49 PDT DEBUG2 remoteWorkerThread_3: no sets need
syncing for this event
2008-05-02 09:34:49 PDT DEBUG2 remoteWorkerThread_1: forward confirm
3,94 received by 1
2008-05-02 09:34:49 PDT DEBUG2 remoteWorkerThread_1: forward confirm
4,93 received by 1
2008-05-02 09:34:49 PDT DEBUG2 remoteWorkerThread_1: forward confirm
4,93 received by 3
2008-05-02 09:34:49 PDT DEBUG2 remoteListenThread_2: LISTEN
2008-05-02 09:34:49 PDT DEBUG2 remoteWorkerThread_2: forward confirm
3,94 received by 2
2008-05-02 09:34:51 PDT DEBUG2 remoteListenThread_2: queue event 2,104 SYNC
2008-05-02 09:34:51 PDT DEBUG2 remoteListenThread_2: UNLISTEN
2008-05-02 09:34:51 PDT DEBUG2 remoteWorkerThread_2: Received event 2,104 SYNC
2008-05-02 09:34:51 PDT DEBUG2 calc sync size - last time: 1 last
length: 2019 ideal: 29 proposed size: 3
2008-05-02 09:34:51 PDT DEBUG2 remoteWorkerThread_2: SYNC 104 processing
2008-05-02 09:34:51 PDT DEBUG2 remoteWorkerThread_2: syncing set 1
with 9 table(s) from provider 2
2008-05-02 09:34:51 PDT DEBUG4  ssy_action_list value:
2008-05-02 09:34:51 PDT DEBUG2  ssy_action_list length: 0
2008-05-02 09:34:51 PDT DEBUG2 remoteWorkerThread_2: current local
log_status is 0
2008-05-02 09:34:51 PDT DEBUG3 remoteWorkerThread_2: activate helper 2
2008-05-02 09:34:51 PDT DEBUG4 remoteWorkerThread_2: waiting for log data
2008-05-02 09:34:51 PDT DEBUG4 remoteHelperThread_2_2: got work to do
2008-05-02 09:34:51 PDT DEBUG2 remoteWorkerThread_2_2: current remote
log_status = 0
2008-05-02 09:34:51 PDT DEBUG4 remoteHelperThread_2_2: allocate line buffers
2008-05-02 09:34:51 PDT DEBUG4 remoteHelperThread_2_2: fetch from cursor
2008-05-02 09:34:51 PDT DEBUG2 remoteHelperThread_2_2: 0.002 seconds
delay for first row
2008-05-02 09:34:51 PDT DEBUG4 remoteHelperThread_2_2: fetched 0 log rows
2008-05-02 09:34:51 PDT DEBUG4 remoteHelperThread_2_2: return 10
unused line buffers
2008-05-02 09:34:51 PDT DEBUG2 remoteHelperThread_2_2: 0.002 seconds
until close cursor
2008-05-02 09:34:51 PDT DEBUG2 remoteHelperThread_2_2: inserts=0
updates=0 deletes=0
2008-05-02 09:34:51 PDT DEBUG4 remoteHelperThread_2_2: change helper
thread status
2008-05-02 09:34:51 PDT DEBUG4 remoteHelperThread_2_2: send DONE/ERROR
line to worker
2008-05-02 09:34:51 PDT DEBUG3 remoteWorkerThread_2: helper 2 finished
2008-05-02 09:34:51 PDT DEBUG4 remoteWorkerThread_2: returning lines to pool
2008-05-02 09:34:51 PDT DEBUG3 remoteWorkerThread_2: all helpers done.
2008-05-02 09:34:51 PDT DEBUG4 remoteWorkerThread_2: changing helper 2 to IDLE
2008-05-02 09:34:51 PDT DEBUG4 remoteWorkerThread_2: cleanup
2008-05-02 09:34:51 PDT DEBUG4 remoteHelperThread_2_2: waiting for work
2008-05-02 09:34:51 PDT DEBUG2 remoteWorkerThread_2: new sl_rowid_seq
value: 2000000000000000
2008-05-02 09:34:51 PDT DEBUG2 remoteWorkerThread_2: SYNC 104 done in
0.010 seconds
2008-05-02 09:34:51 PDT DEBUG2 localListenThread: Received event 4,93 SYNC
2008-05-02 09:34:52 PDT DEBUG4 version for "dbname=clsdb
host=devidb04.domain.com user=postgres password=SECURED" is 80205
2008-05-02 09:34:52 PDT ERROR  remoteListenThread_3:
db_getLocalNodeId() returned 2 - wrong database?
From cbbrowne at ca.afilias.info  Fri May  2 13:28:35 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri May  2 13:28:52 2008
Subject: [Slony1-general] 1.2.14rc still does not appear to handle
	switchover cleanly
In-Reply-To: <82646.48814.qm@web25802.mail.ukl.yahoo.com> (Glyn Astill's
	message of "Fri, 2 May 2008 09:07:18 +0000 (GMT)")
References: <82646.48814.qm@web25802.mail.ukl.yahoo.com>
Message-ID: <60od7o1mdo.fsf@dba2.int.libertyrms.com>

Glyn Astill <glynastill@yahoo.co.uk> writes:
> Do we need to apply this patch? We have 3 nodes, 1 origin with 2 subscribers to it.
>
> I'd have expected such a patch would have been applied to the
> 1.2.14rc tarball? I guess I don''t understand something about the
> release procedures etc.

I think what happened is that I made a mistake when applying the
patch, which meant that the "rc" tarball was short a change.

I'll make sure this is in place; will pursue releasing 1.2.14 early
next week.
-- 
output = reverse("ofni.secnanifxunil" "@" "enworbbc")
http://www3.sympatico.ca/cbbrowne/sap.html
I always try to do things in chronological order. 
From cbbrowne at ca.afilias.info  Fri May  2 13:36:35 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri May  2 13:36:46 2008
Subject: [Slony1-general] configure:7134: error: Headers for libpqserver
	are not found in the includeserverdir
In-Reply-To: <a34d0ed40805011101o700c0846v65d770d9796409dd@mail.gmail.com>
	(Giuseppe Laccone's message of "Thu, 1 May 2008 20:01:03 +0200")
References: <a34d0ed40805011101o700c0846v65d770d9796409dd@mail.gmail.com>
Message-ID: <60k5ic1m0c.fsf@dba2.int.libertyrms.com>

"Giuseppe Laccone" <giuseppe.laccone@gmail.com> writes:
> hi all,
> I am trying to compile slony1 with PG8.1.11 for win32 with mingwin32
> and msys as suggested in the
> documentation:
>
> ./configure --with-pgbindir=/usr/local/pgsql/bin
> --with-pgincludedir=/usr/local/pgsql/include
> --with-pgincludeserverdir=/usr/local/pgsql/include/server
> --with-pgconfigdir=/usr/local/pgsql/bin
>
> Unfortunately I  receive always the same error even is I specify
> pgincludeserverdir:

Well, in looking at the config log (thanks for including it!), the
following seems to be suggesting that:

a) Yes, you *do* have the server #includes installed (I can see it
   referencing PostgreSQL server includes, successfully).

   So the problem *isn't* relating to not having properly referenced
   pgincludeserverdir.

b) ERROR being redefined seems troublesome.

It seems as though you have some extra #includes being drawn in that
are causing a conflict.

In particular, you probably ought to see what is in
".../include/wingdi.h", as that is evidently in conflict with
definitions in PostgreSQL #include <server/utils/elog.h>.  I have no
idea what "wingdi.h" is; evidently it's not compatible...

> C:/msys/1.0/local/pgsql/include/server/utils/elog.h:40:1: warning:
> "ERROR" redefined
>
> In file included from
> c:/mingw/bin/../lib/gcc/mingw32/3.4.5/../../../../include/windows.h:52,
>
>                  from C:/msys/1.0/local/pgsql/include/server/pg_config_os.h:7,
>
>                  from C:/msys/1.0/local/pgsql/include/server/c.h:85,
>
>                  from C:/msys/1.0/local/pgsql/include/server/postgres.h:48,
>
>                  from conftest.c:71:
>
> c:/mingw/bin/../lib/gcc/mingw32/3.4.5/../../../../include/wingdi.h:313:1:
> warning: this is the location of
> the previous definition

-- 
(format nil "~S@~S" "cbbrowne" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From giuseppe.laccone at gmail.com  Sun May  4 13:09:52 2008
From: giuseppe.laccone at gmail.com (Giuseppe Laccone)
Date: Sun May  4 13:10:04 2008
Subject: [Slony1-general] configure:7134: error: Headers for libpqserver
	are not found in the includeserverdir
In-Reply-To: <a34d0ed40805041306y6b4a2438l2f1a83de1cb2232e@mail.gmail.com>
References: <a34d0ed40805011101o700c0846v65d770d9796409dd@mail.gmail.com>
	<60k5ic1m0c.fsf@dba2.int.libertyrms.com>
	<a34d0ed40805041306y6b4a2438l2f1a83de1cb2232e@mail.gmail.com>
Message-ID: <a34d0ed40805041309m589b7a2fjb37fb658a9979dfc@mail.gmail.com>

Thanks a lot Christopher for your attention!!

  I will try to recompile it again, even if I have specifyed the
  location with --with-pgincludeserverdir=/usr/local/pgsql/include/server,
  it seems to me that here there aren't  some requested server headers.

  Do you know if there is somone who has compiled PG8.1.15 with slony1
  1.1.5 for Windows?

  I need this particular version on the master because the linux client
  is supporting just this version of slony and as I know from the
  documentation it's not possible to synchronize 2 diffenrent slony
  versions. Do you have any suggestion?


  Thanks a lot in advance!

  regards
  Giuseppe




  On Fri, May 2, 2008 at 10:36 PM, Christopher Browne
  <cbbrowne@ca.afilias.info> wrote:
  > "Giuseppe Laccone" <giuseppe.laccone@gmail.com> writes:
  >  > hi all,
  >  > I am trying to compile slony1 with PG8.1.11 for win32 with mingwin32
  >  > and msys as suggested in the
  >  > documentation:
  >  >
  >  > ./configure --with-pgbindir=/usr/local/pgsql/bin
  >  > --with-pgincludedir=/usr/local/pgsql/include
  >  > --with-pgincludeserverdir=/usr/local/pgsql/include/server
  >  > --with-pgconfigdir=/usr/local/pgsql/bin
  >  >
  >  > Unfortunately I  receive always the same error even is I specify
  >  > pgincludeserverdir:
  >
  >  Well, in looking at the config log (thanks for including it!), the
  >  following seems to be suggesting that:
  >
  >  a) Yes, you *do* have the server #includes installed (I can see it
  >    referencing PostgreSQL server includes, successfully).
  >
  >    So the problem *isn't* relating to not having properly referenced
  >    pgincludeserverdir.
  >
  >  b) ERROR being redefined seems troublesome.
  >
  >  It seems as though you have some extra #includes being drawn in that
  >  are causing a conflict.
  >
  >  In particular, you probably ought to see what is in
  >  ".../include/wingdi.h", as that is evidently in conflict with
  >  definitions in PostgreSQL #include <server/utils/elog.h>.  I have no
  >  idea what "wingdi.h" is; evidently it's not compatible...
  >
  >
  >
  >  > C:/msys/1.0/local/pgsql/include/server/utils/elog.h:40:1: warning:
  >  > "ERROR" redefined
  >  >
  >  > In file included from
  >  > c:/mingw/bin/../lib/gcc/mingw32/3.4.5/../../../../include/windows.h:52,
  >  >
  >  >                  from
C:/msys/1.0/local/pgsql/include/server/pg_config_os.h:7,
  >  >
  >  >                  from C:/msys/1.0/local/pgsql/include/server/c.h:85,
  >  >
  >  >                  from
C:/msys/1.0/local/pgsql/include/server/postgres.h:48,
  >  >
  >  >                  from conftest.c:71:
  >  >
  >  > c:/mingw/bin/../lib/gcc/mingw32/3.4.5/../../../../include/wingdi.h:313:1:
  >  > warning: this is the location of
  >  > the previous definition
  >
  >  --
  >  (format nil "~S@~S" "cbbrowne" "ca.afilias.info")
  >  <http://dba2.int.libertyrms.com/>
  >  Christopher Browne
  >  (416) 673-4124 (land)
  >
From slonymaillist at gmail.com  Mon May  5 03:39:08 2008
From: slonymaillist at gmail.com (Jakub K)
Date: Mon May  5 03:39:38 2008
Subject: [Slony1-general] Beginner question about slony - push or pull
	replication?
Message-ID: <baea41c70805050339n2b78aabdjde43d14d0643a189@mail.gmail.com>

Hello,

i am new in slony and i have question, about how it's works.

Q: slony is pull or push replication? or both?
I set replication using slonik, than start slon daemon on subsrciber (sub)
and publisher (pub).
And now, subsrciber is asking for data or publisher is sending data? How
often?
-S SYNC check interval means how often are data asking (sub) or sending
(pub)

Thanks Jakub.

*
*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080505/=
216e1e69/attachment.htm
From cbbrowne at ca.afilias.info  Mon May  5 07:20:20 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon May  5 07:20:57 2008
Subject: [Slony1-general] Beginner question about slony - push or pull
	replication?
In-Reply-To: <baea41c70805050339n2b78aabdjde43d14d0643a189@mail.gmail.com>
	(Jakub K.'s message of "Mon, 5 May 2008 12:39:08 +0200")
References: <baea41c70805050339n2b78aabdjde43d14d0643a189@mail.gmail.com>
Message-ID: <603aow25p7.fsf@dba2.int.libertyrms.com>

"Jakub K" <slonymaillist@gmail.com> writes:
> Hello,
> i am new in slony and i have question, about how it's works.
> Q: slony is pull or push replication? or both?

When it combines "publish" and "subscribe," that implies a bit of
both, though it mostly looks like "pull," as that's where the bulk of
the work takes place.

> I set replication using slonik, than start slon daemon on subsrciber
> (sub) and publisher (pub).  And now, subsrciber is asking for data
> or publisher is sending data? How often?  -S SYNC check interval
> means how often are data asking (sub) or sending (pub)

"How often" is essentially controlled by the origin; it is there that
the "check interval" is used to determine if a SYNC event should be
published.

  -> If, within the sync_interval period, replication work has arrived,
     the origin will generate a SYNC event.

  -> If the origin is quiet (e.g. - not receiving any replicable work),
     there will still be a SYNC event each sync_interval_timeout
     milliseconds.

The SYNC event is published via the LISTEN/NOTIFY system in
PostgreSQL, which will cause a quiescent subscriber to wake up and
know it has new work to do.  That's when the "pulling" takes place.
-- 
output = reverse("moc.enworbbc" "@" "enworbbc")
http://www3.sympatico.ca/cbbrowne/emacs.html
Horribly wedging my very own personal machine gives me a comfortable,
familiar, Lisp-machine feeling (like an old shoe), laced with that racy
hint of raw, cold-boot power.
From stephane.schildknecht at postgresqlfr.org  Mon May  5 08:38:09 2008
From: stephane.schildknecht at postgresqlfr.org (=?UTF-8?B?IlN0w6lwaGFuZSBBLiBTY2hpbGRrbmVjaHQi?=)
Date: Mon May  5 08:38:20 2008
Subject: [Slony1-general] sl_status for cross replication
Message-ID: <481F29E1.9080605@postgresqlfr.org>

Hi,

I put up a replication composed of 2 sets, each one subscribed by different nodes.

(that is I have 5 nodes, 2 sets

First set is replicating that way :
1 -> 2 -> 3 -> 4
     \-> 5

Second set :
4 -> 3 -> 2 -> 1)

First, I wonder which set the information given by sl_status on different nodes
refers to, and how I could get relevant information about each replicated set.

Second, if I have to failover node 1 or node 4, how would I have to proceed?

Would it be enough to fail from 1 to 2 and drop node 1? (It seems indeed that
patched 1.2.14 version works that way, but I'd prefer be sure.)

Best regards,
-- 
St?phane SCHILDKNECHT
Pr?sident de PostgreSQLFr
T?l. 09 53 69 97 12
http://www.postgresql.fr
From glynastill at yahoo.co.uk  Thu May  8 04:54:59 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Thu May  8 04:55:32 2008
Subject: [Slony1-general] subscribers failing with "child terminated status:
	11"
Message-ID: <643025.37899.qm@web25810.mail.ukl.yahoo.com>

Running slony 1.2.14rc, pg8.3.1 on debian. I've just dropped a set and am tyring to recreate it, Anyone got any idea whats going off here?

2008-05-08_124040 BST DEBUG4 remoteWorkerThread_3: update provider configuration
2008-05-08_124040 BST DEBUG4 version for "dbname=SEE user=slony host=192.168.240.11" is 80301
2008-05-08_124040 BST DEBUG1 remoteListenThread_3: connected to 'dbname=SEE host=192.168.240.12 user=slony'
2008-05-08_124040 BST DEBUG1 remoteListenThread_1: connected to 'dbname=SEE host=192.168.240.10 user=slony'
2008-05-08_124040 BST DEBUG2 remoteWorkerThread_3: forward confirm 3,189839 received by 1
2008-05-08_124040 BST DEBUG2 remoteWorkerThread_3: forward confirm 1,316706 received by 3
2008-05-08_124040 BST DEBUG2 remoteWorkerThread_3: forward confirm 2,189839 received by 3
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316707 RESET_CONFIG
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316708 RESET_CONFIG
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316709 RESET_CONFIG
2008-05-08_124040 BST DEBUG2 remoteWorkerThread_3: forward confirm 2,189837 received by 1
2008-05-08_124040 BST DEBUG2 remoteWorkerThread_1: Received event 1,316707 RESET_CONFIG
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316710 DROP_SET
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316711 STORE_SET
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316712 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316713 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316714 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316715 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316716 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316717 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316718 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316719 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316720 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316721 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316722 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316723 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316724 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316725 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316726 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316727 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316728 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316729 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316730 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316731 SYNC
2008-05-08_124040 BST DEBUG2 remoteListenThread_1: queue event 1,316732 SYNC
2008-05-08_124040 BST DEBUG2 slon: child terminated status: 11; pid: 23811, current worker pid: 23811
2008-05-08_124040 BST DEBUG1 slon: restart of worker in 10 seconds



      __________________________________________________________
Sent from Yahoo! Mail.
A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html

From postgres at dac.e4ward.com  Thu May  8 20:39:29 2008
From: postgres at dac.e4ward.com (postgres@dac.e4ward.com)
Date: Thu May  8 20:39:49 2008
Subject: [Slony1-general] Dropping the master node
Message-ID: <3387742770-1790@e4ward.com>

hi all.

Just a quick question...

I currently have a master-slave setup with postgres 8.1.10 and slony
1.2.12. I'm needing to drop slony usage on BOTH nodes for a while. I
have experience dropping slave nodes using DROP NODE... but I can't
find any info about its usage on master nodes.

Any advice?

Thanks
From victor.aluko at gmail.com  Fri May  9 05:22:44 2008
From: victor.aluko at gmail.com (ajcity)
Date: Fri May  9 05:22:48 2008
Subject: [Slony1-general] Need help on changing Slave node and adding
	another slave node
Message-ID: <17146792.post@talk.nabble.com>


 Hi all,
  I have a slight issue and I need advice on what to do.
  I already have a local machine (machineX) replicating from I remote
server(machineY). Now, my company just got another server(machineZ) with
higher config and I want to replicate the data on the machineX onto the
machineZ then setup the new server to replicate from the machineY and the X
should replicate from the machineY.
  1) Can I just change the IP of machineX to the the IP of machineY,
subscribe machineZ then replicate and when this is done, I should change the
IP of machineZ to that of machineX, and then subscribe machineX with a
different IP?
  2) Or do I just pull-down the structure and start everything again from
scratch with the appropriate config?
  I certainly dont wanna take option 2) but am really wondering how else I
can go about it.
  
  Thanks Guys.

   Victor 
-- 
View this message in context: http://www.nabble.com/Need-help-on-changing-Slave-node-and-adding-another-slave-node-tp17146792p17146792.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From cbbrowne at ca.afilias.info  Fri May  9 08:32:28 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri May  9 08:32:35 2008
Subject: [Slony1-general] Dropping the master node
In-Reply-To: <3387742770-1790@e4ward.com> (postgres@dac.e4ward.com's message
	of "Fri, 9 May 2008 00:39:29 -0300")
References: <3387742770-1790@e4ward.com>
Message-ID: <60d4nvh4s3.fsf@dba2.int.libertyrms.com>

<postgres@dac.e4ward.com> writes:
> Just a quick question...
>
> I currently have a master-slave setup with postgres 8.1.10 and slony
> 1.2.12. I'm needing to drop slony usage on BOTH nodes for a while. I
> have experience dropping slave nodes using DROP NODE... but I can't
> find any info about its usage on master nodes.
>
> Any advice?

Nodes do not, in and of themselves, have a role of either "master" or
"slave"; they're all just nodes.  That, for a particular replication
set, they are performing "master" or "slave" activities is distinct
from that.

You can just do DROP NODE, whether you think it's a "master" or a
"slave."
-- 
output = ("cbbrowne" "@" "linuxfinances.info")
http://linuxdatabases.info/info/linuxdistributions.html
Those who do not learn from history, loop.
From ajs at crankycanuck.ca  Fri May  9 09:19:28 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri May  9 09:19:52 2008
Subject: [Slony1-general] Need help on changing Slave node and adding
	another slave node
In-Reply-To: <17146792.post@talk.nabble.com>
References: <17146792.post@talk.nabble.com>
Message-ID: <20080509161928.GB11033@crankycanuck.ca>

On Fri, May 09, 2008 at 05:22:44AM -0700, ajcity wrote:
> 
>  Hi all,
>   I have a slight issue and I need advice on what to do.
>   I already have a local machine (machineX) replicating from I remote
> server(machineY). Now, my company just got another server(machineZ) with
> higher config and I want to replicate the data on the machineX onto the
> machineZ then setup the new server to replicate from the machineY and the X
> should replicate from the machineY.

So you currently have Y->X, and you want to perform Y->X->Z, and then later
Y->Z->X?  

>   1) Can I just change the IP of machineX to the the IP of machineY,
> subscribe machineZ then replicate and when this is done, I should change the
> IP of machineZ to that of machineX, and then subscribe machineX with a
> different IP?
>   2) Or do I just pull-down the structure and start everything again from
> scratch with the appropriate config?

Don't do either.  Subscribe Z using provider X, and then later move the
subscription from X to Y, and then X's subscription from Y to Z.  This is
all part of the design of Slony.

A

From victor.aluko at gmail.com  Fri May  9 11:38:44 2008
From: victor.aluko at gmail.com (ajcity)
Date: Fri May  9 11:38:55 2008
Subject: [Slony1-general] Need help on changing Slave node and adding
	another slave node
In-Reply-To: <20080509161928.GB11033@crankycanuck.ca>
References: <17146792.post@talk.nabble.com>
	<20080509161928.GB11033@crankycanuck.ca>
Message-ID: <17154203.post@talk.nabble.com>




Andrew Sullivan wrote:
> 
> On Fri, May 09, 2008 at 05:22:44AM -0700, ajcity wrote:
>> 
>>  Hi all,
>>   I have a slight issue and I need advice on what to do.
>>   I already have a local machine (machineX) replicating from I remote
>> server(machineY). Now, my company just got another server(machineZ) with
>> higher config and I want to replicate the data on the machineX onto the
>> machineZ then setup the new server to replicate from the machineY and the
>> X
>> should replicate from the machineY.
> 
> So you currently have Y->X, and you want to perform Y->X->Z, and then
> later
> Y->Z->X?  
> 

Not quite but close to it. I wanna move the data on X->Z and then instead of
using Y->X->Z, I want to use Y->Z->X



>>   1) Can I just change the IP of machineX to the the IP of machineY,
>> subscribe machineZ then replicate and when this is done, I should change
>> the
>> IP of machineZ to that of machineX, and then subscribe machineX with a
>> different IP?
>>   2) Or do I just pull-down the structure and start everything again from
>> scratch with the appropriate config?
> 
> Don't do either.  Subscribe Z using provider X, and then later move the
> subscription from X to Y, and then X's subscription from Y to Z.  This is
> all part of the design of Slony.
> 
> A
> 
Ok. Sounds like it might work but what if I wanted to change the IP of
machineZ to that of machineX and vice versa cos I wanna remove the public IP
on machineX and set it for machineZ; can I do this without disturbing Slony
or do I have to consult the network admin?

  Victor
-- 
View this message in context: http://www.nabble.com/Need-help-on-changing-Slave-node-and-adding-another-slave-node-tp17146792p17154203.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From ajs at crankycanuck.ca  Fri May  9 12:09:52 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri May  9 12:10:22 2008
Subject: [Slony1-general] Need help on changing Slave node and adding
	another slave node
In-Reply-To: <17154203.post@talk.nabble.com>
References: <17146792.post@talk.nabble.com>
	<20080509161928.GB11033@crankycanuck.ca>
	<17154203.post@talk.nabble.com>
Message-ID: <20080509190952.GC11033@crankycanuck.ca>

On Fri, May 09, 2008 at 11:38:44AM -0700, ajcity wrote:
> Ok. Sounds like it might work but what if I wanted to change the IP of
> machineZ to that of machineX and vice versa cos I wanna remove the public IP
> on machineX and set it for machineZ; can I do this without disturbing Slony
> or do I have to consult the network admin?

No, that won't work.  You'll confuse Slony.  It will be surprised that
records it thought got to one database never made it there at all.  Don't
try to subvert Slony's controls by cute work-arounds.  They almost always
break things.  (This is a significant source of much of the trouble that
some users end up having, in fact.  Slony is complicated enough without you
trying to outsmart it.)

A

From troy at troywolf.com  Fri May  9 12:37:11 2008
From: troy at troywolf.com (Troy Wolf)
Date: Fri May  9 12:37:22 2008
Subject: [Slony1-general] SUBSCRIBE problem and old transactions
Message-ID: <e0d7c3f50805091237l6e8fc11fq5bcd9a5a3442ede0@mail.gmail.com>

I am having what I think is the same problem described in section 2.3
from http://www.pgadmin.org/docs/1.4/slony/faq.html#id2602106:

=== QUOTE ====================================================
2.3.

I'm trying to get a slave subscribed, and get the following messages
in the logs:

DEBUG1 copy_set 1
DEBUG1 remoteWorkerThread_1: connected to provider DB
WARN	remoteWorkerThread_1: transactions earlier than XID 127314958 are
still in progress
WARN	remoteWorkerThread_1: data copy for set 1 failed - sleep 60 seconds

There is evidently some reasonably old outstanding transaction
blocking Slony-I from processing the sync. You might want to take a
look at pg_locks to see what's up:

sampledb=# select * from pg_locks where transaction is not null order
by transaction;
 relation | database | transaction |  pid    |     mode      | granted
----------+----------+-------------+---------+---------------+---------
          |          |   127314921 | 2605100 | ExclusiveLock | t
          |          |   127326504 | 5660904 | ExclusiveLock | t
(2 rows)

See? 127314921 is indeed older than 127314958, and it's still running.

A long running G/L report, a runaway RT3 query, a pg_dump, all will
open up transactions that may run for substantial periods of time.
Until they complete, or are interrupted, you will continue to see the
message " data copy for set 1 failed - sleep 60 seconds ".

By the way, if there is more than one database on the PostgreSQL
cluster, and activity is taking place on the OTHER database, that will
lead to there being "transactions earlier than XID whatever" being
found to be still in progress. The fact that it's a separate database
on the cluster is irrelevant; Slony-I will wait until those old
transactions terminate.
=== END QUOTE ===============================================

I am perplexed by the last paragraph above. I may be naive, but why in
the world does Slony wait on transactions that are on other databases
that have no relation to any replicated objects?

We have a schema that has several relatively small tables with lots of
foreign-key relationships. The schema also has a couple of relatively
large history tables that have zero foreign-key relationships. Many
times (but not all the time!), Slony will be unable to SYNC while a
large index is being built on one of these large tables. I repeat that
these large tables are not part of any replication set and have no FKs
to any replicated objects. WHY does Slony wait behind these
transactions?

Thank you.
From victor.aluko at gmail.com  Fri May  9 12:44:50 2008
From: victor.aluko at gmail.com (ajcity)
Date: Fri May  9 12:45:02 2008
Subject: [Slony1-general] Need help on changing Slave node and adding
	another slave node
In-Reply-To: <20080509190952.GC11033@crankycanuck.ca>
References: <17146792.post@talk.nabble.com>
	<20080509161928.GB11033@crankycanuck.ca>
	<17154203.post@talk.nabble.com>
	<20080509190952.GC11033@crankycanuck.ca>
Message-ID: <17155247.post@talk.nabble.com>



> Ok. Sounds like it might work but what if I wanted to change the IP of
> machineZ to that of machineX and vice versa cos I wanna remove the public
> IP
> on machineX and set it for machineZ; can I do this without disturbing
> Slony
> or do I have to consult the network admin?

No, that won't work.  You'll confuse Slony.  It will be surprised that
records it thought got to one database never made it there at all.  Don't
try to subvert Slony's controls by cute work-arounds.  They almost always
break things.  (This is a significant source of much of the trouble that
some users end up having, in fact.  Slony is complicated enough without you
trying to outsmart it.)

Point taken!!
But am a bit concerned since postgres on machineZ is fresh without any data
in it and there have been several rows of replicated data on machineX. Will
MOVE SET automatically cause all the data to be added into the postgres on
machineZ?

  Victor




-- 
View this message in context: http://www.nabble.com/Need-help-on-changing-Slave-node-and-adding-another-slave-node-tp17146792p17155247.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From victor.aluko at gmail.com  Fri May  9 12:46:22 2008
From: victor.aluko at gmail.com (ajcity)
Date: Fri May  9 12:46:33 2008
Subject: [Slony1-general] Need help on changing Slave node and adding
	another slave node
In-Reply-To: <17155247.post@talk.nabble.com>
References: <17146792.post@talk.nabble.com>
	<20080509161928.GB11033@crankycanuck.ca>
	<17154203.post@talk.nabble.com>
	<20080509190952.GC11033@crankycanuck.ca>
	<17155247.post@talk.nabble.com>
Message-ID: <17155273.post@talk.nabble.com>





> 
>> Ok. Sounds like it might work but what if I wanted to change the IP of
>> machineZ to that of machineX and vice versa cos I wanna remove the public
>> IP
>> on machineX and set it for machineZ; can I do this without disturbing
>> Slony
>> or do I have to consult the network admin?
> 
> No, that won't work.  You'll confuse Slony.  It will be surprised that
> records it thought got to one database never made it there at all.  Don't
> try to subvert Slony's controls by cute work-arounds.  They almost always
> break things.  (This is a significant source of much of the trouble that
> some users end up having, in fact.  Slony is complicated enough without
> you
> trying to outsmart it.)
> 

Point taken!!
But am a bit concerned since postgres on machineZ is fresh without any data
in it and there have been several rows of replicated data on machineX. Will
MOVE SET automatically cause all the data to be added into the postgres on
machineZ?

  Victor




-- 
View this message in context: http://www.nabble.com/Need-help-on-changing-Slave-node-and-adding-another-slave-node-tp17146792p17155273.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From kevin at kevinkempterllc.com  Fri May  9 13:12:04 2008
From: kevin at kevinkempterllc.com (kevin kempter)
Date: Fri May  9 13:12:19 2008
Subject: [Slony1-general] Cluster management question
Message-ID: <7AA76A39-D9DB-4B58-B073-EA077D18F040@kevinkempterllc.com>

Hi List;

I have a scenario we would like to implement (see below) , although I  
don't know if its an 'out of the box' scenario supported by SLONY. Any  
advice, links to relevant info, etc would be greatly appreciated.

Thanks in advance for your help..


==================================
we plan to have 2 environments
1) a Production "stack" with a SLONY Master and one slave
2) a Staging "stack" that mimic's production.

Here's what I'd like to pull off :

1) for initial deployment setup a single SLONY cluster with 3 slaves
    The master will be a node in the Staging environment

2) once we like what we have we'll do the following:
   - do a switch so one of the slaves in the Production environment  
becomes the master
   - remove the 2 nodes - which are now both slaves in the Staging  
environment from the cluster

3) re-shape the 2 nodes now removed from the cluster into their own 2- 
node cluster making one of them the master and retaining the data set  
currently in the database on that server.
*********
   - This is my first issue - How can I do this ?
**********

4) We'll then develop against the staging cluster which now has it's  
own 2-node SLONY cluster and we'll create exec scripts as needed for  
DDL changes

5) when we're ready to release a new version we want to do this:
   - drop the cluster attached to the Staging environment and add them  
back into the Production environment and let them sync
**************
   - My thought is that we'll simply drop the databases, and go  
through the standard SLONY setup to set these nodes up as slaves to  
the Production Master
*************

   - once the sync is done turn off user access to the app (on the  
Production environment)
   - with no changes coming into the db on either side do this:
        -  break the nodes back into 2 separate clusters (like in step  
3)
        - apply (re-apply) the DDL and code changes to the Staging  
environment that took place during our development cycle
        - Point customers to the Staging environment which now becomes  
the Production environment and the former Production environment  
becomes Staging

6) rinse and repeat as needed

So my only real question is how to do #3

Anyone have any Ideas ?



From ajs at crankycanuck.ca  Fri May  9 13:26:12 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri May  9 13:26:46 2008
Subject: [Slony1-general] Need help on changing Slave node and adding
	another slave node
In-Reply-To: <17155273.post@talk.nabble.com>
References: <17146792.post@talk.nabble.com>
	<20080509161928.GB11033@crankycanuck.ca>
	<17154203.post@talk.nabble.com>
	<20080509190952.GC11033@crankycanuck.ca>
	<17155247.post@talk.nabble.com> <17155273.post@talk.nabble.com>
Message-ID: <20080509202612.GA12064@crankycanuck.ca>

On Fri, May 09, 2008 at 12:46:22PM -0700, ajcity wrote:
> Point taken!!
> But am a bit concerned since postgres on machineZ is fresh without any data
> in it and there have been several rows of replicated data on machineX. Will
> MOVE SET automatically cause all the data to be added into the postgres on
> machineZ?

When you subscribe Z to the cluster, it will get a current version of the
database.

A

From troy at troywolf.com  Fri May  9 13:29:38 2008
From: troy at troywolf.com (Troy Wolf)
Date: Fri May  9 13:29:50 2008
Subject: [Slony1-general] More info on subscribe failure
Message-ID: <e0d7c3f50805091329w61959a76r2d46695a3beee464@mail.gmail.com>

So I have:

1. Stop the listeners on both nodes
2. Uninstall slony on both nodes (_slony schema no longer exists after this)
3. Initialize the cluster (re-install on both nodes)
4. Start the slon listeners on both nodes
5. create the set (tables and sequences in a single set)

At this point, there are no errors in the slon log on the
subscriber--all looks normal and good.

6. Subscribe the subscriber to the origin for the single set.

At this point, we get a couple of messages we've not seen before in
all our previous testing and production slony use. For every table in
the set, we get this message:

2008-05-09 16:05:52 EDT DEBUG3 remoteWorkerThread_1: table
"my_schema"."one_of_my_tables" does not require Slony-I serial key

I should point out that I'm not using the special SERIAL option that
tells slony to create a unique key. All my tables have either a PK or
a UC that I explicitly specify in my add table lines like so:

set add table (set id=1, origin=1, id=0010, fully qualified name =
'my_schema.my_table', key = 'my_table_uc1');

Then for the first sequence in the set, which we've given ID = 10, we
get this series of messages in the slon log:

2008-05-09 16:05:52 EDT DEBUG2 remoteWorkerThread_1: all tables for
set 1 found on subscriber
2008-05-09 16:05:52 EDT DEBUG2 remoteWorkerThread_1: copy sequence
"my_schema"."foobar_seq"
2008-05-09 16:05:52 EDT ERROR  remoteWorkerThread_1: "select
"_slony".setAddSequence_int(1, 10, '"my_schema"."foobar_seq"',
'my_schema.foobar_seq')" PGRES_FATAL_ERROR ERROR:  Slony-I:
setAddSequence_int(): sequence ID 10 has already been assigned
2008-05-09 16:05:52 EDT WARN   remoteWorkerThread_1: data copy for set
1 failed - sleep 60 seconds
WARNING:  there is no transaction in progress

No, my create set script does not have more than one sequence with
ID=10 in it, and no, I'm not calling the script twice. I've been
searching online resources, and although I find relevant information,
I have not found a solution. I hope one of you gurus can point me in
the right direction.

Thank you!
From martin at marquesminen.com.ar  Fri May  9 17:31:50 2008
From: martin at marquesminen.com.ar (Martin Marques)
Date: Fri May  9 17:32:14 2008
Subject: [Slony1-general] Slony-I testing, doc problems and master activity
Message-ID: <4824ECF6.3020800@marquesminen.com.ar>

I'm doing some tests with Slony-I on 2 PG 8.3 servers (Debian testing)
and I stumbled with some issues related to the Slony docs (from the
slony.info page).

For example, I configured /etc/slony1/prueba/slon.conf and
/etc/slony1/slon_tools.conf correctly to replicate three tables I have
in the "prueba" database.

Doing the steps in http://slony.info/documentation/firstdb.html in the
section "Using the altperl scripts" I found what I think are doc errors:

$ slonik_create_set 1

This line above should be, AFAICS:

$ slonik_create_set 1 | slonik

And:

$ slonik_subscribe_set  2 | slonik

is missing one parameter for slonik_subscribe_set, so it should be
something like:

$ slonik_subscribe_set  1 2 | slonik

Now, like that I got replication working great, at least that's what I
think. ;-)

The other problems, and which are the things that make me think I may
have done something wrong is that the nodes, master and slaves, DB have
quite a good amount of load, mostly rolled-back slony transactions. It
doesn't bloat the server, but I was interested in knowing if all those
queries are right.

Also, but maybe for someone else, the init script /etc/init.d/slony1
doesn't work right (I have to slon_start the nodes my hand one by one).


From drees76 at gmail.com  Fri May  9 18:02:41 2008
From: drees76 at gmail.com (David Rees)
Date: Fri May  9 18:03:19 2008
Subject: [Slony1-general] Slony-I testing, doc problems and master activity
In-Reply-To: <4824ECF6.3020800@marquesminen.com.ar>
References: <4824ECF6.3020800@marquesminen.com.ar>
Message-ID: <72dbd3150805091802r5d13dc3fmef06e3778f976469@mail.gmail.com>

On Fri, May 9, 2008 at 5:31 PM, Martin Marques
<martin@marquesminen.com.ar> wrote:
> Doing the steps in http://slony.info/documentation/firstdb.html in the
> section "Using the altperl scripts" I found what I think are doc errors:
>
> $ slonik_create_set 1
>
> This line above should be, AFAICS:
>
> $ slonik_create_set 1 | slonik

Yep, it should.

> And:
>
> $ slonik_subscribe_set  2 | slonik
>
> is missing one parameter for slonik_subscribe_set, so it should be
> something like:
>
> $ slonik_subscribe_set  1 2 | slonik

Yep, that, too. One of the slony devs will hopefully fix that soon. :-)

> The other problems, and which are the things that make me think I may
> have done something wrong is that the nodes, master and slaves, DB have
> quite a good amount of load, mostly rolled-back slony transactions. It
> doesn't bloat the server, but I was interested in knowing if all those
> queries are right.

It kind of sounds like the subscription isn't succeeding and slony is
just copying data to node 2, failing, and retrying. Are you sure that
replication is working? What do the slony logs say?

> Also, but maybe for someone else, the init script /etc/init.d/slony1
> doesn't work right (I have to slon_start the nodes my hand one by one).

Are you running slony repackaged from Debian or built from source? If
the former, you should probably file a bug with Debian.

-Dave
From martin at marquesminen.com.ar  Fri May  9 18:31:37 2008
From: martin at marquesminen.com.ar (Martin Marques)
Date: Fri May  9 18:32:02 2008
Subject: [Slony1-general] Slony-I testing, doc problems and master activity
In-Reply-To: <72dbd3150805091802r5d13dc3fmef06e3778f976469@mail.gmail.com>
References: <4824ECF6.3020800@marquesminen.com.ar>
	<72dbd3150805091802r5d13dc3fmef06e3778f976469@mail.gmail.com>
Message-ID: <4824FAF9.80807@marquesminen.com.ar>

David Rees escribi?:
> On Fri, May 9, 2008 at 5:31 PM, Martin Marques
> <martin@marquesminen.com.ar> wrote:
> 
>> The other problems, and which are the things that make me think I may
>> have done something wrong is that the nodes, master and slaves, DB have
>> quite a good amount of load, mostly rolled-back slony transactions. It
>> doesn't bloat the server, but I was interested in knowing if all those
>> queries are right.
> 
> It kind of sounds like the subscription isn't succeeding and slony is
> just copying data to node 2, failing, and retrying. Are you sure that
> replication is working? What do the slony logs say?

Lets see. from node1 log I get:

2008-05-09 22:10:43 ART DEBUG2 calc sync size - last time: 1 last 
length: 11003 ideal: 5 proposed size: 3
2008-05-09 22:10:43 ART DEBUG2 remoteWorkerThread_2: SYNC 2029 processing
2008-05-09 22:10:43 ART DEBUG2 remoteWorkerThread_2: no sets need 
syncing for this event
2008-05-09 22:10:43 ART DEBUG2 remoteWorkerThread_2: forward confirm 
1,2029 received by 2
2008-05-09 22:10:50 ART DEBUG2 syncThread: new sl_action_seq 11 - SYNC 2030
2008-05-09 22:10:52 ART DEBUG2 localListenThread: Received event 1,2030 SYNC
2008-05-09 22:10:54 ART DEBUG2 remoteListenThread_2: queue event 2,2030 SYNC
2008-05-09 22:10:54 ART DEBUG2 remoteWorkerThread_2: Received event 
2,2030 SYNC

 From node2 log:

2008-05-09 22:25:26 ART DEBUG2 remoteListenThread_1: queue event 1,2117 SYNC
2008-05-09 22:25:26 ART DEBUG2 remoteWorkerThread_1: Received event 
1,2117 SYNC
2008-05-09 22:25:26 ART DEBUG2 calc sync size - last time: 1 last 
length: 11008 ideal: 5 proposed size: 3
2008-05-09 22:25:26 ART DEBUG2 remoteWorkerThread_1: SYNC 2117 processing
2008-05-09 22:25:26 ART DEBUG2 remoteWorkerThread_1: syncing set 1 with 
3 table(s) from provider 1
2008-05-09 22:25:26 ART DEBUG2  ssy_action_list length: 0
2008-05-09 22:25:26 ART DEBUG2 remoteWorkerThread_1: current local 
log_status is 0
2008-05-09 22:25:26 ART DEBUG2 remoteWorkerThread_1_1: current remote 
log_status = 0
2008-05-09 22:25:26 ART DEBUG2 remoteHelperThread_1_1: 0.001 seconds 
delay for first row
2008-05-09 22:25:26 ART DEBUG2 remoteHelperThread_1_1: 0.002 seconds 
until close cursor
2008-05-09 22:25:26 ART DEBUG2 remoteHelperThread_1_1: inserts=0 
updates=0 deletes=0
2008-05-09 22:25:26 ART DEBUG2 remoteWorkerThread_1: new sl_rowid_seq 
value: 1000000000000000
2008-05-09 22:25:26 ART DEBUG2 remoteWorkerThread_1: SYNC 2117 done in 
0.007 seconds

These happen each 10 seconds.

On postgresql.log I get:

2008-05-09 22:25:26 ART [27507]: [28515-1] LOG:  sentencia: select 
ev_origin, ev_seqno, ev_timestamp,        ev_minxid, ev_maxxid, ev_xip, 
        e
v_type,        ev_data1, ev_data2,        ev_data3, ev_data4, 
ev_data5, ev_data6,        ev_data7, ev_data8 from 
"_replicaprueba".sl_event e
  where (e.ev_origin = '1' and e.ev_seqno > '2116') order by 
e.ev_origin, e.ev_seqno
2008-05-09 22:25:26 ART [27507]: [28516-1] LOG:  duraci?n: 0.607 ms
2008-05-09 22:25:26 ART [27507]: [28517-1] LOG:  sentencia: select 
con_origin, con_received,     max(con_seqno) as con_seqno, 
max(con_timestamp
) as con_timestamp from "_replicaprueba".sl_confirm where con_received 
<> 2 group by con_origin, con_received
2008-05-09 22:25:26 ART [27507]: [28518-1] LOG:  duraci?n: 0.493 ms
2008-05-09 22:25:26 ART [27507]: [28519-1] LOG:  sentencia: select 
ev_origin, ev_seqno, ev_timestamp,        ev_minxid, ev_maxxid, ev_xip, 
        e
v_type,        ev_data1, ev_data2,        ev_data3, ev_data4, 
ev_data5, ev_data6,        ev_data7, ev_data8 from 
"_replicaprueba".sl_event e
  where (e.ev_origin = '1' and e.ev_seqno > '2117') order by 
e.ev_origin, e.ev_seqno
2008-05-09 22:25:26 ART [27507]: [28520-1] LOG:  duraci?n: 0.428 ms
2008-05-09 22:25:26 ART [27507]: [28521-1] LOG:  sentencia: select 
con_origin, con_received,     max(con_seqno) as con_seqno, 
max(con_timestamp
) as con_timestamp from "_replicaprueba".sl_confirm where con_received 
<> 2 group by con_origin, con_received
2008-05-09 22:25:26 ART [27507]: [28522-1] LOG:  duraci?n: 0.428 ms
2008-05-09 22:25:26 ART [27516]: [29549-1] LOG:  sentencia: start 
transaction; set enable_seqscan = off; set enable_indexscan = on;
2008-05-09 22:25:26 ART [27516]: [29550-1] LOG:  duraci?n: 0.142 ms
2008-05-09 22:25:26 ART [27516]: [29551-1] LOG:  sentencia: select 
last_value from "_replicaprueba".sl_log_status
2008-05-09 22:25:26 ART [27516]: [29552-1] LOG:  duraci?n: 0.209 ms
2008-05-09 22:25:26 ART [27516]: [29553-1] LOG:  sentencia: declare LOG 
cursor for select     log_origin, log_xid, log_tableid,     log_actionseq,
log_cmdtype,     octet_length(log_cmddata),     case when 
octet_length(log_cmddata) <= 8192         then log_cmddata         else 
null end from "_r
eplicaprueba".sl_log_1 where log_origin = 1 and (  (
          log_tableid in (1,2,3)
             and (log_xid < '128705')
             and (log_xid >= '128702')
         ) ) order by log_actionseq;
2008-05-09 22:25:26 ART [27516]: [29554-1] LOG:  duraci?n: 0.445 ms
2008-05-09 22:25:26 ART [27516]: [29555-1] LOG:  sentencia: fetch 100 
from LOG;
2008-05-09 22:25:26 ART [27516]: [29556-1] LOG:  duraci?n: 0.101 ms
2008-05-09 22:25:26 ART [27516]: [29557-1] LOG:  sentencia: close LOG;
2008-05-09 22:25:26 ART [27516]: [29558-1] LOG:  duraci?n: 0.061 ms
2008-05-09 22:25:26 ART [27516]: [29559-1] LOG:  sentencia: rollback 
transaction; set enable_seqscan = default; set enable_indexscan = default;
2008-05-09 22:25:26 ART [27516]: [29560-1] LOG:  duraci?n: 0.118 ms

What are these?

From victor.aluko at gmail.com  Sat May 10 14:37:49 2008
From: victor.aluko at gmail.com (ajcity)
Date: Sat May 10 14:38:02 2008
Subject: [Slony1-general] Need help on changing Slave node and adding
	another slave node
In-Reply-To: <20080509202612.GA12064@crankycanuck.ca>
References: <17146792.post@talk.nabble.com>
	<20080509161928.GB11033@crankycanuck.ca>
	<17154203.post@talk.nabble.com>
	<20080509190952.GC11033@crankycanuck.ca>
	<17155247.post@talk.nabble.com> <17155273.post@talk.nabble.com>
	<20080509202612.GA12064@crankycanuck.ca>
Message-ID: <17167817.post@talk.nabble.com>




Andrew Sullivan wrote:
> 
> On Fri, May 09, 2008 at 12:46:22PM -0700, ajcity wrote:
>> Point taken!!
>> But am a bit concerned since postgres on machineZ is fresh without any
>> data
>> in it and there have been several rows of replicated data on machineX.
>> Will
>> MOVE SET automatically cause all the data to be added into the postgres
>> on
>> machineZ?
> 
> When you subscribe Z to the cluster, it will get a current version of the
> database.
> 
> A
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

Got it. found a way around it which might not be very elegant but works.
I dropped the clusters on X and Y, set the IP of X to Z so Z reads the old
IP of X (which has a public IP assigned to it; thats the main reason I
wanted to change the IPs), gave X a new IP.
Then I subscribed Z to Y and subscribed X to Y with the provider as Z.
So far it seems to be working; I guess I'll raise this topic again if I have
anyother problem with it

  Thanks

  Victor
-- 
View this message in context: http://www.nabble.com/Need-help-on-changing-Slave-node-and-adding-another-slave-node-tp17146792p17167817.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From slonymaillist at gmail.com  Sun May 11 08:40:11 2008
From: slonymaillist at gmail.com (Jakub K)
Date: Sun May 11 08:40:18 2008
Subject: [Slony1-general] Beginner question about slony - push or pull
	replication?
In-Reply-To: <603aow25p7.fsf@dba2.int.libertyrms.com>
References: <baea41c70805050339n2b78aabdjde43d14d0643a189@mail.gmail.com>
	<603aow25p7.fsf@dba2.int.libertyrms.com>
Message-ID: <baea41c70805110840j3d5ae4d6te6cc8a56736464a3@mail.gmail.com>

Hello,

if i understand well, publisher have two basic parameters for
generating SYNC events:

1) SYNC check interval
each SYNC check interval default 2 seconds slon looks to replication
log if there is new work (new means changes after las SYNC)  generate
SYNC
2) SYNC interval timeout
genereate SYNC each interval no matter if there is new work for
replication or not

Subsciber LISTEN and wake up + ask for changes when SYNC from
publisher comes. If SYNC was created by interval timeout it is
possible that subscriber get no new data. SYNC event is saying to
subscribers do pull replication and ask publisher for data.

In slon documentation is note that subscribers generate SYNC too - it
means that only sub that generate SYNC ask for new data or this SYNC
is send to all subs and all subs received this SYNC and ask for new
data?


Thanks a lot, Jakub.

2008/5/5 Christopher Browne <cbbrowne@ca.afilias.info>:
>
> "Jakub K" <slonymaillist@gmail.com> writes:
> > Hello,
> > i am new in slony and i have question, about how it's works.
> > Q: slony is pull or push replication? or both?
>
> When it combines "publish" and "subscribe," that implies a bit of
> both, though it mostly looks like "pull," as that's where the bulk of
> the work takes place.
>
>
> > I set replication using slonik, than start slon daemon on subsrciber
> > (sub) and publisher (pub).  And now, subsrciber is asking for data
> > or publisher is sending data? How often?  -S SYNC check interval
> > means how often are data asking (sub) or sending (pub)
>
> "How often" is essentially controlled by the origin; it is there that
> the "check interval" is used to determine if a SYNC event should be
> published.
>
>  -> If, within the sync_interval period, replication work has arrived,
>     the origin will generate a SYNC event.
>
>  -> If the origin is quiet (e.g. - not receiving any replicable work),
>     there will still be a SYNC event each sync_interval_timeout
>     milliseconds.
>
> The SYNC event is published via the LISTEN/NOTIFY system in
> PostgreSQL, which will cause a quiescent subscriber to wake up and
> know it has new work to do.  That's when the "pulling" takes place.
> --
> output = reverse("moc.enworbbc" "@" "enworbbc")
> http://www3.sympatico.ca/cbbrowne/emacs.html
> Horribly wedging my very own personal machine gives me a comfortable,
> familiar, Lisp-machine feeling (like an old shoe), laced with that racy
> hint of raw, cold-boot power.
>
From cbbrowne at ca.afilias.info  Mon May 12 08:09:28 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon May 12 08:09:34 2008
Subject: [Slony1-general] Slony-I testing, doc problems and master activity
In-Reply-To: <72dbd3150805091802r5d13dc3fmef06e3778f976469@mail.gmail.com>
	(David Rees's message of "Fri, 9 May 2008 18:02:41 -0700")
References: <4824ECF6.3020800@marquesminen.com.ar>
	<72dbd3150805091802r5d13dc3fmef06e3778f976469@mail.gmail.com>
Message-ID: <60lk2fimon.fsf@dba2.int.libertyrms.com>

"David Rees" <drees76@gmail.com> writes:
> On Fri, May 9, 2008 at 5:31 PM, Martin Marques
> <martin@marquesminen.com.ar> wrote:
>> Doing the steps in http://slony.info/documentation/firstdb.html in the
>> section "Using the altperl scripts" I found what I think are doc errors:
>>
>> $ slonik_create_set 1
>>
>> This line above should be, AFAICS:
>>
>> $ slonik_create_set 1 | slonik
>
> Yep, it should.
>
>> And:
>>
>> $ slonik_subscribe_set  2 | slonik
>>
>> is missing one parameter for slonik_subscribe_set, so it should be
>> something like:
>>
>> $ slonik_subscribe_set  1 2 | slonik
>
> Yep, that, too. One of the slony devs will hopefully fix that soon. :-)

Fixed; will get committed shortly...

Thanks for pointing it out!
-- 
(format nil "~S@~S" "cbbrowne" "linuxfinances.info")
http://www3.sympatico.ca/cbbrowne/sgml.html
When aiming for the common denominator, be prepared for the occasional
division by zero.
From cbbrowne at ca.afilias.info  Mon May 12 08:19:31 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon May 12 08:19:40 2008
Subject: [Slony1-general] Beginner question about slony - push or pull
	replication?
In-Reply-To: <baea41c70805110840j3d5ae4d6te6cc8a56736464a3@mail.gmail.com>
	(Jakub K.'s message of "Sun, 11 May 2008 17:40:11 +0200")
References: <baea41c70805050339n2b78aabdjde43d14d0643a189@mail.gmail.com>
	<603aow25p7.fsf@dba2.int.libertyrms.com>
	<baea41c70805110840j3d5ae4d6te6cc8a56736464a3@mail.gmail.com>
Message-ID: <60hcd3im7w.fsf@dba2.int.libertyrms.com>

"Jakub K" <slonymaillist@gmail.com> writes:
> Hello,
>
> if i understand well, publisher have two basic parameters for
> generating SYNC events:
>
> 1) SYNC check interval
> each SYNC check interval default 2 seconds slon looks to replication
> log if there is new work (new means changes after las SYNC)  generate
> SYNC
> 2) SYNC interval timeout
> genereate SYNC each interval no matter if there is new work for
> replication or not

That's pretty well duplicating what the docs say; certainly seems
consistent...

> Subsciber LISTEN and wake up + ask for changes when SYNC from
> publisher comes. If SYNC was created by interval timeout it is
> possible that subscriber get no new data. SYNC event is saying to
> subscribers do pull replication and ask publisher for data.

Yes, you're understanding that correctly.

The reason why we have the timeout, and generate a SYNC even though it
is not necessitated by replication data, is so that there is an
ongoing indication given that the subscribers are actually keeping up.

Supposing we didn't have the SYNC interval timeout, then, if there was
a period of time when the origin was not receiving updates, there
would be *nothing* meaningful to indicate that the subscribers are
indeed up to date.

This would manifest pretty visibly in the view sl_status; without the
"workless SYNCs," a quiescent origin would lead to it looking like the
subscribers were falling increasingly and steadily behind, which would
not change until an update was made.  It's hard to tell the difference
between that and having slons fallen over.

> In slon documentation is note that subscribers generate SYNC too - it
> means that only sub that generate SYNC ask for new data or this SYNC
> is send to all subs and all subs received this SYNC and ask for new
> data?

All the nodes send around SYNCs; if there is no corresponding
subscription to work on, those SYNCs are ignored.

I'm glad to make sure that explanations for things get published, so
that the design choices aren't just mysteries.
-- 
let name="cbbrowne" and tld="cbbrowne.com" in name ^ "@" ^ tld;;
http://cbbrowne.com/info/spreadsheets.html
Nobody can fix the economy.  Nobody can be trusted with their finger
on the button.  Nobody's perfect.  VOTE FOR NOBODY.
From mgruetzn at HTWM.De  Mon May 12 08:50:53 2008
From: mgruetzn at HTWM.De (Michael Gruetzner)
Date: Mon May 12 08:51:03 2008
Subject: [Slony1-general] Origin/Provider understanding issue
Message-ID: <DF83DA74-0CFB-43D7-84C8-6734A111A105@htwm.de>

Hi,

I've read the documentation item 5.5 at http://slony.info/documentation/concepts.html 
.
If I understand this correctly, it would not work, if a user updates a  
table belonging to
a specific replication set on a node which is _not_ the origin of that  
replication set.

Would it be possible to define multiple origin nodes for one  
replication set to allow
applications to update the same table on multiple nodes?

Thank you very much in advance for your suggestions.
Best regards,
Michael 
From tmblue at gmail.com  Mon May 12 09:02:22 2008
From: tmblue at gmail.com (Tory M Blue)
Date: Mon May 12 09:02:28 2008
Subject: [Slony1-general] Subscriber nodes, full mesh or not
Message-ID: <8a547c840805120902u7b00f286paa37068fd9562bb3@mail.gmail.com>

Hello,

I've just started to make some changes to our slon environment and
wanted to run something by the group, to decide if I'm buying myself
anything or looking for trouble.

Currently I have a 4 node system

Master
Slave
Qslave1
Qslave2

(Qslaves are query only)

Currently all 4 nodes talk to each other and at this point in time,
I'm not sure why it was configured this way. I would think that it's a
slight performance hit to have the Qservers talk to each other, when
they will never exchange any data.

So is there a reason for each node in a cluster to talk to each other,
or will I be okay removing the communication  (Paths) between the 2
Qslave servers?


Thanks

Tory
From ahodgson at simkin.ca  Mon May 12 09:19:19 2008
From: ahodgson at simkin.ca (Alan Hodgson)
Date: Mon May 12 09:19:29 2008
Subject: [Slony1-general] Subscriber nodes, full mesh or not
In-Reply-To: <8a547c840805120902u7b00f286paa37068fd9562bb3@mail.gmail.com>
References: <8a547c840805120902u7b00f286paa37068fd9562bb3@mail.gmail.com>
Message-ID: <200805120919.19222@hal.medialogik.com>

On Monday 12 May 2008, "Tory M Blue" <tmblue@gmail.com> wrote:
> So is there a reason for each node in a cluster to talk to each other,
> or will I be okay removing the communication  (Paths) between the 2
> Qslave servers?

That'll be fine. They can exchange messages via a shared upstream.


-- 
Alan
From cbbrowne at ca.afilias.info  Mon May 12 10:18:49 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon May 12 10:18:57 2008
Subject: [Slony1-general] Origin/Provider understanding issue
In-Reply-To: <DF83DA74-0CFB-43D7-84C8-6734A111A105@htwm.de> (Michael
	Gruetzner's message of "Mon, 12 May 2008 17:50:53 +0200")
References: <DF83DA74-0CFB-43D7-84C8-6734A111A105@htwm.de>
Message-ID: <604p93igp2.fsf@dba2.int.libertyrms.com>

Michael Gruetzner <mgruetzn@HTWM.De> writes:
> I've read the documentation item 5.5 at
> http://slony.info/documentation/concepts.html .
> If I understand this correctly, it would not work, if a user updates a
> table belonging to
> a specific replication set on a node which is _not_ the origin of that
> replication set.

Correct; you cannot do updates on a non-origin node because Slony-I
adds in a trigger using a function called deny_access() which prevents
processes other than slon from doing Insert/Update/Delete.

> Would it be possible to define multiple origin nodes for one
> replication set to allow applications to update the same table on
> multiple nodes?

You're asking for multimaster replication; Slony-I was *not* designed
as a multimaster replication system.

There is some thinking taking place as to how to describe some limited
sorts of update scenarios where it might be made permissible to do
something like that, but that's at the "we don't really have useful
thoughts yet" stage.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://linuxfinances.info/info/
Hard work pays off in the long run. Laziness pays off now. 
From cbbrowne at ca.afilias.info  Mon May 12 10:26:25 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon May 12 10:26:32 2008
Subject: [Slony1-general] Subscriber nodes, full mesh or not
In-Reply-To: <8a547c840805120902u7b00f286paa37068fd9562bb3@mail.gmail.com>
	(Tory M. Blue's message of "Mon, 12 May 2008 09:02:22 -0700")
References: <8a547c840805120902u7b00f286paa37068fd9562bb3@mail.gmail.com>
Message-ID: <60y76fh1ry.fsf@dba2.int.libertyrms.com>

"Tory M Blue" <tmblue@gmail.com> writes:
> I've just started to make some changes to our slon environment and
> wanted to run something by the group, to decide if I'm buying myself
> anything or looking for trouble.
>
> Currently I have a 4 node system
>
> Master
> Slave
> Qslave1
> Qslave2
>
> (Qslaves are query only)
>
> Currently all 4 nodes talk to each other and at this point in time,
> I'm not sure why it was configured this way. I would think that it's a
> slight performance hit to have the Qservers talk to each other, when
> they will never exchange any data.
>
> So is there a reason for each node in a cluster to talk to each other,
> or will I be okay removing the communication  (Paths) between the 2
> Qslave servers?

It's preferable NOT to have a "full communications mesh," as that
leads (particularly as the number of nodes increases) to nodes opening
way more connections and doing way more DBMS work.

The *minimum* easy-to-determine[1] set of paths that you need is to
have a path from each subscriber to its provider, and
from each provider to its subscriber.

Thanks for asking; that gave me reason to mull it over in my head
again, and I think I now know what I want to write up in the docs on
this.

Notes:

[1]  This represents more than a "strict" minimum; the "strict" minimum
would involve having the mandatory path from each subscriber to its
provider (as above), plus a set of "spanning paths" to allow events to
get from the subscribers back to the origin.  The "minimal" form of
the latter *probably* is to have a path from each subscriber that is
not a provider that points back to the origin node.
-- 
"cbbrowne","@","linuxfinances.info"
http://linuxfinances.info/info/spiritual.html
Why are they called apartments, when they're all stuck together? 
From wmoran at collaborativefusion.com  Mon May 12 11:04:04 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Mon May 12 11:04:32 2008
Subject: [Slony1-general] Re: Small tool to make Slony management easier
In-Reply-To: <1208637733.24575.31.camel@danespc.home>
References: <1207952918.16726.33.camel@danespc.home>
	<20080412085543.a9ce1734.wmoran@collaborativefusion.com>
	<1208637733.24575.31.camel@danespc.home>
Message-ID: <20080512140404.2b6defdb.wmoran@collaborativefusion.com>

In response to Dane Miller <dane@greatschools.net>:

> On Sat, 2008-04-12 at 08:55 -0400, Bill Moran wrote:
> > > > I've been putting this together over the last couple weeks for
> > > > the reasons listed on the HTML page:
> > > >
> > http://people.collaborativefusion.com/~wmoran/PostgreSQL/slony_switchover.html
> > > > 
> > > > I'm interested to hear how useful this is to others, and of course
> > > > suggestions for improvement are welcome.
> > 
> > Are you running Linux by any chance?  This script was developed and
> > is deployed for us on FreeBSD, and I've verified that it works as
> > advertised on that platform.  I've _attempted_ to make sure that it
> > will work on Linux as well, but I've not had the opportunity to test
> > it.  Testing and feedback is encouraged.
> 
> I was able to do some testing on Linux this week.  I setup three nodes
> running Debian 4.0 (Etch), Postgresql 8.1, and Slony 1.2.1-1.  In the
> end, I didn't succeed at moving the replication origin between nodes
> using slony_switchover.sh (although slonik_move_set worked).  
> 
> I fixed the first issue I encountered (bug #3 below) by adding extra
> functionality to the script. However, the problem that I got stuck on
> was that sl_path didn't contain connection info between my two
> subscriber nodes, node 30 and node 31.  
> 
> Here's what my sl_path table looks like:
> 
> community=# select pa_server,pa_client,pa_conninfo from sl_path ;
>  pa_server | pa_client |  pa_conninfo (truncated)         
> -----------+-----------+---------------------------------------
>         30 |        29 | host=172.16.32.130 dbname=community 
>         29 |        30 | host=172.16.32.129 dbname=community 
>         29 |        31 | host=172.16.32.129 dbname=community 
>         31 |        29 | host=172.16.32.131 dbname=community 
> (4 rows)
> 
> Notice that there are no entries for (pa_server, pa_client) = (31, 30)
> or (30, 31).  Not sure if sl_path is missing entries due to a mistake I
> made in configuring replication, or perhaps a bug in the version of the
> slonik_* scripts packaged for Debian 4.0.  In any case, on my FreeBSD
> production machines sl_path is complete, so this isn't a blocker for my
> using this in production.

While I consider missing of paths a bug in ones configuration, I also
think this script should handle it gracefully, so I'm adding a check
to have it bail with a sane error message if it misses any.

> Here are the bugs I found.  Only the first is Linux-specific.  Hope this
> is helpful feedback.
> 
> 1. absolute path to psql, line 89

I threw in a `which psql`, which should take care of that in all but the
strangest of cases.

> 2. Comparison operator '-eq' should be '=' at line 102

Oops ... although the fix for #3 obviates the need for that bit of
code altogether.

> 3. slony_switchover.sh fails when node ids don't follow the sequence 1,
> 2, 3, .... For example, my slons have node ids 29, 30, and 31.  As a
> result, the following comparison always fails at line 109:
> 
> for I in `$JOT $NUMNODES`;
> do
>   if [ $ME -eq $I ]   <--- 109
>   then

Yeah.  Don't know why I didn't consider that possibility.  At first I
thought this was going to be really difficult to fix, but it only
took a little bit of banging around to make it extract that list from
the DB.

I've uploaded an updated version.  Testing and feedback are encouraged:
http://people.collaborativefusion.com/~wmoran/PostgreSQL/slony_switchover.html

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From glynastill at yahoo.co.uk  Tue May 13 05:12:03 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue May 13 05:12:06 2008
Subject: [Slony1-general] Stripping out slony after / before / during
	pg_restore?
Message-ID: <349621.46525.qm@web25806.mail.ukl.yahoo.com>

Hi people,

I'm setting us up a separate staging / test server and I want to read in a pg_dump of our current origin stripping out all the slony stuff.

I was thinking this could serve two purposes a) test out backups restore properly and b) provide us with us with the staging / test server

What's the best way to remove all the slony bits?

I was thinking read in the dump, then use uninstall node - but I'd rather not have to run the slon daemons.

Or should I just leave all the slony stuff in there... would it cause us any problems? There'd be no slons running and the next night it's all wiped and restored again...

Anyone got any ideas? Anyone got something similar already?

Cheers
Glyn


      __________________________________________________________
Sent from Yahoo! Mail.
A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html
From stephane.schildknecht at postgresqlfr.org  Tue May 13 05:50:43 2008
From: stephane.schildknecht at postgresqlfr.org (=?UTF-8?B?IlN0w6lwaGFuZSBBLiBTY2hpbGRrbmVjaHQi?=)
Date: Tue May 13 05:50:49 2008
Subject: [Slony1-general] Stripping out slony after / before / during
	pg_restore?
In-Reply-To: <349621.46525.qm@web25806.mail.ukl.yahoo.com>
References: <349621.46525.qm@web25806.mail.ukl.yahoo.com>
Message-ID: <48298EA3.4060402@postgresqlfr.org>

Glyn Astill a ?crit :
> Hi people,
> 
> I'm setting us up a separate staging / test server and I want to read in a pg_dump of our current origin stripping out all the slony stuff.
> 
> I was thinking this could serve two purposes a) test out backups restore properly and b) provide us with us with the staging / test server
> 
> What's the best way to remove all the slony bits?
> 
> I was thinking read in the dump, then use uninstall node - but I'd rather not have to run the slon daemons.

You don't have to run any slon daemon to execute uninstallnode().

You just have to execute something like "select _YOURINSTANCE.uninstallnode()"
on the newly restored DB.

You can the drop the entire schema (drop schema _YOURINSTANCE cascade).

> 
> Or should I just leave all the slony stuff in there... would it cause us any problems? There'd be no slons running and the next night it's all wiped and restored again...

Not sure leaving all slony stuf is really a good option as it does :
- modify the catalog schema
- log every modification in data as slony triggers will stay up and active.

> 
> Anyone got any ideas? Anyone got something similar already?

I do restore the whole master db and uninstall/drop every slony stuff.

Best regards,
-- 
St?phane SCHILDKNECHT
Pr?sident de PostgreSQLFr
T?l. 09 53 69 97 12
http://www.postgresql.fr
From glynastill at yahoo.co.uk  Tue May 13 06:02:54 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue May 13 06:02:59 2008
Subject: [Slony1-general] Re: [GENERAL] Stripping out slony after / before /
	during pg_restore?
Message-ID: <740090.32367.qm@web25803.mail.ukl.yahoo.com>

Thanks guys, that's exactly what I wanted.

----- Original Message ----
> From: Richard Huxton <dev@archonet.com>
> To: Glyn Astill <glynastill@yahoo.co.uk>
> Cc: slony1-general@lists.slony.info; pgsql-general@postgresql.org
> Sent: Tuesday, 13 May, 2008 1:34:18 PM
> Subject: Re: [GENERAL] Stripping out slony after / before / during pg_restore?
> 
> Glyn Astill wrote:
> > Hi people,
> > 
> > I'm setting us up a separate staging / test server and I want to read
> > in a pg_dump of our current origin stripping out all the slony stuff.
> > 
> > I was thinking this could serve two purposes a) test out backups
> > restore properly and b) provide us with us with the staging / test
> > server
> > 
> > What's the best way to remove all the slony bits?
> 
> Well, you can always just drop the slony schema (with a cascade) - that 
> should do it.
> 
> --
>    Richard Huxton



      __________________________________________________________
Sent from Yahoo! Mail.
A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html
From glynastill at yahoo.co.uk  Tue May 13 06:26:22 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue May 13 06:26:26 2008
Subject: [Slony1-general] Stripping out slony after / before / during
	pg_restore?
Message-ID: <385026.59970.qm@web25808.mail.ukl.yahoo.com>

Aha, does uninstallnode() clean up anything that drop cascade will not?

----- Original Message ----
> From: St?phane A. Schildknecht <stephane.schildknecht@postgresqlfr.org>
> To: Glyn Astill <glynastill@yahoo.co.uk>
> Cc: slony1-general@lists.slony.info
> Sent: Tuesday, 13 May, 2008 1:50:43 PM
> Subject: Re: [Slony1-general] Stripping out slony after / before / during pg_restore?
> 
> Glyn Astill a ?crit :
> > Hi people,
> > 
> > I'm setting us up a separate staging / test server and I want to read in a 
> pg_dump of our current origin stripping out all the slony stuff.
> > 
> > I was thinking this could serve two purposes a) test out backups restore 
> properly and b) provide us with us with the staging / test server
> > 
> > What's the best way to remove all the slony bits?
> > 
> > I was thinking read in the dump, then use uninstall node - but I'd rather not 
> have to run the slon daemons.
> 
> You don't have to run any slon daemon to execute uninstallnode().
> 
> You just have to execute something like "select _YOURINSTANCE.uninstallnode()"
> on the newly restored DB.
> 
> You can the drop the entire schema (drop schema _YOURINSTANCE cascade).
> 
> > 
> > Or should I just leave all the slony stuff in there... would it cause us any 
> problems? There'd be no slons running and the next night it's all wiped and 
> restored again...
> 
> Not sure leaving all slony stuf is really a good option as it does :
> - modify the catalog schema
> - log every modification in data as slony triggers will stay up and active.
> 
> > 
> > Anyone got any ideas? Anyone got something similar already?
> 
> I do restore the whole master db and uninstall/drop every slony stuff.
> 
> Best regards,
> -- 
> St?phane SCHILDKNECHT
> Pr?sident de PostgreSQLFr
> T?l. 09 53 69 97 12
> http://www.postgresql.fr



      __________________________________________________________
Sent from Yahoo! Mail.
A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html
From vivek at khera.org  Tue May 13 07:50:40 2008
From: vivek at khera.org (Vivek Khera)
Date: Tue May 13 07:50:45 2008
Subject: [Slony1-general] Re: Stripping out slony after / before / during
	pg_restore?
In-Reply-To: <65937bea0805130537h28b80c79k950d707d8dfd342d@mail.gmail.com>
References: <349621.46525.qm@web25806.mail.ukl.yahoo.com>
	<65937bea0805130537h28b80c79k950d707d8dfd342d@mail.gmail.com>
Message-ID: <2A5288FC-B727-4A6D-A371-24CC12817E6E@khera.org>

Here's how you do it on restore step from a pg_dump in -Fc format.

pg_restore -l dumpfile > list
edit the file "list" to remove references to slony objects
pg_restore -L list <other options you want> dumpfile

From cbbrowne at ca.afilias.info  Tue May 13 08:42:39 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue May 13 08:42:50 2008
Subject: [Slony1-general] Stripping out slony after / before / during
	pg_restore?
In-Reply-To: <385026.59970.qm@web25808.mail.ukl.yahoo.com> (Glyn Astill's
	message of "Tue, 13 May 2008 13:26:22 +0000 (GMT)")
References: <385026.59970.qm@web25808.mail.ukl.yahoo.com>
Message-ID: <60mymugqhc.fsf@dba2.int.libertyrms.com>

Glyn Astill <glynastill@yahoo.co.uk> writes:
> Aha, does uninstallnode() clean up anything that drop cascade will not?

Yes...

1.  It cleans up the hiding of foreign keys on subscriber nodes.

2.  If you used TABLE ADD KEY to fabricate a primary key, it drops
that.
-- 
(reverse (concatenate 'string "gro.mca" "@" "enworbbc"))
http://www3.sympatico.ca/cbbrowne/x.html
"Not  me, guy. I  read the  Bash man  page each  day like  a Jehovah's
Witness reads  the Bible.  No  wait, the Bash  man page IS  the bible.
Excuse    me..."    (More   on    confusing   aliases,    taken   from
comp.os.linux.misc)
From glynastill at yahoo.co.uk  Tue May 13 09:17:56 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue May 13 09:18:04 2008
Subject: [Slony1-general] Stripping out slony after / before / during
	pg_restore?
Message-ID: <28132.92448.qm@web25808.mail.ukl.yahoo.com>

Ah, thanks Christopher

*However* running uninstallnode doesn't seem to work here when I've dumped my database, then read it back in on another server. I get:

ERROR:  Slony-I: alterTableRestore(): Table with id 1000 not found
CONTEXT:  SQL statement "SELECT  "_main_replication".alterTableRestore( $1 )"
PL/pgSQL function "uninstallnode" line 14 at PERFORM

However table ID 1000 does exist.

I've had a look at alterTableRestore, does slony rely on the oid of the table on the original
server? 

I'm refering to "pg_catalog".pg_class.oid in this query in altertablerestore(integer) that seems to give me the message:

select T.tab_reloid, T.tab_set, T.tab_altered,
            S.set_origin, PGX.indexrelid,
            "_main_replication".slon_quote_brute(PGN.nspname) || '.' ||
            "_main_replication".slon_quote_brute(PGC.relname) as tab_fqname
            into v_tab_row
            from "_main_replication".sl_table T, "_main_replication".sl_set S,
                "pg_catalog".pg_class PGC, "pg_catalog".pg_namespace PGN,
                "pg_catalog".pg_index PGX, "pg_catalog".pg_class PGXC
            where T.tab_id = p_tab_id
                and T.tab_set = S.set_id
                and T.tab_reloid = PGC.oid
                and PGC.relnamespace = PGN.oid
                and PGX.indrelid = T.tab_reloid
                and PGX.indexrelid = PGXC.oid
                and PGXC.relname = T.tab_idxname
                for update;
    if not found then
        raise exception 'Slony-I: alterTableRestore(): Table with id % not found', p_tab_id;
    end if;

I've been told that you can't dump out the oids of objects with pg_dump, so I can't get around this.


----- Original Message ----
> From: Christopher Browne <cbbrowne@ca.afilias.info>
> To: Glyn Astill <glynastill@yahoo.co.uk>
> Cc: slony1-general@lists.slony.info
> Sent: Tuesday, 13 May, 2008 4:42:39 PM
> Subject: Re: [Slony1-general] Stripping out slony after / before / during pg_restore?
> 
> Glyn Astill writes:
> > Aha, does uninstallnode() clean up anything that drop cascade will not?
> 
> Yes...
> 
> 1.  It cleans up the hiding of foreign keys on subscriber nodes.
> 
> 2.  If you used TABLE ADD KEY to fabricate a primary key, it drops
> that.
> -- 
> (reverse (concatenate 'string "gro.mca" "@" "enworbbc"))
> http://www3.sympatico.ca/cbbrowne/x.html
> "Not  me, guy. I  read the  Bash man  page each  day like  a Jehovah's
> Witness reads  the Bible.  No  wait, the Bash  man page IS  the bible.
> Excuse    me..."    (More   on    confusing   aliases,    taken   from
> comp.os.linux.misc)



      __________________________________________________________
Sent from Yahoo! Mail.
A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html
From glynastill at yahoo.co.uk  Wed May 14 04:56:56 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Wed May 14 04:57:23 2008
Subject: [Slony1-general] Stripping out slony after / before / during
	pg_restore?
Message-ID: <271441.43821.qm@web25802.mail.ukl.yahoo.com>

Anyone know if I'm right or wrong in the message below? Does altertablerestore(integer) use object oids?

----- Original Message ----
> From: Glyn Astill <glynastill@yahoo.co.uk>
> To: Christopher Browne <cbbrowne@ca.afilias.info>
> Cc: slony1-general@lists.slony.info
> Sent: Tuesday, 13 May, 2008 5:17:56 PM
> Subject: Re: [Slony1-general] Stripping out slony after / before / during pg_restore?
> 
> Ah, thanks Christopher
> 
> *However* running uninstallnode doesn't seem to work here when I've dumped my 
> database, then read it back in on another server. I get:
> 
> ERROR:  Slony-I: alterTableRestore(): Table with id 1000 not found
> CONTEXT:  SQL statement "SELECT  "_main_replication".alterTableRestore( $1 )"
> PL/pgSQL function "uninstallnode" line 14 at PERFORM
> 
> However table ID 1000 does exist.
> 
> I've had a look at alterTableRestore, does slony rely on the oid of the table on 
> the original
> server? 
> 
> I'm refering to "pg_catalog".pg_class.oid in this query in 
> altertablerestore(integer) that seems to give me the message:
> 
> select T.tab_reloid, T.tab_set, T.tab_altered,
>             S.set_origin, PGX.indexrelid,
>             "_main_replication".slon_quote_brute(PGN.nspname) || '.' ||
>             "_main_replication".slon_quote_brute(PGC.relname) as tab_fqname
>             into v_tab_row
>             from "_main_replication".sl_table T, "_main_replication".sl_set S,
>                 "pg_catalog".pg_class PGC, "pg_catalog".pg_namespace PGN,
>                 "pg_catalog".pg_index PGX, "pg_catalog".pg_class PGXC
>             where T.tab_id = p_tab_id
>                 and T.tab_set = S.set_id
>                 and T.tab_reloid = PGC.oid
>                 and PGC.relnamespace = PGN.oid
>                 and PGX.indrelid = T.tab_reloid
>                 and PGX.indexrelid = PGXC.oid
>                 and PGXC.relname = T.tab_idxname
>                 for update;
>     if not found then
>         raise exception 'Slony-I: alterTableRestore(): Table with id % not 
> found', p_tab_id;
>     end if;
> 
> I've been told that you can't dump out the oids of objects with pg_dump, so I 
> can't get around this.
> 
> 
> ----- Original Message ----
> > From: Christopher Browne 
> > To: Glyn Astill 
> > Cc: slony1-general@lists.slony.info
> > Sent: Tuesday, 13 May, 2008 4:42:39 PM
> > Subject: Re: [Slony1-general] Stripping out slony after / before / during 
> pg_restore?
> > 
> > Glyn Astill writes:
> > > Aha, does uninstallnode() clean up anything that drop cascade will not?
> > 
> > Yes...
> > 
> > 1.  It cleans up the hiding of foreign keys on subscriber nodes.
> > 
> > 2.  If you used TABLE ADD KEY to fabricate a primary key, it drops
> > that.
> > -- 
> > (reverse (concatenate 'string "gro.mca" "@" "enworbbc"))
> > http://www3.sympatico.ca/cbbrowne/x.html
> > "Not  me, guy. I  read the  Bash man  page each  day like  a Jehovah's
> > Witness reads  the Bible.  No  wait, the Bash  man page IS  the bible.
> > Excuse    me..."    (More   on    confusing   aliases,    taken   from
> > comp.os.linux.misc)
> 
> 
> 
>       __________________________________________________________
> Sent from Yahoo! Mail.
> A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general



      __________________________________________________________
Sent from Yahoo! Mail.
A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html
From cbbrowne at ca.afilias.info  Wed May 14 07:55:02 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed May 14 07:55:09 2008
Subject: [Slony1-general] Stripping out slony after / before / during
	pg_restore?
In-Reply-To: <271441.43821.qm@web25802.mail.ukl.yahoo.com> (Glyn Astill's
	message of "Wed, 14 May 2008 11:56:56 +0000 (GMT)")
References: <271441.43821.qm@web25802.mail.ukl.yahoo.com>
Message-ID: <608wydgcl5.fsf@dba2.int.libertyrms.com>

Glyn Astill <glynastill@yahoo.co.uk> writes:
> Anyone know if I'm right or wrong in the message below? Does
> altertablerestore(integer) use object oids?

It should depend on the oid on the local server *that was an active
node.*

Thus, if you do a pg_dump, and restore it, somewhere else, the oids
will doubtless all be wrong.
-- 
"cbbrowne","@","linuxfinances.info"
http://cbbrowne.com/info/sgml.html
Windows and Icons and Mice, OH MY!
From glynastill at yahoo.co.uk  Wed May 14 08:33:00 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Wed May 14 08:33:08 2008
Subject: [Slony1-general] Stripping out slony after / before / during
	pg_restore?
Message-ID: <953190.64996.qm@web25803.mail.ukl.yahoo.com>

Okay thanks.

----- Original Message ----
> From: Christopher Browne <cbbrowne@ca.afilias.info>
> To: Glyn Astill <glynastill@yahoo.co.uk>
> Cc: Christopher Browne <cbbrowne@ca.afilias.info>; slony1-general@lists.slony.info
> Sent: Wednesday, 14 May, 2008 3:55:02 PM
> Subject: Re: [Slony1-general] Stripping out slony after / before / during pg_restore?
> 
> Glyn Astill writes:
> > Anyone know if I'm right or wrong in the message below? Does
> > altertablerestore(integer) use object oids?
> 
> It should depend on the oid on the local server *that was an active
> node.*
> 
> Thus, if you do a pg_dump, and restore it, somewhere else, the oids
> will doubtless all be wrong.
> -- 
> "cbbrowne","@","linuxfinances.info"
> http://cbbrowne.com/info/sgml.html
> Windows and Icons and Mice, OH MY!



      __________________________________________________________
Sent from Yahoo! Mail.
A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html
From JanWieck at Yahoo.com  Wed May 14 10:25:59 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed May 14 10:26:23 2008
Subject: [Slony1-general] Dropping the master node
In-Reply-To: <60d4nvh4s3.fsf@dba2.int.libertyrms.com>
References: <3387742770-1790@e4ward.com>
	<60d4nvh4s3.fsf@dba2.int.libertyrms.com>
Message-ID: <482B20A7.8060307@Yahoo.com>

On 5/9/2008 11:32 AM, Christopher Browne wrote:
> <postgres@dac.e4ward.com> writes:
>> Just a quick question...
>>
>> I currently have a master-slave setup with postgres 8.1.10 and slony
>> 1.2.12. I'm needing to drop slony usage on BOTH nodes for a while. I
>> have experience dropping slave nodes using DROP NODE... but I can't
>> find any info about its usage on master nodes.
>>
>> Any advice?
> 
> Nodes do not, in and of themselves, have a role of either "master" or
> "slave"; they're all just nodes.  That, for a particular replication
> set, they are performing "master" or "slave" activities is distinct
> from that.
> 
> You can just do DROP NODE, whether you think it's a "master" or a
> "slave."

If you want to remove all traces of slony from all the nodes, UNINSTALL 
NODE is what you are looking for.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From shahaf at redfin.com  Wed May 14 20:00:07 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Wed May 14 20:00:29 2008
Subject: [Slony1-general] Using Postgres' continuous archiving and
	point-in-time recovery as a form of replication
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB22033E870D@mail-1.rf.lan>

I just read this page:
http://www.postgresql.org/docs/8.2/static/continuous-archiving.html

 

And it made me wonder whether this mechanism of continuous archiving and
point-in-time recovery (PITR) can be used for replication.  In other
words, Postgres already generates write-ahead-log files for all
operations.  If you can transmit those files to another machine and
replay them, continuously, then you have a form of replication.

 

But, it's not clear if this is possible.  The article linked above talks
about doing continuous archiving, but it doesn't talk about doing
continuous recovery.  Instead, it talks about doing a single
point-in-time recovery.  Moreover, this recovery requires the target
("slave") machine to be offline during the recovery process.

 

Does anyone know whether it's possible to use Postgres' archiving and
PITR for replication?

 

By the way, if it can, it seems that:

1.       It would have the advantage that various operations are handled
in a more natural manner than with Slony (e.g. DDL)

2.       It would have the disadvantage that you can't be choosy about
what to replicate - you get the entire DB cluster, or nothing at all.

Is that correct?

 

Thanks!

 

--Shahaf

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080514/691fdc85/attachment.htm
From guillaume at lelarge.info  Wed May 14 23:03:24 2008
From: guillaume at lelarge.info (Guillaume Lelarge)
Date: Wed May 14 23:03:50 2008
Subject: [Slony1-general] Using Postgres' continuous archiving
	and	point-in-time recovery as a form of replication
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB22033E870D@mail-1.rf.lan>
References: <082D8A131DF72A4D88C908A1AD3DEB22033E870D@mail-1.rf.lan>
Message-ID: <482BD22C.6040604@lelarge.info>

Shahaf Abileah a ?crit :
> I just read this page: 
> http://www.postgresql.org/docs/8.2/static/continuous-archiving.html
> 
> And it made me wonder whether this mechanism of continuous archiving and 
> point-in-time recovery (PITR) can be used for replication.  In other 
> words, Postgres already generates write-ahead-log files for all 
> operations.  If you can transmit those files to another machine and 
> replay them, continuously, then you have a form of replication.
> 
> But, it?s not clear if this is possible.  The article linked above talks 
> about doing continuous archiving, but it doesn?t talk about doing 
> continuous recovery.  Instead, it talks about doing a single 
> point-in-time recovery.  Moreover, this recovery requires the target 
> (?slave?) machine to be offline during the recovery process.
> 

Continuous recovery is possible, see :
   http://www.postgresql.org/docs/8.2/static/warm-standby.html

But you're right, during the recovery the slave is not available for 
connections.

> Does anyone know whether it?s possible to use Postgres? archiving and 
> PITR for replication?
> 

Yes, it's called Log Shipping or Warm Server Standby.

> By the way, if it can, it seems that:
> 
> 1.       It would have the advantage that various operations are handled 
> in a more natural manner than with Slony (e.g. DDL)
> 
> 2.       It would have the disadvantage that you can?t be choosy about 
> what to replicate ? you get the entire DB cluster, or nothing at all.
> 
> Is that correct?
> 

Yes.

Regards.


-- 
Guillaume.
  http://www.postgresqlfr.org
  http://dalibo.com
From simon at 2ndquadrant.com  Thu May 15 00:51:19 2008
From: simon at 2ndquadrant.com (Simon Riggs)
Date: Thu May 15 00:50:08 2008
Subject: [Slony1-general] Using Postgres' continuous archiving and
	point-in-time recovery as a form of replication
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB22033E870D@mail-1.rf.lan>
References: <082D8A131DF72A4D88C908A1AD3DEB22033E870D@mail-1.rf.lan>
Message-ID: <1210837879.1218.275.camel@ebony.site>

On Wed, 2008-05-14 at 20:00 -0700, Shahaf Abileah wrote:

> By the way, if it can, it seems that:
> 
> 1.      It would have the advantage that various operations are
> handled in a more natural manner than with Slony (e.g. DDL)
> 
> 2.      It would have the disadvantage that you can?t be choosy about
> what to replicate ? you get the entire DB cluster, or nothing at all.
> 
> Is that correct?

Those are correct but of course whether they are advantages or
disadvantages depends upon the situation and your requirements.

Slony has many features designed to fine-tune the replication process,
so the phrase "more natural manner" isn't fair. "without needing to run
a command" would be more neutral viewpoint.

-- 
  Simon Riggs
  2ndQuadrant  http://www.2ndQuadrant.com

From shahaf at redfin.com  Thu May 15 00:51:32 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Thu May 15 00:51:57 2008
Subject: [Slony1-general] Using Postgres' continuous archiving
	andpoint-in-time recovery as a form of replication
In-Reply-To: <1210837879.1218.275.camel@ebony.site>
References: <082D8A131DF72A4D88C908A1AD3DEB22033E870D@mail-1.rf.lan>
	<1210837879.1218.275.camel@ebony.site>
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB22033E8748@mail-1.rf.lan>

Fair enough.  Sorry if I offended anyone.  We've been using Slony at www.redfin.com for a few months now, and it gets the job done for us, so thank you to everyone who contributed to this project.

--S

-----Original Message-----
From: Simon Riggs [mailto:simon@2ndquadrant.com] 
Sent: Thursday, May 15, 2008 12:51 AM
To: Shahaf Abileah
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] Using Postgres' continuous archiving andpoint-in-time recovery as a form of replication

On Wed, 2008-05-14 at 20:00 -0700, Shahaf Abileah wrote:

> By the way, if it can, it seems that:
> 
> 1.      It would have the advantage that various operations are
> handled in a more natural manner than with Slony (e.g. DDL)
> 
> 2.      It would have the disadvantage that you can?t be choosy about
> what to replicate ? you get the entire DB cluster, or nothing at all.
> 
> Is that correct?

Those are correct but of course whether they are advantages or
disadvantages depends upon the situation and your requirements.

Slony has many features designed to fine-tune the replication process,
so the phrase "more natural manner" isn't fair. "without needing to run
a command" would be more neutral viewpoint.

-- 
  Simon Riggs
  2ndQuadrant  http://www.2ndQuadrant.com


From rafael.domiciano at gmail.com  Thu May 15 06:29:31 2008
From: rafael.domiciano at gmail.com (Rafael Domiciano)
Date: Thu May 15 06:29:36 2008
Subject: [Slony1-general] Slony Queue Monitoring
Message-ID: <3a0028490805150629y6a67b211k52fd0789ffa3649d@mail.gmail.com>

Hi,

I need a little help.
I just installed Slony-I in my office and it's working great.
But I have a proccess that brings to me the queue of several events of the
enterprise for monitoring, easing the support when something fails.
The queue of those events I get doing a SQL select in determinates tables
that suply me the information.
But in the Slony-I I can't find the table that suply me that kind of
information, and I don't find in the net someone in the same situation.
If someone knows how to do it, I could pay a candy hehehe....

Thanks,

By the way, I am using by know the Log of Slony.

Rafael
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080515/=
ebb1af5a/attachment.htm
From cbbrowne at ca.afilias.info  Thu May 15 07:27:27 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu May 15 07:27:33 2008
Subject: [Slony1-general] Using Postgres' continuous archiving and
	point-in-time recovery as a form of replication
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB22033E870D@mail-1.rf.lan> (Shahaf
	Abileah's message of "Wed, 14 May 2008 20:00:07 -0700")
References: <082D8A131DF72A4D88C908A1AD3DEB22033E870D@mail-1.rf.lan>
Message-ID: <60ve1ffxrk.fsf@dba2.int.libertyrms.com>

"Shahaf Abileah" <shahaf@redfin.com> writes:
> I just read this page: http://www.postgresql.org/docs/8.2/static/continuous-archiving.html:p>
>
> And it made me wonder whether this mechanism of continuous archiving
> and point-in-time recovery (PITR) can be used for replication.? In
> other words, Postgres already generates write-ahead-log files for
> all operations.? If you can transmit those files to another machine
> and replay them, continuously, then you have a form of
> replication.:p>

Yes, it is "a form of replication," in a somewhat analagous (but, for
many scenarios, superior) way to the way that using pg_dump to dump
and psql to restore a database may be considered "a form of
replication."

> But, it's not clear if this is possible.? The article linked above
> talks about doing continuous archiving, but it doesn't talk about
> doing continuous recovery.? Instead, it talks about doing a single
> point-in-time recovery.? Moreover, this recovery requires the target
> (``slave'') machine to be offline during the recovery process.:p>

PITR is certainly "possible" to use.

Whether its behaviour is suitable for your requirements is another
matter.  As noted, the "slave" is held offline throughout the recovery
process, which means it can't be used to satisfy queries, as is the
case with Slony-I subscribers.

> Does anyone know whether it's possible to use Postgres' archiving
> and PITR for replication?:p>

As noted above, yes, it certainly can, as long as it is acceptable to
not be able to use slaves that are in recovery mode.

We've had at least one case where we have considered using PITR as a
replication solution since its tradeoffs all seemed satisfactory.
(The system for which it was considered hasn't gone into production,
which is why I say "considered," rather than "deployed.")

> By the way, if it can, it seems that::p>

> 1.??????It would have the advantage that various operations are
> handled in a more natural manner than with Slony (e.g. DDL):p>
>
> 2.??????It would have the disadvantage that you can't be choosy
> about what to replicate -- you get the entire DB cluster, or nothing
> at all.:p>
>
> Is that correct?:p>

Yes, those would seem to be merits and demerits of using PITR as a
replication system.
-- 
output = reverse("ofni.sesabatadxunil" "@" "enworbbc")
http://www3.sympatico.ca/cbbrowne/advocacy.html
The quickest way to a man's heart is through his chest, with an axe. 
From tmblue at gmail.com  Thu May 15 22:56:19 2008
From: tmblue at gmail.com (Tory M Blue)
Date: Thu May 15 22:56:42 2008
Subject: [Slony1-general] 1.2.14.rc - status of 1.2.14
Message-ID: <8a547c840805152256w71625c1fg165c4f880633f1cf@mail.gmail.com>

Good evening/morning or other :)

Wondering what is the latest for the 1.2.14 version?

It's making it's way to production now in my environment, but it's
1.2.14rc (patched) since a critical switchover piece missed the
1.2.14rc tarball.

So far no issues and I just rolled it to another staging environment
today. I will be doing further failover/switchover tests before going
to production with it and will report back, but so far it fixed the
brokenness of 1.2.12 and 1.2.13.

Just wondering how long does the group think before they are
comfortable with releasing it, or if someone can tell us or me in
private, if there are any "major" concerns looming that I should be
aware of.

Right now since I must re index weekly, I have to drop/add a node
during each switchback and rebuild it because of the current bug in
1.2.12 (and 1.2.13 and 1.2.14rc (unless the tarball has been updated
in the last 2 weeks) which is horrible, especially for someone running
into this the first time.

Thanks and keep up the good work.

1.2.14 and than next "master master" :) wheeee (okay so one can dream!)

Tory
From glynastill at yahoo.co.uk  Fri May 16 02:48:12 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Fri May 16 02:48:42 2008
Subject: [Slony1-general] 1.2.14.rc - status of 1.2.14
Message-ID: <33893.81927.qm@web25807.mail.ukl.yahoo.com>

I was just about to ask the same, as we're in a similar situation here pretty close to going into production.

I don't want to pester as I'm sure you chaps are busy, but a little update would be good.

----- Original Message ----
> From: Tory M Blue <tmblue@gmail.com>
> To: slony1-general@lists.slony.info
> Sent: Friday, 16 May, 2008 6:56:19 AM
> Subject: [Slony1-general] 1.2.14.rc - status of 1.2.14
> 
> Good evening/morning or other :)
> 
> Wondering what is the latest for the 1.2.14 version?
> 
> It's making it's way to production now in my environment, but it's
> 1.2.14rc (patched) since a critical switchover piece missed the
> 1.2.14rc tarball.
> 
> So far no issues and I just rolled it to another staging environment
> today. I will be doing further failover/switchover tests before going
> to production with it and will report back, but so far it fixed the
> brokenness of 1.2.12 and 1.2.13.
> 
> Just wondering how long does the group think before they are
> comfortable with releasing it, or if someone can tell us or me in
> private, if there are any "major" concerns looming that I should be
> aware of.
> 
> Right now since I must re index weekly, I have to drop/add a node
> during each switchback and rebuild it because of the current bug in
> 1.2.12 (and 1.2.13 and 1.2.14rc (unless the tarball has been updated
> in the last 2 weeks) which is horrible, especially for someone running
> into this the first time.
> 
> Thanks and keep up the good work.
> 
> 1.2.14 and than next "master master" :) wheeee (okay so one can dream!)
> 
> Tory
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general



      __________________________________________________________
Sent from Yahoo! Mail.
A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html
From ajs at crankycanuck.ca  Fri May 16 07:03:15 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri May 16 07:03:38 2008
Subject: [Slony1-general] 1.2.14.rc - status of 1.2.14
In-Reply-To: <8a547c840805152256w71625c1fg165c4f880633f1cf@mail.gmail.com>
References: <8a547c840805152256w71625c1fg165c4f880633f1cf@mail.gmail.com>
Message-ID: <20080516140315.GA21387@crankycanuck.ca>

On Thu, May 15, 2008 at 10:56:19PM -0700, Tory M Blue wrote:
> 
> Wondering what is the latest for the 1.2.14 version?
> 
> Just wondering how long does the group think before they are
> comfortable with releasing it, or if someone can tell us or me in
> private, if there are any "major" concerns looming that I should be
> aware of.

I know that Chris announced here he was planning to ship it last week. 
Chris?

A

From jc at oxado.com  Fri May 16 07:16:32 2008
From: jc at oxado.com (Jacques Caron)
Date: Fri May 16 07:25:08 2008
Subject: [Slony1-general] Using Postgres' continuous archiving and
	point-in-time recovery as a form of replication
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB22033E870D@mail-1.rf.lan>
References: <082D8A131DF72A4D88C908A1AD3DEB22033E870D@mail-1.rf.lan>
Message-ID: <20080516142457.564A1120F012@zeus.directinfos.com>

At 05:00 15/05/2008, Shahaf Abileah wrote:
>By the way, if it can, it seems that:
>1.       It would have the advantage that various operations are 
>handled in a more natural manner than with Slony (e.g. DDL)
>2.       It would have the disadvantage that you can't be choosy 
>about what to replicate ? you get the entire DB cluster, or nothing at all.
>Is that correct?

AFAIK, another important drawback is that you can't use it between 
different (major) versions of Postgresql, or different platforms (at 
the very least platforms with different word sizes, alignment 
constraints, byte order, etc., but also different compile-time 
settings...), as it's a binary format based on the on-disk 
representation of data. So you can't use it for transparent upgrade scenarios.

Correct me if I'm wrong.

Jacques.

From ajs at crankycanuck.ca  Fri May 16 07:29:14 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri May 16 07:29:25 2008
Subject: [Slony1-general] Using Postgres' continuous archiving and
	point-in-time recovery as a form of replication
In-Reply-To: <20080516142457.564A1120F012@zeus.directinfos.com>
References: <082D8A131DF72A4D88C908A1AD3DEB22033E870D@mail-1.rf.lan>
	<20080516142457.564A1120F012@zeus.directinfos.com>
Message-ID: <20080516142914.GB21387@crankycanuck.ca>

On Fri, May 16, 2008 at 04:16:32PM +0200, Jacques Caron wrote:

> representation of data. So you can't use it for transparent upgrade 
> scenarios.
> 
> Correct me if I'm wrong.

No, you're right.  The only real way to upgrade Postgres today without a
complete dump and restore is some sort of trigger-based replication (like
Slony).  I don't know whether Bucardo or Londiste can do the same trick, but
their design suggests they might be able to.  I also used erserver in the
past to do this, although that was way harder and much scarier.

A
From cbbrowne at ca.afilias.info  Fri May 16 09:17:02 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri May 16 09:17:11 2008
Subject: [Slony1-general] 1.2.14.rc - status of 1.2.14
In-Reply-To: <20080516140315.GA21387@crankycanuck.ca> (Andrew Sullivan's
	message of "Fri, 16 May 2008 10:03:15 -0400")
References: <8a547c840805152256w71625c1fg165c4f880633f1cf@mail.gmail.com>
	<20080516140315.GA21387@crankycanuck.ca>
Message-ID: <60hccyfcld.fsf@dba2.int.libertyrms.com>

Andrew Sullivan <ajs@crankycanuck.ca> writes:
> On Thu, May 15, 2008 at 10:56:19PM -0700, Tory M Blue wrote:
>> 
>> Wondering what is the latest for the 1.2.14 version?
>> 
>> Just wondering how long does the group think before they are
>> comfortable with releasing it, or if someone can tell us or me in
>> private, if there are any "major" concerns looming that I should be
>> aware of.
>
> I know that Chris announced here he was planning to ship it last week. 
> Chris?

Today.  I have been run off my feet on other things all week.

I got a couple changes from Richard Yen in the other day (to the log
shipper component), and have done tests on all the PostgreSQL
versions, with success; will see about getting the build today.
-- 
(reverse (concatenate 'string "moc.enworbbc" "@" "enworbbc"))
http://cbbrowne.com/info/advocacy.html
"Nightmares - Ha!  The way my life's been going lately,
 Who'd notice?"  -- Londo Mollari
From cbbrowne at ca.afilias.info  Fri May 16 10:51:57 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri May 16 10:52:07 2008
Subject: [Slony1-general] Version 1.2.14 available
Message-ID: <607iduf876.fsf@dba2.int.libertyrms.com>

Here are the release notes; as always, see the "front page" at slony.info to get
URLs...

RELEASE 1.2.14
- Fix typo in configure-replication.sh (missing CR)

- Per bug #35, search the Slony share dir for scripts
  before falling back to the PG share dir on 8.0+
  http://www.slony.info/bugzilla/show_bug.cgi?id=35

- Change test framework to write out the test name into
  $TEMPDIR/TestName

- Patch that seems to resolve a race condition with
  ACCEPT_SET

  http://lists.slony.info/pipermail/slony1-general/2008-March/007655.html

- Fix bug #49 - mishandling by slony_logshipper of quotes &
  backslashes.

- Fix bug #50 - slony_logshipper had a variable access *after*
  memory was freed
-- 
(reverse (concatenate 'string "ofni.sesabatadxunil" "@" "enworbbc"))
http://www3.sympatico.ca/cbbrowne/slony.html
10.0 times 0.1 is hardly ever 1.0.
From jeff at frostconsultingllc.com  Fri May 16 11:41:59 2008
From: jeff at frostconsultingllc.com (Jeff Frost)
Date: Fri May 16 11:42:21 2008
Subject: [Slony1-general] Version 1.2.14 available
In-Reply-To: <607iduf876.fsf@dba2.int.libertyrms.com>
References: <607iduf876.fsf@dba2.int.libertyrms.com>
Message-ID: <Pine.LNX.4.64.0805161139340.8269@discord.home.frostconsultingllc.com>

On Fri, 16 May 2008, Christopher Browne wrote:

> Here are the release notes; as always, see the "front page" at slony.info to get
> URLs...
>
> RELEASE 1.2.14

Hrmmm..I get this when trying to update functions:

./upgrade.slonik:5: Possible unsupported PostgreSQL version 8.3, 
defaulting to 8.1 support

Thats' updating from 1.2.14rc to 1.2.14.

This is what is said in my configure:

pg_config says pg_bindir is /usr/bin/
pg_config says pg_libdir is /usr/lib64/
pg_config says pg_includedir is /usr/include/
pg_config says pg_pkglibdir is /usr/lib64/pgsql/
pg_config says pg_includeserverdir is /usr/include/pgsql/server/
checking for correct version of PostgreSQL... 8.3
pg_config says pg_sharedir is /usr/share/pgsql/

so that seems fine.



-- 
Jeff Frost, Owner 	<jeff@frostconsultingllc.com>
Frost Consulting, LLC 	http://www.frostconsultingllc.com/
Phone: 650-780-7908	FAX: 650-649-1954
From cbbrowne at ca.afilias.info  Fri May 16 13:42:26 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri May 16 13:42:43 2008
Subject: [Slony1-general] Version 1.2.14 available
In-Reply-To: <Pine.LNX.4.64.0805161139340.8269@discord.home.frostconsultingllc.com>
	(Jeff Frost's message of "Fri, 16 May 2008 11:41:59 -0700 (PDT)")
References: <607iduf876.fsf@dba2.int.libertyrms.com>
	<Pine.LNX.4.64.0805161139340.8269@discord.home.frostconsultingllc.com>
Message-ID: <60prrmdlql.fsf@dba2.int.libertyrms.com>

Jeff Frost <jeff@frostconsultingllc.com> writes:
> On Fri, 16 May 2008, Christopher Browne wrote:
>
>> Here are the release notes; as always, see the "front page" at slony.info to get
>> URLs...
>>
>> RELEASE 1.2.14
>
> Hrmmm..I get this when trying to update functions:
>
> ./upgrade.slonik:5: Possible unsupported PostgreSQL version 8.3,
> defaulting to 8.1 support
>
> Thats' updating from 1.2.14rc to 1.2.14.
>
> This is what is said in my configure:
>
> pg_config says pg_bindir is /usr/bin/
> pg_config says pg_libdir is /usr/lib64/
> pg_config says pg_includedir is /usr/include/
> pg_config says pg_pkglibdir is /usr/lib64/pgsql/
> pg_config says pg_includeserverdir is /usr/include/pgsql/server/
> checking for correct version of PostgreSQL... 8.3
> pg_config says pg_sharedir is /usr/share/pgsql/
>
> so that seems fine.

On the "good news" side, 8.1 support should work out fine, as the
internals (e.g. - stored procs) do not vary between 8.1 and 8.3.

But the line of code that should be recognizing version "80300" evidently isn't.

This is the bit of logic that should be catching your version:  (slonik.c)
	else if ((adminfo->pg_version >= 80100) && adminfo->pg_version < 80400)	/* 8.1, 8.2 and 8.3 */

Actually, there's one thing that makes me suspicious about this: your
message differs from what it should.

It should read:

./upgrade.slonik:5: Possible unsupported PostgreSQL version (80300) 8.3, defaulting to 8.1 support

But perhaps I'm being overpicky...
-- 
select 'cbbrowne' || '@' || 'cbbrowne.com';
http://linuxdatabases.info/info/x.html
"A train stops at  a train station, a bus stops at  a bus station.  On
my desk I have a work station..."
From jeff at frostconsultingllc.com  Fri May 16 14:39:41 2008
From: jeff at frostconsultingllc.com (Jeff Frost)
Date: Fri May 16 14:39:59 2008
Subject: [Slony1-general] Version 1.2.14 available
In-Reply-To: <60prrmdlql.fsf@dba2.int.libertyrms.com>
References: <607iduf876.fsf@dba2.int.libertyrms.com>
	<Pine.LNX.4.64.0805161139340.8269@discord.home.frostconsultingllc.com>
	<60prrmdlql.fsf@dba2.int.libertyrms.com>
Message-ID: <Pine.LNX.4.64.0805161433560.8269@discord.home.frostconsultingllc.com>

On Fri, 16 May 2008, Christopher Browne wrote:

> Jeff Frost <jeff@frostconsultingllc.com> writes:
>> On Fri, 16 May 2008, Christopher Browne wrote:
>>
>>> Here are the release notes; as always, see the "front page" at slony.info to get
>>> URLs...
>>>
>>> RELEASE 1.2.14
>>
>> Hrmmm..I get this when trying to update functions:
>>
>> ./upgrade.slonik:5: Possible unsupported PostgreSQL version 8.3,
>> defaulting to 8.1 support
>>
>> Thats' updating from 1.2.14rc to 1.2.14.
>>
>> This is what is said in my configure:
>>
>> pg_config says pg_bindir is /usr/bin/
>> pg_config says pg_libdir is /usr/lib64/
>> pg_config says pg_includedir is /usr/include/
>> pg_config says pg_pkglibdir is /usr/lib64/pgsql/
>> pg_config says pg_includeserverdir is /usr/include/pgsql/server/
>> checking for correct version of PostgreSQL... 8.3
>> pg_config says pg_sharedir is /usr/share/pgsql/
>>
>> so that seems fine.
>
> On the "good news" side, 8.1 support should work out fine, as the
> internals (e.g. - stored procs) do not vary between 8.1 and 8.3.
>
> But the line of code that should be recognizing version "80300" evidently isn't.
>
> This is the bit of logic that should be catching your version:  (slonik.c)
> 	else if ((adminfo->pg_version >= 80100) && adminfo->pg_version < 80400)	/* 8.1, 8.2 and 8.3 */
>
> Actually, there's one thing that makes me suspicious about this: your
> message differs from what it should.
>
> It should read:
>
> ./upgrade.slonik:5: Possible unsupported PostgreSQL version (80300) 8.3, defaulting to 8.1 support
>
> But perhaps I'm being overpicky...

Oh, you think it should be putting out the (80300) as well?  Maybe it's 
somehow running the wrong version?  I thought that myself and so I went and 
moved the 1.2.14 binaries and re-ran the install.  I configured with the 
following line:

./configure --prefix=/usr/local/slony1-1.2.14 --with-perltools=/usr/local/slony1-1.2.14/perl

Then I removed the /usr/local/slony symlink to 1.2.14rc and pointed it at 
/usr/local/slony1-1.2.14.  You think it's possible I ended up with two 
versions of the .so files somewhere?

I only found this file:

/usr/lib64/pgsql/slony1_funcs.so

Anything you'd like me to do to investigate further?

-- 
Jeff Frost, Owner 	<jeff@frostconsultingllc.com>
Frost Consulting, LLC 	http://www.frostconsultingllc.com/
Phone: 650-780-7908	FAX: 650-649-1954
From drees76 at gmail.com  Fri May 16 15:02:26 2008
From: drees76 at gmail.com (David Rees)
Date: Fri May 16 15:02:40 2008
Subject: [Slony1-general] Version 1.2.14 available
In-Reply-To: <Pine.LNX.4.64.0805161433560.8269@discord.home.frostconsultingllc.com>
References: <607iduf876.fsf@dba2.int.libertyrms.com>
	<Pine.LNX.4.64.0805161139340.8269@discord.home.frostconsultingllc.com>
	<60prrmdlql.fsf@dba2.int.libertyrms.com>
	<Pine.LNX.4.64.0805161433560.8269@discord.home.frostconsultingllc.com>
Message-ID: <72dbd3150805161502v670b5ca1u36f133c832ae4295@mail.gmail.com>

On Fri, May 16, 2008 at 2:39 PM, Jeff Frost <jeff@frostconsultingllc.com> wrote:
> On Fri, 16 May 2008, Christopher Browne wrote:
>> Jeff Frost <jeff@frostconsultingllc.com> writes:
>>> Hrmmm..I get this when trying to update functions:
>>>
>>> ./upgrade.slonik:5: Possible unsupported PostgreSQL version 8.3,
>>> defaulting to 8.1 support
>>>
>>> Thats' updating from 1.2.14rc to 1.2.14.
>>
>> On the "good news" side, 8.1 support should work out fine, as the
>> internals (e.g. - stored procs) do not vary between 8.1 and 8.3.
>>
>> It should read:
>>
>> ./upgrade.slonik:5: Possible unsupported PostgreSQL version (80300) 8.3,
>> defaulting to 8.1 support
>>
> Oh, you think it should be putting out the (80300) as well?  Maybe it's
> somehow running the wrong version?  I thought that myself and so I went and
> moved the 1.2.14 binaries and re-ran the install.  I configured with the
> following line:

Fwiw, I'm seeing the exact same thing as Jeff - no (80300) in my error
message as well.

I am upgrading a Postgres 8.3.1 cluster from Slony1-1.2.13 to 1.2.14.
Slony is getting installed under a new prefix from the old install.

-Dave
From drees76 at gmail.com  Fri May 16 15:18:44 2008
From: drees76 at gmail.com (David Rees)
Date: Fri May 16 15:18:59 2008
Subject: [Slony1-general] Version 1.2.14 available
In-Reply-To: <72dbd3150805161502v670b5ca1u36f133c832ae4295@mail.gmail.com>
References: <607iduf876.fsf@dba2.int.libertyrms.com>
	<Pine.LNX.4.64.0805161139340.8269@discord.home.frostconsultingllc.com>
	<60prrmdlql.fsf@dba2.int.libertyrms.com>
	<Pine.LNX.4.64.0805161433560.8269@discord.home.frostconsultingllc.com>
	<72dbd3150805161502v670b5ca1u36f133c832ae4295@mail.gmail.com>
Message-ID: <72dbd3150805161518r6e6f7d8emea83e6c987972524@mail.gmail.com>

On Fri, May 16, 2008 at 3:02 PM, David Rees <drees76@gmail.com> wrote:
> On Fri, May 16, 2008 at 2:39 PM, Jeff Frost <jeff@frostconsultingllc.com>=
 wrote:
>> On Fri, 16 May 2008, Christopher Browne wrote:
>>> Jeff Frost <jeff@frostconsultingllc.com> writes:
>>>> Hrmmm..I get this when trying to update functions:
>>>>
>>>> ./upgrade.slonik:5: Possible unsupported PostgreSQL version 8.3,
>>>> defaulting to 8.1 support

Looks like there is two places where the versions are checked in
slonik.c, around like 1900 and around line 2000. The code should
probably be moved in to a common function, but attached is a quick
patch to make them match.

At this point the message is completely harmless and only cosmetic.

-Dave
-------------- next part --------------
A non-text attachment was scrubbed...
Name: slonik.c.patch
Type: text/x-patch
Size: 1231 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080516=
/13c41087/slonik.c.bin
From hannu at krosing.net  Sun May 18 23:27:44 2008
From: hannu at krosing.net (Hannu Krosing)
Date: Mon May 19 00:35:53 2008
Subject: [Slony1-general] Using Postgres' continuous archiving and
	point-in-time recovery as a form of replication
In-Reply-To: <20080516142914.GB21387@crankycanuck.ca>
References: <082D8A131DF72A4D88C908A1AD3DEB22033E870D@mail-1.rf.lan>
	<20080516142457.564A1120F012@zeus.directinfos.com>
	<20080516142914.GB21387@crankycanuck.ca>
Message-ID: <1211178464.8174.1.camel@huvostro>

On Fri, 2008-05-16 at 10:29 -0400, Andrew Sullivan wrote:
> On Fri, May 16, 2008 at 04:16:32PM +0200, Jacques Caron wrote:
> 
> > representation of data. So you can't use it for transparent upgrade 
> > scenarios.
> > 
> > Correct me if I'm wrong.
> 
> No, you're right.  The only real way to upgrade Postgres today without a
> complete dump and restore is some sort of trigger-based replication (like
> Slony).  I don't know whether Bucardo or Londiste can do the same trick, but
> their design suggests they might be able to.  

Yes, Londiste is used for that also.

> I also used erserver in the
> past to do this, although that was way harder and much scarier.

-----
Hannu


From Glen.Edmonds at tafmo.com  Mon May 19 21:09:28 2008
From: Glen.Edmonds at tafmo.com (Glen Edmonds)
Date: Mon May 19 21:09:53 2008
Subject: [Slony1-general] True cluster
Message-ID: <634262AC6200FF4598AEF683D6CDFEC51D280B@seth.tafmo.net>

Hi,

 

If this is not an appropriate forum for these comments, please let me
know the best place to propose enhancements to functionality.

 

Here's my basic problem with slony and why I think it is not yet
"industrial strength":

 

Despite what the home page says, Slony is absolutely not a clustering
solution.  It is a replication solution.  For any database to have true
high availability (achievable 24/7 up time), it must have a clustering
solution.  Put simply, a cluster has these things:

 

1.	A cluster is comprised of any number of servers that behaves
like a single server and that is addressable by a single consistent
address
2.	A server may be added to the cluster at any time without any
downtime.  When a server is added, it brings itself up to date
automatically
3.	A server may be removed from the cluster at any time without any
downtime
4.	The failure of a server is detected by the cluster and it is
automatically excluded from the cluster without any downtime

 

Basically, you can add and remove servers whenever you like, and because
there's no single point of failure, it can stay up 24/7.

 

Slony can not do any of these things.  That's why I don't use postgres
for the transaction system - we have a 24/7 operation and we can't
guarantee achieving our SLAs with postgres.

 

Regards,

Glen

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080520/49d0bda2/attachment.htm
From wmoran at collaborativefusion.com  Tue May 20 05:29:00 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Tue May 20 05:29:15 2008
Subject: [Slony1-general] True cluster
In-Reply-To: <634262AC6200FF4598AEF683D6CDFEC51D280B@seth.tafmo.net>
References: <634262AC6200FF4598AEF683D6CDFEC51D280B@seth.tafmo.net>
Message-ID: <20080520082900.44ef044c.wmoran@collaborativefusion.com>


You're arguments are very interesting.  When will you be releasing your
code improvements?

In response to "Glen Edmonds" <Glen.Edmonds@tafmo.com>:
> 
> If this is not an appropriate forum for these comments, please let me
> know the best place to propose enhancements to functionality.
> 
> Here's my basic problem with slony and why I think it is not yet
> "industrial strength":
> 
>  
> 
> Despite what the home page says, Slony is absolutely not a clustering
> solution.  It is a replication solution.  For any database to have true
> high availability (achievable 24/7 up time), it must have a clustering
> solution.  Put simply, a cluster has these things:
> 
>  
> 
> 1.	A cluster is comprised of any number of servers that behaves
> like a single server and that is addressable by a single consistent
> address
> 2.	A server may be added to the cluster at any time without any
> downtime.  When a server is added, it brings itself up to date
> automatically
> 3.	A server may be removed from the cluster at any time without any
> downtime
> 4.	The failure of a server is detected by the cluster and it is
> automatically excluded from the cluster without any downtime
> 
>  
> 
> Basically, you can add and remove servers whenever you like, and because
> there's no single point of failure, it can stay up 24/7.
> 
>  
> 
> Slony can not do any of these things.  That's why I don't use postgres
> for the transaction system - we have a 24/7 operation and we can't
> guarantee achieving our SLAs with postgres.
> 
>  
> 
> Regards,
> 
> Glen
> 
>  
> 
>  
> 
> 


-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023

****************************************************************
IMPORTANT: This message contains confidential information and is
intended only for the individual named. If the reader of this
message is not an intended recipient (or the individual
responsible for the delivery of this message to an intended
recipient), please be advised that any re-use, dissemination,
distribution or copying of this message is prohibited. Please
notify the sender immediately by e-mail if you have received
this e-mail by mistake and delete this e-mail from your system.
E-mail transmission cannot be guaranteed to be secure or
error-free as information could be intercepted, corrupted, lost,
destroyed, arrive late or incomplete, or contain viruses. The
sender therefore does not accept liability for any errors or
omissions in the contents of this message, which arise as a
result of e-mail transmission.
****************************************************************
From charles.duffy at gmail.com  Tue May 20 05:41:36 2008
From: charles.duffy at gmail.com (Charles Duffy)
Date: Tue May 20 05:41:38 2008
Subject: [Slony1-general] Merging databases into schema to minimise number
	of slony clusters
Message-ID: <dfdaea8f0805200541y11f6ea08q822c6fefb32081e4@mail.gmail.com>

Hi,

I have 4 separate Postgres databases (within the same pg cluster) that
I need to replicate across a WAN link. This will be a simple two-node
"master-slave" cluster for the time being.

I'm planning to move these databases to four separate schema within
the same pg database, so that I don't need to have four Slony clusters
configured. I'm planning on putting each schema in its own replication
set. I'm thinking the 4-cluster scenario would be a bad thing, since
it would complicate administration, and having four Slon daemons each
side of the link might make for more network traffic.

Has anybody been in a similar situation? Any comments on whether or
not what I've described above is the best way to approach the
situation?

Thanks,

Charles Duffy
From peter.geoghegan86 at gmail.com  Tue May 20 12:30:50 2008
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Tue May 20 12:31:03 2008
Subject: [Slony1-general] Hybrid MS Windows/ Linux cluster
Message-ID: <db471ace0805201230h78b79d4ag525dd8db59156788@mail.gmail.com>

Hello,

It is my intention to create a hybrid Windows/Linux Slony cluster. The
master will run MS windows, and each of the slaves will run Linux. The
master currently runs Postgres 8.3.1 with the latest version of Slony
(as supplied by the "stack builder") , and the slaves run 8.2.4 with a
slightly earlier version of Slony. I believe that configuring Slony is
difficult, with many variables to consider, so I think it wise to
approach this absolutely methodically and circumspectly.

According to the "best practices" section of the slony-I
documentation, "Running all of the slon daemons on a central server
for each network has proven preferable". It seems like a good idea to
manage all slon processes from a central location, so I'll aim to do
this. This cluster will be part of an application that will hopefully
be distributed widely, so ease of installation/ maintenance is
particularly important.

My questions are:

1. As I've said, my current set up has slightly different versions of
slony-I and PostgreSQL. Will this be problematic? Will it prove
essential, or prudent to harmonise versions?

2. According to the "Slon daemons" section of the Slony documentation,
"On Windows? when running as a service things are slightly different.
One slon service is installed, and a separate configuration file
registered for each node to be serviced by that machine. The main
service then manages the individual slons itself". Would it be
possible to control the entire cluster through this one Windows/master
based slon process, with no trace of Slony on the slaves other then
the triggers and so on that the Slony configuration script leaves in
the node's database?

Regards,
Peter Geoghegan
From wmoran at collaborativefusion.com  Tue May 20 12:43:20 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Tue May 20 12:43:44 2008
Subject: [Slony1-general] Hybrid MS Windows/ Linux cluster
In-Reply-To: <db471ace0805201230h78b79d4ag525dd8db59156788@mail.gmail.com>
References: <db471ace0805201230h78b79d4ag525dd8db59156788@mail.gmail.com>
Message-ID: <20080520154320.843130df.wmoran@collaborativefusion.com>

In response to "Peter Geoghegan" <peter.geoghegan86@gmail.com>:

> Hello,
> 
> It is my intention to create a hybrid Windows/Linux Slony cluster. The
> master will run MS windows, and each of the slaves will run Linux. The
> master currently runs Postgres 8.3.1 with the latest version of Slony
> (as supplied by the "stack builder") , and the slaves run 8.2.4 with a
> slightly earlier version of Slony. I believe that configuring Slony is
> difficult, with many variables to consider, so I think it wise to
> approach this absolutely methodically and circumspectly.
> 
> According to the "best practices" section of the slony-I
> documentation, "Running all of the slon daemons on a central server
> for each network has proven preferable". It seems like a good idea to
> manage all slon processes from a central location, so I'll aim to do
> this. This cluster will be part of an application that will hopefully
> be distributed widely, so ease of installation/ maintenance is
> particularly important.
> 
> My questions are:
> 
> 1. As I've said, my current set up has slightly different versions of
> slony-I and PostgreSQL. Will this be problematic? Will it prove
> essential, or prudent to harmonise versions?

It won't work.  The version of Slony has to be exactly the same on all
cluster nodes.

The varying version of PostgreSQL will probably work, but you should test
to be sure.

> 2. According to the "Slon daemons" section of the Slony documentation,
> "On Windows? when running as a service things are slightly different.
> One slon service is installed, and a separate configuration file
> registered for each node to be serviced by that machine. The main
> service then manages the individual slons itself". Would it be
> possible to control the entire cluster through this one Windows/master
> based slon process, with no trace of Slony on the slaves other then
> the triggers and so on that the Slony configuration script leaves in
> the node's database?

You'll still need Slony installed on each of the slaves because the
triggers call procedures written in C and stored in shared objects.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From peter.geoghegan86 at gmail.com  Tue May 20 13:11:05 2008
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Tue May 20 13:11:17 2008
Subject: [Slony1-general] Hybrid MS Windows/ Linux cluster
In-Reply-To: <20080520154320.843130df.wmoran@collaborativefusion.com>
References: <db471ace0805201230h78b79d4ag525dd8db59156788@mail.gmail.com>
	<20080520154320.843130df.wmoran@collaborativefusion.com>
Message-ID: <db471ace0805201311i247ab6c7q616d3f129a9cb03c@mail.gmail.com>

Thanks for that Bill, it's most helpful.

> You'll still need Slony installed on each of the slaves because the
> triggers call procedures written in C and stored in shared objects.

But, once the .so files are present, I should be able to configure the
slon daemons from one central location, the MS Windows master, without
having the slon daemon running/ configured on the slave nodes, right?
Is this "best practice"?

Thanks,
Peter Geoghegan
From ajs at crankycanuck.ca  Tue May 20 14:34:40 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue May 20 14:35:15 2008
Subject: [Slony1-general] True cluster
In-Reply-To: <634262AC6200FF4598AEF683D6CDFEC51D280B@seth.tafmo.net>
References: <634262AC6200FF4598AEF683D6CDFEC51D280B@seth.tafmo.net>
Message-ID: <20080520213440.GA10897@crankycanuck.ca>

On Tue, May 20, 2008 at 02:09:28PM +1000, Glen Edmonds wrote:
> 
> Here's my basic problem with slony and why I think it is not yet
> "industrial strength":
>  
> Despite what the home page says, Slony is absolutely not a clustering
> solution.  It is a replication solution.  For any database to have true
> high availability (achievable 24/7 up time), it must have a clustering
> solution.  Put simply, a cluster has these things:

Well, I think this depends pretty heavily on whatever local definition of
"cluster", "industrial", and "24/7 uptime" you have.

It sounds like what you want is a multi-machine cluster of databases with
multiple members in read/write mode, with some kind of failure detection
that takes over transactions in the event of the loss of a cluster member. 

As I like to tell customers, if you actually need Oracle's RAC, go buy it.
It's an excellent product.  In spite of what some sales people may initially
tell you, it _will not_ provide the fabled five nines, and Oracle Corp.
actually sign a contract with you guaranteeing it does last I checked.  That
said, it seems like a good system to me, and I've never heard anything from
users of it that suggested it didn't work reasonably well ("except when it
doesn't", of course) in local-net scenarios.  (I have heard several very
painful stories about metronet deployments.)

If you want to do this with Postgres, I encourage you to go find Markus, and
start sponsoring work (or pay him to release his work) on Postgres-R.

If you want to describe your actual goals rather than describe a specific
implementation of some technology, then I may have some suggestions on what
you can do to realise those goals.  I don't expect any of it will look much
like Oracle's much-patented system, however.

I will note one other thing, however:

> Basically, you can add and remove servers whenever you like, and because
> there's no single point of failure, it can stay up 24/7.

I think you have a remarkably na?ve view of how reliable systems are
designed.  Hint: a very complicated technology with "no single point of
failure" does not entail a reliable system. 

A
From Glen.Edmonds at tafmo.com  Tue May 20 18:00:53 2008
From: Glen.Edmonds at tafmo.com (Glen Edmonds)
Date: Tue May 20 18:01:14 2008
Subject: [Slony1-general] RE: True clustering
Message-ID: <634262AC6200FF4598AEF683D6CDFEC51D280E@seth.tafmo.net>

> You're arguments are very interesting.  When will you be releasing
your
code improvements?

Bill's comment is fair - my note was not constructive.  My apologies.

I should have added:

1. Is there any plan to enhance Slony to become a clustering solution?

2. If not, are there any other "official" open source projects for
clustering solution to postgres?  If not, would anyone be interested in
starting one with me?

-- Glen

-----Original Message-----
From: slony1-general-request@lists.slony.info
[mailto:slony1-general-request@lists.slony.info] 
Sent: Wednesday, 21 May 2008 5:00 AM
To: slony1-general@lists.slony.info
Subject: Slony1-general Digest, Vol 15, Issue 19

Send Slony1-general mailing list submissions to
	slony1-general@lists.slony.info

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.slony.info/mailman/listinfo/slony1-general
or, via email, send a message with subject or body 'help' to
	slony1-general-request@lists.slony.info

You can reach the person managing the list at
	slony1-general-owner@lists.slony.info

When replying, please edit your Subject line so it is more specific
than "Re: Contents of Slony1-general digest..."


Today's Topics:

   1. True cluster (Glen Edmonds)
   2. Re: True cluster (Bill Moran)


----------------------------------------------------------------------

Message: 1
Date: Tue, 20 May 2008 14:09:28 +1000
From: "Glen Edmonds" <Glen.Edmonds@tafmo.com>
Subject: [Slony1-general] True cluster
To: <slony1-general@lists.slony.info>
Message-ID: <634262AC6200FF4598AEF683D6CDFEC51D280B@seth.tafmo.net>
Content-Type: text/plain; charset="us-ascii"

Hi,


If this is not an appropriate forum for these comments, please let me
know the best place to propose enhancements to functionality.

 

Here's my basic problem with slony and why I think it is not yet
"industrial strength":

 

Despite what the home page says, Slony is absolutely not a clustering
solution.  It is a replication solution.  For any database to have true
high availability (achievable 24/7 up time), it must have a clustering
solution.  Put simply, a cluster has these things:

 

1.	A cluster is comprised of any number of servers that behaves
like a single server and that is addressable by a single consistent
address
2.	A server may be added to the cluster at any time without any
downtime.  When a server is added, it brings itself up to date
automatically
3.	A server may be removed from the cluster at any time without any
downtime
4.	The failure of a server is detected by the cluster and it is
automatically excluded from the cluster without any downtime

 

Basically, you can add and remove servers whenever you like, and because
there's no single point of failure, it can stay up 24/7.
 

Slony can not do any of these things.  That's why I don't use postgres
for the transaction system - we have a 24/7 operation and we can't
guarantee achieving our SLAs with postgres.

 

Regards,

Glen

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL:
http://lists.slony.info/pipermail/slony1-general/attachments/20080520/49
d0bda2/attachment.html

------------------------------

Message: 2
Date: Tue, 20 May 2008 08:29:00 -0400
From: Bill Moran <wmoran@collaborativefusion.com>
Subject: Re: [Slony1-general] True cluster
To: "Glen Edmonds" <Glen.Edmonds@tafmo.com>
Cc: slony1-general@lists.slony.info
Message-ID: <20080520082900.44ef044c.wmoran@collaborativefusion.com>
Content-Type: text/plain; charset=US-ASCII


You're arguments are very interesting.  When will you be releasing your
code improvements?

In response to "Glen Edmonds" <Glen.Edmonds@tafmo.com>:
> 
> If this is not an appropriate forum for these comments, please let me
> know the best place to propose enhancements to functionality.
> 
> Here's my basic problem with slony and why I think it is not yet
> "industrial strength":
> 
>  
> 
> Despite what the home page says, Slony is absolutely not a clustering
> solution.  It is a replication solution.  For any database to have
true
> high availability (achievable 24/7 up time), it must have a clustering
> solution.  Put simply, a cluster has these things:
> 
>  
> 
> 1.	A cluster is comprised of any number of servers that behaves
> like a single server and that is addressable by a single consistent
> address
> 2.	A server may be added to the cluster at any time without any
> downtime.  When a server is added, it brings itself up to date
> automatically
> 3.	A server may be removed from the cluster at any time without any
> downtime
> 4.	The failure of a server is detected by the cluster and it is
> automatically excluded from the cluster without any downtime
> 
>  
> 
> Basically, you can add and remove servers whenever you like, and
because
> there's no single point of failure, it can stay up 24/7.
> 
>  
> 
> Slony can not do any of these things.  That's why I don't use postgres
> for the transaction system - we have a 24/7 operation and we can't
> guarantee achieving our SLAs with postgres.
> 
>  
> 
> Regards,
> 
> Glen
> 
>  


-- 

Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023


------------------------------

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


End of Slony1-general Digest, Vol 15, Issue 19
**********************************************


From wmoran at collaborativefusion.com  Wed May 21 05:39:45 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Wed May 21 05:40:00 2008
Subject: [Slony1-general] RE: True clustering
In-Reply-To: <634262AC6200FF4598AEF683D6CDFEC51D280E@seth.tafmo.net>
References: <634262AC6200FF4598AEF683D6CDFEC51D280E@seth.tafmo.net>
Message-ID: <20080521083945.59cd42f8.wmoran@collaborativefusion.com>

In response to "Glen Edmonds" <Glen.Edmonds@tafmo.com>:

> > You're arguments are very interesting.  When will you be releasing
> your
> code improvements?
> 
> Bill's comment is fair - my note was not constructive.  My apologies.
> 
> I should have added:
> 
> 1. Is there any plan to enhance Slony to become a clustering solution?

First off, you're apparently using the term "cluster" to mean something
very specific.  I point this out because your use of it is _not_ typical
in my experience, and simply saying "clustering" without further
qualification of what you think that means is going to lead to confusion
in many conversations.

To my knowledge, there is no desire or intention to change the design
of Slony I.  It does an excellent job of what it was designed to do,
so _I_ can't see any reason to change it radically, and I don't know
of anyone who does.

> 2. If not, are there any other "official" open source projects for
> clustering solution to postgres?  If not, would anyone be interested in
> starting one with me?

What do you mean by "official"?

pgpool is the typical solution for synchronous multi-master replication,
if that's what you're looking for.

Slony-II attempted to create asynchronous multi-master replication, but
my understand was that the development team ran into serious performance
problems with the design, and the project has been stalled for quite a
while.  Postgres-R has picked up that banner, but I've been a little
out of touch and couldn't tell you how far along they are.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From rafael.domiciano at gmail.com  Wed May 21 12:14:40 2008
From: rafael.domiciano at gmail.com (Rafael Domiciano)
Date: Wed May 21 12:14:54 2008
Subject: [Slony1-general] Slony Vacuuming DB
Message-ID: <3a0028490805211214k7f9911dcv4af3babe73ced309@mail.gmail.com>

Hi there,

Frequently Slony is vacuuming my database, and I'm having to kill his
proccess because it's slowing the client-side application.
I tryied to find option that disables the vacuum but i'm unsucessful.
Someone know how to stop vacuum's Slony?

Tnks,

Rafael Domiciano
Senff Ltda.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080521/=
e4954d03/attachment.htm
From wmoran at collaborativefusion.com  Wed May 21 12:23:27 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Wed May 21 12:23:51 2008
Subject: [Slony1-general] Slony Vacuuming DB
In-Reply-To: <3a0028490805211214k7f9911dcv4af3babe73ced309@mail.gmail.com>
References: <3a0028490805211214k7f9911dcv4af3babe73ced309@mail.gmail.com>
Message-ID: <20080521152327.3b31a869.wmoran@collaborativefusion.com>

In response to "Rafael Domiciano" <rafael.domiciano@gmail.com>:
> 
> Frequently Slony is vacuuming my database, and I'm having to kill his
> proccess because it's slowing the client-side application.
> I tryied to find option that disables the vacuum but i'm unsucessful.
> Someone know how to stop vacuum's Slony?

This should be enough information for you to address any issues:
http://cbbrowne.com/info/maintenance.html

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From rafael.domiciano at gmail.com  Wed May 21 12:54:26 2008
From: rafael.domiciano at gmail.com (Rafael Domiciano)
Date: Wed May 21 12:54:41 2008
Subject: [Slony1-general] Slony Vacuuming DB
In-Reply-To: <20080521152327.3b31a869.wmoran@collaborativefusion.com>
References: <3a0028490805211214k7f9911dcv4af3babe73ced309@mail.gmail.com>
	<20080521152327.3b31a869.wmoran@collaborativefusion.com>
Message-ID: <3a0028490805211254x1f6be478g39710febe3333074@mail.gmail.com>

My postgres set up is not configured to run autovacuum, I ran it at
midnigth.
Today, Slony not vacuumed my DB yet.
I had a big queue on Slony in the last week, could that happened because
this?

2008/5/21 Bill Moran <wmoran@collaborativefusion.com>:

> In response to "Rafael Domiciano" <rafael.domiciano@gmail.com>:
> >
> > Frequently Slony is vacuuming my database, and I'm having to kill his
> > proccess because it's slowing the client-side application.
> > I tryied to find option that disables the vacuum but i'm unsucessful.
> > Someone know how to stop vacuum's Slony?
>
> This should be enough information for you to address any issues:
> http://cbbrowne.com/info/maintenance.html
>
> --
> Bill Moran
> Collaborative Fusion Inc.
> http://people.collaborativefusion.com/~wmoran/<http://people.collaborativ=
efusion.com/%7Ewmoran/>
>
> wmoran@collaborativefusion.com
> Phone: 412-422-3463x4023
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080521/=
01f66775/attachment.htm
From wmoran at collaborativefusion.com  Wed May 21 13:13:30 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Wed May 21 13:13:54 2008
Subject: [Slony1-general] Slony Vacuuming DB
In-Reply-To: <3a0028490805211254x1f6be478g39710febe3333074@mail.gmail.com>
References: <3a0028490805211214k7f9911dcv4af3babe73ced309@mail.gmail.com>
	<20080521152327.3b31a869.wmoran@collaborativefusion.com>
	<3a0028490805211254x1f6be478g39710febe3333074@mail.gmail.com>
Message-ID: <20080521161330.e01ea74a.wmoran@collaborativefusion.com>

In response to "Rafael Domiciano" <rafael.domiciano@gmail.com>:

> My postgres set up is not configured to run autovacuum, I ran it at
> midnigth.
> Today, Slony not vacuumed my DB yet.
> I had a big queue on Slony in the last week, could that happened because
> this?

I think you may be confused.

As mentioned in the page I directed you to, Slony does not vacuum "your
database", it only vacuums the tables that it uses for the purpose of
replication.

If this is slowing your database down, then you have improperly configured
your PostgreSQL server, or insufficiently sized your hardware to handle
replication.

In any event, it sounds like more of a PostgreSQL performance question than
a Slony question.

> 
> 2008/5/21 Bill Moran <wmoran@collaborativefusion.com>:
> 
> > In response to "Rafael Domiciano" <rafael.domiciano@gmail.com>:
> > >
> > > Frequently Slony is vacuuming my database, and I'm having to kill his
> > > proccess because it's slowing the client-side application.
> > > I tryied to find option that disables the vacuum but i'm unsucessful.
> > > Someone know how to stop vacuum's Slony?
> >
> > This should be enough information for you to address any issues:
> > http://cbbrowne.com/info/maintenance.html
> >
> > --
> > Bill Moran
> > Collaborative Fusion Inc.
> > http://people.collaborativefusion.com/~wmoran/<http://people.collaborativefusion.com/%7Ewmoran/>
> >
> > wmoran@collaborativefusion.com
> > Phone: 412-422-3463x4023
> >
> 


-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023

****************************************************************
IMPORTANT: This message contains confidential information and is
intended only for the individual named. If the reader of this
message is not an intended recipient (or the individual
responsible for the delivery of this message to an intended
recipient), please be advised that any re-use, dissemination,
distribution or copying of this message is prohibited. Please
notify the sender immediately by e-mail if you have received
this e-mail by mistake and delete this e-mail from your system.
E-mail transmission cannot be guaranteed to be secure or
error-free as information could be intercepted, corrupted, lost,
destroyed, arrive late or incomplete, or contain viruses. The
sender therefore does not accept liability for any errors or
omissions in the contents of this message, which arise as a
result of e-mail transmission.
****************************************************************
From wmoran at collaborativefusion.com  Wed May 21 14:45:32 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Wed May 21 14:59:12 2008
Subject: [Slony1-general] Slony Vacuuming DB
In-Reply-To: <3a0028490805211342h25e58c03s936fa4d60e28bf09@mail.gmail.com>
References: <3a0028490805211214k7f9911dcv4af3babe73ced309@mail.gmail.com>
	<20080521152327.3b31a869.wmoran@collaborativefusion.com>
	<3a0028490805211254x1f6be478g39710febe3333074@mail.gmail.com>
	<20080521161330.e01ea74a.wmoran@collaborativefusion.com>
	<3a0028490805211342h25e58c03s936fa4d60e28bf09@mail.gmail.com>
Message-ID: <20080521174532.b12924dd.wmoran@collaborativefusion.com>


Please keep the mailing list in the conversation, and please don't
top-post.

In response to "Rafael Domiciano" <rafael.domiciano@gmail.com>:
>
> Bill, are you saying that even autovacuum in postgresql.conf is disabled,
> Slony does vacuum in the tables that it uses?

Yes.  Slony is not autovaccum, they are independent of one another.

> So, there's how to disabled the vacuum's Slony?

There is no way that I know of.

> I have a proccess at
> midnigth that does a "vacuumdb -a -v -z -f", so I don't need Slony's vacuum
> at midday.

That statement is incorrect.

First of, scheduled VACUUM FULLs are not recommended as they cause index
bloating, require aggressive locking, put a heavy load on the server, and
are not an effective method of ongoing maintenance.

Secondly, unless you have a lot more information than you're providing,
you don't _know_ that vacuuming once a day is often enough, and I _highly_
doubt that it is.

> 
> 2008/5/21 Bill Moran <wmoran@collaborativefusion.com>:
> 
> > In response to "Rafael Domiciano" <rafael.domiciano@gmail.com>:
> >
> > > My postgres set up is not configured to run autovacuum, I ran it at
> > > midnigth.
> > > Today, Slony not vacuumed my DB yet.
> > > I had a big queue on Slony in the last week, could that happened because
> > > this?
> >
> > I think you may be confused.
> >
> > As mentioned in the page I directed you to, Slony does not vacuum "your
> > database", it only vacuums the tables that it uses for the purpose of
> > replication.
> >
> > If this is slowing your database down, then you have improperly configured
> > your PostgreSQL server, or insufficiently sized your hardware to handle
> > replication.
> >
> > In any event, it sounds like more of a PostgreSQL performance question than
> > a Slony question.
> >
> > >
> > > 2008/5/21 Bill Moran <wmoran@collaborativefusion.com>:
> > >
> > > > In response to "Rafael Domiciano" <rafael.domiciano@gmail.com>:
> > > > >
> > > > > Frequently Slony is vacuuming my database, and I'm having to kill his
> > > > > proccess because it's slowing the client-side application.
> > > > > I tryied to find option that disables the vacuum but i'm unsucessful.
> > > > > Someone know how to stop vacuum's Slony?
> > > >
> > > > This should be enough information for you to address any issues:
> > > > http://cbbrowne.com/info/maintenance.html
> > > >
> > > > --
> > > > Bill Moran
> > > > Collaborative Fusion Inc.
> > > > http://people.collaborativefusion.com/~wmoran/<http://people.collaborativefusion.com/%7Ewmoran/>
> > <http://people.collaborativefusion.com/%7Ewmoran/>
> > > >
> > > > wmoran@collaborativefusion.com
> > > > Phone: 412-422-3463x4023
> > > >
> > >
> >
> >
> > --
> > Bill Moran
> > Collaborative Fusion Inc.
> > http://people.collaborativefusion.com/~wmoran/<http://people.collaborativefusion.com/%7Ewmoran/>
> >
> > wmoran@collaborativefusion.com
> > Phone: 412-422-3463x4023
> >
> > ****************************************************************
> > IMPORTANT: This message contains confidential information and is
> > intended only for the individual named. If the reader of this
> > message is not an intended recipient (or the individual
> > responsible for the delivery of this message to an intended
> > recipient), please be advised that any re-use, dissemination,
> > distribution or copying of this message is prohibited. Please
> > notify the sender immediately by e-mail if you have received
> > this e-mail by mistake and delete this e-mail from your system.
> > E-mail transmission cannot be guaranteed to be secure or
> > error-free as information could be intercepted, corrupted, lost,
> > destroyed, arrive late or incomplete, or contain viruses. The
> > sender therefore does not accept liability for any errors or
> > omissions in the contents of this message, which arise as a
> > result of e-mail transmission.
> > ****************************************************************
> >
> 


-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023

****************************************************************
IMPORTANT: This message contains confidential information and is
intended only for the individual named. If the reader of this
message is not an intended recipient (or the individual
responsible for the delivery of this message to an intended
recipient), please be advised that any re-use, dissemination,
distribution or copying of this message is prohibited. Please
notify the sender immediately by e-mail if you have received
this e-mail by mistake and delete this e-mail from your system.
E-mail transmission cannot be guaranteed to be secure or
error-free as information could be intercepted, corrupted, lost,
destroyed, arrive late or incomplete, or contain viruses. The
sender therefore does not accept liability for any errors or
omissions in the contents of this message, which arise as a
result of e-mail transmission.
****************************************************************
From guillaume.cheramy at gmail.com  Thu May 22 06:56:53 2008
From: guillaume.cheramy at gmail.com (CHERAMY Guillaume)
Date: Thu May 22 06:57:03 2008
Subject: [Slony1-general] slonik_subscribe_set error
Message-ID: <48357BA5.2050908@gmail.com>

Hello,

    i a newby with slony and for my test i use this doc :
http://slony.info/documentation/firstdb.html

I have got 2 servers with on each postgres.

All is good, but i have got an error when i want to subscribe my set '1'
for node 2:

postgres@xxxxx:~$ slonik_subscribe_set 1 2 | slonik
<stdin>:4: PGRES_FATAL_ERROR select "_test_basetest2".subscribeSet(1, 1,
2, 't');  - ERROR:  Slony-I: subscribeSet(): set 1 not found

Thanks for your help

-- 

                        ''~``
                        ( o o )
+------------------.oooO--(_)--Oooo.---------------------+
| Guillaume Ch?ramy - Guidtz			         |
| E-Mail : guillaume.cheramy@gmail.com                   |
| 		     (   )                               |
+---------------------\ (----(   )-----------------------+
                       \_)    ) /
                             (_/

From cbbrowne at ca.afilias.info  Thu May 22 07:33:23 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu May 22 07:33:37 2008
Subject: [Slony1-general] Version 1.2.14 available
In-Reply-To: <72dbd3150805161518r6e6f7d8emea83e6c987972524@mail.gmail.com>
	(David Rees's message of "Fri, 16 May 2008 15:18:44 -0700")
References: <607iduf876.fsf@dba2.int.libertyrms.com>
	<Pine.LNX.4.64.0805161139340.8269@discord.home.frostconsultingllc.com>
	<60prrmdlql.fsf@dba2.int.libertyrms.com>
	<Pine.LNX.4.64.0805161433560.8269@discord.home.frostconsultingllc.com>
	<72dbd3150805161502v670b5ca1u36f133c832ae4295@mail.gmail.com>
	<72dbd3150805161518r6e6f7d8emea83e6c987972524@mail.gmail.com>
Message-ID: <60k5hms91o.fsf@dba2.int.libertyrms.com>

"David Rees" <drees76@gmail.com> writes:
> On Fri, May 16, 2008 at 3:02 PM, David Rees <drees76@gmail.com> wrote:
>> On Fri, May 16, 2008 at 2:39 PM, Jeff Frost <jeff@frostconsultingllc.com> wrote:
>>> On Fri, 16 May 2008, Christopher Browne wrote:
>>>> Jeff Frost <jeff@frostconsultingllc.com> writes:
>>>>> Hrmmm..I get this when trying to update functions:
>>>>>
>>>>> ./upgrade.slonik:5: Possible unsupported PostgreSQL version 8.3,
>>>>> defaulting to 8.1 support
>
> Looks like there is two places where the versions are checked in
> slonik.c, around like 1900 and around line 2000. The code should
> probably be moved in to a common function, but attached is a quick
> patch to make them match.
>
> At this point the message is completely harmless and only cosmetic.

Right you are!  Happily, the message is, as you say, harmless and
cosmetic.  The code is doing the right things anyways.  I'll see about
patching this (not today; I'm at PGCon) so it appears to behave right,
instead of merely doing so :-).

It is indeed irritating that this code occurs twice; it is essentially
eliminated in v2.0, so I don't think it's worth going to much effort
to do the "big fix."
-- 
"cbbrowne","@","acm.org"
http://cbbrowne.com/info/oses.html
"I still maintain the point that designing a monolithic kernel in 1991
is  a fundamental error.   Be thankful  you are  not my  student.  You
would not get a high grade  for such a design :-)" -- Andrew Tanenbaum
to Linus Torvalds
From cbbrowne at ca.afilias.info  Thu May 22 07:56:00 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu May 22 07:56:06 2008
Subject: [Slony1-general] Merging databases into schema to minimise number
	of slony clusters
In-Reply-To: <dfdaea8f0805200541y11f6ea08q822c6fefb32081e4@mail.gmail.com>
	(Charles Duffy's message of "Tue, 20 May 2008 22:41:36 +1000")
References: <dfdaea8f0805200541y11f6ea08q822c6fefb32081e4@mail.gmail.com>
Message-ID: <60fxsas7zz.fsf@dba2.int.libertyrms.com>

"Charles Duffy" <charles.duffy@gmail.com> writes:
> I have 4 separate Postgres databases (within the same pg cluster) that
> I need to replicate across a WAN link. This will be a simple two-node
> "master-slave" cluster for the time being.
>
> I'm planning to move these databases to four separate schema within
> the same pg database, so that I don't need to have four Slony clusters
> configured. I'm planning on putting each schema in its own replication
> set. I'm thinking the 4-cluster scenario would be a bad thing, since
> it would complicate administration, and having four Slon daemons each
> side of the link might make for more network traffic.
>
> Has anybody been in a similar situation? Any comments on whether or
> not what I've described above is the best way to approach the
> situation?

That sounds pretty appropriate.

I can't guarantee there will be *no* issues with such a configuration;
it warrants some testing, but it otta work, and your plan sounds
right.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://www3.sympatico.ca/cbbrowne/nonrdbms.html
Rules  of  the  Evil  Overlord  #231.   "Mythical  guardians  will  be
instructed to  ask visitors name,  purpose of visit, and  whether they
have an appointment instead of ancient riddles.
<http://www.eviloverlord.com/>
From cbbrowne at ca.afilias.info  Thu May 22 08:01:30 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu May 22 08:01:36 2008
Subject: [Slony1-general] Hybrid MS Windows/ Linux cluster
In-Reply-To: <db471ace0805201230h78b79d4ag525dd8db59156788@mail.gmail.com>
	(Peter Geoghegan's message of "Tue, 20 May 2008 20:30:50 +0100")
References: <db471ace0805201230h78b79d4ag525dd8db59156788@mail.gmail.com>
Message-ID: <60bq2ys7qt.fsf@dba2.int.libertyrms.com>

"Peter Geoghegan" <peter.geoghegan86@gmail.com> writes:
> 1. As I've said, my current set up has slightly different versions of
> slony-I and PostgreSQL. Will this be problematic? Will it prove
> essential, or prudent to harmonise versions?

Varying versions of Slony-I *WILL* bite you; each cluster MUST use an
identical version of Slony-I across *ALL* the databases in the
cluster.  Slony-I does not accept ANY variation in version numbering;
it will reject connecting to databases any time the slon finds the
database is configured for a different version.

On the other hand, Slony-I will accomodate multiple PostgreSQL
releases.

> 2. According to the "Slon daemons" section of the Slony documentation,
> "On Windows. when running as a service things are slightly different.
> One slon service is installed, and a separate configuration file
> registered for each node to be serviced by that machine. The main
> service then manages the individual slons itself". Would it be
> possible to control the entire cluster through this one Windows/master
> based slon process, with no trace of Slony on the slaves other then
> the triggers and so on that the Slony configuration script leaves in
> the node's database?

You MUST have the appropriate version of Slony-I built into each
server's PostgreSQL build.
-- 
output = reverse("gro.mca" "@" "enworbbc")
http://www3.sympatico.ca/cbbrowne/nonrdbms.html
Rules of the Evil Overlord #12. "One of my advisors will be an average
five-year-old child. Any flaws in my plan that he is able to spot will
be corrected before implementation.  <http://www.eviloverlord.com/>
From cbbrowne at ca.afilias.info  Thu May 22 08:38:51 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu May 22 08:39:00 2008
Subject: [Slony1-general] RE: True clustering
In-Reply-To: <634262AC6200FF4598AEF683D6CDFEC51D280E@seth.tafmo.net> (Glen
	Edmonds's message of "Wed, 21 May 2008 11:00:53 +1000")
References: <634262AC6200FF4598AEF683D6CDFEC51D280E@seth.tafmo.net>
Message-ID: <607idms60k.fsf@dba2.int.libertyrms.com>

"Glen Edmonds" <Glen.Edmonds@tafmo.com> writes:
>> You're arguments are very interesting.  When will you be releasing
> your
> code improvements?
>
> Bill's comment is fair - my note was not constructive.  My apologies.
>
> I should have added:
>
> 1. Is there any plan to enhance Slony to become a clustering solution?
>
> 2. If not, are there any other "official" open source projects for
> clustering solution to postgres?  If not, would anyone be interested in
> starting one with me?

There is a really fundamental problem with this idea, which is that
"clustering" of this sort tends to require integration with the
operating system.

Because PostgreSQL is an OS-agnostic project (save for looking at the
world from a POSIX perspective), and Slony-I tries to be, too, that
makes it a fair bit challenging to turn either into "a clustering
solution."

You'll get a lot of "push-back" on attempts to change either project
because of the preference for that "agnosticism."
-- 
"cbbrowne","@","acm.org"
http://www3.sympatico.ca/cbbrowne/x.html
There are two kinds of people in the world: People who think there are
two kinds of people and people who don't.
From cbbrowne at ca.afilias.info  Thu May 22 08:56:06 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu May 22 08:56:14 2008
Subject: [Slony1-general] Slony Vacuuming DB
In-Reply-To: <3a0028490805211214k7f9911dcv4af3babe73ced309@mail.gmail.com>
	(Rafael Domiciano's message of "Wed, 21 May 2008 16:14:40 -0300")
References: <3a0028490805211214k7f9911dcv4af3babe73ced309@mail.gmail.com>
Message-ID: <60zlqiqqnd.fsf@dba2.int.libertyrms.com>

"Rafael Domiciano" <rafael.domiciano@gmail.com> writes:
> Frequently Slony is vacuuming my database, and I'm having to kill his proccess because it's slowing the client-side application.
> I tryied to find option that disables the vacuum but i'm unsucessful.
> Someone know how to stop vacuum's Slony?

You can look at the documentation for the slon command, and find how
to do this.

However, it is entirely probable that this is a Really Unwise thing to
do.

Slony-I does not vacuum anything of your application's tables; it only
runs VACUUM and ANALYZE against its own internal tables.

Those tables need to be vacuumed quite frequently.

"Rafael Domiciano" <rafael.domiciano@gmail.com> writes:
> I have a proccess at 
> midnigth that does a "vacuumdb -a -v -z -f", so I don't need Slony's vacuum 
> at midday. 

That is absolutely incorrect.

Slony-I's internal tables MUST be vacuumed WAY more frequently than
once per day.  If you suppress the vacuuming that Slony-I does, and do
not run autovacuum or something similar, you WILL experience big
problems with replication.

We didn't code Slony-I to run VACUUM on its tables just for fun - we
did so because it was *necessary*.  You suppress the VACUUMing at your
peril.

If you examine the documentation, you can find a way to suppress the
vacuuming, but I decline to explain where, as I am reasonably certain
from the details that you have given that it would have *terrible*
implications for your environment.
-- 
output = reverse("gro.mca" "@" "enworbbc")
http://linuxfinances.info/info/bestpractices.html
"Programming today  is a race  between software engineers  striving to
build bigger and better  idiot-proof programs, and the Universe trying
to  produce  bigger  and  better  idiots.  So  far,  the  Universe  is
winning."  -- Rich Cook
From cbbrowne at ca.afilias.info  Thu May 22 09:06:45 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu May 22 09:06:53 2008
Subject: [Slony1-general] True cluster
In-Reply-To: <20080520213440.GA10897@crankycanuck.ca> (Andrew Sullivan's
	message of "Tue, 20 May 2008 17:34:40 -0400")
References: <634262AC6200FF4598AEF683D6CDFEC51D280B@seth.tafmo.net>
	<20080520213440.GA10897@crankycanuck.ca>
Message-ID: <60ve16qq5m.fsf@dba2.int.libertyrms.com>

Andrew Sullivan <ajs@crankycanuck.ca> writes:
> On Tue, May 20, 2008 at 02:09:28PM +1000, Glen Edmonds wrote:
>> 
>> Here's my basic problem with slony and why I think it is not yet
>> "industrial strength":
>>  
>> Despite what the home page says, Slony is absolutely not a clustering
>> solution.  It is a replication solution.  For any database to have true
>> high availability (achievable 24/7 up time), it must have a clustering
>> solution.  Put simply, a cluster has these things:
>
> Well, I think this depends pretty heavily on whatever local definition of
> "cluster", "industrial", and "24/7 uptime" you have.
>
> It sounds like what you want is a multi-machine cluster of databases with
> multiple members in read/write mode, with some kind of failure detection
> that takes over transactions in the event of the loss of a cluster member. 

A base bit of the design rules this sort of thing out:
-----------------
1.3. What Slony-I is not

    * Slony-I is not a network management system.

    * Slony-I does not have any functionality within it to detect a
      node failure, nor to automatically promote a node to a master or
      other data origin.
-----------------

HA *requires* a set of functionality to evaluate these things, and
those things tend to be quite platform-specific.

PostgreSQL and Slony-I are both intended to be platform-agnostic,
which means that quite a bit of the apparatus necessary can't get
integrated into the project, at least, not tightly.
-- 
(format nil "~S@~S" "cbbrowne" "linuxdatabases.info")
http://www3.sympatico.ca/cbbrowne/spiritual.html
Rules of the Evil Overlord #203.  "I will not employ an evil wizard if
he has a sleazy mustache." <http://www.eviloverlord.com/>
From salmanb at quietcaresystems.com  Thu May 22 09:25:20 2008
From: salmanb at quietcaresystems.com (salman)
Date: Thu May 22 09:25:29 2008
Subject: [Slony1-general] Slony Vacuuming DB
In-Reply-To: <60zlqiqqnd.fsf@dba2.int.libertyrms.com>
References: <3a0028490805211214k7f9911dcv4af3babe73ced309@mail.gmail.com>
	<60zlqiqqnd.fsf@dba2.int.libertyrms.com>
Message-ID: <48359E70.5000404@quietcaresystems.com>



Christopher Browne wrote:
> "Rafael Domiciano" <rafael.domiciano@gmail.com> writes:
>> Frequently Slony is vacuuming my database, and I'm having to kill his proccess because it's slowing the client-side application.
>> I tryied to find option that disables the vacuum but i'm unsucessful.
>> Someone know how to stop vacuum's Slony?
> 

Not sure what kind of a shop you run and how many 
inserts/updates/deletes you do during a day -- we do about 5.5M inserts 
a day, and we archive ~3-4M records a day (dumped to disk, removed from 
production db) and the slony vacuum jobs do not slow anything down for us.

The only time we have slowdown is when one particular script leaves an 
'idle in transaction' query open for a number of hours -- once that 
happens, slony's fetch requests and the growing log table really drag 
the server down. But, the problem isn't created by slony here, 
obviously, it's one of our scripts.

-salman
From stepha at wni.co.jp  Thu May 22 22:37:11 2008
From: stepha at wni.co.jp (Stephane LAPIE)
Date: Thu May 22 22:37:53 2008
Subject: [Slony1-general] Slow replication issue
Message-ID: <48365807.1000605@wni.co.jp>

Hello,

We have been using Slony-I for nearly two years now for our database 
system, with the following cluster :
- 6 Postgres 8.2.4 servers, four in Asia, two in Europe
- Communication occurs over a VPN, and every server can reach the others
- Roundtrip average is 300ms for Europe and Asia servers
- Data providers are in Asia, 3 million row updates / day average 
(mostly split in 4 big updates / day), and at peak time, 100 updates/s

Replication basically occurs fine, to the exception that the European 
servers accumulate lag for around two, three hours (six in worst cases 
observed) when data processing occurs in Asia. We tried raising 
"sync_group_maxsize" in every slon.conf to 24, which diminished the 
number of events replication was lagging behind, but did not diminish 
the lag time (Both are observed through the _SCHEMA.sl_status view, and 
made into a graph through MRTG)

By reducing processed data quantities of 30%, we could reduce the lag 
time by half. We also checked the VPN status, and there is no obvious 
anomaly nor QoS. The only startling point would be that bandwidth usage 
is extremely low, and that reducing processed data quantities did not 
make the bandwidth usage fall any lower.

It looks as though as Slony is actually limiting itself to not over-use 
the network's bandwidth, but actually ends up using nearly nothing. 
Given our current network environment, we could perfectly cope with 
Slony using 10 times more bandwidth with no problem.

Is there a way to force Slony out of it's "don't go too hard on the 
network" mode (such as a setting in slon.conf)?

Thanks in advance,
-- 
Stephane LAPIE
Condapter / WITH EPC / Europe
Weathernews, Inc.
From hannu at krosing.net  Fri May 23 01:27:07 2008
From: hannu at krosing.net (Hannu Krosing)
Date: Fri May 23 01:27:40 2008
Subject: [Slony1-general] Slow replication issue
In-Reply-To: <48365807.1000605@wni.co.jp>
References: <48365807.1000605@wni.co.jp>
Message-ID: <1211531227.6789.41.camel@huvostro>

On Fri, 2008-05-23 at 14:37 +0900, Stephane LAPIE wrote:
> Hello,
> 
> We have been using Slony-I for nearly two years now for our database 
> system, with the following cluster :
> - 6 Postgres 8.2.4 servers, four in Asia, two in Europe
> - Communication occurs over a VPN, and every server can reach the others
> - Roundtrip average is 300ms for Europe and Asia servers
> - Data providers are in Asia, 3 million row updates / day average 
> (mostly split in 4 big updates / day), and at peak time, 100 updates/s
> 
> Replication basically occurs fine, to the exception that the European 
> servers accumulate lag for around two, three hours (six in worst cases 
> observed) when data processing occurs in Asia. We tried raising 
> "sync_group_maxsize" in every slon.conf to 24, which diminished the 
> number of events replication was lagging behind, but did not diminish 
> the lag time (Both are observed through the _SCHEMA.sl_status view, and 
> made into a graph through MRTG)

My experience is that for high-latency links it is crucial to have the
slon daemon doing the inserts/updates/deletes running on the receiving
side (close to the slave tables) so that the actual data updating
queries do not happen over WAN links.

You may need to split your replication to 2 separate slony clusters
executed by separate slony daemons to achieve this.

> By reducing processed data quantities of 30%, we could reduce the lag 
> time by half. We also checked the VPN status, and there is no obvious 
> anomaly nor QoS. The only startling point would be that bandwidth usage 
> is extremely low, and that reducing processed data quantities did not 
> make the bandwidth usage fall any lower.
> 
> It looks as though as Slony is actually limiting itself to not over-use 
> the network's bandwidth, but actually ends up using nearly nothing. 
> Given our current network environment, we could perfectly cope with 
> Slony using 10 times more bandwidth with no problem.
> 
> Is there a way to force Slony out of it's "don't go too hard on the 
> network" mode (such as a setting in slon.conf)?
> 
> Thanks in advance,

From stepha at wni.co.jp  Fri May 23 03:57:45 2008
From: stepha at wni.co.jp (Stephane LAPIE)
Date: Fri May 23 03:59:29 2008
Subject: [Slony1-general] Slow replication issue
In-Reply-To: <1211531227.6789.41.camel@huvostro>
References: <48365807.1000605@wni.co.jp> <1211531227.6789.41.camel@huvostro>
Message-ID: <4836A329.2080208@wni.co.jp>

Hannu Krosing wrote:
> My experience is that for high-latency links it is crucial to have the
> slon daemon doing the inserts/updates/deletes running on the receiving
> side (close to the slave tables) so that the actual data updating
> queries do not happen over WAN links.
> 
> You may need to split your replication to 2 separate slony clusters
> executed by separate slony daemons to achieve this.

Oh, yes, I forgot mentioning, I already run a slon daemon on each server.

As for the splitting, wouldn't having the processing done on two 
servers, and with only two separate data sets (each server being master 
for one data set) do the same? I noticed that for two data sets 
originating from different servers, the set with virtually no 
replication load compared to the other fared pretty well.
-- 
Stephane LAPIE
Condapter / WITH EPC / Europe
Weathernews, Inc.
From hannu at krosing.net  Fri May 23 04:09:06 2008
From: hannu at krosing.net (Hannu Krosing)
Date: Fri May 23 04:09:44 2008
Subject: [Slony1-general] Slow replication issue
In-Reply-To: <4836A329.2080208@wni.co.jp>
References: <48365807.1000605@wni.co.jp> <1211531227.6789.41.camel@huvostro>
	<4836A329.2080208@wni.co.jp>
Message-ID: <1211540946.6789.49.camel@huvostro>

On Fri, 2008-05-23 at 19:57 +0900, Stephane LAPIE wrote:

> Oh, yes, I forgot mentioning, I already run a slon daemon on each server.
> 
> As for the splitting, wouldn't having the processing done on two 
> servers, and with only two separate data sets (each server being master 
> for one data set) do the same? 

Not sure, check from logs, which daemon does the actual data updating.

I have not been running slony for quite a long time. I last used it at
Skype a few years ago before we moved to our own implementation -
Londiste/pgQ from SkyTools. The main reason was that our cluster got too
big to manage with slony.

> I noticed that for two data sets 
> originating from different servers, the set with virtually no 
> replication load compared to the other fared pretty well.

-------------
Hannu


From henry at zen.co.za  Fri May 23 05:47:54 2008
From: henry at zen.co.za (Henry)
Date: Fri May 23 05:48:02 2008
Subject: [Slony1-general] Slow replication issue
In-Reply-To: <1211540946.6789.49.camel@huvostro>
References: <48365807.1000605@wni.co.jp> <1211531227.6789.41.camel@huvostro>
	<4836A329.2080208@wni.co.jp> <1211540946.6789.49.camel@huvostro>
Message-ID: <63176.196.23.181.69.1211546874.squirrel@zenmail.co.za>

On Fri, May 23, 2008 1:09 pm, Hannu Krosing wrote:
> On Fri, 2008-05-23 at 19:57 +0900, Stephane LAPIE wrote:
> I have not been running slony for quite a long time. I last used it at
> Skype a few years ago before we moved to our own implementation -
> Londiste/pgQ from SkyTools. The main reason was that our cluster got too
> big to manage with slony.

Sadly, the documentation for SkyTools is *appalling*.  I took a look some
time ago and ran away pronto.

I think the origional question still stands:  how do you get slony to pump
more data and take full advantage of available network bandwidth.  In our
case, we have a large cluster with gigabit networking, yet usage is a
paltry few megabits/s (so large updates take hours to days to replicate).

I recall asking the same question some months ago, but no real solution
surfaced.  It appears to be pretty much a case of "that's the way slony
works."

Regards
Henry

From cbbrowne at ca.afilias.info  Fri May 23 08:23:08 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri May 23 08:23:20 2008
Subject: [Slony1-general] Slow replication issue
In-Reply-To: <63176.196.23.181.69.1211546874.squirrel@zenmail.co.za>
	(henry@zen.co.za's message of "Fri,
	23 May 2008 14:47:54 +0200 (SAST)")
References: <48365807.1000605@wni.co.jp> <1211531227.6789.41.camel@huvostro>
	<4836A329.2080208@wni.co.jp> <1211540946.6789.49.camel@huvostro>
	<63176.196.23.181.69.1211546874.squirrel@zenmail.co.za>
Message-ID: <607idlqc2r.fsf@dba2.int.libertyrms.com>

"Henry" <henry@zen.co.za> writes:
> On Fri, May 23, 2008 1:09 pm, Hannu Krosing wrote:
>> On Fri, 2008-05-23 at 19:57 +0900, Stephane LAPIE wrote:
>> I have not been running slony for quite a long time. I last used it at
>> Skype a few years ago before we moved to our own implementation -
>> Londiste/pgQ from SkyTools. The main reason was that our cluster got too
>> big to manage with slony.
>
> Sadly, the documentation for SkyTools is *appalling*.  I took a look some
> time ago and ran away pronto.
>
> I think the origional question still stands:  how do you get slony to pump
> more data and take full advantage of available network bandwidth.  In our
> case, we have a large cluster with gigabit networking, yet usage is a
> paltry few megabits/s (so large updates take hours to days to replicate).
>
> I recall asking the same question some months ago, but no real solution
> surfaced.  It appears to be pretty much a case of "that's the way slony
> works."

The recommendation to run the slon near the node it is managing was a
good one; that will cut down on the round-tripping between slon and
local node.

The other thing that comes to mind is that maybe increasing the number
of rows fetched by the cursor from the provider would be of some help.
Default value for SLON_FETCH_DATA_SIZE is 100, which means it's
FETCHing 100 rows from the cursor each time; there may be value in
increasing that somewhat.

The other thing to do is to try to see what your cluster is busy with.
Looking at pg_stat_activity on both the provider and the subscriber
should give an idea of where the bottleneck is; one of them is likely
to consistently be IDLE, and that should tell us what to look at.

I'm not sure but that maybe something's flakey with your network; that
would also be an explanation for the problem, and Slony-I can't
resolve that!
-- 
output = ("cbbrowne" "@" "linuxdatabases.info")
http://cbbrowne.com/info/x.html
"I would rather spend 10 hours reading someone else's source code than
10  minutes listening  to Musak  waiting for  technical  support which
isn't." -- Dr. Greg Wettstein, Roger Maris Cancer Center
From henry at zen.co.za  Fri May 23 10:12:44 2008
From: henry at zen.co.za (Henry)
Date: Fri May 23 10:12:56 2008
Subject: [Slony1-general] Slow replication issue
In-Reply-To: <607idlqc2r.fsf@dba2.int.libertyrms.com>
References: <48365807.1000605@wni.co.jp> <1211531227.6789.41.camel@huvostro>
	<4836A329.2080208@wni.co.jp> <1211540946.6789.49.camel@huvostro>
	<63176.196.23.181.69.1211546874.squirrel@zenmail.co.za>
	<607idlqc2r.fsf@dba2.int.libertyrms.com>
Message-ID: <64297.196.23.181.69.1211562764.squirrel@zenmail.co.za>


On Fri, May 23, 2008 5:23 pm, Christopher Browne wrote:
>> case, we have a large cluster with gigabit networking, yet usage is a
>> paltry few megabits/s (so large updates take hours to days to
>> replicate).

> I'm not sure but that maybe something's flakey with your network; that
> would also be an explanation for the problem, and Slony-I can't
> resolve that!

:) fair enough.  Let me rephrase:  I can saturate my network with rsync,
but Slony doesn't seem to follow suit (even bumping SLON_FETCH_DATA_SIZE).
 I do understand, however, that pumping data between nodes isn't the only
processing being done.

I'll say this about Slony though:  the documentation is /comprehensive/. 
I think the only area it's lacking is in problem resolution.

/aside:  The CommandPrompt crew will (apparently) release Mammoth
Replicator as open source soon.  I'm keen to give it a swing to see how
their non-trigger approach performs.  Our system is
excessive-replication-lag intolerant...

Regards
Henry

From cbbrowne at ca.afilias.info  Fri May 23 10:33:35 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri May 23 10:33:43 2008
Subject: [Slony1-general] Slow replication issue
In-Reply-To: <64297.196.23.181.69.1211562764.squirrel@zenmail.co.za>
	(henry@zen.co.za's message of "Fri,
	23 May 2008 19:12:44 +0200 (SAST)")
References: <48365807.1000605@wni.co.jp> <1211531227.6789.41.camel@huvostro>
	<4836A329.2080208@wni.co.jp> <1211540946.6789.49.camel@huvostro>
	<63176.196.23.181.69.1211546874.squirrel@zenmail.co.za>
	<607idlqc2r.fsf@dba2.int.libertyrms.com>
	<64297.196.23.181.69.1211562764.squirrel@zenmail.co.za>
Message-ID: <60y761orgw.fsf@dba2.int.libertyrms.com>

"Henry" <henry@zen.co.za> writes:
> On Fri, May 23, 2008 5:23 pm, Christopher Browne wrote:
>>> case, we have a large cluster with gigabit networking, yet usage is a
>>> paltry few megabits/s (so large updates take hours to days to
>>> replicate).
>
>> I'm not sure but that maybe something's flakey with your network; that
>> would also be an explanation for the problem, and Slony-I can't
>> resolve that!
>
> :) fair enough.  Let me rephrase:  I can saturate my network with rsync,
> but Slony doesn't seem to follow suit (even bumping SLON_FETCH_DATA_SIZE).
>  I do understand, however, that pumping data between nodes isn't the only
> processing being done.

I'm just trying to be comprehensive about the possibilities ;-).  

If you can take a peek at pg_stat_activity, you may be able to track
down where the bottleneck is, which might help us help you further.
-- 
output = ("cbbrowne" "@" "cbbrowne.com")
http://cbbrowne.com/info/oses.html
"I will not be numbered, stamped, briefed, debriefed, or filed!"
-- Number Six
From JanWieck at Yahoo.com  Fri May 23 11:05:00 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Fri May 23 11:05:49 2008
Subject: [Slony1-general] Slow replication issue
Message-ID: <200805231805.m4NI5Mrj032900@jupiter.jannicash.info>

Slony has no mechanisms built in that would artificially limit any bandwidth use. So what you observe must have some other roots.

The slony connections are all regular libpq database connections. So you might test this by using psql or pg_dump running on one of those subscribers, connecting to the appropriate data provider. If that can utilize more bandwidth, then the problem lies within the replica itself and something else must be limiting it from reading from the network faster.


Jan

--
Anyone who trades liberty for security deserves neither liberty nor security. -- Benjamin Franklin 

-----Original Message-----

From:  Stephane LAPIE <stepha@wni.co.jp>
Subj:  [Slony1-general] Slow replication issue
Date:  Fri May 23, 2008 1:37
Size:  1K
To:  slony1-general@lists.slony.info

Hello,

We have been using Slony-I for nearly two years now for our database 
system, with the following cluster :
- 6 Postgres 8.2.4 servers, four in Asia, two in Europe
- Communication occurs over a VPN, and every server can reach the others
- Roundtrip average is 300ms for Europe and Asia servers
- Data providers are in Asia, 3 million row updates / day average 
(mostly split in 4 big updates / day), and at peak time, 100 updates/s

Replication basically occurs fine, to the exception that the European 
servers accumulate lag for around two, three hours (six in worst cases 
observed) when data processing occurs in Asia. We tried raising 
"sync_group_maxsize" in every slon.conf to 24, which diminished the 
number of events replication was lagging behind, but did not diminish 
the lag time (Both are observed through the _SCHEMA.sl_status view, and 
made into a graph through MRTG)

By reducing processed data quantities of 30%, we could reduce the lag 
time by half. We also checked the VPN status, and there is no obvious 
anomaly nor QoS. The only startling point would be that bandwidth usage 
is extremely low, and that reducing processed data quantities did not 
make the bandwidth usage fall any lower.

It looks as though as Slony is actually limiting itself to not over-use 
the network's bandwidth, but actually ends up using nearly nothing. 
Given our current network environment, we could perfectly cope with 
Slony using 10 times more bandwidth with no problem.

Is there a way to force Slony out of it's "don't go too hard on the 
network" mode (such as a setting in slon.conf)?

Thanks in advance,
-- 
Stephane LAPIE
Condapter / WITH EPC / Europe
Weathernews, Inc.
_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


From tmblue at gmail.com  Sat May 24 11:16:56 2008
From: tmblue at gmail.com (Tory M Blue)
Date: Sat May 24 11:17:05 2008
Subject: [Slony1-general] auto vac output: Page Slots
Message-ID: <8a547c840805241116r200e4bd1we74f57545090fe51@mail.gmail.com>

Good morning,

I have a question re some slon output that I'm seeing.

I'm wondering if my configuration needs tweaking if I'm seeing these
kind of numbers during/after an autovac

DETAIL:  A total of 227168 page slots are in use (including overhead).
227168 page slots are required to track all free space.
Current limits are:  1087500 page slots, 430 relations, using 6401 kB.
VACUUM

Does the higher than needed page slots affect anything, performance,
resources etc? Is it something I should tune for or, no?

Thanks
Tory
From tmblue at gmail.com  Sat May 24 11:26:10 2008
From: tmblue at gmail.com (Tory M Blue)
Date: Sat May 24 11:26:19 2008
Subject: [Slony1-general] Slon Bloat or question re master and slave
	reported bloat
Message-ID: <8a547c840805241126rca59df4v217b14adf3d9c66c@mail.gmail.com>

So full of questions today and it's a Saturday, well guess that's the
nature of the beast.

Postgres 8.3.1 slon 1.2.14

Question is why there seems to be such a difference in size between
the master and the slave.

SLAVE
POSTGRES_BLOAT OK: DB "clsdb" (host:idb02.domain.com) table
cls.listings rows:1608406 pages:335434 shouldbe:222939 (1.5X) wasted
size:921559040 (879 MB) | time=2.32  cls.listings=921559040

MASTER
POSTGRES_BLOAT OK: DB "clsdb" (host:idb01.domain.com) table
cls.listings rows:1599038 pages:292044 shouldbe:222460 (1.3X) wasted
size:570032128 (544 MB) | time=2.57  cls.listings=570032128

Yes I checked the autovacs and they ran 2 hours apart but during a
time of low traffic, this would not explain the pretty significant
difference in wasted space.. I'm wondering where I should start to
look and or if I should waste any effort on tracking this down?

The hardware is identical, the settings/conf are identical, I was just
thinking that the db's would track more closely.

Thanks

Tory
From M.Eriksson at albourne.com  Sun May 25 23:25:34 2008
From: M.Eriksson at albourne.com (Martin Eriksson)
Date: Sun May 25 23:26:01 2008
Subject: [Slony1-general] pg_dump/pg_restore keeping all slony stuff intact?
In-Reply-To: <26864996.01211783042391.JavaMail.meriksso@martin-Ubuntu>
Message-ID: <24113997.21211783128950.JavaMail.meriksso@martin-Ubuntu>

Hi people,

I'm wondering if there is a way to shut down a slave slony node take a dump using pg_dump of the database on that node
dropping that database then using pg_restore to restore the database and then happily just fire up slony again.

I tried this and it did not work.

The reason why I ask is that we got a few slave nodes distributed around the world and some nodes are on rather slow links mainly due to the geographic location. The database is around 40 gigs so just re-creating the node and letting it replicate across is just not possible the slave node will never catch up. replicating 40 gigs across to these nodes will take several days and that is not really an option.

we are trying to do 2 things.

1. upgrade to postgres 8.3.1 (currently on 8.2.4)
2. adding a new slave node in a remote location. (its ALOT faster to FedEx a harddrive with the dump and pg_restore it there then the full replicaiton would ever be)

but as i said I've had no luck using pg_dump and pg_restore to re-create a wiped slave node :(

what i've tried so far on a test system is:

1. replicate the database (1 Master 1 Slave)
2. Verify replication is working correctly
3. shutting down slony (for both slave and master, using slon_kill)
4. use "pg_dump -Z 9 -Fc -o db > /tmp/slonyDump.dmp" on the salve db (tried without the -o as well)
5. re-create the db and then load the dump it into it with "pg_restore -Fc -d db /tmp/slonyDump.dmp"

then i try to just start it again, with the slon_start for both master and slave)

but I get:

2008-05-23 18:35:30 EEST ERROR  remoteWorkerThread_1: "select "_db_cluster".sequenceSetValue(1,1,'581','32'); " PGRES_FATAL_ERROR ERROR:  Slony-I: sequenceSetValue(): sequence 1 not found

so maybe either my pg_dump or my pg_restore was not doing a complete job.

I'm open to try more or less anything so please if anyone got some ideas of a way to restore a database in a way that will make slony startup and not force a full replication?
From glynastill at yahoo.co.uk  Sun May 25 23:59:12 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Sun May 25 23:59:37 2008
Subject: [Slony1-general] pg_dump/pg_restore keeping all slony stuff
	intact?
Message-ID: <303230.26167.qm@web25803.mail.ukl.yahoo.com>

I looked into this briefly a few weeks ago.

From what I could see slony references the OIDs of individual objects, and since it's not possible to maintain object OIDs across a pg_dump and pg_restore the slony schema is in effect "shagged".

----- Original Message ----
> From: Martin Eriksson <M.Eriksson@albourne.com>
> To: slony1-general@lists.slony.info
> Sent: Monday, 26 May, 2008 7:25:34 AM
> Subject: [Slony1-general] pg_dump/pg_restore keeping all slony stuff intact?
> 
> Hi people,
> 
> I'm wondering if there is a way to shut down a slave slony node take a dump 
> using pg_dump of the database on that node
> dropping that database then using pg_restore to restore the database and then 
> happily just fire up slony again.
> 
> I tried this and it did not work.
> 
> The reason why I ask is that we got a few slave nodes distributed around the 
> world and some nodes are on rather slow links mainly due to the geographic 
> location. The database is around 40 gigs so just re-creating the node and 
> letting it replicate across is just not possible the slave node will never catch 
> up. replicating 40 gigs across to these nodes will take several days and that is 
> not really an option.
> 
> we are trying to do 2 things.
> 
> 1. upgrade to postgres 8.3.1 (currently on 8.2.4)
> 2. adding a new slave node in a remote location. (its ALOT faster to FedEx a 
> harddrive with the dump and pg_restore it there then the full replicaiton would 
> ever be)
> 
> but as i said I've had no luck using pg_dump and pg_restore to re-create a wiped 
> slave node :(
> 
> what i've tried so far on a test system is:
> 
> 1. replicate the database (1 Master 1 Slave)
> 2. Verify replication is working correctly
> 3. shutting down slony (for both slave and master, using slon_kill)
> 4. use "pg_dump -Z 9 -Fc -o db > /tmp/slonyDump.dmp" on the salve db (tried 
> without the -o as well)
> 5. re-create the db and then load the dump it into it with "pg_restore -Fc -d db 
> /tmp/slonyDump.dmp"
> 
> then i try to just start it again, with the slon_start for both master and 
> slave)
> 
> but I get:
> 
> 2008-05-23 18:35:30 EEST ERROR  remoteWorkerThread_1: "select 
> "_db_cluster".sequenceSetValue(1,1,'581','32'); " PGRES_FATAL_ERROR ERROR:  
> Slony-I: sequenceSetValue(): sequence 1 not found
> 
> so maybe either my pg_dump or my pg_restore was not doing a complete job.
> 
> I'm open to try more or less anything so please if anyone got some ideas of a 
> way to restore a database in a way that will make slony startup and not force a 
> full replication?
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general



      __________________________________________________________
Sent from Yahoo! Mail.
A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html
From M.Eriksson at albourne.com  Mon May 26 00:23:47 2008
From: M.Eriksson at albourne.com (Martin Eriksson)
Date: Mon May 26 00:24:17 2008
Subject: [Slony1-general] pg_dump/pg_restore keeping all slony stuff
	intact?
In-Reply-To: <303230.26167.qm@web25803.mail.ukl.yahoo.com>
Message-ID: <10987593.81211786623163.JavaMail.meriksso@martin-Ubuntu>

that doesn't sound to encouraging! 

maybe I'll try the following:

got Master node M, and Slave node S.

on the machine where i got node S i create a new postgres installation and start up a second postgres creating a new database on it and calling it N1.

start up a slony deamon for N1 saying that S is the master and since its all locally it goes pretty fast to replicate across all data to N1, then when all synced up I switch it over to to use M as the master instead of S and then cutting out S from the cluster leaving N1 in its place.

and setting up a new site in a similar way create a new postgres installation on a machine that already has a slave node, replicate it over same way as above then just copy the whole data directory onto a portable hard drive take it to the new location and just copy it in and fire it up as a new node (would require some restarts of the master node i guess to know of the added slave) but hopefully doable?


----- Original Message -----
From: "Glyn Astill" <glynastill@yahoo.co.uk>
To: "Martin Eriksson" <M.Eriksson@albourne.com>, slony1-general@lists.slony.info
Sent: Monday, May 26, 2008 9:59:12 AM GMT +02:00 Athens, Beirut, Bucharest, Istanbul
Subject: Re: [Slony1-general] pg_dump/pg_restore keeping all slony stuff intact?

I looked into this briefly a few weeks ago.

>From what I could see slony references the OIDs of individual objects, and since it's not possible to maintain object OIDs across a pg_dump and pg_restore the slony schema is in effect "shagged".

----- Original Message ----
> From: Martin Eriksson <M.Eriksson@albourne.com>
> To: slony1-general@lists.slony.info
> Sent: Monday, 26 May, 2008 7:25:34 AM
> Subject: [Slony1-general] pg_dump/pg_restore keeping all slony stuff intact?
> 
> Hi people,
> 
> I'm wondering if there is a way to shut down a slave slony node take a dump 
> using pg_dump of the database on that node
> dropping that database then using pg_restore to restore the database and then 
> happily just fire up slony again.
> 
> I tried this and it did not work.
> 
> The reason why I ask is that we got a few slave nodes distributed around the 
> world and some nodes are on rather slow links mainly due to the geographic 
> location. The database is around 40 gigs so just re-creating the node and 
> letting it replicate across is just not possible the slave node will never catch 
> up. replicating 40 gigs across to these nodes will take several days and that is 
> not really an option.
> 
> we are trying to do 2 things.
> 
> 1. upgrade to postgres 8.3.1 (currently on 8.2.4)
> 2. adding a new slave node in a remote location. (its ALOT faster to FedEx a 
> harddrive with the dump and pg_restore it there then the full replicaiton would 
> ever be)
> 
> but as i said I've had no luck using pg_dump and pg_restore to re-create a wiped 
> slave node :(
> 
> what i've tried so far on a test system is:
> 
> 1. replicate the database (1 Master 1 Slave)
> 2. Verify replication is working correctly
> 3. shutting down slony (for both slave and master, using slon_kill)
> 4. use "pg_dump -Z 9 -Fc -o db > /tmp/slonyDump.dmp" on the salve db (tried 
> without the -o as well)
> 5. re-create the db and then load the dump it into it with "pg_restore -Fc -d db 
> /tmp/slonyDump.dmp"
> 
> then i try to just start it again, with the slon_start for both master and 
> slave)
> 
> but I get:
> 
> 2008-05-23 18:35:30 EEST ERROR  remoteWorkerThread_1: "select 
> "_db_cluster".sequenceSetValue(1,1,'581','32'); " PGRES_FATAL_ERROR ERROR:  
> Slony-I: sequenceSetValue(): sequence 1 not found
> 
> so maybe either my pg_dump or my pg_restore was not doing a complete job.
> 
> I'm open to try more or less anything so please if anyone got some ideas of a 
> way to restore a database in a way that will make slony startup and not force a 
> full replication?
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general



      __________________________________________________________
Sent from Yahoo! Mail.
A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html
_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general
From glynastill at yahoo.co.uk  Mon May 26 03:00:19 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Mon May 26 03:00:48 2008
Subject: [Slony1-general] pg_dump/pg_restore keeping all slony stuff
	intact?
Message-ID: <423994.27411.qm@web25806.mail.ukl.yahoo.com>

I'm not sure that would work, if N1 was subscribed to S and S subscribed to M,  then the replication clusters would be separate wouldn't they?

Can you have downtime on the subscribers? Maybe you could take down S, create a new temporary S at the same site as M, replicate it, back up the data directory send it over to the the site where the real S is and restore the data and bring S back up.

----- Original Message ----
> From: Martin Eriksson <M.Eriksson@albourne.com>
> To: slony1-general@lists.slony.info
> Sent: Monday, 26 May, 2008 8:23:47 AM
> Subject: Re: [Slony1-general] pg_dump/pg_restore keeping all slony stuff intact?
> 
> that doesn't sound to encouraging! 
> 
> maybe I'll try the following:
> 
> got Master node M, and Slave node S.
> 
> on the machine where i got node S i create a new postgres installation and start 
> up a second postgres creating a new database on it and calling it N1.
> 
> start up a slony deamon for N1 saying that S is the master and since its all 
> locally it goes pretty fast to replicate across all data to N1, then when all 
> synced up I switch it over to to use M as the master instead of S and then 
> cutting out S from the cluster leaving N1 in its place.
> 
> and setting up a new site in a similar way create a new postgres installation on 
> a machine that already has a slave node, replicate it over same way as above 
> then just copy the whole data directory onto a portable hard drive take it to 
> the new location and just copy it in and fire it up as a new node (would require 
> some restarts of the master node i guess to know of the added slave) but 
> hopefully doable?
> 
> 
> ----- Original Message -----
> From: "Glyn Astill" 
> To: "Martin Eriksson" , slony1-general@lists.slony.info
> Sent: Monday, May 26, 2008 9:59:12 AM GMT +02:00 Athens, Beirut, Bucharest, 
> Istanbul
> Subject: Re: [Slony1-general] pg_dump/pg_restore keeping all slony stuff intact?
> 
> I looked into this briefly a few weeks ago.
> 
> >From what I could see slony references the OIDs of individual objects, and 
> since it's not possible to maintain object OIDs across a pg_dump and pg_restore 
> the slony schema is in effect "shagged".
> 
> ----- Original Message ----
> > From: Martin Eriksson 
> > To: slony1-general@lists.slony.info
> > Sent: Monday, 26 May, 2008 7:25:34 AM
> > Subject: [Slony1-general] pg_dump/pg_restore keeping all slony stuff intact?
> > 
> > Hi people,
> > 
> > I'm wondering if there is a way to shut down a slave slony node take a dump 
> > using pg_dump of the database on that node
> > dropping that database then using pg_restore to restore the database and then 
> > happily just fire up slony again.
> > 
> > I tried this and it did not work.
> > 
> > The reason why I ask is that we got a few slave nodes distributed around the 
> > world and some nodes are on rather slow links mainly due to the geographic 
> > location. The database is around 40 gigs so just re-creating the node and 
> > letting it replicate across is just not possible the slave node will never 
> catch 
> > up. replicating 40 gigs across to these nodes will take several days and that 
> is 
> > not really an option.
> > 
> > we are trying to do 2 things.
> > 
> > 1. upgrade to postgres 8.3.1 (currently on 8.2.4)
> > 2. adding a new slave node in a remote location. (its ALOT faster to FedEx a 
> > harddrive with the dump and pg_restore it there then the full replicaiton 
> would 
> > ever be)
> > 
> > but as i said I've had no luck using pg_dump and pg_restore to re-create a 
> wiped 
> > slave node :(
> > 
> > what i've tried so far on a test system is:
> > 
> > 1. replicate the database (1 Master 1 Slave)
> > 2. Verify replication is working correctly
> > 3. shutting down slony (for both slave and master, using slon_kill)
> > 4. use "pg_dump -Z 9 -Fc -o db > /tmp/slonyDump.dmp" on the salve db (tried 
> > without the -o as well)
> > 5. re-create the db and then load the dump it into it with "pg_restore -Fc -d 
> db 
> > /tmp/slonyDump.dmp"
> > 
> > then i try to just start it again, with the slon_start for both master and 
> > slave)
> > 
> > but I get:
> > 
> > 2008-05-23 18:35:30 EEST ERROR  remoteWorkerThread_1: "select 
> > "_db_cluster".sequenceSetValue(1,1,'581','32'); " PGRES_FATAL_ERROR ERROR:  
> > Slony-I: sequenceSetValue(): sequence 1 not found
> > 
> > so maybe either my pg_dump or my pg_restore was not doing a complete job.
> > 
> > I'm open to try more or less anything so please if anyone got some ideas of a 
> > way to restore a database in a way that will make slony startup and not force 
> a 
> > full replication?
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general@lists.slony.info
> > http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 
> 
>       __________________________________________________________
> Sent from Yahoo! Mail.
> A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general



      __________________________________________________________
Sent from Yahoo! Mail.
A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html
From M.Eriksson at albourne.com  Mon May 26 03:51:39 2008
From: M.Eriksson at albourne.com (Martin Eriksson)
Date: Mon May 26 03:52:15 2008
Subject: [Slony1-general] pg_dump/pg_restore keeping all slony stuff
	intact?
In-Reply-To: <6337683.161211798760594.JavaMail.meriksso@martin-Ubuntu>
Message-ID: <19651328.181211799096826.JavaMail.meriksso@martin-Ubuntu>

Ya i guess that is true :/

setting up a temp database next to the master, add a new temp node with 8.3.1 and replicate to it then copy the data directory onto some mobile device and take it to the other site install 8.3.1 and just dump it in the data directory.. switch over that node to the new one and let it catch up on the replication..

just that when you talk to people and you mention copying the data directory most posgres people frown big time.. and I agree its not exactly a "nice" way of doing it but sort of feels like the only way currently..

we can allow for some down time, slave node specific down time we could allow for up to 48h as other nodes can step in but it will be slower for users, and the main master we could have down for 4-6h if properly scheduled so at least we are not working under 0 downtime pressure, some downtime is allowed is managed properly.



----- Original Message -----
From: "Glyn Astill" <glynastill@yahoo.co.uk>
To: "Martin Eriksson" <M.Eriksson@albourne.com>, slony1-general@lists.slony.info
Sent: Monday, May 26, 2008 1:00:19 PM GMT +02:00 Athens, Beirut, Bucharest, Istanbul
Subject: Re: [Slony1-general] pg_dump/pg_restore keeping all slony stuff intact?

I'm not sure that would work, if N1 was subscribed to S and S subscribed to M,  then the replication clusters would be separate wouldn't they?

Can you have downtime on the subscribers? Maybe you could take down S, create a new temporary S at the same site as M, replicate it, back up the data directory send it over to the the site where the real S is and restore the data and bring S back up.

----- Original Message ----
> From: Martin Eriksson <M.Eriksson@albourne.com>
> To: slony1-general@lists.slony.info
> Sent: Monday, 26 May, 2008 8:23:47 AM
> Subject: Re: [Slony1-general] pg_dump/pg_restore keeping all slony stuff intact?
> 
> that doesn't sound to encouraging! 
> 
> maybe I'll try the following:
> 
> got Master node M, and Slave node S.
> 
> on the machine where i got node S i create a new postgres installation and start 
> up a second postgres creating a new database on it and calling it N1.
> 
> start up a slony deamon for N1 saying that S is the master and since its all 
> locally it goes pretty fast to replicate across all data to N1, then when all 
> synced up I switch it over to to use M as the master instead of S and then 
> cutting out S from the cluster leaving N1 in its place.
> 
> and setting up a new site in a similar way create a new postgres installation on 
> a machine that already has a slave node, replicate it over same way as above 
> then just copy the whole data directory onto a portable hard drive take it to 
> the new location and just copy it in and fire it up as a new node (would require 
> some restarts of the master node i guess to know of the added slave) but 
> hopefully doable?
> 
> 
> ----- Original Message -----
> From: "Glyn Astill" 
> To: "Martin Eriksson" , slony1-general@lists.slony.info
> Sent: Monday, May 26, 2008 9:59:12 AM GMT +02:00 Athens, Beirut, Bucharest, 
> Istanbul
> Subject: Re: [Slony1-general] pg_dump/pg_restore keeping all slony stuff intact?
> 
> I looked into this briefly a few weeks ago.
> 
> >From what I could see slony references the OIDs of individual objects, and 
> since it's not possible to maintain object OIDs across a pg_dump and pg_restore 
> the slony schema is in effect "shagged".
> 
> ----- Original Message ----
> > From: Martin Eriksson 
> > To: slony1-general@lists.slony.info
> > Sent: Monday, 26 May, 2008 7:25:34 AM
> > Subject: [Slony1-general] pg_dump/pg_restore keeping all slony stuff intact?
> > 
> > Hi people,
> > 
> > I'm wondering if there is a way to shut down a slave slony node take a dump 
> > using pg_dump of the database on that node
> > dropping that database then using pg_restore to restore the database and then 
> > happily just fire up slony again.
> > 
> > I tried this and it did not work.
> > 
> > The reason why I ask is that we got a few slave nodes distributed around the 
> > world and some nodes are on rather slow links mainly due to the geographic 
> > location. The database is around 40 gigs so just re-creating the node and 
> > letting it replicate across is just not possible the slave node will never 
> catch 
> > up. replicating 40 gigs across to these nodes will take several days and that 
> is 
> > not really an option.
> > 
> > we are trying to do 2 things.
> > 
> > 1. upgrade to postgres 8.3.1 (currently on 8.2.4)
> > 2. adding a new slave node in a remote location. (its ALOT faster to FedEx a 
> > harddrive with the dump and pg_restore it there then the full replicaiton 
> would 
> > ever be)
> > 
> > but as i said I've had no luck using pg_dump and pg_restore to re-create a 
> wiped 
> > slave node :(
> > 
> > what i've tried so far on a test system is:
> > 
> > 1. replicate the database (1 Master 1 Slave)
> > 2. Verify replication is working correctly
> > 3. shutting down slony (for both slave and master, using slon_kill)
> > 4. use "pg_dump -Z 9 -Fc -o db > /tmp/slonyDump.dmp" on the salve db (tried 
> > without the -o as well)
> > 5. re-create the db and then load the dump it into it with "pg_restore -Fc -d 
> db 
> > /tmp/slonyDump.dmp"
> > 
> > then i try to just start it again, with the slon_start for both master and 
> > slave)
> > 
> > but I get:
> > 
> > 2008-05-23 18:35:30 EEST ERROR  remoteWorkerThread_1: "select 
> > "_db_cluster".sequenceSetValue(1,1,'581','32'); " PGRES_FATAL_ERROR ERROR:  
> > Slony-I: sequenceSetValue(): sequence 1 not found
> > 
> > so maybe either my pg_dump or my pg_restore was not doing a complete job.
> > 
> > I'm open to try more or less anything so please if anyone got some ideas of a 
> > way to restore a database in a way that will make slony startup and not force 
> a 
> > full replication?
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general@lists.slony.info
> > http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 
> 
>       __________________________________________________________
> Sent from Yahoo! Mail.
> A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general



      __________________________________________________________
Sent from Yahoo! Mail.
A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html
_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general
From rafael.domiciano at gmail.com  Mon May 26 05:02:51 2008
From: rafael.domiciano at gmail.com (Rafael Domiciano)
Date: Mon May 26 05:03:21 2008
Subject: [Slony1-general] Slony Vacuuming DB
In-Reply-To: <48359E70.5000404@quietcaresystems.com>
References: <3a0028490805211214k7f9911dcv4af3babe73ced309@mail.gmail.com>
	<60zlqiqqnd.fsf@dba2.int.libertyrms.com>
	<48359E70.5000404@quietcaresystems.com>
Message-ID: <3a0028490805260502k67b5598oaf2eff39748d9e60@mail.gmail.com>

Bill,
Sorry about that, I didn't see that the answer was just for you.

Bill,
You said that VACUUM FULLs are not an effective method of ongoing
maintenance. But VACUUM FULL is used by more than 5 years, so VACUUM is a
culture in the company. The DBA function is new in the company, and I have
been reading everything I find about Postgres, DB, Slony, Maintenance, and
so on.
I would like to know if you could pass me a forum or a list of messages on
Postgres or maintenance in DB.
If you can, I would be very grateful.

People,
I'll give a new search in the documentation to find something about slony's
vacuum and how to stop that.
The problem is that I have an external application, and when the Slony's
vacuum is running the application begins to slow down and give time-out.
I think necessary to run the vacuum when it asks, but while the company
decides not replace the hardware, I have to do that anyway. However, they
are warned that this could affect the replication.

If someone discover how to do that I thank you very much.

I thank all.
Rafael Domiciano


2008/5/22 salman <salmanb@quietcaresystems.com>:

>
>
> Christopher Browne wrote:
>
>> "Rafael Domiciano" <rafael.domiciano@gmail.com> writes:
>>
>>> Frequently Slony is vacuuming my database, and I'm having to kill his
>>> proccess because it's slowing the client-side application.
>>> I tryied to find option that disables the vacuum but i'm unsucessful.
>>> Someone know how to stop vacuum's Slony?
>>>
>>
>>
> Not sure what kind of a shop you run and how many inserts/updates/deletes
> you do during a day -- we do about 5.5M inserts a day, and we archive ~3-=
4M
> records a day (dumped to disk, removed from production db) and the slony
> vacuum jobs do not slow anything down for us.
>
> The only time we have slowdown is when one particular script leaves an
> 'idle in transaction' query open for a number of hours -- once that happe=
ns,
> slony's fetch requests and the growing log table really drag the server
> down. But, the problem isn't created by slony here, obviously, it's one of
> our scripts.
>
> -salman
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080526/=
d1dd6aee/attachment.htm
From wmoran at collaborativefusion.com  Mon May 26 05:57:28 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Mon May 26 05:57:34 2008
Subject: [Slony1-general] Slony Vacuuming DB
In-Reply-To: <3a0028490805260502k67b5598oaf2eff39748d9e60@mail.gmail.com>
References: <3a0028490805211214k7f9911dcv4af3babe73ced309@mail.gmail.com>
	<60zlqiqqnd.fsf@dba2.int.libertyrms.com>
	<48359E70.5000404@quietcaresystems.com>
	<3a0028490805260502k67b5598oaf2eff39748d9e60@mail.gmail.com>
Message-ID: <20080526085728.d010b569.wmoran@collaborativefusion.com>

"Rafael Domiciano" <rafael.domiciano@gmail.com> wrote:
> 
> Bill,
> You said that VACUUM FULLs are not an effective method of ongoing
> maintenance. But VACUUM FULL is used by more than 5 years, so VACUUM is a
> culture in the company. The DBA function is new in the company, and I have
> been reading everything I find about Postgres, DB, Slony, Maintenance, and
> so on.
> I would like to know if you could pass me a forum or a list of messages on
> Postgres or maintenance in DB.
> If you can, I would be very grateful.

http://www.postgresql.org/docs/8.3/static/routine-vacuuming.html

> People,
> I'll give a new search in the documentation to find something about slony's
> vacuum and how to stop that.
> The problem is that I have an external application, and when the Slony's
> vacuum is running the application begins to slow down and give time-out.
> I think necessary to run the vacuum when it asks, but while the company
> decides not replace the hardware, I have to do that anyway. However, they
> are warned that this could affect the replication.

You're still going down the wrong road.  While it's possible that your
hardware is inadequate for the job, it's also possible that the same person
who thought VACUUM FULL was a good idea also thinks lots of other incorrect
things.  Before trying to redesign Slony and operating counter to the
advice of the very smart people who designed it, I would do a basic audit
of your PostgreSQL config settings and tune the server.  It's quite possible
that a little bit of postgresql.conf tuning will bring this thing back up
to par.  See:
http://www.postgresql.org/docs/8.3/static/admin.html

You may also want to join the pgsql-perform mailing list.

I'd be willing to bet that if you disable Slony's vacuum, you'll
make the situation worse.  Please try it, I'd love to hear your results.

-- 
Bill Moran
Collaborative Fusion Inc.

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From cbbrowne at ca.afilias.info  Mon May 26 07:46:51 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon May 26 07:46:58 2008
Subject: [Slony1-general] auto vac output: Page Slots
In-Reply-To: <8a547c840805241116r200e4bd1we74f57545090fe51@mail.gmail.com>
	(Tory M. Blue's message of "Sat, 24 May 2008 11:16:56 -0700")
References: <8a547c840805241116r200e4bd1we74f57545090fe51@mail.gmail.com>
Message-ID: <60d4n9f7hg.fsf@dba2.int.libertyrms.com>

"Tory M Blue" <tmblue@gmail.com> writes:
> Good morning,
>
> I have a question re some slon output that I'm seeing.
>
> I'm wondering if my configuration needs tweaking if I'm seeing these
> kind of numbers during/after an autovac
>
> DETAIL:  A total of 227168 page slots are in use (including overhead).
> 227168 page slots are required to track all free space.
> Current limits are:  1087500 page slots, 430 relations, using 6401 kB.
> VACUUM
>
> Does the higher than needed page slots affect anything, performance,
> resources etc? Is it something I should tune for or, no?

This isn't terribly Slony-I-related, but that's fine...

Having more page slots than needed does cost you some memory; 6 bytes
per page.  If that space isn't in use, it wastes you 6 bytes per
unused page.

Sounds like you're wasting ~5MB of memory, which doesn't sound too
serious.
-- 
(reverse (concatenate 'string "moc.enworbbc" "@" "enworbbc"))
http://linuxfinances.info/info/internet.html
Do not worry  about the bullet that  has got your name on  it. It will
hit you and it will kill  you, no questions asked. The rounds to worry
about are the ones marked: TO WHOM IT MAY CONCERN.
From rafael.domiciano at gmail.com  Mon May 26 10:54:41 2008
From: rafael.domiciano at gmail.com (Rafael Domiciano)
Date: Mon May 26 10:54:53 2008
Subject: [Slony1-general] "Cannot get node list"
Message-ID: <3a0028490805261054y16b11931n1af76ef1a0534bfc@mail.gmail.com>

Hello People,

I have a serious problem.
The Slony began to make the error below, I tried to restart it to see if
returned but continued to give the same thing.
I tried in several places, but found nothing. Even talking to my Network
Administrator forming in Linux to see if it would be a problem in the
machine, but he told me what could be a problem in the DB.

I tried the following:
* Database restart in the slave
* Recreated table sl_node in the master
* Vacuumed and reindexed tables sl_node and sl_confirm (Master / Slave)
* Reset the Postgres service in the slave

SQL that the Slony does and has given the problem is (I get it in the src):

        /*
         * Read configuration table sl_node
         */
        dstring_init(&query);
        slon_mkquery(&query,
                                 "select no_id, no_active, no_comment, "
                                 "    (select coalesce(max(con_seqno),0)
from %s.sl_confirm "
                                 "        where con_origin =3D no_id and
con_received =3D %d) "
                                 "        as last_event "
                                 "from %s.sl_node "
                                 "order by no_id; ",
                                 rtcfg_namespace, rtcfg_nodeid,
rtcfg_namespace);
        res =3D PQexec(startup_conn, dstring_data(&query));
        if (PQresultStatus(res) !=3D PGRES_TUPLES_OK)
        {
                slon_log(SLON_FATAL, "main: Cannot get node list - %s\n",
                                 PQresultErrorMessage(res));
                PQclear(res);
                dstring_free(&query);
                slon_retry();
        }

2008-05-26 14:45:43 BRT DEBUG2 slon_retry() from pid=3D23678
2008-05-26 14:45:43 BRT DEBUG1 slon: retry requested
2008-05-26 14:45:43 BRT DEBUG2 slon: notify worker process to shutdown
2008-05-26 14:45:43 BRT DEBUG2 slon: child terminated status: 0; pid: 23678,
current worker pid: 23678
2008-05-26 14:45:43 BRT DEBUG1 slon: restart of worker
2008-05-26 14:45:43 BRT CONFIG main: slon version 1.2.8 starting up
2008-05-26 14:45:43 BRT DEBUG2 slon: watchdog process started
2008-05-26 14:45:43 BRT DEBUG2 slon: watchdog ready - pid =3D 23464
2008-05-26 14:45:43 BRT DEBUG2 slon: worker process created - pid =3D 23681
2008-05-26 14:45:44 BRT CONFIG main: local node id =3D 2
2008-05-26 14:45:44 BRT DEBUG2 main: main process started
2008-05-26 14:45:44 BRT CONFIG main: launching sched_start_mainloop
2008-05-26 14:45:44 BRT CONFIG main: loading current cluster configuration

2008-05-26 14:45:44 BRT FATAL  main: Cannot get node list - ERROR: cannot
open segment 1 from relation 1663/242628616/279475826 (target block
250120132): File or directory not found

2008-05-26 14:45:44 BRT DEBUG2 slon_retry() from pid=3D23681
2008-05-26 14:45:44 BRT DEBUG1 slon: retry requested
2008-05-26 14:45:44 BRT DEBUG2 slon: notify worker process to shutdown
2008-05-26 14:45:44 BRT DEBUG2 slon: child terminated status: 0; pid: 23681,
current worker pid: 23681
2008-05-26 14:45:44 BRT DEBUG1 slon: restart of worker
2008-05-26 14:45:44 BRT CONFIG main: slon version 1.2.8 starting up
2008-05-26 14:45:44 BRT DEBUG2 slon: watchdog process started
2008-05-26 14:45:44 BRT DEBUG2 slon: watchdog ready - pid =3D 23464
2008-05-26 14:45:44 BRT DEBUG2 slon: worker process created - pid =3D 23685

If anyone has any ideas, I thank

Tnks
Rafael Domiciano
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080526/=
0ecfb80c/attachment.htm
From ajs at crankycanuck.ca  Mon May 26 11:19:05 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon May 26 11:19:36 2008
Subject: [Slony1-general] Slony Vacuuming DB
In-Reply-To: <3a0028490805260502k67b5598oaf2eff39748d9e60@mail.gmail.com>
References: <3a0028490805211214k7f9911dcv4af3babe73ced309@mail.gmail.com>
	<60zlqiqqnd.fsf@dba2.int.libertyrms.com>
	<48359E70.5000404@quietcaresystems.com>
	<3a0028490805260502k67b5598oaf2eff39748d9e60@mail.gmail.com>
Message-ID: <20080526181905.GB14857@crankycanuck.ca>

On Mon, May 26, 2008 at 09:02:51AM -0300, Rafael Domiciano wrote:

> maintenance. But VACUUM FULL is used by more than 5 years, so VACUUM is a
> culture in the company. 

VACUUM is good.  VACUUM FULL is bad.  Even if they've been using it more
than 5 years, it's a bad idea.  (There are plenty of things that people did
for long periods of time that we no longer think are a good idea.  Some of
these are big -- selling people into slavery, say -- and some are small --
running open SMTP relays.  The thing is that the world changes, and saying
that you've done something a long time is not a good argument on its own.)

> I would like to know if you could pass me a forum or a list of messages on
> Postgres or maintenance in DB.

The manual has a long section on this.

> I'll give a new search in the documentation to find something about slony's
> vacuum and how to stop that.

Don't.  It's a bad idea.

> The problem is that I have an external application, and when the Slony's
> vacuum is running the application begins to slow down and give time-out.

I suspect what's happening is that you are running up against your
hardware's limitations.  Try collecting iostat during the period.  You will
likely find that you are at or near 100% of your disk's transfer capacity. 
That's what you need to fix.  You will cause worse problems if you don't
allow Slony to do its vacuuming on its own schedule.

A
From cbbrowne at ca.afilias.info  Mon May 26 12:15:02 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon May 26 12:17:09 2008
Subject: [Slony1-general] Slony Vacuuming DB
In-Reply-To: <3a0028490805260502k67b5598oaf2eff39748d9e60@mail.gmail.com>
	(Rafael Domiciano's message of "Mon, 26 May 2008 09:02:51 -0300")
References: <3a0028490805211214k7f9911dcv4af3babe73ced309@mail.gmail.com>
	<60zlqiqqnd.fsf@dba2.int.libertyrms.com>
	<48359E70.5000404@quietcaresystems.com>
	<3a0028490805260502k67b5598oaf2eff39748d9e60@mail.gmail.com>
Message-ID: <608wxwg9mx.fsf@dba2.int.libertyrms.com>

"Rafael Domiciano" <rafael.domiciano@gmail.com> writes:
> You said that VACUUM FULLs are not an effective method of ongoing
> maintenance. But VACUUM FULL is used by more than 5 years, so VACUUM
> is a culture in the company. The DBA function is new in the company,
> and I have been reading everything I find about Postgres, DB, Slony,
> Maintenance, and so on.

Cultures get addicted to all manner of traditions, but times change,
and sometimes, the nature of technology turns what was once a fine
idea into a very bad one.

"VACUUM FULL" is an example of something that used to be fine and
necessary that is now a Really Terrible tradition to continue.

If you try to continue this tradition, in the context of Slony-I, you
will find your system performance worsening, and find that replication
is not working as it should.

This is not a case where it is quaint and cute to keep to the old
traditions - this is a case where you will cause plenty of unnecessary
pain by trying to ignore five years worth of development effort that
has taken place.

> I would like to know if you could pass me a forum or a list of
> messages on Postgres or maintenance in DB.  If you can, I would be
> very grateful.

Look to the documentation:
   http://www.postgresql.org/docs/8.3/static/maintenance.html
-- 
output = ("cbbrowne" "@" "linuxdatabases.info")
http://cbbrowne.com/info/sgml.html
It isn't that  physicists enjoy physics  more than they enjoy sex, its
that they enjoy sex more when they are thinking of physics.
From ajs at crankycanuck.ca  Mon May 26 12:46:45 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon May 26 12:47:18 2008
Subject: [Slony1-general] "Cannot get node list"
In-Reply-To: <3a0028490805261054y16b11931n1af76ef1a0534bfc@mail.gmail.com>
References: <3a0028490805261054y16b11931n1af76ef1a0534bfc@mail.gmail.com>
Message-ID: <20080526194645.GC14857@crankycanuck.ca>

On Mon, May 26, 2008 at 02:54:41PM -0300, Rafael Domiciano wrote:
> 
> 2008-05-26 14:45:44 BRT FATAL  main: Cannot get node list - ERROR: cannot
> open segment 1 from relation 1663/242628616/279475826 (target block
> 250120132): File or directory not found

That looks like a postgres error.  It's trying to open a particular block,
and can't.  You have a database problem.

A

From tmblue at gmail.com  Mon May 26 13:02:14 2008
From: tmblue at gmail.com (Tory M Blue)
Date: Mon May 26 13:02:26 2008
Subject: [Slony1-general] auto vac output: Page Slots
In-Reply-To: <60d4n9f7hg.fsf@dba2.int.libertyrms.com>
References: <8a547c840805241116r200e4bd1we74f57545090fe51@mail.gmail.com>
	<60d4n9f7hg.fsf@dba2.int.libertyrms.com>
Message-ID: <8a547c840805261302y1b89a93u3543278d536c0549@mail.gmail.com>

> This isn't terribly Slony-I-related, but that's fine...
>
> Having more page slots than needed does cost you some memory; 6 bytes
> per page.  If that space isn't in use, it wastes you 6 bytes per
> unused page.
>
> Sounds like you're wasting ~5MB of memory, which doesn't sound too
> serious.
> --
> (reverse (concatenate 'string "moc.enworbbc" "@" "enworbbc"))
> http://linuxfinances.info/info/internet.html
> Do not worry  about the bullet that  has got your name on  it. It will
> hit you and it will kill  you, no questions asked. The rounds to worry
> about are the ones marked: TO WHOM IT MAY CONCERN.
>


Thanks Chris, ya the last 2 are suspect at best for being on the slon
list. This one for sure and didn't even think about it until I hit the
send button on my second email. I'll be a bit more careful and get my
slon and postgres centric questions routed correctly :) Thanks

Tory
From cbbrowne at ca.afilias.info  Mon May 26 14:42:53 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon May 26 14:43:07 2008
Subject: [Slony1-general] Removing Listen/Notify Usage for events
Message-ID: <60r6boeo82.fsf@dba2.int.libertyrms.com>

I have removed code that generates + listens to LISTEN/NOTIFY events
for Events + Confirms.

remote_listen.c now functions solely via polling, which we already
knew worked fine, as we have had a polling mode for quite a while in
the 2.0 (e.g. - CVS HEAD) code tree.

http://lists.slony.info/pipermail/slony1-commit/2008-May/002288.html
http://lists.slony.info/pipermail/slony1-commit/2008-May/002289.html

The slon _Restart LISTEN/NOTIFY handling has been left alone, as the
effects are quite independent.  Making sure that *that* is handled
rightly will be left as a further exercise to be handled separately.

Note that we *do* need to revise that, probably to remove the
_Restart; in PostgreSQL 8.4, there will no longer be a pg_listener
table, with the result that some of the usages surrounding the "slon
restart" notification type will break.  

Note Andrew Dunstan's presentation at PGCon:
   <http://www.pgcon.org/2008/schedule/events/107.en.html>
-- 
(reverse (concatenate 'string "gro.mca" "@" "enworbbc"))
http://cbbrowne.com/info/sgml.html
"... the open research model is justified. There is a passage in the
Bible (John 8:32, and on a plaque in CIA HQ), "And ye shall know the
truth, and the truth shall set ye free." -- Dave Dittrich 
From vivek at khera.org  Tue May 27 07:26:32 2008
From: vivek at khera.org (Vivek Khera)
Date: Tue May 27 07:26:38 2008
Subject: [Slony1-general] Slow replication issue
In-Reply-To: <200805231805.m4NI5Mrj032900@jupiter.jannicash.info>
References: <200805231805.m4NI5Mrj032900@jupiter.jannicash.info>
Message-ID: <2724D707-547C-4E53-9580-8D4EFB975A65@khera.org>


On May 23, 2008, at 2:05 PM, Jan Wieck wrote:

> The slony connections are all regular libpq database connections. So  
> you might test this by using psql or pg_dump running on one of those  
> subscribers, connecting to the appropriate data provider. If that  
> can utilize more bandwidth, then the problem lies within the replica  
> itself and something else must be limiting it from reading from the  
> network faster.

Every time I dig into why our replication is lagging severely (more  
than 10-15 mintues) I find that I'm spending a lot of time inside  
"FETCH 100" queries, and then a lot of time between them, as well (ie,  
applying the updates to the replica).  It feels as of pg isn't running  
at full speed, but I don't have the numbers to prove it.

From vivek at khera.org  Tue May 27 07:28:12 2008
From: vivek at khera.org (Vivek Khera)
Date: Tue May 27 07:28:18 2008
Subject: [Slony1-general] Removing Listen/Notify Usage for events
In-Reply-To: <60r6boeo82.fsf@dba2.int.libertyrms.com>
References: <60r6boeo82.fsf@dba2.int.libertyrms.com>
Message-ID: <AD894662-CB33-45E0-B6B6-E619362627A1@khera.org>


On May 26, 2008, at 5:42 PM, Christopher Browne wrote:

> I have removed code that generates + listens to LISTEN/NOTIFY events
> for Events + Confirms.
>
> remote_listen.c now functions solely via polling, which we already
> knew worked fine, as we have had a polling mode for quite a while in
> the 2.0 (e.g. - CVS HEAD) code tree.

This won't affect the 1.x branch, right?  No major changes should  
happen there.
From cbbrowne at ca.afilias.info  Tue May 27 07:50:47 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue May 27 07:50:54 2008
Subject: [Slony1-general] Removing Listen/Notify Usage for events
In-Reply-To: <AD894662-CB33-45E0-B6B6-E619362627A1@khera.org> (Vivek Khera's
	message of "Tue, 27 May 2008 10:28:12 -0400")
References: <60r6boeo82.fsf@dba2.int.libertyrms.com>
	<AD894662-CB33-45E0-B6B6-E619362627A1@khera.org>
Message-ID: <60mymber7c.fsf@dba2.int.libertyrms.com>

Vivek Khera <vivek@khera.org> writes:
> On May 26, 2008, at 5:42 PM, Christopher Browne wrote:
>
>> I have removed code that generates + listens to LISTEN/NOTIFY events
>> for Events + Confirms.
>>
>> remote_listen.c now functions solely via polling, which we already
>> knew worked fine, as we have had a polling mode for quite a while in
>> the 2.0 (e.g. - CVS HEAD) code tree.
>
> This won't affect the 1.x branch, right?  No major changes should
> happen there.

Correct.  This is only in -HEAD.

It is worth observing that the relevant forthcoming change in 8.4
(e.g. - that pg_listener goes away) means that Slony-I 1.1 and 1.2
won't be compatible with 8.4 without repeating this rework with them.

I don't think I care at all about 1.1 not being compatible; I am a bit
more concerned about 1.2...
-- 
select 'cbbrowne' || '@' || 'acm.org';
http://cbbrowne.com/info/nonrdbms.html
Rules of the Evil Overlord #58.  "If it becomes necessary to escape, I
will  never stop  to  pose  dramatically and  toss  off a  one-liner."
<http://www.eviloverlord.com/>
From greg at endpoint.com  Wed May 28 09:03:48 2008
From: greg at endpoint.com (Greg Sabino Mullane)
Date: Wed May 28 09:05:00 2008
Subject: [Slony1-general] Slon Bloat or question re master and slave
	reported bloat
In-Reply-To: <8a547c840805241126rca59df4v217b14adf3d9c66c@mail.gmail.com>
References: <8a547c840805241126rca59df4v217b14adf3d9c66c@mail.gmail.com>
Message-ID: <20080528120348.66714f81@greg-laptop>


> Question is why there seems to be such a difference in size between
> the master and the slave.
...
> cls.listings rows:1608406 pages:335434 shouldbe:222939 (1.5X) wasted
...
> cls.listings rows:1599038 pages:292044 shouldbe:222460 (1.3X) wasted
...
> Yes I checked the autovacs and they ran 2 hours apart but during a
> time of low traffic, this would not explain the pretty significant
> difference in wasted space.. I'm wondering where I should start to
> look and or if I should waste any effort on tracking this down?

Try ANALYZING both right before running the check, to get the best row and
pages estimates. At the very least, the number of rows should match much
closer than they are. It may just be that the vacuum on the slave side did
not run as recently as the master, or could not completely remove all dead
tuples. Compare ANALYZE VERBOSE on both sides for a closer look at the
differences.

-- 
Greg Sabino Mullane greg@endpoint.com
End Point Corporation
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 163 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080528/27fc6091/signature.pgp
From skelton at apple.com  Wed May 28 14:34:29 2008
From: skelton at apple.com (Bruce Skelton)
Date: Wed May 28 14:34:44 2008
Subject: [Slony1-general] logswitch_finish() still in progress
Message-ID: <C2C35FDD-F0F3-449A-A968-B25A9E6C0CB3@apple.com>

Over the last 30 hours there have been a tremendous amount of changes  
against my db. This morning there are 5.6 million entries in sl_log_1.  
Over time the fetch log statements are  getting longer in duration.

Is there any hope the log switch/truncate will complete?

LOG:  00000: duration: 7768.196 ms  statement: fetch 100 from LOG;
LOCATION:  exec_simple_query, postgres.c:1015
LOG:  00000: duration: 7775.775 ms  statement: fetch 100 from LOG;
LOCATION:  exec_simple_query, postgres.c:1015
LOG:  00000: duration: 7764.928 ms  statement: fetch 100 from LOG;
LOCATION:  exec_simple_query, postgres.c:1015


STATEMENT:  delete from "_mycluster_procluster".sl_log_1 where  
log_origin = '1' and log_xid < '252358837'; delete from  
"_mycluster_procluster".sl_log_2 where log_origin = '1' and log_xid <  
'252358837'; delete from "_mycluster_procluster".sl_seqlog where  
seql_origin = '1' and seql_ev_seqno < '647367'; select  
"_mycluster_procluster".logswitch_finish();
NOTICE:  00000: Slony-I: log switch to sl_log_2 still in progress -  
sl_log_1 not truncated
LOCATION:  exec_stmt_raise, pl_exec.c:2149
STATEMENT:  delete from "_mycluster_procluster".sl_log_1 where  
log_origin = '2' and log_xid < '220973324'; delete from  
"_mycluster_procluster".sl_log_2 where log_origin = '2' and log_xid <  
'220973324'; delete from "_mycluster_procluster".sl_seqlog where  
seql_origin = '2' and seql_ev_seqno < '605163'; select  
"_mycluster_procluster".logswitch_finish();

Thanks in advance,

bruce
From slony at mbreslow.net  Wed May 28 19:28:28 2008
From: slony at mbreslow.net (Marc)
Date: Wed May 28 19:28:49 2008
Subject: [Slony1-general] sl_status view not working for me
Message-ID: <809128960805281928j636b2bc7g9f30d8f26855cb47@mail.gmail.com>

Using slony 1.2.14 on pgsql 8.3.1

Replication is working.  I don't see any errors in the log files and changes
made to the master are indeed propegated to the slave.

What isn't working is

   select * from _mycluster.sl_status

I've narrowed the issue down to sl_confirm containing:
con_origin,con_received,con_seqno,con_timestamp
1;2;0;"2008-05-28 18:49:30.947064"
2;1;0;"2008-05-28 18:49:30.947497"

con_seqno is 0 for both rows and this is joined to ev_seqno in sl_event
which is an int between 1 and 1266.

Any thoughts on what could be wrong with my cluster?

Thanks,
---Marc
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080528/=
67ec26a4/attachment.htm
From m.eriksson at albourne.com  Thu May 29 04:14:19 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Thu May 29 04:15:00 2008
Subject: [Slony1-general] 3 postgres instances and 3 slony on same machine,
	having some issues
Message-ID: <483E900B.9060701@albourne.com>

Hi,

I got a little test system where i got 2 postgres servers running, 
master = 8.2.4, slave1 = 8.2.4

both goth their own instance of slony as well (1.2.14)
and replication works fine between them

So i intended to try a migration to 8.3.1. where i add the 8.3.1 as new 
node to the master let it replicate completely then uninstall the old 
slave1 node.
of course each database run on a different port.

so i created a 3rd postgres installation of 8.3.1, and also installed 
slony 1.2.14 on that one.
create the db and schema.
add the node info in the config file then try to issue

slonik_store_node --config /tmp/slon_Test.conf 3 | /data/pgsql/bin/slonik
(this point to the 8.2.4 installation of the slonik_store_node)

but i get:
<stdin>:7: PGRES_FATAL_ERROR load '$libdir/xxid';  - ERROR:  
incompatible library "/data/pgsql-8.3.1/lib/xxid.so": version mismatch
DETAIL:  Server is version 8.3, library is version 8.2.
<stdin>:7: Error: the extension for the xxid data type cannot be loaded 
in database 'host=t1 dbname=slonyT user=u1 port=5434'

same if i try to use the slonik from the 8.3.1 slony installation..


I know its not exactly ideal to be running this much one one machine.. 
but if anyone got some suggestions it would be most appreciated.

or maybe i shouldn't add the 8.3.1 node in this way?





From m.eriksson at albourne.com  Thu May 29 05:41:13 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Thu May 29 05:41:19 2008
Subject: [Slony1-general] 3 postgres instances and 3 slony on same machine,
	having some issues
In-Reply-To: <483E93A2.7060808@postgresqlfr.org>
References: <483E900B.9060701@albourne.com> <483E93A2.7060808@postgresqlfr.org>
Message-ID: <483EA469.2010108@albourne.com>

St?phane A. Schildknecht wrote:
> Martin Eriksson a ?crit :
>   
>> Hi,
>>
>> I got a little test system where i got 2 postgres servers running,
>> master = 8.2.4, slave1 = 8.2.4
>>
>> both goth their own instance of slony as well (1.2.14)
>> and replication works fine between them
>>
>> So i intended to try a migration to 8.3.1. where i add the 8.3.1 as new
>> node to the master let it replicate completely then uninstall the old
>> slave1 node.
>> of course each database run on a different port.
>>
>> so i created a 3rd postgres installation of 8.3.1, and also installed
>> slony 1.2.14 on that one.
>> create the db and schema.
>> add the node info in the config file then try to issue
>>
>> slonik_store_node --config /tmp/slon_Test.conf 3 | /data/pgsql/bin/slonik
>> (this point to the 8.2.4 installation of the slonik_store_node)
>>
>> but i get:
>> <stdin>:7: PGRES_FATAL_ERROR load '$libdir/xxid';  - ERROR: 
>> incompatible library "/data/pgsql-8.3.1/lib/xxid.so": version mismatch
>> DETAIL:  Server is version 8.3, library is version 8.2.
>> <stdin>:7: Error: the extension for the xxid data type cannot be loaded
>> in database 'host=t1 dbname=slonyT user=u1 port=5434'
>>
>> same if i try to use the slonik from the 8.3.1 slony installation..
>>
>>
>> I know its not exactly ideal to be running this much one one machine..
>> but if anyone got some suggestions it would be most appreciated.
>>
>> or maybe i shouldn't add the 8.3.1 node in this way?
>>     
>
> It seems to me slony for 8.3.1 has been compiled using information coming from
> pg_config installed by 8.2. Am I wrong ?
>
> Regards,
>   
well i built it the following way:
while in the slony1-1.2.14 source folder:
./configure --prefix=/data/pgsql-8.3.1 
--with-perltools=/data/pgsql-8.3.1/slony 
--with-pgconfigdir=/data/pgsql-8.3.1/bin

make && make install

which feels like it should have used the 8.3.1 postgres correctly...

but maybe im missing something....
because obviously it doesn't like the lib....

the slony for the other two 2.8.4 dbs i built the same way but changing 
to the respective paths.

basically i built first for the master which is using the standard 
/data/pgsql/ then second one using /data/pgsql-8.2.4-test/ and last 
/data/pgsql-8.3.1/

started up slon_start for master and then node 2.
added the node 3 (8.3) in the config file, then tried the 
slonik_store_node and got the above message..


From stephane.schildknecht at postgresqlfr.org  Thu May 29 05:49:38 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Thu May 29 05:49:47 2008
Subject: [Slony1-general] 3 postgres instances and 3 slony on same machine,
	having some issues
In-Reply-To: <483EA469.2010108@albourne.com>
References: <483E900B.9060701@albourne.com> <483E93A2.7060808@postgresqlfr.org>
	<483EA469.2010108@albourne.com>
Message-ID: <483EA662.3010600@postgresqlfr.org>

Martin Eriksson a ?crit :
>>   
> well i built it the following way:
> while in the slony1-1.2.14 source folder:
> ./configure --prefix=/data/pgsql-8.3.1
> --with-perltools=/data/pgsql-8.3.1/slony
> --with-pgconfigdir=/data/pgsql-8.3.1/bin

What did you get for lines beginning  with "pg_config says..." ?

From m.eriksson at albourne.com  Thu May 29 05:51:46 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Thu May 29 05:51:53 2008
Subject: [Slony1-general] 3 postgres instances and 3 slony on same machine,
	having some issues
In-Reply-To: <483EA662.3010600@postgresqlfr.org>
References: <483E900B.9060701@albourne.com>
	<483E93A2.7060808@postgresqlfr.org>	<483EA469.2010108@albourne.com>
	<483EA662.3010600@postgresqlfr.org>
Message-ID: <483EA6E2.1080406@albourne.com>

St?phane A. Schildknecht wrote:
> Martin Eriksson a ?crit :
>   
>>>   
>>>       
>> well i built it the following way:
>> while in the slony1-1.2.14 source folder:
>> ./configure --prefix=/data/pgsql-8.3.1
>> --with-perltools=/data/pgsql-8.3.1/slony
>> --with-pgconfigdir=/data/pgsql-8.3.1/bin
>>     
>
> What did you get for lines beginning  with "pg_config says..." ?
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>   
Checking for pg_config... /data/pgsql-8.3.1/bin/pg_config
pg_config says pg_bindir is /data/pgsql-8.3.1/bin/
pg_config says pg_libdir is /data/pgsql-8.3.1/lib/
pg_config says pg_includedir is /data/pgsql-8.3.1/include/
pg_config says pg_pkglibdir is /data/pgsql-8.3.1/lib/
pg_config says pg_includeserverdir is /data/pgsql-8.3.1/include/server/
checking for correct version of PostgreSQL... 8.3
pg_config says pg_sharedir is /data/pgsql-8.3.1/share/


so that looks all good....


From stephane.schildknecht at postgresqlfr.org  Thu May 29 08:00:50 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Thu May 29 08:00:59 2008
Subject: [Slony1-general] 3 postgres instances and 3 slony on same machine,
	having some issues
In-Reply-To: <483EA6E2.1080406@albourne.com>
References: <483E900B.9060701@albourne.com>	<483E93A2.7060808@postgresqlfr.org>	<483EA469.2010108@albourne.com>	<483EA662.3010600@postgresqlfr.org>
	<483EA6E2.1080406@albourne.com>
Message-ID: <483EC522.7070404@postgresqlfr.org>

Martin Eriksson a ?crit :
> St?phane A. Schildknecht wrote:
>> Martin Eriksson a ?crit :
>>  
>>>>         
>>> well i built it the following way:
>>> while in the slony1-1.2.14 source folder:
>>> ./configure --prefix=/data/pgsql-8.3.1
>>> --with-perltools=/data/pgsql-8.3.1/slony
>>> --with-pgconfigdir=/data/pgsql-8.3.1/bin
>>>     
>>
>> What did you get for lines beginning  with "pg_config says..." ?
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>   
> Checking for pg_config... /data/pgsql-8.3.1/bin/pg_config
> pg_config says pg_bindir is /data/pgsql-8.3.1/bin/
> pg_config says pg_libdir is /data/pgsql-8.3.1/lib/
> pg_config says pg_includedir is /data/pgsql-8.3.1/include/
> pg_config says pg_pkglibdir is /data/pgsql-8.3.1/lib/
> pg_config says pg_includeserverdir is /data/pgsql-8.3.1/include/server/
> checking for correct version of PostgreSQL... 8.3
> pg_config says pg_sharedir is /data/pgsql-8.3.1/share/
> 
> 
> so that looks all good....

Simple question... Did you make clean before compiling for 8.3 ?

If so, I don't know what the problem could be.
From tmblue at gmail.com  Thu May 29 10:26:15 2008
From: tmblue at gmail.com (Tory M Blue)
Date: Thu May 29 10:26:25 2008
Subject: [Slony1-general] Slave sl_path and sl_listen (was/is full mesh)
Message-ID: <8a547c840805291026q2deff394n93acd5bede1d6d7f@mail.gmail.com>

Okay

Per a few emails back I asked about full mesh and it was determined or
re certified to not be a great idea, especially when one scales, and
it's apparently not needed at any point.

So I've tried to get my 4 box system to drop some of this activity.

Master
Slave
QSLAVE1
QSLAVE2

I've tried to remove any events from taking place from 3 to 4 and vice
versa. I still want the master and slave to talk to both QSlaves (a
bit nervous I guess about removing Slave to Qslave communications)
(since there will be occasions for switchover).

I'm still seeing on Slave1 the following (slave 1 is node 3, slave 2 is node 4)

2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: SYNC 5318 done in
0.011 seconds
2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: forward confirm
4,4775 received by 1
2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: forward confirm
4,4775 received by 2
2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: forward confirm
3,6219 received by 4
2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: forward confirm
2,4937 received by 4
2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: forward confirm
3,6219 received by 2
2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: forward confirm
3,6219 received by 1
2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: forward confirm
1,5318 received by 2
2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: forward confirm
2,4937 received by 1

So it appears even though I show : sl_path=10, sl_listen=26

That I've not correctly or successfully removed some of the chatter
between the nodes.

So, where would I have assigned these paths? We have looked thru our
scripts and cannot locate where we may be setting this and thus need
to remove it.  sl_path seems right, sl_listen not so sure about.

Thanks
Tory
From JanWieck at Yahoo.com  Thu May 29 11:03:43 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu May 29 11:04:04 2008
Subject: [Slony1-general] Slave sl_path and sl_listen (was/is full mesh)
In-Reply-To: <8a547c840805291026q2deff394n93acd5bede1d6d7f@mail.gmail.com>
References: <8a547c840805291026q2deff394n93acd5bede1d6d7f@mail.gmail.com>
Message-ID: <483EEFFF.3090902@Yahoo.com>

On 5/29/2008 1:26 PM, Tory M Blue wrote:
> Okay
> 
> Per a few emails back I asked about full mesh and it was determined or
> re certified to not be a great idea, especially when one scales, and
> it's apparently not needed at any point.
> 
> So I've tried to get my 4 box system to drop some of this activity.
> 
> Master
> Slave
> QSLAVE1
> QSLAVE2
> 
> I've tried to remove any events from taking place from 3 to 4 and vice
> versa. I still want the master and slave to talk to both QSlaves (a
> bit nervous I guess about removing Slave to Qslave communications)
> (since there will be occasions for switchover).
> 
> I'm still seeing on Slave1 the following (slave 1 is node 3, slave 2 is node 4)
> 
> 2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: SYNC 5318 done in
> 0.011 seconds
> 2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: forward confirm
> 4,4775 received by 1
> 2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: forward confirm
> 4,4775 received by 2
> 2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: forward confirm
> 3,6219 received by 4
> 2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: forward confirm
> 2,4937 received by 4
> 2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: forward confirm
> 3,6219 received by 2
> 2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: forward confirm
> 3,6219 received by 1
> 2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: forward confirm
> 1,5318 received by 2
> 2008-05-29 10:22:02 PDT DEBUG2 remoteWorkerThread_1: forward confirm
> 2,4937 received by 1
> 
> So it appears even though I show : sl_path=10, sl_listen=26
> 
> That I've not correctly or successfully removed some of the chatter
> between the nodes.
> 
> So, where would I have assigned these paths? We have looked thru our
> scripts and cannot locate where we may be setting this and thus need
> to remove it.  sl_path seems right, sl_listen not so sure about.

Configuring your path network to match your designed flow of data does 
not affect the necessity that all nodes need to confirm the events of 
all other nodes. Those messages are simply propagated through the 
existing paths from node to node.

The major point of keeping your path network the same as your data flow 
is to make sure events are not sent ahead of available data on the data 
providers. Slon does handle this, but it involves timeouts and retries.

Slonik does handle reshaping the path network at failover/switchover 
time just fine, so there is no need to worry about not being able to 
switch/fail if the network isn't configured ahead of time.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From JanWieck at Yahoo.com  Thu May 29 12:26:31 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu May 29 12:27:51 2008
Subject: [Slony1-general] Slow replication issue
In-Reply-To: <2724D707-547C-4E53-9580-8D4EFB975A65@khera.org>
References: <200805231805.m4NI5Mrj032900@jupiter.jannicash.info>
	<2724D707-547C-4E53-9580-8D4EFB975A65@khera.org>
Message-ID: <483F0367.4090405@Yahoo.com>

On 5/27/2008 10:26 AM, Vivek Khera wrote:
> On May 23, 2008, at 2:05 PM, Jan Wieck wrote:
> 
>> The slony connections are all regular libpq database connections. So  
>> you might test this by using psql or pg_dump running on one of those  
>> subscribers, connecting to the appropriate data provider. If that  
>> can utilize more bandwidth, then the problem lies within the replica  
>> itself and something else must be limiting it from reading from the  
>> network faster.
> 
> Every time I dig into why our replication is lagging severely (more  
> than 10-15 mintues) I find that I'm spending a lot of time inside  
> "FETCH 100" queries, and then a lot of time between them, as well (ie,  
> applying the updates to the replica).  It feels as of pg isn't running  
> at full speed, but I don't have the numbers to prove it.

Which is surprising, because the entire (complex I might add) 
architecture of helper threads doing the fetch, placing result rows into 
buffers and shoveling them towards the remote_worker thread was supposed 
to reduce those times, where the remote_worker is "waiting" for rows 
instead of full-bore applying changes.

One thing that comes to mind would be if the slon is actually running on 
the same box as any of the involved databases. In that case I can think 
of a scenario where the remote helper is buffering quite well ahead ... 
and since the local DB is heavily under fire the OS thinks it's a good 
idea to page those buffers out. To make better educated guesswork we 
probably need a few more DEBUG points where the remote worker and 
helpers are issuing messages when the internal buffer exceeds some high 
water mark or when (after exceeding high) falls back below some low 
water mark. That would help us very much to fine tune the actual amount 
of buffers and the fetch size (both of which should be config options).

Another thing to look for is the dreaded "delay for first row". That is 
the time it takes the data provider from when the subscriber asks for 
the current SYNC's log rows until the data provider actually returns the 
first FETCH chunk. That is definitely time that the subscriber is doing 
nothing but twiddling thumbs. Unfortunately, it is going to add a lot 
more complexity in the worker/helper architecture to cure that problem. 
But I would like to hear what are typical ratios of "delay for first 
row" / "time for entire sync" out in the field. Because if that delay 
accounts for a large portion of the entire sync processing we better 
look into improving that part, however complex the solution to it might 
be. Again, it would help to have some better logging here that simply 
states the percentage of time spent waiting during the sync processing. 
That would be the sum of all delays caused when the worker is waiting 
for log rows.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From JanWieck at Yahoo.com  Thu May 29 12:36:42 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu May 29 12:37:11 2008
Subject: [Slony1-general] logswitch_finish() still in progress
In-Reply-To: <C2C35FDD-F0F3-449A-A968-B25A9E6C0CB3@apple.com>
References: <C2C35FDD-F0F3-449A-A968-B25A9E6C0CB3@apple.com>
Message-ID: <483F05CA.7020507@Yahoo.com>

On 5/28/2008 5:34 PM, Bruce Skelton wrote:
> Over the last 30 hours there have been a tremendous amount of changes  
> against my db. This morning there are 5.6 million entries in sl_log_1.  
> Over time the fetch log statements are  getting longer in duration.
> 
> Is there any hope the log switch/truncate will complete?

A log switch can be held up indefinitely by one single node not catching 
up or at least its event confirmations not making it back to the origin. 
Do you see anything suspicious in sl_status on the origin?


Jan

> 
> LOG:  00000: duration: 7768.196 ms  statement: fetch 100 from LOG;
> LOCATION:  exec_simple_query, postgres.c:1015
> LOG:  00000: duration: 7775.775 ms  statement: fetch 100 from LOG;
> LOCATION:  exec_simple_query, postgres.c:1015
> LOG:  00000: duration: 7764.928 ms  statement: fetch 100 from LOG;
> LOCATION:  exec_simple_query, postgres.c:1015
> 
> 
> STATEMENT:  delete from "_mycluster_procluster".sl_log_1 where  
> log_origin = '1' and log_xid < '252358837'; delete from  
> "_mycluster_procluster".sl_log_2 where log_origin = '1' and log_xid <  
> '252358837'; delete from "_mycluster_procluster".sl_seqlog where  
> seql_origin = '1' and seql_ev_seqno < '647367'; select  
> "_mycluster_procluster".logswitch_finish();
> NOTICE:  00000: Slony-I: log switch to sl_log_2 still in progress -  
> sl_log_1 not truncated
> LOCATION:  exec_stmt_raise, pl_exec.c:2149
> STATEMENT:  delete from "_mycluster_procluster".sl_log_1 where  
> log_origin = '2' and log_xid < '220973324'; delete from  
> "_mycluster_procluster".sl_log_2 where log_origin = '2' and log_xid <  
> '220973324'; delete from "_mycluster_procluster".sl_seqlog where  
> seql_origin = '2' and seql_ev_seqno < '605163'; select  
> "_mycluster_procluster".logswitch_finish();
> 
> Thanks in advance,
> 
> bruce
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From skelton at apple.com  Thu May 29 13:22:30 2008
From: skelton at apple.com (Bruce Skelton)
Date: Thu May 29 13:22:43 2008
Subject: [Slony1-general] logswitch_finish() still in progress
In-Reply-To: <483F05CA.7020507@Yahoo.com>
References: <C2C35FDD-F0F3-449A-A968-B25A9E6C0CB3@apple.com>
	<483F05CA.7020507@Yahoo.com>
Message-ID: <56A98331-ABFB-4C0A-A637-4AD0403DFF75@apple.com>

On May 29, 2008, at 12:36 PM, Jan Wieck wrote:

>> Over the last 30 hours there have been a tremendous amount of  
>> changes  against my db. This morning there are 5.6 million entries  
>> in sl_log_1.  Over time the fetch log statements are  getting  
>> longer in duration.
>> Is there any hope the log switch/truncate will complete?
>
> A log switch can be held up indefinitely by one single node not  
> catching up or at least its event confirmations not making it back  
> to the origin. Do you see anything suspicious in sl_status on the  
> origin?
>
>
> Jan
I don't see anything suspicious. Other that my lagtime has almost  
doubled.

mycluster=# SELECT * from _mycluster.sl_status ;
  st_origin | st_received | st_last_event |      st_last_event_ts       
| st_last_received |    st_last_received_ts     |  
st_last_received_event_ts  | st_lag_num_events |   st_lag_time
-----------+-------------+---------------+---------------------------- 
+------------------+---------------------------- 
+----------------------------+-------------------+-----------------
          1 |           2 |        659998 | 2008-05-29 13:11:49.256412  
|           659996 | 2008-05-29 13:11:27.181933 | 2008-05-29  
13:11:33.224593 |                 2 | 00:00:19.023239
(1 row)

mycluster=# SELECT * from _mycluster.sl_event_seq ;
  sequence_name | last_value | increment_by |      max_value      |  
min_value | cache_value | log_cnt | is_cycled | is_called
---------------+------------+--------------+--------------------- 
+-----------+-------------+---------+-----------+-----------
  sl_event_seq  |     659998 |            1 | 9223372036854775807  
|         1 |           1 |      12 | f


Thanks,
bruce
>
>
>> LOG:  00000: duration: 7768.196 ms  statement: fetch 100 from LOG;
>> LOCATION:  exec_simple_query, postgres.c:1015
>> LOG:  00000: duration: 7775.775 ms  statement: fetch 100 from LOG;
>> LOCATION:  exec_simple_query, postgres.c:1015
>> LOG:  00000: duration: 7764.928 ms  statement: fetch 100 from LOG;
>> LOCATION:  exec_simple_query, postgres.c:1015
>> STATEMENT:  delete from "_mycluster_procluster".sl_log_1 where   
>> log_origin = '1' and log_xid < '252358837'; delete from   
>> "_mycluster_procluster".sl_log_2 where log_origin = '1' and log_xid  
>> <  '252358837'; delete from "_mycluster_procluster".sl_seqlog  
>> where  seql_origin = '1' and seql_ev_seqno < '647367'; select   
>> "_mycluster_procluster".logswitch_finish();
>> NOTICE:  00000: Slony-I: log switch to sl_log_2 still in progress  
>> -  sl_log_1 not truncated
>> LOCATION:  exec_stmt_raise, pl_exec.c:2149
>> STATEMENT:  delete from "_mycluster_procluster".sl_log_1 where   
>> log_origin = '2' and log_xid < '220973324'; delete from   
>> "_mycluster_procluster".sl_log_2 where log_origin = '2' and log_xid  
>> <  '220973324'; delete from "_mycluster_procluster".sl_seqlog  
>> where  seql_origin = '2' and seql_ev_seqno < '605163'; select   
>> "_mycluster_procluster".logswitch_finish();
>> Thanks in advance,
>> bruce



From tmblue at gmail.com  Thu May 29 21:38:27 2008
From: tmblue at gmail.com (Tory M Blue)
Date: Thu May 29 21:38:48 2008
Subject: [Slony1-general] db_getLocalNodeId() returned 2 - wrong database?
Message-ID: <8a547c840805292138safe41bchf35a8b4260fe6fba@mail.gmail.com>

Hello "again"

So I posted this error before, when playing with 1.2.14(rc) + patch
and never got a response.

I'm only seeing this error on node 4 (Qslavehost2). I don't see this
on node 3 (Qslavehost1).. The configs are identical, other than how we
start slon (qslavehost1 vs qslavehost2 (variables)).. and only after a
switchover (moving from master to slave)

Wondering why I may be seeing this error? Still seems to replicate
okay, just whining about wrong host/database

2008-05-29 21:28:14 PDT DEBUG4 remoteHelperThread_2_2: waiting for work
2008-05-29 21:28:14 PDT DEBUG2 remoteWorkerThread_2: new sl_rowid_seq
value: 2000000000000000
2008-05-29 21:28:14 PDT DEBUG2 remoteWorkerThread_2: SYNC 75553 done
in 0.006 seconds
2008-05-29 21:28:14 PDT DEBUG2 remoteWorkerThread_2: forward confirm
4,75687 received by 2
2008-05-29 21:28:15 PDT DEBUG4 version for "dbname=clsdb
host=idb02.domain.com user=postgres password=SECURED" is 80301
2008-05-29 21:28:15 PDT ERROR  remoteListenThread_3:
db_getLocalNodeId() returned 2 - wrong database?
2008-05-29 21:28:16 PDT DEBUG2 remoteListenThread_1: LISTEN
2008-05-29 21:28:18 PDT DEBUG2 localListenThread: Received event 4,75687 SYNC
2008-05-29 21:28:19 PDT DEBUG2 remoteWorkerThread_2: forward confirm
2,75553 received by 3
2008-05-29 21:28:19 PDT DEBUG2 remoteWorkerThread_2: forward confirm
4,75687 received by 3
2008-05-29 21:28:23 PDT DEBUG2 syncThread: new sl_action_seq 1 - SYNC 75688
2008-05-29 21:28:25 PDT DEBUG4 version for "dbname=clsdb
host=idb02.domain.com user=postgres password=SECURED" is 80301
2008-05-29 21:28:25 PDT ERROR  remoteListenThread_3:
db_getLocalNodeId() returned 2 - wrong database?
From m.eriksson at albourne.com  Fri May 30 00:31:24 2008
From: m.eriksson at albourne.com (Martin Eriksson)
Date: Fri May 30 00:31:49 2008
Subject: [Slony1-general] 3 postgres instances and 3 slony on same machine,
	having some issues
In-Reply-To: <483EC522.7070404@postgresqlfr.org>
References: <483E900B.9060701@albourne.com>	<483E93A2.7060808@postgresqlfr.org>	<483EA469.2010108@albourne.com>	<483EA662.3010600@postgresqlfr.org>
	<483EA6E2.1080406@albourne.com> <483EC522.7070404@postgresqlfr.org>
Message-ID: <483FAD4C.4050001@albourne.com>

St?phane A. Schildknecht wrote:
> Martin Eriksson a ?crit :
>   
>> St?phane A. Schildknecht wrote:
>>     
>>> Martin Eriksson a ?crit :
>>>  
>>>       
>>>>>         
>>>>>           
>>>> well i built it the following way:
>>>> while in the slony1-1.2.14 source folder:
>>>> ./configure --prefix=/data/pgsql-8.3.1
>>>> --with-perltools=/data/pgsql-8.3.1/slony
>>>> --with-pgconfigdir=/data/pgsql-8.3.1/bin
>>>>     
>>>>         
>>> What did you get for lines beginning  with "pg_config says..." ?
>>>
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general@lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>   
>>>       
>> Checking for pg_config... /data/pgsql-8.3.1/bin/pg_config
>> pg_config says pg_bindir is /data/pgsql-8.3.1/bin/
>> pg_config says pg_libdir is /data/pgsql-8.3.1/lib/
>> pg_config says pg_includedir is /data/pgsql-8.3.1/include/
>> pg_config says pg_pkglibdir is /data/pgsql-8.3.1/lib/
>> pg_config says pg_includeserverdir is /data/pgsql-8.3.1/include/server/
>> checking for correct version of PostgreSQL... 8.3
>> pg_config says pg_sharedir is /data/pgsql-8.3.1/share/
>>
>>
>> so that looks all good....
>>     
>
> Simple question... Did you make clean before compiling for 8.3 ?
>
> If so, I don't know what the problem could be.
>   
thanks, for making me feel stupid hehe :) you are right!!!!

grrrr, long days is my only excuse.

now it works as it should, Thanks a bunch!


From lists at serioustechnology.com  Fri May 30 08:10:30 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Fri May 30 08:10:38 2008
Subject: [Slony1-general] adding tables/triggers to a slony slave
Message-ID: <484018E6.60208@serioustechnology.com>

We are about to revisit replication via slony since we now have upgraded 
our databases to 8.3.1.  There are a couple of issues I need to address 
though.

First, is it possible, and will it cause any problems to add a table to 
the slave that will not exist in the master.  We won't be replicating 
data to this table.

Is it possible to add triggers to the slave tables.  These triggers will 
update the aforementioned table we will add.

Finally, we will need to set the slony triggers to 'fire always' on our 
master as we have a process in our production environment which turns 
off all triggers on a regular basis.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From ajs at crankycanuck.ca  Fri May 30 08:50:41 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri May 30 08:51:06 2008
Subject: [Slony1-general] adding tables/triggers to a slony slave
In-Reply-To: <484018E6.60208@serioustechnology.com>
References: <484018E6.60208@serioustechnology.com>
Message-ID: <20080530155041.GA11328@crankycanuck.ca>

On Fri, May 30, 2008 at 11:10:30AM -0400, Geoffrey wrote:

> First, is it possible, and will it cause any problems to add a table to 
> the slave that will not exist in the master.  We won't be replicating 
> data to this table.

That's fine.
 
> Is it possible to add triggers to the slave tables.  These triggers will 
> update the aforementioned table we will add.

You need to review the documentation on STORE TRIGGER.   Also, pay attention
to this area, because it will be different when you upgrade to 2.x.

> Finally, we will need to set the slony triggers to 'fire always' on our 
> master as we have a process in our production environment which turns 
> off all triggers on a regular basis.

You need to fix that process.  If you go around and thump them, things will
break.  There's no setting that allows you to say, "Even if someone turns
you off, fire anyway."

A
From greg at endpoint.com  Fri May 30 09:08:50 2008
From: greg at endpoint.com (Greg Sabino Mullane)
Date: Fri May 30 09:09:36 2008
Subject: [Slony1-general] adding tables/triggers to a slony slave
In-Reply-To: <20080530155041.GA11328@crankycanuck.ca>
References: <484018E6.60208@serioustechnology.com>
	<20080530155041.GA11328@crankycanuck.ca>
Message-ID: <20080530120850.1ed9cdd9@greg-laptop>


> > Finally, we will need to set the slony triggers to 'fire always' on
> > our master as we have a process in our production environment which
> > turns off all triggers on a regular basis.
> 
> You need to fix that process.  If you go around and thump them, things
> will break.  There's no setting that allows you to say, "Even if someone
> turns you off, fire anyway."

Andrew is right that you need to fix this, but all hope is not lost, as
you may be able to make (careful) use of the new session_replication_role
variable in 8.3 of Postgres and a future version of Slony:

http://www.postgresql.org/docs/8.3/interactive/sql-altertable.html

However, "turning off all triggers on a regular basis" is probably the
wrong solution to your particular problem, and at best will cause serious
locking issues, and may cause database corruption. You might want to post
what you are trying to do on pgsql-general and see if a better solution
can be found.

-- 
Greg Sabino Mullane greg@endpoint.com
End Point Corporation
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 163 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080530/9d2d9c49/signature-0001.pgp
From jc at oxado.com  Fri May 30 09:26:25 2008
From: jc at oxado.com (Jacques Caron)
Date: Fri May 30 09:26:49 2008
Subject: [Slony1-general] Slow replication issue
In-Reply-To: <48365807.1000605@wni.co.jp>
References: <48365807.1000605@wni.co.jp>
Message-ID: <20080530162644.0DE69120F049@zeus.directinfos.com>

Hi,

At 07:37 23/05/2008, Stephane LAPIE wrote:
>Replication basically occurs fine, to the exception that the 
>European servers accumulate lag for around two, three hours (six in 
>worst cases observed) when data processing occurs in Asia. We tried 
>raising "sync_group_maxsize" in every slon.conf to 24, which 
>diminished the number of events replication was lagging behind, but 
>did not diminish the lag time (Both are observed through the 
>_SCHEMA.sl_status view, and made into a graph through MRTG)

You might want to bump up sync_group_maxsize to a much higher value 
(we have it set to 1000, with desired_sync_time set to 0, on some 
boxes), that can help quite a bit when the slave is lagging a bit too 
much. That will allow slon to get more data from each cursor, so you 
save on cursor preparation time, and it should also cope a little bit 
better with the "high" network latency.

Also remember that Slony does not really like long transactions, so 
make sure you don't keep transactions open for too long for no good 
reason. But if that were the issue you would probably have the same 
problem for "local" replication.

>By reducing processed data quantities of 30%, we could reduce the 
>lag time by half. We also checked the VPN status, and there is no 
>obvious anomaly nor QoS. The only startling point would be that 
>bandwidth usage is extremely low, and that reducing processed data 
>quantities did not make the bandwidth usage fall any lower.
>
>It looks as though as Slony is actually limiting itself to not 
>over-use the network's bandwidth, but actually ends up using nearly 
>nothing. Given our current network environment, we could perfectly 
>cope with Slony using 10 times more bandwidth with no problem.

I think it's more of a latency issue that anything. Having 300 ms 
instead of a few ms on each roundtrip can make quite a difference 
when you have lots of roundtrips, so the idea is to reduce the number 
of roundtrips as much as possible. This includes having local slons 
(already the case), maximizing the number of events handled on each 
loop (sync_group_maxsize), maximizing the number of log items 
processed on each fetch (as advised by Christopher).

Jacques.

From lists at serioustechnology.com  Fri May 30 09:41:01 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Fri May 30 09:41:09 2008
Subject: [Slony1-general] adding tables/triggers to a slony slave
In-Reply-To: <20080530155041.GA11328@crankycanuck.ca>
References: <484018E6.60208@serioustechnology.com>
	<20080530155041.GA11328@crankycanuck.ca>
Message-ID: <48402E1D.5000402@serioustechnology.com>

Andrew Sullivan wrote:

> You need to fix that process.  If you go around and thump them, things will
> break.  There's no setting that allows you to say, "Even if someone turns
> you off, fire anyway."

I agree, we need to fix this, but the replication is a higher priority. 
  Thus, are you saying "this won't work," or are you saying "this is 
dangerous?"

Are you saying that if we set the slony triggers to 'always fire', the 
will not fire if we 'disable all triggers?'

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From ajs at crankycanuck.ca  Fri May 30 09:56:18 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri May 30 09:56:35 2008
Subject: [Slony1-general] adding tables/triggers to a slony slave
In-Reply-To: <48402E1D.5000402@serioustechnology.com>
References: <484018E6.60208@serioustechnology.com>
	<20080530155041.GA11328@crankycanuck.ca>
	<48402E1D.5000402@serioustechnology.com>
Message-ID: <20080530165618.GB11328@crankycanuck.ca>

On Fri, May 30, 2008 at 12:41:01PM -0400, Geoffrey wrote:
> Are you saying that if we set the slony triggers to 'always fire', the 
> will not fire if we 'disable all triggers?'

As far as I know, even the changes in 8.3 don't allow you to say, "Even if I
tell you to do so later, don't stop firing this trigger."  You could just
disable the _other_ triggers.

A
From lists at serioustechnology.com  Fri May 30 13:07:20 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Fri May 30 13:07:32 2008
Subject: [Slony1-general] order of operations when setting up/starting slony
Message-ID: <48405E78.80809@serioustechnology.com>

I'm wanting to rearrange the way I setup and start slony, but I wanted 
to make sure that my new solution works.  Comments/corrections would be 
appreciated.  My assumption is that the actual replication of data is 
initiated by the slon_start scripts, thus I want to get everything set 
up and then start the replication process.

Currently approach:


slonik_init_cluster --config $CONFIG

slon_start --config $CONFIG 1
slon_start --config $CONFIG 2

slonik_create_set --config $CONFIG 1
slonik_create_set --config $CONFIG 2

slonik_subscribe_set --config $CONFIG 1 2
slonik_subscribe_set --config $CONFIG 2 2


What I want to do is:

slonik_init_cluster --config $CONFIG

slonik_create_set --config $CONFIG 1
slonik_create_set --config $CONFIG 2

slonik_subscribe_set --config $CONFIG 1 2
slonik_subscribe_set --config $CONFIG 2 2

slon_start --config $CONFIG 1
slon_start --config $CONFIG 2

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From gmr at myyearbook.com  Sat May 31 22:38:51 2008
From: gmr at myyearbook.com (Gavin M. Roy)
Date: Sat May 31 22:39:16 2008
Subject: [Slony1-general] segfault in slony 1.2.14 misc.c
Message-ID: <af1bce590805312238w436fb8f7u385a684c4ce19e6@mail.gmail.com>

Hey,
I found a problem in misc.c (line 175) causing segfaults with slon.  Here's
the breakdown of what happened and what was found:

Started seeing this after a long import (200M row table subscribe)

2008-06-01 01:08:18 EDT DEBUG2 slon: child terminated status: 11; pid:
31047, current worker pid: 31047
2008-06-01 01:08:18 EDT DEBUG1 slon: restart of worker in 10 seconds

dmesg was reporting:

slon[30612]: segfault at 0000000000273139 rip 0000003b22876150 rsp
0000000041ded7a8 error 4

(not the same segfault pid, but they were all the same addresses)

I broke out gdb and found the following backtrace on the child:

[Switching to LWP 31128]
0x0000003b22876150 in strlen () from /lib64/libc.so.6
(gdb) bt
#0  0x0000003b22876150 in strlen () from /lib64/libc.so.6
#1  0x0000003b2284605b in vfprintf () from /lib64/libc.so.6
#2  0x0000003b2286728a in vsnprintf () from /lib64/libc.so.6
#3  0x0000000000417488 in slon_log (level=3D<value optimized out>,
fmt=3D0x41ec38 " ssy_action_list value: %s\n") at misc.c:175
#4  0x00000000004094a6 in sync_event (node=3D0x7cb3d70, local_conn=3D<value
optimized out>, wd=3D0x7cb3840, event=3D0x2aaaac0010f0) at remote_worker.c:=
4333
#5  0x000000000040bb03 in remoteWorkerThread_main (cdata=3D<value optimized
out>) at remote_worker.c:630
#6  0x0000003b234062f7 in start_thread () from /lib64/libpthread.so.0
#7  0x0000003b228ce85d in clone () from /lib64/libc.so.6
(gdb)

I went into main.c and had it printf some of the output before the segfault
and found:

2008-06-01 01:15:49 EDT DEBUG2 remoteListenThread_1: queue event 1,1342 SYNC
2008-06-01 01:15:49 EDT DEBUG4  ssy_action_list value:
'52977','52978','52979','52980','52981','52982','52983','52984','52985','52=
986','52987','52988','52989','52990','52991','52992','52993','52994','52995=
','52996','52997','52998','52999','53000','53001','53002','53003','53004','=
53005','53006','53007','53008','53009','53010','53011','53012','53013','530=
14','53015','53016','53017','53018','53019','53020','53021','53022','53023'=
,'53024','53025','53026','53027','53028','53029','53030','53031','53032','5=
3033','53034','53035','53036','53037','53038','53039','53040','53041','5304=
2','53043','53044','53045','53046','53047','53048','53049','53050','53051',=
'53052','53053','53054','53055','53056','53057','53058','53059','53060','53=
061','53062','53063','53064','53065','53066','53067','53068','53069','53070=
','53071','53072','53073','53074','53075','53076','53077','53078','53079','=
53080','53081','53082','53083','53084','53085','53086','53087','53088','530=
89','53090','53091','53092','53093','53094','53095','53096','53097','53098'=
,'53099','53100','53101','53102','53103','53104','53105','53106','53107','5=
3108','53109','53110','53111','53112','53113','53114','53115','53116','5311=
7','53118','53119','53120','53121','53122','53123','53124','53125','53126',=
'53127','53128','53129','53130','53131','53132','53133','53134','53135','53=
136','53137','53138','53139','53140','53141','53142','53143','53144','53145=
','53146','53147','53148','53149','53150','53151','53152','53153','53154','=
53155','53156','53157','53158','53159','53160','53161','53162','53163','531=
64','53165','53166','53167','53168','53169','53170','53171','53172','53173'=
,'53174','53175','53176','53177','53178','53179','53180','53181','53182','5=
3183','53184','53185','53186','53187','53188','53189','53190','53191','5319=
2','53193','53194','53195','53196','53197','53198','53199','53200','53201',=
'53202','53203','53204','53205','53206','53207','53208','53209','53210','53=
211','53212','53213','53214','53215','53216','53217','53218','53219','53220=
','53221','53222','53223','53224','53225','53226','53227','53228','53229','=
53230','53231','53232','53233','53234','53235','53236','53237','53238','532=
39','53240','53241','53242','53243','53244','53245','53246','53247','53248'=
,'53249','53250','53251','53252','53253','53254','53255','53256','53257','5=
3258','53259','53260','53261','53262','53263','53264','53265','53266','5326=
7','53268','53269','53270','53271','53272','53273','53274','53275','53276',=
'53277','53278','53279','53280','53281','53282','53283','53284','53285','53=
286','53287','53288','53289','53290','53291','53292','53293','53294','53295=
','53296','53297','53298','53299','53300','53301','53302','53303','53304','=
53305','53306','53307','53308','53309','53310','53311','53312','53313','533=
14','53315','53316','53317','53318','53319','53320','53321','53322','53323'=
,'53324','53325','53326','53327','53328','53329','53330','53331','53332','5=
3333','53334','53335','53336','53337','53338','53339','53340','53341','5334=
2','53343','53344','53345','53346','53347','53348','53349','53350','53351',=
'53352','53353','53354','53355','53356','53357','53358','53359','53360','53=
361','53362','53363','53364','53365','53366','53367','53368','53369','53370=
','53371','53372','53373','53374','53375','53376','53377','53378','53379','=
53380','53381','53382','53383','53384','53385','53386','53387','53388','533=
89','53390','53391','53392','53393','53394','53395','53396','53397','53398'=
,'53399','53400','53401','53402','53403','53404','53405','53406','53407','5=
3408','53409','53410','53411','53412','53413','53414','53415','53416','5341=
7','53418','53419','53420','53421','53422','53423','53424','53425','53426',=
'53427','53428','53429','53430','53431','53432','53433','53434','53435','53=
436','53437','53438','53439','53440','53441','53442','53443','53444','53445=
','53446','53447','53448','53449','53450','53451','53452','53453','53454','=
53455','53456','53457','53458','53459','53460','53461','53462','53463','534=
64','53465','53466','53467','53468','53469','53470','53471','53472','53473'=
,'53474','53475','53476','53477','53478','53479','53480','53481','53482','5=
3483','53484','53485','53486','53487','53488','53489','53490','53491','5349=
2','53493','53494','53495','53496','53497','53498','53499','53500','53501',=
'53502','53503','53504','53505','53506','53507','53508','53509','53510','53=
511','53512','53513','53514','53515','53516','53517','53518','53519','53520=
','53521','53522','53523','53524','53525','53526','53527','53528','53529','=
53530','53531','53532','53533','53534','53535','53536','53537','53538','535=
39','53540','53541','53542','53543','53544','53545','53546','53547','53548'=
,'53549','53550','53551','53552','53553','53554','53555','53556','53557','5=
3558','53559','53560','53561','53562','53563','53564','53565','53566','5356=
7','53568','53569','53570','53571','53572','53573','53574','53575','53576',=
'53577','53578','53579','53580','53581','53582','53583','53584','53585','53=
586','53587','53588','53589','53590','53591','53592','53593','53594','53595=
','53596','53597','53598','53599','53600','53601','53602','53603','53604','=
53605','53606','53607','53608','53609','53610','53611','53612','53613','536=
14','53615','53616','53617','53618','53619','53620','53621','53622','53623'=
,'53624','53625','53626','53627','53628','53629','53630','53631','53632','5=
3633','53634','53635','53636','53637','53638','53639','53640','53641','5364=
2','53643','53644','53645','53646','53647','53648','53649','53650','53651',=
'53652','53653','53654','53655','53656','53657','53658','53659','53660','53=
661','53662','53663','53664','53665','53666','53667','53668','53669','53670=
','53671','53672','53673','53674','53675','53676','53677','53678','53679','=
53680','53681','53682','53683','53684','53685','53686','53687','53688','536=
89','53690','53691','53692','53693','53694','53695','53696','53697','53698'=
,'53699','53700','53701','53702','53703','53704','53705','53706','53707','5=
3708','53709','53710','53711','53712','53713','53714','53715','53716','5371=
7','53718','53719','53720','53721','53722','53723','53724','53725','53726',=
'53727','53728','53729','53730','53731','53732','53733','53734','53735','53=
736','53737','53738','53739','53740','53741','53742','53743','53744','53745=
','53746','53747','53748','53749','53750','53751','53752','53753','53754','=
53755','53756','53757','53758','53759','53760','53761','53762','53763','537=
64','53765','53766','53767','53768','53769','53770','53771','53772','53773'=
,'53774','53775','53776','53777','53778','53779','53780','53781','53782','5=
3783','53784','53785','53786','53787','53788','53789','53790','53791','5379=
2','53793','53794','53795','53796','53797','53798','53799','53800','53801',=
'53802','53803','53804','53805','53806','53807','53808','53809','53810','53=
811','53812','53813','53814','53815','53816','53817','53818','53819','53820=
','53821','53822','53823','53824','53825','53826','53827','53828','53829','=
53830','53831','53832','53833','53834','53835','53836','53837','53838','538=
39','53840','53841','53842','53843','53844','53845','53846','53847','53848'=
,'53849','53850','53851','53852','53853','53854','53855','53856','53857','5=
3858','53859','53860','53861','53862','53863','53864','53865','53866','5386=
7','53868','53869','53870','53871','53872','53873','53874','53875','53876',=
'53877','53878','53879','53880','53881','53882','53883','53884','53885','53=
886','53887','53888','53889','53890','53891','53892','53893','53894','53895=
','53896','53897','53898','53899','53900','53901','53902','53903','53904','=
53905','53906','53907','53908','53909','53910','53911','53912','53913','539=
14','53915','53916','53917','53918','53919','53920','53921','53922','53923'=
,'53924','53925','53926','53927','53928','53929','53930','53931','53932','5=
3933','53934','53935','53936','53937','53938','53939','53940','53941','5394=
2','53943','53944','53945','53946','53947','53948','53949','53950','53951',=
'53952','53953','53954','53955','53956','53957','53958','53959','53960','53=
961','53962','53963','53964','53965','53966','53967','53968','53969','53970=
','53971','53972','53973','53974','53975','53976','53977','53978','53979','=
53980','53981','53982','53983','53984','53985','53986','53987','53988','539=
89','53990','53991','53992','53993',
2008-06-01 01:15:49 EDT DEBUG2 slon: child terminated status: 11; pid:
31548, current worker pid: 31548

The error looks like it was the DEBUG4 output of all the action list, as it
segfaults in building the string in the while loop on main.c:175.

Sorry I don't have a fix, I just commented out the loop for now, I could
have switched to debug1 or so though, it appears.  I was more concerned with
getting up and running than fixing.  If I have time tomorrow I might try and
figure out what can be done.  My off the cuff suggestion is when outbuf > a
max, split the message, send this one, and construct a new message.

Cheers,

Gavin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080601/=
1faad7d1/attachment-0001.htm
From dev at archonet.com  Tue May 13 05:34:23 2008
From: dev at archonet.com (Richard Huxton)
Date: Thu Sep 25 08:27:18 2008
Subject: [Slony1-general] Re: [GENERAL] Stripping out slony after / before /
	during pg_restore?
In-Reply-To: <349621.46525.qm@web25806.mail.ukl.yahoo.com>
References: <349621.46525.qm@web25806.mail.ukl.yahoo.com>
Message-ID: <48298ACA.1050002@archonet.com>

Glyn Astill wrote:
> Hi people,
> 
> I'm setting us up a separate staging / test server and I want to read
> in a pg_dump of our current origin stripping out all the slony stuff.
> 
> I was thinking this could serve two purposes a) test out backups
> restore properly and b) provide us with us with the staging / test
> server
> 
> What's the best way to remove all the slony bits?

Well, you can always just drop the slony schema (with a cascade) - that 
should do it.

--
   Richard Huxton
From singh.gurjeet at gmail.com  Tue May 13 05:37:42 2008
From: singh.gurjeet at gmail.com (Gurjeet Singh)
Date: Thu Sep 25 08:27:20 2008
Subject: [Slony1-general] Re: [GENERAL] Stripping out slony after / before /
	during pg_restore?
In-Reply-To: <349621.46525.qm@web25806.mail.ukl.yahoo.com>
References: <349621.46525.qm@web25806.mail.ukl.yahoo.com>
Message-ID: <65937bea0805130537h28b80c79k950d707d8dfd342d@mail.gmail.com>

On Tue, May 13, 2008 at 5:42 PM, Glyn Astill <glynastill@yahoo.co.uk> wrote:

> Hi people,
>
> I'm setting us up a separate staging / test server and I want to read in a
> pg_dump of our current origin stripping out all the slony stuff.
>
> I was thinking this could serve two purposes a) test out backups restore
> properly and b) provide us with us with the staging / test server
>
> What's the best way to remove all the slony bits?
>
> I was thinking read in the dump, then use uninstall node - but I'd rather
> not have to run the slon daemons.
>
> Or should I just leave all the slony stuff in there... would it cause us
> any problems? There'd be no slons running and the next night it's all wip=
ed
> and restored again...
>
> Anyone got any ideas? Anyone got something similar already?
>

    You need to have a slon daemon running, configured to monitor the
restored database, and the essential settings for this to work are: host
name, port-number, database name and the Slony cluster name. Since you do
not have a slon daemon for the restored database, I guess you are fine after
restoring the database.

    If you really need to be sure that this restored database does not take
part in replication, you can go ahead an DROP CASCADE the replication schema
from the database. For eg. if your Slony cluster name was my_repl_cluster,
then you can connect to the restored database and issue 'DROP SCHEMA
_my_repl_cluster CASCADE;' to get rid of the replication information. Now,
even if there's a slon daemon running for this DB, it won't be able to do
anything; you can eye the slon's log to see the warnings it will generate.

Best regards,

-- =

gurjeet[.singh]@EnterpriseDB.com
singh.gurjeet@{ gmail | hotmail | indiatimes | yahoo }.com

EnterpriseDB http://www.enterprisedb.com

Mail sent from my BlackLaptop device
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080513/=
fd696237/attachment-0001.htm
From BENEZRAE at ORU.com  Wed May 21 08:57:15 2008
From: BENEZRAE at ORU.com (Benezra, Eric )
Date: Thu Sep 25 08:27:20 2008
Subject: [Slony1-general] Downgrading to a previous version of Slony
Message-ID: <1D90589BAB0AB947B2688E135A546498130A4495@r900ex03.conedison.net>

Hello everyone,

We are looking to replicate from Linux to Windows. The current version
of Slony that we have installed on the Linux machine is 1.2.13 and the
Windows machine is running Slony 1.2.12. I am aware that the versions
have to be the same.

Am I correct to assume that there is currently no version 1.2.13 for
Windows? If so, where can I find it?

If not, we are looking to downgrade the Slony version on the Linux
installation back to 1.2.12, so that the two installations are
compatible.

Is there a specific way to uninstall Slony (on Linux) in such a way that
it eliminates all references to the previous installations?

Thanks to anyone who can help answer these questions!

Eric


-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080521/1b72a70c/attachment-0001.htm
From dim at hi-media.com  Sat May 24 01:54:34 2008
From: dim at hi-media.com (Dimitri Fontaine)
Date: Thu Sep 25 08:27:21 2008
Subject: [Slony1-general] Slow replication issue
In-Reply-To: <63176.196.23.181.69.1211546874.squirrel@zenmail.co.za>
References: <48365807.1000605@wni.co.jp> <1211531227.6789.41.camel@huvostro>
	<4836A329.2080208@wni.co.jp> <1211540946.6789.49.camel@huvostro>
	<63176.196.23.181.69.1211546874.squirrel@zenmail.co.za>
Message-ID: <8F9DC669-B518-4BE7-BA82-9583D9DEACE6@hi-media.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi,

Le 23 mai 08 ? 14:47, Henry a ?crit :
> Sadly, the documentation for SkyTools is *appalling*.  I took a look  
> some
> time ago and ran away pronto.

Some amount of effort have been made to this topic, please consider  
having another look:
   http://skytools.projects.postgresql.org/doc/
   http://skytools.projects.postgresql.org/doc/ 
londiste.cmdline.html#toc3

You'll still find that londiste/skytools have much less documentation  
than Slony offers, and that ties to the fact londiste is much simpler  
than Slony. I think the design is, and I'm quite confident the  
learning, deploying and maintaining is (much simpler).

As a result, you might find out --- almost wild guess here --- that  
londiste won't accomodate to your precise needs as much as Slony is  
able too, that's how it's able to be that much simpler.

Hope this helps,
- --
dim
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.9 (Darwin)

iEYEARECAAYFAkg316UACgkQlBXRlnbh1bmGwgCgvPyEFF5kp4z+kbORgdZDckTw
0YoAniScoBYqtWaQxeT3lM9gUlN4iE5x
=RgEx
-----END PGP SIGNATURE-----
