From bijayant4u at yahoo.com  Tue Jul  1 00:00:19 2008
From: bijayant4u at yahoo.com (bijayant kumar)
Date: Tue Jul  1 00:00:54 2008
Subject: [Slony1-general] Node is not initialized properly
In-Reply-To: <4868F5F8.3060008@albourne.com>
Message-ID: <165778.64945.qm@web32705.mail.mud.yahoo.com>

Thanks to all for the reply. Now i think that i am coming closer to run slonik. When i did as suggested by all

bijayant ~ # slonik_init_cluster --config /etc/slon_tools.conf | slonik

<stdin>:6: PGRES_FATAL_ERROR load '$libdir/xxid';  - ERROR:  could not access file "$libdir/xxid": No such file or directory

<stdin>:6: Error: the extension for the xxid data type cannot be loaded in database 'host=192.168.99.23 dbname=bijayant user=bijayant port=5432 password=bijayant'

<stdin>:6: ERROR: no admin conninfo for node 134598992

The first two lines of the error i couldnot understand.
The "admin conninfo" error, i think this parameter should be present in conf file, right? But in my case what should be there, as i am already giving the username and password to connect to the postgresql database.

In this problem thread one gentleman has asked me to check whether "_bijayant" schema is created or not? I am sorry to ask but i really dont know about this. I have only created a user,database and table in the postgresql database nothing else. When we create database, schema is also created automatically, right?

Please suggest me what should i do next. Sorry but i am very new to slonik and database. I am trying hard to understand the concept

Thanks & Regards,

Bijayant Kumar


--- On Mon, 30/6/08, Martin Eriksson <m.eriksson@albourne.com> wrote:

> From: Martin Eriksson <m.eriksson@albourne.com>
> Subject: Re: [Slony1-general] Node is not initialized properly
> To: 
> Cc: slony1-general@lists.slony.info
> Date: Monday, 30 June, 2008, 8:34 PM
> If you only run it like that, it only prints what it will
> execute, to 
> actually execute this you need to | it to the slonik app..
> 
> eg. /data/pgsql/slony/slonik_init_cluster --config
> <file> | 
> /data/pgsql/bin/slonik
> 
> all scripts starting with "slonik_" doesn't
> actually do anything 
> themself, its just a way to format a command correctly for
> the slonik 
> parser, which actually does the work..
> 
> 
> 
> bijayant kumar wrote:
> > Thanks for the reply. I executed the command
> "slonik_init_cluster".It gives the output like
> >
> > # INIT CLUSTER
> > cluster name = bijayant;
> >  node 1 admin conninfo='host=192.168.99.23
> dbname=bijayant user=bijayant port=5432
> password=bijayant';
> >  node 2 admin conninfo='host=192.168.99.134
> dbname=bijayant user=bijayant port=5432
> password=bijayant';
> >   init cluster (id = 1, comment = 'Node 1 -
> bijayant@192.168.99.23');
> >
> > # STORE NODE
> >   store node (id = 2, event node = 1, comment =
> 'Node 2 - bijayant@192.168.99.134');
> >   echo 'Set up replication nodes';
> >
> > # STORE PATH
> >   echo 'Next: configure paths for each
> node/origin';
> >   store path (server = 1, client = 2, conninfo =
> 'host=192.168.99.23 dbname=bijayant user=bijayant
> port=5432 password=bijayant');
> >   store path (server = 2, client = 1, conninfo =
> 'host=192.168.99.134 dbname=bijayant user=bijayant
> port=5432 password=bijayant');
> >   echo 'Replication nodes prepared';
> >   echo 'Please start a slon replication daemon for
> each node';
> >
> > After that i run the slonik daemon and got the error
> mentioned.
> >
> >
> > Bijayant Kumar
> >
> >
> > --- On Mon, 30/6/08, Martin Eriksson
> <m.eriksson@albourne.com> wrote:
> >
> >   
> >> From: Martin Eriksson
> <m.eriksson@albourne.com>
> >> Subject: Re: [Slony1-general] Node is not
> initialized properly
> >> To: 
> >> Cc: slony1-general@lists.slony.info
> >> Date: Monday, 30 June, 2008, 7:18 PM
> >> Very basic, but saw no mention of it in the
> e-mail,
> >>
> >> I assume you ran the
> "slonik_init_cluster" before
> >> trying to start the 
> >> slon daemons? as that does the slony setup on each
> of the
> >> dbs
> >>
> >>
> >>
> >> bijayant kumar wrote:
> >>     
> >>> Hello list,
> >>>
> >>> I am a very new user of slony1 so please
> forgive me if
> >>>       
> >> i am asking very stupid question. 
> >>     
> >>> I have installed Postgresql and Slony1 on two
> gentoo
> >>>       
> >> machine. Postgresql is working fine no problem at
> all. I
> >> want to use Slony1 to replicate the two databases
> across
> >> the systems. To understand the slony1 concept i
> made a 
> >>     
> >>> test database with one single table and only
> one entry
> >>>       
> >> into it. But when i start slony1 i get error like
> >>     
> >>> 2008-06-30 15:54:55 IST ERROR  cannot get
> >>>       
> >> sl_local_node_id - ERROR:  schema
> "_bijayant"
> >> does not exist
> >>     
> >>> 2008-06-30 15:54:55 IST FATAL  main: Node is
> not
> >>>       
> >> initialized properly - sleep 10s
> >>     
> >>> Now i am giving here my configuration details.
> >>>
> >>> /* Master Server */
> >>> IP Address 192.168.99.23
> >>> Database name bijayant
> >>> Table name kavach
> >>>
> >>> bijayant=# select * from kavach;
> >>>  id |   name   | designation |   address
> >>> ----+----------+-------------+-------------
> >>>   1 | Bijayant | consultant  | Lakkasandra
> >>> (1 row)
> >>>
> >>>
> >>> vi /etc/slon_tools.conf
> >>>
> >>> if ($ENV{"SLONYNODES"}) {
> >>>     require $ENV{"SLONYNODES"};
> >>> } else {
> >>>     $CLUSTER_NAME = 'bijayant';
> >>>     $LOGDIR = '/var/log/slony';
> >>>     $MASTERNODE = 1;
> >>>     add_node(node     => 1,
> >>>              host     =>
> '192.168.99.23',
> >>>              dbname   =>
> 'bijayant',
> >>>              port     => 5432,
> >>>              user     =>
> 'bijayant',
> >>>              password =>
> 'bijayant');
> >>>
> >>>     add_node(node     => 2,
> >>>              host     =>
> '192.168.99.134',
> >>>              dbname   =>
> 'bijayant',
> >>>              port     => 5432,
> >>>              user     =>
> 'bijayant',
> >>>              password =>
> 'bijayant',
> >>>              parent => 1
> >>>              );
> >>> }
> >>>
> >>> $SLONY_SETS = {
> >>>                 "set1" => {
> >>>                            "set_id"
> => 1,
> >>>                           
> "pkeyedtables"
> >>>       
> >> => [
> >>     
> >>>                           
> 'public.kavach',
> >>>                            ],
> >>>         },
> >>> if ($ENV{"SLONYSET"}) {
> >>>     require $ENV{"SLONYSET"};
> >>> }
> >>>
> >>> 1;
> >>>
> >>> vi /etc/conf.d/slony1
> >>> USER=postgres
> >>> CLUSTER=bijayant
> >>> DBUSER=bijayant
> >>> DBNAME=bijayant
> >>> DBHOST=192.168.99.23
> >>> LOGFILE=/var/lib/postgresql/data/slony1.log
> >>> LOGLEVEL=4
> >>>
> >>> /* On the Slave Server */
> >>>
> >>> IP Address 192.168.99.134
> >>> Database name bijayant
> >>> Table name kavach
> >>>
> >>> vi /etc/slon_tools.conf
> >>>
> >>> if ($ENV{"SLONYNODES"}) {
> >>>     require $ENV{"SLONYNODES"};
> >>> } else {
> >>>     $CLUSTER_NAME = 'bijayant';
> >>>     $LOGDIR = '/var/log/slony';
> >>>     # SYNC check interval (slon -s option)
> >>>     # $SYNC_CHECK_INTERVAL = 1000;
> >>>     $MASTERNODE = 1;
> >>>     add_node(node     => 1,
> >>>              host     =>
> '192.168.99.23',
> >>>              dbname   =>
> 'bijayant',
> >>>              port     => 5432,
> >>>              user     =>
> 'bijayant',
> >>>              password =>
> 'bijayant');
> >>>
> >>>     add_node(node     => 2,
> >>>              host     =>
> '192.168.99.134',
> >>>              dbname   =>
> 'bijayant',
> >>>              port     => 5432,
> >>>              user     =>
> 'bijayant',
> >>>              password =>
> 'bijayant',
> >>>              parent => 1
> >>>              );
> >>> }
> >>> $SLONY_SETS = {
> >>>     "set1" => {
> >>>                "set_id" => 1,
> >>>                 "pkeyedtables" =>
> [
> >>>                                  
> >>>       
> >> 'public.kavach',
> >>     
> >>>                                   ],
> >>>               },
> >>> };
> >>>
> >>> if ($ENV{"SLONYSET"}) {
> >>>     require $ENV{"SLONYSET"};
> >>> }
> >>>
> >>> 1;
> >>>
> >>> vi /etc/conf.d/slony1
> >>> USER=postgres
> >>> CLUSTER=bijayant
> >>> DBUSER=bijayant
> >>> DBNAME=bijayant
> >>> DBHOST=192.168.99.23
> >>> LOGFILE=/var/lib/postgresql/data/slony1.log
> >>> LOGLEVEL=4 
> >>>
> >>> When i start the slony1 /etc/init.d/slony1
> start on
> >>>       
> >> both the machine it generates lots of logs with
> errors line
> >> like
> >>     
> >>> 2008-06-30 15:54:55 IST DEBUG2 slon_retry()
> from
> >>>       
> >> pid=19007
> >>     
> >>> 2008-06-30 15:54:55 IST DEBUG1 slon: retry
> requested
> >>> 2008-06-30 15:54:55 IST DEBUG2 slon: notify
> worker
> >>>       
> >> process to shutdown
> >>     
> >>> 2008-06-30 15:54:55 IST DEBUG2 slon: child
> terminated
> >>>       
> >> status: 0; pid: 19007, current worker pid: 19007
> >>     
> >>> 2008-06-30 15:54:55 IST DEBUG1 slon: restart
> of worker
> >>> 2008-06-30 15:54:55 IST CONFIG main: slon
> version
> >>>       
> >> 1.2.10 starting up
> >>     
> >>> 2008-06-30 15:54:55 IST DEBUG2 slon: watchdog
> process
> >>>       
> >> started
> >>     
> >>> 2008-06-30 15:54:55 IST DEBUG2 slon: watchdog
> ready -
> >>>       
> >> pid = 19005
> >>     
> >>> 2008-06-30 15:54:55 IST DEBUG2 slon: worker
> process
> >>>       
> >> created - pid = 19034
> >>     
> >>> 2008-06-30 15:54:55 IST ERROR  cannot get
> >>>       
> >> sl_local_node_id - ERROR:  schema
> "_bijayant"
> >> does not exist
> >>     
> >>> 2008-06-30 15:54:55 IST FATAL  main: Node is
> not
> >>>       
> >> initialized properly - sleep 10s
> >>     
> >>> I am sure that i am not understanding some
> basic
> >>>       
> >> things about the slony1. Can anybody help me to
> understand
> >> the logic, i will be very helpful for you all.
> Please tell
> >> me what i am doing wrong here, what should i do. I
> have
> >> read the documentation at the website but not able
> to
> >> understand fully. Please help me out.
> >>     
> >>> Thanks & Regards,
> >>> Bijayant Kumar
> >>>
> >>> Send instant messages to your online friends
> >>>       
> >> http://uk.messenger.yahoo.com 
> >>     
> >>>
> _______________________________________________
> >>> Slony1-general mailing list
> >>> Slony1-general@lists.slony.info
> >>>
> >>>       
> >>
> http://lists.slony.info/mailman/listinfo/slony1-general
> >>     
> >>>   
> >>>       
> >> _______________________________________________
> >> Slony1-general mailing list
> >> Slony1-general@lists.slony.info
> >>
> http://lists.slony.info/mailman/listinfo/slony1-general
> >>     
> >
> > Send instant messages to your online friends
> http://uk.messenger.yahoo.com 
> >   
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

Send instant messages to your online friends http://uk.messenger.yahoo.com 
From glynastill at yahoo.co.uk  Tue Jul  1 01:52:59 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue Jul  1 01:53:27 2008
Subject: [Slony1-general] Node is not initialized properly
Message-ID: <736067.70588.qm@web25806.mail.ukl.yahoo.com>

I'd be checking to see where xxid.so was on the system, and if I had an older version lurking somewhere.

I'd also make sure bijayant was a database superuser.



----- Original Message ----
> From: bijayant kumar <bijayant4u@yahoo.com>
> To: Martin Eriksson <m.eriksson@albourne.com>
> Cc: slony1-general@lists.slony.info
> Sent: Tuesday, 1 July, 2008 8:00:19 AM
> Subject: Re: [Slony1-general] Node is not initialized properly
> 
> Thanks to all for the reply. Now i think that i am coming closer to run slonik. 
> When i did as suggested by all
> 
> bijayant ~ # slonik_init_cluster --config /etc/slon_tools.conf | slonik
> 
> :6: PGRES_FATAL_ERROR load '$libdir/xxid';  - ERROR:  could not access 
> file "$libdir/xxid": No such file or directory
> 
> :6: Error: the extension for the xxid data type cannot be loaded in 
> database 'host=192.168.99.23 dbname=bijayant user=bijayant port=5432 
> password=bijayant'
> 
> :6: ERROR: no admin conninfo for node 134598992
> 
> The first two lines of the error i couldnot understand.
> The "admin conninfo" error, i think this parameter should be present in conf 
> file, right? But in my case what should be there, as i am already giving the 
> username and password to connect to the postgresql database.
> 
> In this problem thread one gentleman has asked me to check whether "_bijayant" 
> schema is created or not? I am sorry to ask but i really dont know about this. I 
> have only created a user,database and table in the postgresql database nothing 
> else. When we create database, schema is also created automatically, right?
> 
> Please suggest me what should i do next. Sorry but i am very new to slonik and 
> database. I am trying hard to understand the concept
> 
> Thanks & Regards,
> 
> Bijayant Kumar
> 
> 
> --- On Mon, 30/6/08, Martin Eriksson wrote:
> 
> > From: Martin Eriksson 
> > Subject: Re: [Slony1-general] Node is not initialized properly
> > To: 
> > Cc: slony1-general@lists.slony.info
> > Date: Monday, 30 June, 2008, 8:34 PM
> > If you only run it like that, it only prints what it will
> > execute, to 
> > actually execute this you need to | it to the slonik app..
> > 
> > eg. /data/pgsql/slony/slonik_init_cluster --config
> > | 
> > /data/pgsql/bin/slonik
> > 
> > all scripts starting with "slonik_" doesn't
> > actually do anything 
> > themself, its just a way to format a command correctly for
> > the slonik 
> > parser, which actually does the work..
> > 
> > 
> > 
> > bijayant kumar wrote:
> > > Thanks for the reply. I executed the command
> > "slonik_init_cluster".It gives the output like
> > >
> > > # INIT CLUSTER
> > > cluster name = bijayant;
> > >  node 1 admin conninfo='host=192.168.99.23
> > dbname=bijayant user=bijayant port=5432
> > password=bijayant';
> > >  node 2 admin conninfo='host=192.168.99.134
> > dbname=bijayant user=bijayant port=5432
> > password=bijayant';
> > >   init cluster (id = 1, comment = 'Node 1 -
> > bijayant@192.168.99.23');
> > >
> > > # STORE NODE
> > >   store node (id = 2, event node = 1, comment =
> > 'Node 2 - bijayant@192.168.99.134');
> > >   echo 'Set up replication nodes';
> > >
> > > # STORE PATH
> > >   echo 'Next: configure paths for each
> > node/origin';
> > >   store path (server = 1, client = 2, conninfo =
> > 'host=192.168.99.23 dbname=bijayant user=bijayant
> > port=5432 password=bijayant');
> > >   store path (server = 2, client = 1, conninfo =
> > 'host=192.168.99.134 dbname=bijayant user=bijayant
> > port=5432 password=bijayant');
> > >   echo 'Replication nodes prepared';
> > >   echo 'Please start a slon replication daemon for
> > each node';
> > >
> > > After that i run the slonik daemon and got the error
> > mentioned.
> > >
> > >
> > > Bijayant Kumar
> > >
> > >
> > > --- On Mon, 30/6/08, Martin Eriksson
> > wrote:
> > >
> > >  
> > >> From: Martin Eriksson
> > 
> > >> Subject: Re: [Slony1-general] Node is not
> > initialized properly
> > >> To: 
> > >> Cc: slony1-general@lists.slony.info
> > >> Date: Monday, 30 June, 2008, 7:18 PM
> > >> Very basic, but saw no mention of it in the
> > e-mail,
> > >>
> > >> I assume you ran the
> > "slonik_init_cluster" before
> > >> trying to start the 
> > >> slon daemons? as that does the slony setup on each
> > of the
> > >> dbs
> > >>
> > >>
> > >>
> > >> bijayant kumar wrote:
> > >>    
> > >>> Hello list,
> > >>>
> > >>> I am a very new user of slony1 so please
> > forgive me if
> > >>>      
> > >> i am asking very stupid question. 
> > >>    
> > >>> I have installed Postgresql and Slony1 on two
> > gentoo
> > >>>      
> > >> machine. Postgresql is working fine no problem at
> > all. I
> > >> want to use Slony1 to replicate the two databases
> > across
> > >> the systems. To understand the slony1 concept i
> > made a 
> > >>    
> > >>> test database with one single table and only
> > one entry
> > >>>      
> > >> into it. But when i start slony1 i get error like
> > >>    
> > >>> 2008-06-30 15:54:55 IST ERROR  cannot get
> > >>>      
> > >> sl_local_node_id - ERROR:  schema
> > "_bijayant"
> > >> does not exist
> > >>    
> > >>> 2008-06-30 15:54:55 IST FATAL  main: Node is
> > not
> > >>>      
> > >> initialized properly - sleep 10s
> > >>    
> > >>> Now i am giving here my configuration details.
> > >>>
> > >>> /* Master Server */
> > >>> IP Address 192.168.99.23
> > >>> Database name bijayant
> > >>> Table name kavach
> > >>>
> > >>> bijayant=# select * from kavach;
> > >>>  id |   name   | designation |   address
> > >>> ----+----------+-------------+-------------
> > >>>   1 | Bijayant | consultant  | Lakkasandra
> > >>> (1 row)
> > >>>
> > >>>
> > >>> vi /etc/slon_tools.conf
> > >>>
> > >>> if ($ENV{"SLONYNODES"}) {
> > >>>     require $ENV{"SLONYNODES"};
> > >>> } else {
> > >>>     $CLUSTER_NAME = 'bijayant';
> > >>>     $LOGDIR = '/var/log/slony';
> > >>>     $MASTERNODE = 1;
> > >>>     add_node(node     => 1,
> > >>>              host     =>
> > '192.168.99.23',
> > >>>              dbname   =>
> > 'bijayant',
> > >>>              port     => 5432,
> > >>>              user     =>
> > 'bijayant',
> > >>>              password =>
> > 'bijayant');
> > >>>
> > >>>     add_node(node     => 2,
> > >>>              host     =>
> > '192.168.99.134',
> > >>>              dbname   =>
> > 'bijayant',
> > >>>              port     => 5432,
> > >>>              user     =>
> > 'bijayant',
> > >>>              password =>
> > 'bijayant',
> > >>>              parent => 1
> > >>>              );
> > >>> }
> > >>>
> > >>> $SLONY_SETS = {
> > >>>                 "set1" => {
> > >>>                            "set_id"
> > => 1,
> > >>>                          
> > "pkeyedtables"
> > >>>      
> > >> => [
> > >>    
> > >>>                          
> > 'public.kavach',
> > >>>                            ],
> > >>>         },
> > >>> if ($ENV{"SLONYSET"}) {
> > >>>     require $ENV{"SLONYSET"};
> > >>> }
> > >>>
> > >>> 1;
> > >>>
> > >>> vi /etc/conf.d/slony1
> > >>> USER=postgres
> > >>> CLUSTER=bijayant
> > >>> DBUSER=bijayant
> > >>> DBNAME=bijayant
> > >>> DBHOST=192.168.99.23
> > >>> LOGFILE=/var/lib/postgresql/data/slony1.log
> > >>> LOGLEVEL=4
> > >>>
> > >>> /* On the Slave Server */
> > >>>
> > >>> IP Address 192.168.99.134
> > >>> Database name bijayant
> > >>> Table name kavach
> > >>>
> > >>> vi /etc/slon_tools.conf
> > >>>
> > >>> if ($ENV{"SLONYNODES"}) {
> > >>>     require $ENV{"SLONYNODES"};
> > >>> } else {
> > >>>     $CLUSTER_NAME = 'bijayant';
> > >>>     $LOGDIR = '/var/log/slony';
> > >>>     # SYNC check interval (slon -s option)
> > >>>     # $SYNC_CHECK_INTERVAL = 1000;
> > >>>     $MASTERNODE = 1;
> > >>>     add_node(node     => 1,
> > >>>              host     =>
> > '192.168.99.23',
> > >>>              dbname   =>
> > 'bijayant',
> > >>>              port     => 5432,
> > >>>              user     =>
> > 'bijayant',
> > >>>              password =>
> > 'bijayant');
> > >>>
> > >>>     add_node(node     => 2,
> > >>>              host     =>
> > '192.168.99.134',
> > >>>              dbname   =>
> > 'bijayant',
> > >>>              port     => 5432,
> > >>>              user     =>
> > 'bijayant',
> > >>>              password =>
> > 'bijayant',
> > >>>              parent => 1
> > >>>              );
> > >>> }
> > >>> $SLONY_SETS = {
> > >>>     "set1" => {
> > >>>                "set_id" => 1,
> > >>>                 "pkeyedtables" =>
> > [
> > >>>                                  
> > >>>      
> > >> 'public.kavach',
> > >>    
> > >>>                                   ],
> > >>>               },
> > >>> };
> > >>>
> > >>> if ($ENV{"SLONYSET"}) {
> > >>>     require $ENV{"SLONYSET"};
> > >>> }
> > >>>
> > >>> 1;
> > >>>
> > >>> vi /etc/conf.d/slony1
> > >>> USER=postgres
> > >>> CLUSTER=bijayant
> > >>> DBUSER=bijayant
> > >>> DBNAME=bijayant
> > >>> DBHOST=192.168.99.23
> > >>> LOGFILE=/var/lib/postgresql/data/slony1.log
> > >>> LOGLEVEL=4 
> > >>>
> > >>> When i start the slony1 /etc/init.d/slony1
> > start on
> > >>>      
> > >> both the machine it generates lots of logs with
> > errors line
> > >> like
> > >>    
> > >>> 2008-06-30 15:54:55 IST DEBUG2 slon_retry()
> > from
> > >>>      
> > >> pid=19007
> > >>    
> > >>> 2008-06-30 15:54:55 IST DEBUG1 slon: retry
> > requested
> > >>> 2008-06-30 15:54:55 IST DEBUG2 slon: notify
> > worker
> > >>>      
> > >> process to shutdown
> > >>    
> > >>> 2008-06-30 15:54:55 IST DEBUG2 slon: child
> > terminated
> > >>>      
> > >> status: 0; pid: 19007, current worker pid: 19007
> > >>    
> > >>> 2008-06-30 15:54:55 IST DEBUG1 slon: restart
> > of worker
> > >>> 2008-06-30 15:54:55 IST CONFIG main: slon
> > version
> > >>>      
> > >> 1.2.10 starting up
> > >>    
> > >>> 2008-06-30 15:54:55 IST DEBUG2 slon: watchdog
> > process
> > >>>      
> > >> started
> > >>    
> > >>> 2008-06-30 15:54:55 IST DEBUG2 slon: watchdog
> > ready -
> > >>>      
> > >> pid = 19005
> > >>    
> > >>> 2008-06-30 15:54:55 IST DEBUG2 slon: worker
> > process
> > >>>      
> > >> created - pid = 19034
> > >>    
> > >>> 2008-06-30 15:54:55 IST ERROR  cannot get
> > >>>      
> > >> sl_local_node_id - ERROR:  schema
> > "_bijayant"
> > >> does not exist
> > >>    
> > >>> 2008-06-30 15:54:55 IST FATAL  main: Node is
> > not
> > >>>      
> > >> initialized properly - sleep 10s
> > >>    
> > >>> I am sure that i am not understanding some
> > basic
> > >>>      
> > >> things about the slony1. Can anybody help me to
> > understand
> > >> the logic, i will be very helpful for you all.
> > Please tell
> > >> me what i am doing wrong here, what should i do. I
> > have
> > >> read the documentation at the website but not able
> > to
> > >> understand fully. Please help me out.
> > >>    
> > >>> Thanks & Regards,
> > >>> Bijayant Kumar
> > >>>
> > >>> Send instant messages to your online friends
> > >>>      
> > >> http://uk.messenger.yahoo.com 
> > >>    
> > >>>
> > _______________________________________________
> > >>> Slony1-general mailing list
> > >>> Slony1-general@lists.slony.info
> > >>>
> > >>>      
> > >>
> > http://lists.slony.info/mailman/listinfo/slony1-general
> > >>    
> > >>>  
> > >>>      
> > >> _______________________________________________
> > >> Slony1-general mailing list
> > >> Slony1-general@lists.slony.info
> > >>
> > http://lists.slony.info/mailman/listinfo/slony1-general
> > >>    
> > >
> > > Send instant messages to your online friends
> > http://uk.messenger.yahoo.com 
> > >  
> > 
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general@lists.slony.info
> > http://lists.slony.info/mailman/listinfo/slony1-general
> 
> Send instant messages to your online friends http://uk.messenger.yahoo.com
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general



      __________________________________________________________
Not happy with your email address?.
Get the one you really want - millions of new email addresses available now at Yahoo! http://uk.docs.yahoo.com/ymail/new.html
From bijayant4u at yahoo.com  Tue Jul  1 02:35:38 2008
From: bijayant4u at yahoo.com (bijayant kumar)
Date: Tue Jul  1 02:36:06 2008
Subject: [Slony1-general] Node is not initialized properly
In-Reply-To: <736067.70588.qm@web25806.mail.ukl.yahoo.com>
Message-ID: <866360.89616.qm@web32702.mail.mud.yahoo.com>

Hi,
The user "bijayant" is a super user. I have created this user like

postgres@bijayant ~ $ createuser bijayant
Shall the new user be allowed to create databases? (y/n) y
Shall the new user be allowed to create more new users? (y/n) y
CREATE USER

It means that bijayant is a super user right? Or i am doing some silly thing. If i will use user as pgsql than what to use for password.

bijayant ~ # pg_config --libdir
/usr/lib

bijayant ~ # pg_config --pkglibdir
/usr/lib/postgresql

In the Master Database server the xxid.so file is in 
/usr/local/pgsql/lib/xxid.so. I have copied to this file to /usr/lib/postgresql/ and /usr/lib/ also. But no luck.

On the Slave Server, its in /usr/lib64/postgresql/xxid.so.

Is it creating the problem. How to resolve this problem.
I have installed postgres and slony1 by the emerge utility of gentoo.

Thanks & Regards,

Bijayant Kumar


--- On Tue, 1/7/08, Glyn Astill <glynastill@yahoo.co.uk> wrote:

> From: Glyn Astill <glynastill@yahoo.co.uk>
> Subject: Re: [Slony1-general] Node is not initialized properly
> To: bijayant4u@yahoo.com, "Martin Eriksson" <m.eriksson@albourne.com>
> Cc: slony1-general@lists.slony.info
> Date: Tuesday, 1 July, 2008, 2:22 PM
> I'd be checking to see where xxid.so was on the system,
> and if I had an older version lurking somewhere.
> 
> I'd also make sure bijayant was a database superuser.
> 
> 
> 
> ----- Original Message ----
> > From: bijayant kumar <bijayant4u@yahoo.com>
> > To: Martin Eriksson <m.eriksson@albourne.com>
> > Cc: slony1-general@lists.slony.info
> > Sent: Tuesday, 1 July, 2008 8:00:19 AM
> > Subject: Re: [Slony1-general] Node is not initialized
> properly
> > 
> > Thanks to all for the reply. Now i think that i am
> coming closer to run slonik. 
> > When i did as suggested by all
> > 
> > bijayant ~ # slonik_init_cluster --config
> /etc/slon_tools.conf | slonik
> > 
> > :6: PGRES_FATAL_ERROR load '$libdir/xxid';  -
> ERROR:  could not access 
> > file "$libdir/xxid": No such file or
> directory
> > 
> > :6: Error: the extension for the xxid data type cannot
> be loaded in 
> > database 'host=192.168.99.23 dbname=bijayant
> user=bijayant port=5432 
> > password=bijayant'
> > 
> > :6: ERROR: no admin conninfo for node 134598992
> > 
> > The first two lines of the error i couldnot
> understand.
> > The "admin conninfo" error, i think this
> parameter should be present in conf 
> > file, right? But in my case what should be there, as i
> am already giving the 
> > username and password to connect to the postgresql
> database.
> > 
> > In this problem thread one gentleman has asked me to
> check whether "_bijayant" 
> > schema is created or not? I am sorry to ask but i
> really dont know about this. I 
> > have only created a user,database and table in the
> postgresql database nothing 
> > else. When we create database, schema is also created
> automatically, right?
> > 
> > Please suggest me what should i do next. Sorry but i
> am very new to slonik and 
> > database. I am trying hard to understand the concept
> > 
> > Thanks & Regards,
> > 
> > Bijayant Kumar
> > 
> > 
> > --- On Mon, 30/6/08, Martin Eriksson wrote:
> > 
> > > From: Martin Eriksson 
> > > Subject: Re: [Slony1-general] Node is not
> initialized properly
> > > To: 
> > > Cc: slony1-general@lists.slony.info
> > > Date: Monday, 30 June, 2008, 8:34 PM
> > > If you only run it like that, it only prints what
> it will
> > > execute, to 
> > > actually execute this you need to | it to the
> slonik app..
> > > 
> > > eg. /data/pgsql/slony/slonik_init_cluster
> --config
> > > | 
> > > /data/pgsql/bin/slonik
> > > 
> > > all scripts starting with "slonik_"
> doesn't
> > > actually do anything 
> > > themself, its just a way to format a command
> correctly for
> > > the slonik 
> > > parser, which actually does the work..
> > > 
> > > 
> > > 
> > > bijayant kumar wrote:
> > > > Thanks for the reply. I executed the command
> > > "slonik_init_cluster".It gives the
> output like
> > > >
> > > > # INIT CLUSTER
> > > > cluster name = bijayant;
> > > >  node 1 admin
> conninfo='host=192.168.99.23
> > > dbname=bijayant user=bijayant port=5432
> > > password=bijayant';
> > > >  node 2 admin
> conninfo='host=192.168.99.134
> > > dbname=bijayant user=bijayant port=5432
> > > password=bijayant';
> > > >   init cluster (id = 1, comment = 'Node
> 1 -
> > > bijayant@192.168.99.23');
> > > >
> > > > # STORE NODE
> > > >   store node (id = 2, event node = 1,
> comment =
> > > 'Node 2 - bijayant@192.168.99.134');
> > > >   echo 'Set up replication nodes';
> > > >
> > > > # STORE PATH
> > > >   echo 'Next: configure paths for each
> > > node/origin';
> > > >   store path (server = 1, client = 2,
> conninfo =
> > > 'host=192.168.99.23 dbname=bijayant
> user=bijayant
> > > port=5432 password=bijayant');
> > > >   store path (server = 2, client = 1,
> conninfo =
> > > 'host=192.168.99.134 dbname=bijayant
> user=bijayant
> > > port=5432 password=bijayant');
> > > >   echo 'Replication nodes prepared';
> > > >   echo 'Please start a slon replication
> daemon for
> > > each node';
> > > >
> > > > After that i run the slonik daemon and got
> the error
> > > mentioned.
> > > >
> > > >
> > > > Bijayant Kumar
> > > >
> > > >
> > > > --- On Mon, 30/6/08, Martin Eriksson
> > > wrote:
> > > >
> > > >  
> > > >> From: Martin Eriksson
> > > 
> > > >> Subject: Re: [Slony1-general] Node is
> not
> > > initialized properly
> > > >> To: 
> > > >> Cc: slony1-general@lists.slony.info
> > > >> Date: Monday, 30 June, 2008, 7:18 PM
> > > >> Very basic, but saw no mention of it in
> the
> > > e-mail,
> > > >>
> > > >> I assume you ran the
> > > "slonik_init_cluster" before
> > > >> trying to start the 
> > > >> slon daemons? as that does the slony
> setup on each
> > > of the
> > > >> dbs
> > > >>
> > > >>
> > > >>
> > > >> bijayant kumar wrote:
> > > >>    
> > > >>> Hello list,
> > > >>>
> > > >>> I am a very new user of slony1 so
> please
> > > forgive me if
> > > >>>      
> > > >> i am asking very stupid question. 
> > > >>    
> > > >>> I have installed Postgresql and
> Slony1 on two
> > > gentoo
> > > >>>      
> > > >> machine. Postgresql is working fine no
> problem at
> > > all. I
> > > >> want to use Slony1 to replicate the two
> databases
> > > across
> > > >> the systems. To understand the slony1
> concept i
> > > made a 
> > > >>    
> > > >>> test database with one single table
> and only
> > > one entry
> > > >>>      
> > > >> into it. But when i start slony1 i get
> error like
> > > >>    
> > > >>> 2008-06-30 15:54:55 IST ERROR 
> cannot get
> > > >>>      
> > > >> sl_local_node_id - ERROR:  schema
> > > "_bijayant"
> > > >> does not exist
> > > >>    
> > > >>> 2008-06-30 15:54:55 IST FATAL  main:
> Node is
> > > not
> > > >>>      
> > > >> initialized properly - sleep 10s
> > > >>    
> > > >>> Now i am giving here my
> configuration details.
> > > >>>
> > > >>> /* Master Server */
> > > >>> IP Address 192.168.99.23
> > > >>> Database name bijayant
> > > >>> Table name kavach
> > > >>>
> > > >>> bijayant=# select * from kavach;
> > > >>>  id |   name   | designation |  
> address
> > > >>>
> ----+----------+-------------+-------------
> > > >>>   1 | Bijayant | consultant  |
> Lakkasandra
> > > >>> (1 row)
> > > >>>
> > > >>>
> > > >>> vi /etc/slon_tools.conf
> > > >>>
> > > >>> if ($ENV{"SLONYNODES"}) {
> > > >>>     require
> $ENV{"SLONYNODES"};
> > > >>> } else {
> > > >>>     $CLUSTER_NAME =
> 'bijayant';
> > > >>>     $LOGDIR =
> '/var/log/slony';
> > > >>>     $MASTERNODE = 1;
> > > >>>     add_node(node     => 1,
> > > >>>              host     =>
> > > '192.168.99.23',
> > > >>>              dbname   =>
> > > 'bijayant',
> > > >>>              port     => 5432,
> > > >>>              user     =>
> > > 'bijayant',
> > > >>>              password =>
> > > 'bijayant');
> > > >>>
> > > >>>     add_node(node     => 2,
> > > >>>              host     =>
> > > '192.168.99.134',
> > > >>>              dbname   =>
> > > 'bijayant',
> > > >>>              port     => 5432,
> > > >>>              user     =>
> > > 'bijayant',
> > > >>>              password =>
> > > 'bijayant',
> > > >>>              parent => 1
> > > >>>              );
> > > >>> }
> > > >>>
> > > >>> $SLONY_SETS = {
> > > >>>                 "set1"
> => {
> > > >>>                           
> "set_id"
> > > => 1,
> > > >>>                          
> > > "pkeyedtables"
> > > >>>      
> > > >> => [
> > > >>    
> > > >>>                          
> > > 'public.kavach',
> > > >>>                            ],
> > > >>>         },
> > > >>> if ($ENV{"SLONYSET"}) {
> > > >>>     require
> $ENV{"SLONYSET"};
> > > >>> }
> > > >>>
> > > >>> 1;
> > > >>>
> > > >>> vi /etc/conf.d/slony1
> > > >>> USER=postgres
> > > >>> CLUSTER=bijayant
> > > >>> DBUSER=bijayant
> > > >>> DBNAME=bijayant
> > > >>> DBHOST=192.168.99.23
> > > >>>
> LOGFILE=/var/lib/postgresql/data/slony1.log
> > > >>> LOGLEVEL=4
> > > >>>
> > > >>> /* On the Slave Server */
> > > >>>
> > > >>> IP Address 192.168.99.134
> > > >>> Database name bijayant
> > > >>> Table name kavach
> > > >>>
> > > >>> vi /etc/slon_tools.conf
> > > >>>
> > > >>> if ($ENV{"SLONYNODES"}) {
> > > >>>     require
> $ENV{"SLONYNODES"};
> > > >>> } else {
> > > >>>     $CLUSTER_NAME =
> 'bijayant';
> > > >>>     $LOGDIR =
> '/var/log/slony';
> > > >>>     # SYNC check interval (slon -s
> option)
> > > >>>     # $SYNC_CHECK_INTERVAL = 1000;
> > > >>>     $MASTERNODE = 1;
> > > >>>     add_node(node     => 1,
> > > >>>              host     =>
> > > '192.168.99.23',
> > > >>>              dbname   =>
> > > 'bijayant',
> > > >>>              port     => 5432,
> > > >>>              user     =>
> > > 'bijayant',
> > > >>>              password =>
> > > 'bijayant');
> > > >>>
> > > >>>     add_node(node     => 2,
> > > >>>              host     =>
> > > '192.168.99.134',
> > > >>>              dbname   =>
> > > 'bijayant',
> > > >>>              port     => 5432,
> > > >>>              user     =>
> > > 'bijayant',
> > > >>>              password =>
> > > 'bijayant',
> > > >>>              parent => 1
> > > >>>              );
> > > >>> }
> > > >>> $SLONY_SETS = {
> > > >>>     "set1" => {
> > > >>>                "set_id"
> => 1,
> > > >>>                
> "pkeyedtables" =>
> > > [
> > > >>>                                  
> > > >>>      
> > > >> 'public.kavach',
> > > >>    
> > > >>>                                   ],
> > > >>>               },
> > > >>> };
> > > >>>
> > > >>> if ($ENV{"SLONYSET"}) {
> > > >>>     require
> $ENV{"SLONYSET"};
> > > >>> }
> > > >>>
> > > >>> 1;
> > > >>>
> > > >>> vi /etc/conf.d/slony1
> > > >>> USER=postgres
> > > >>> CLUSTER=bijayant
> > > >>> DBUSER=bijayant
> > > >>> DBNAME=bijayant
> > > >>> DBHOST=192.168.99.23
> > > >>>
> LOGFILE=/var/lib/postgresql/data/slony1.log
> > > >>> LOGLEVEL=4 
> > > >>>
> > > >>> When i start the slony1
> /etc/init.d/slony1
> > > start on
> > > >>>      
> > > >> both the machine it generates lots of
> logs with
> > > errors line
> > > >> like
> > > >>    
> > > >>> 2008-06-30 15:54:55 IST DEBUG2
> slon_retry()
> > > from
> > > >>>      
> > > >> pid=19007
> > > >>    
> > > >>> 2008-06-30 15:54:55 IST DEBUG1 slon:
> retry
> > > requested
> > > >>> 2008-06-30 15:54:55 IST DEBUG2 slon:
> notify
> > > worker
> > > >>>      
> > > >> process to shutdown
> > > >>    
> > > >>> 2008-06-30 15:54:55 IST DEBUG2 slon:
> child
> > > terminated
> > > >>>      
> > > >> status: 0; pid: 19007, current worker
> pid: 19007
> > > >>    
> > > >>> 2008-06-30 15:54:55 IST DEBUG1 slon:
> restart
> > > of worker
> > > >>> 2008-06-30 15:54:55 IST CONFIG main:
> slon
> > > version
> > > >>>      
> > > >> 1.2.10 starting up
> > > >>    
> > > >>> 2008-06-30 15:54:55 IST DEBUG2 slon:
> watchdog
> > > process
> > > >>>      
> > > >> started
> > > >>    
> > > >>> 2008-06-30 15:54:55 IST DEBUG2 slon:
> watchdog
> > > ready -
> > > >>>      
> > > >> pid = 19005
> > > >>    
> > > >>> 2008-06-30 15:54:55 IST DEBUG2 slon:
> worker
> > > process
> > > >>>      
> > > >> created - pid = 19034
> > > >>    
> > > >>> 2008-06-30 15:54:55 IST ERROR 
> cannot get
> > > >>>      
> > > >> sl_local_node_id - ERROR:  schema
> > > "_bijayant"
> > > >> does not exist
> > > >>    
> > > >>> 2008-06-30 15:54:55 IST FATAL  main:
> Node is
> > > not
> > > >>>      
> > > >> initialized properly - sleep 10s
> > > >>    
> > > >>> I am sure that i am not
> understanding some
> > > basic
> > > >>>      
> > > >> things about the slony1. Can anybody
> help me to
> > > understand
> > > >> the logic, i will be very helpful for
> you all.
> > > Please tell
> > > >> me what i am doing wrong here, what
> should i do. I
> > > have
> > > >> read the documentation at the website
> but not able
> > > to
> > > >> understand fully. Please help me out.
> > > >>    
> > > >>> Thanks & Regards,
> > > >>> Bijayant Kumar
> > > >>>
> > > >>> Send instant messages to your online
> friends
> > > >>>      
> > > >> http://uk.messenger.yahoo.com 
> > > >>    
> > > >>>
> > > _______________________________________________
> > > >>> Slony1-general mailing list
> > > >>> Slony1-general@lists.slony.info
> > > >>>
> > > >>>      
> > > >>
> > >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > > >>    
> > > >>>  
> > > >>>      
> > > >>
> _______________________________________________
> > > >> Slony1-general mailing list
> > > >> Slony1-general@lists.slony.info
> > > >>
> > >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > > >>    
> > > >
> > > > Send instant messages to your online friends
> > > http://uk.messenger.yahoo.com 
> > > >  
> > > 
> > > _______________________________________________
> > > Slony1-general mailing list
> > > Slony1-general@lists.slony.info
> > >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > 
> > Send instant messages to your online friends
> http://uk.messenger.yahoo.com
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general@lists.slony.info
> >
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 
> 
>      
> __________________________________________________________
> Not happy with your email address?.
> Get the one you really want - millions of new email
> addresses available now at Yahoo!
> http://uk.docs.yahoo.com/ymail/new.html

Send instant messages to your online friends http://uk.messenger.yahoo.com 
From liobod.slony at gmail.com  Tue Jul  1 02:50:29 2008
From: liobod.slony at gmail.com (lio bod)
Date: Tue Jul  1 02:50:57 2008
Subject: [Slony1-general] fail over failed
Message-ID: <d4f444290807010250s3b5a238ay8735a4a59441e04a@mail.gmail.com>

Hello world,

It has been a long time that i didn' ask a hand from you clever guys.

My aim is to resubscribe a brand new base to a promoted master after a
fail-over.
Btw, any hint, link on this topic is welcome.

The thing i'm facing is that my fail-over does not even work (it used to be
in my last tests in previous life).

Here's what happens :

$> slonik my_fail_script.slonik
my_fail_script.slonik:8: PGRES_FATAL_ERROR select
"_my_cluster".failedNode(1, 2);  - ERROR:  Slony-I: cannot failover -
subscription for set 1 is not active

Subscription for set 1 is not active? My replication seems ok. At least it
replicates...
So i don't understand the message
What could be wrong on this fail over?

thx,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080701/=
7f632181/attachment.htm
From scetbon at echo.fr  Tue Jul  1 03:01:20 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Tue Jul  1 03:01:52 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from locking all
	tables
Message-ID: <486A0070.2060600@echo.fr>

Hi,

We got a cluster configuration with something like 500 tables. Sometimes 
we have to add columns to tables. Today we're using execute script which 
locks all tables and not only the tables that belong to the set 
specified in this command. We have a lot of DML statement and these 
locks are really disturbing our application.

Are there other possibilities of using EXECUTE SCRIPT ? Is there a bad 
and a good use of this command ?

Thanks
-- 
Cyril SCETBON
From bijayant4u at yahoo.com  Tue Jul  1 05:45:11 2008
From: bijayant4u at yahoo.com (bijayant kumar)
Date: Tue Jul  1 05:45:14 2008
Subject: [Slony1-general] Node is not initialized properly
In-Reply-To: <866360.89616.qm@web32702.mail.mud.yahoo.com>
Message-ID: <663079.27316.qm@web32707.mail.mud.yahoo.com>

Hello to list,

My this problem is solved now. I have re-installed postgresql and slony1 and the problem gone. Now i have another problem. When i run slony on Master Server(192.168.99.23), i can see in the logs

2008-07-01 18:09:50 IST CONFIG enableNode: no_id=2
2008-07-01 18:09:50 IST DEBUG1 remoteWorkerThread_2: thread starts
2008-07-01 18:09:50 IST DEBUG1 remoteListenThread_2: thread starts
2008-07-01 18:09:50 IST DEBUG1 main: running scheduler mainloop
2008-07-01 18:09:50 IST DEBUG1 cleanupThread: thread starts
2008-07-01 18:09:50 IST DEBUG1 syncThread: thread starts
2008-07-01 18:09:50 IST DEBUG1 remoteListenThread_2: connected to 'host=192.168.99.134 dbname=bijayant user=bijayant port=5432 password=bijayant'

But when on second node that is on Slave server(192.168.99.134), i run slony i got

2008-07-01 18:03:07 IST FATAL  Do you already have a slon running against this node?
2008-07-01 18:03:07 IST FATAL  Or perhaps a residual idle backend connection from a dead slon?
2008-07-01 18:03:07 IST DEBUG2 slon_abort() from pid=7871
2008-07-01 18:03:07 IST DEBUG1 slon: shutdown requested
2008-07-01 18:03:07 IST DEBUG2 slon: notify worker process to shutdown
2008-07-01 18:03:27 IST DEBUG1 slon: child termination timeout - kill child
2008-07-01 18:03:27 IST DEBUG2 slon: child terminated status: 9; pid: 7871, current worker pid: 7871
2008-07-01 18:03:27 IST DEBUG1 slon: done
2008-07-01 18:03:27 IST DEBUG2 slon: remove pid file
2008-07-01 18:03:27 IST DEBUG2 slon: exit(0)


I have a exact same configuration(like xerox copy) on both the server. Is it creating the problem? Should i have to change the parameters(/etc/slon_tools.conf) according to Master/Slave.

Please help me, i think i am very near to replicate my first ever database.

Bijayant Kumar


--- On Tue, 1/7/08, bijayant kumar <bijayant4u@yahoo.com> wrote:

> From: bijayant kumar <bijayant4u@yahoo.com>
> Subject: Re: [Slony1-general] Node is not initialized properly
> To: "Martin Eriksson" <m.eriksson@albourne.com>, "Glyn Astill" <glynastill@yahoo.co.uk>
> Cc: slony1-general@lists.slony.info
> Date: Tuesday, 1 July, 2008, 3:05 PM
> Hi,
> The user "bijayant" is a super user. I have
> created this user like
> 
> postgres@bijayant ~ $ createuser bijayant
> Shall the new user be allowed to create databases? (y/n) y
> Shall the new user be allowed to create more new users?
> (y/n) y
> CREATE USER
> 
> It means that bijayant is a super user right? Or i am doing
> some silly thing. If i will use user as pgsql than what to
> use for password.
> 
> bijayant ~ # pg_config --libdir
> /usr/lib
> 
> bijayant ~ # pg_config --pkglibdir
> /usr/lib/postgresql
> 
> In the Master Database server the xxid.so file is in 
> /usr/local/pgsql/lib/xxid.so. I have copied to this file to
> /usr/lib/postgresql/ and /usr/lib/ also. But no luck.
> 
> On the Slave Server, its in /usr/lib64/postgresql/xxid.so.
> 
> Is it creating the problem. How to resolve this problem.
> I have installed postgres and slony1 by the emerge utility
> of gentoo.
> 
> Thanks & Regards,
> 
> Bijayant Kumar
> 
> 
> --- On Tue, 1/7/08, Glyn Astill
> <glynastill@yahoo.co.uk> wrote:
> 
> > From: Glyn Astill <glynastill@yahoo.co.uk>
> > Subject: Re: [Slony1-general] Node is not initialized
> properly
> > To: bijayant4u@yahoo.com, "Martin Eriksson"
> <m.eriksson@albourne.com>
> > Cc: slony1-general@lists.slony.info
> > Date: Tuesday, 1 July, 2008, 2:22 PM
> > I'd be checking to see where xxid.so was on the
> system,
> > and if I had an older version lurking somewhere.
> > 
> > I'd also make sure bijayant was a database
> superuser.
> > 
> > 
> > 
> > ----- Original Message ----
> > > From: bijayant kumar <bijayant4u@yahoo.com>
> > > To: Martin Eriksson
> <m.eriksson@albourne.com>
> > > Cc: slony1-general@lists.slony.info
> > > Sent: Tuesday, 1 July, 2008 8:00:19 AM
> > > Subject: Re: [Slony1-general] Node is not
> initialized
> > properly
> > > 
> > > Thanks to all for the reply. Now i think that i
> am
> > coming closer to run slonik. 
> > > When i did as suggested by all
> > > 
> > > bijayant ~ # slonik_init_cluster --config
> > /etc/slon_tools.conf | slonik
> > > 
> > > :6: PGRES_FATAL_ERROR load
> '$libdir/xxid';  -
> > ERROR:  could not access 
> > > file "$libdir/xxid": No such file or
> > directory
> > > 
> > > :6: Error: the extension for the xxid data type
> cannot
> > be loaded in 
> > > database 'host=192.168.99.23 dbname=bijayant
> > user=bijayant port=5432 
> > > password=bijayant'
> > > 
> > > :6: ERROR: no admin conninfo for node 134598992
> > > 
> > > The first two lines of the error i couldnot
> > understand.
> > > The "admin conninfo" error, i think
> this
> > parameter should be present in conf 
> > > file, right? But in my case what should be there,
> as i
> > am already giving the 
> > > username and password to connect to the
> postgresql
> > database.
> > > 
> > > In this problem thread one gentleman has asked me
> to
> > check whether "_bijayant" 
> > > schema is created or not? I am sorry to ask but i
> > really dont know about this. I 
> > > have only created a user,database and table in
> the
> > postgresql database nothing 
> > > else. When we create database, schema is also
> created
> > automatically, right?
> > > 
> > > Please suggest me what should i do next. Sorry
> but i
> > am very new to slonik and 
> > > database. I am trying hard to understand the
> concept
> > > 
> > > Thanks & Regards,
> > > 
> > > Bijayant Kumar
> > > 
> > > 
> > > --- On Mon, 30/6/08, Martin Eriksson wrote:
> > > 
> > > > From: Martin Eriksson 
> > > > Subject: Re: [Slony1-general] Node is not
> > initialized properly
> > > > To: 
> > > > Cc: slony1-general@lists.slony.info
> > > > Date: Monday, 30 June, 2008, 8:34 PM
> > > > If you only run it like that, it only prints
> what
> > it will
> > > > execute, to 
> > > > actually execute this you need to | it to
> the
> > slonik app..
> > > > 
> > > > eg. /data/pgsql/slony/slonik_init_cluster
> > --config
> > > > | 
> > > > /data/pgsql/bin/slonik
> > > > 
> > > > all scripts starting with
> "slonik_"
> > doesn't
> > > > actually do anything 
> > > > themself, its just a way to format a command
> > correctly for
> > > > the slonik 
> > > > parser, which actually does the work..
> > > > 
> > > > 
> > > > 
> > > > bijayant kumar wrote:
> > > > > Thanks for the reply. I executed the
> command
> > > > "slonik_init_cluster".It gives the
> > output like
> > > > >
> > > > > # INIT CLUSTER
> > > > > cluster name = bijayant;
> > > > >  node 1 admin
> > conninfo='host=192.168.99.23
> > > > dbname=bijayant user=bijayant port=5432
> > > > password=bijayant';
> > > > >  node 2 admin
> > conninfo='host=192.168.99.134
> > > > dbname=bijayant user=bijayant port=5432
> > > > password=bijayant';
> > > > >   init cluster (id = 1, comment =
> 'Node
> > 1 -
> > > > bijayant@192.168.99.23');
> > > > >
> > > > > # STORE NODE
> > > > >   store node (id = 2, event node = 1,
> > comment =
> > > > 'Node 2 - bijayant@192.168.99.134');
> > > > >   echo 'Set up replication
> nodes';
> > > > >
> > > > > # STORE PATH
> > > > >   echo 'Next: configure paths for
> each
> > > > node/origin';
> > > > >   store path (server = 1, client = 2,
> > conninfo =
> > > > 'host=192.168.99.23 dbname=bijayant
> > user=bijayant
> > > > port=5432 password=bijayant');
> > > > >   store path (server = 2, client = 1,
> > conninfo =
> > > > 'host=192.168.99.134 dbname=bijayant
> > user=bijayant
> > > > port=5432 password=bijayant');
> > > > >   echo 'Replication nodes
> prepared';
> > > > >   echo 'Please start a slon
> replication
> > daemon for
> > > > each node';
> > > > >
> > > > > After that i run the slonik daemon and
> got
> > the error
> > > > mentioned.
> > > > >
> > > > >
> > > > > Bijayant Kumar
> > > > >
> > > > >
> > > > > --- On Mon, 30/6/08, Martin Eriksson
> > > > wrote:
> > > > >
> > > > >  
> > > > >> From: Martin Eriksson
> > > > 
> > > > >> Subject: Re: [Slony1-general] Node
> is
> > not
> > > > initialized properly
> > > > >> To: 
> > > > >> Cc: slony1-general@lists.slony.info
> > > > >> Date: Monday, 30 June, 2008, 7:18
> PM
> > > > >> Very basic, but saw no mention of
> it in
> > the
> > > > e-mail,
> > > > >>
> > > > >> I assume you ran the
> > > > "slonik_init_cluster" before
> > > > >> trying to start the 
> > > > >> slon daemons? as that does the
> slony
> > setup on each
> > > > of the
> > > > >> dbs
> > > > >>
> > > > >>
> > > > >>
> > > > >> bijayant kumar wrote:
> > > > >>    
> > > > >>> Hello list,
> > > > >>>
> > > > >>> I am a very new user of slony1
> so
> > please
> > > > forgive me if
> > > > >>>      
> > > > >> i am asking very stupid question. 
> > > > >>    
> > > > >>> I have installed Postgresql and
> > Slony1 on two
> > > > gentoo
> > > > >>>      
> > > > >> machine. Postgresql is working fine
> no
> > problem at
> > > > all. I
> > > > >> want to use Slony1 to replicate the
> two
> > databases
> > > > across
> > > > >> the systems. To understand the
> slony1
> > concept i
> > > > made a 
> > > > >>    
> > > > >>> test database with one single
> table
> > and only
> > > > one entry
> > > > >>>      
> > > > >> into it. But when i start slony1 i
> get
> > error like
> > > > >>    
> > > > >>> 2008-06-30 15:54:55 IST ERROR 
> > cannot get
> > > > >>>      
> > > > >> sl_local_node_id - ERROR:  schema
> > > > "_bijayant"
> > > > >> does not exist
> > > > >>    
> > > > >>> 2008-06-30 15:54:55 IST FATAL 
> main:
> > Node is
> > > > not
> > > > >>>      
> > > > >> initialized properly - sleep 10s
> > > > >>    
> > > > >>> Now i am giving here my
> > configuration details.
> > > > >>>
> > > > >>> /* Master Server */
> > > > >>> IP Address 192.168.99.23
> > > > >>> Database name bijayant
> > > > >>> Table name kavach
> > > > >>>
> > > > >>> bijayant=# select * from
> kavach;
> > > > >>>  id |   name   | designation | 
> 
> > address
> > > > >>>
> > ----+----------+-------------+-------------
> > > > >>>   1 | Bijayant | consultant  |
> > Lakkasandra
> > > > >>> (1 row)
> > > > >>>
> > > > >>>
> > > > >>> vi /etc/slon_tools.conf
> > > > >>>
> > > > >>> if
> ($ENV{"SLONYNODES"}) {
> > > > >>>     require
> > $ENV{"SLONYNODES"};
> > > > >>> } else {
> > > > >>>     $CLUSTER_NAME =
> > 'bijayant';
> > > > >>>     $LOGDIR =
> > '/var/log/slony';
> > > > >>>     $MASTERNODE = 1;
> > > > >>>     add_node(node     => 1,
> > > > >>>              host     =>
> > > > '192.168.99.23',
> > > > >>>              dbname   =>
> > > > 'bijayant',
> > > > >>>              port     =>
> 5432,
> > > > >>>              user     =>
> > > > 'bijayant',
> > > > >>>              password =>
> > > > 'bijayant');
> > > > >>>
> > > > >>>     add_node(node     => 2,
> > > > >>>              host     =>
> > > > '192.168.99.134',
> > > > >>>              dbname   =>
> > > > 'bijayant',
> > > > >>>              port     =>
> 5432,
> > > > >>>              user     =>
> > > > 'bijayant',
> > > > >>>              password =>
> > > > 'bijayant',
> > > > >>>              parent => 1
> > > > >>>              );
> > > > >>> }
> > > > >>>
> > > > >>> $SLONY_SETS = {
> > > > >>>                
> "set1"
> > => {
> > > > >>>                           
> > "set_id"
> > > > => 1,
> > > > >>>                          
> > > > "pkeyedtables"
> > > > >>>      
> > > > >> => [
> > > > >>    
> > > > >>>                          
> > > > 'public.kavach',
> > > > >>>                            ],
> > > > >>>         },
> > > > >>> if ($ENV{"SLONYSET"})
> {
> > > > >>>     require
> > $ENV{"SLONYSET"};
> > > > >>> }
> > > > >>>
> > > > >>> 1;
> > > > >>>
> > > > >>> vi /etc/conf.d/slony1
> > > > >>> USER=postgres
> > > > >>> CLUSTER=bijayant
> > > > >>> DBUSER=bijayant
> > > > >>> DBNAME=bijayant
> > > > >>> DBHOST=192.168.99.23
> > > > >>>
> > LOGFILE=/var/lib/postgresql/data/slony1.log
> > > > >>> LOGLEVEL=4
> > > > >>>
> > > > >>> /* On the Slave Server */
> > > > >>>
> > > > >>> IP Address 192.168.99.134
> > > > >>> Database name bijayant
> > > > >>> Table name kavach
> > > > >>>
> > > > >>> vi /etc/slon_tools.conf
> > > > >>>
> > > > >>> if
> ($ENV{"SLONYNODES"}) {
> > > > >>>     require
> > $ENV{"SLONYNODES"};
> > > > >>> } else {
> > > > >>>     $CLUSTER_NAME =
> > 'bijayant';
> > > > >>>     $LOGDIR =
> > '/var/log/slony';
> > > > >>>     # SYNC check interval (slon
> -s
> > option)
> > > > >>>     # $SYNC_CHECK_INTERVAL =
> 1000;
> > > > >>>     $MASTERNODE = 1;
> > > > >>>     add_node(node     => 1,
> > > > >>>              host     =>
> > > > '192.168.99.23',
> > > > >>>              dbname   =>
> > > > 'bijayant',
> > > > >>>              port     =>
> 5432,
> > > > >>>              user     =>
> > > > 'bijayant',
> > > > >>>              password =>
> > > > 'bijayant');
> > > > >>>
> > > > >>>     add_node(node     => 2,
> > > > >>>              host     =>
> > > > '192.168.99.134',
> > > > >>>              dbname   =>
> > > > 'bijayant',
> > > > >>>              port     =>
> 5432,
> > > > >>>              user     =>
> > > > 'bijayant',
> > > > >>>              password =>
> > > > 'bijayant',
> > > > >>>              parent => 1
> > > > >>>              );
> > > > >>> }
> > > > >>> $SLONY_SETS = {
> > > > >>>     "set1" => {
> > > > >>>               
> "set_id"
> > => 1,
> > > > >>>                
> > "pkeyedtables" =>
> > > > [
> > > > >>>                                
>  
> > > > >>>      
> > > > >> 'public.kavach',
> > > > >>    
> > > > >>>                                
>   ],
> > > > >>>               },
> > > > >>> };
> > > > >>>
> > > > >>> if ($ENV{"SLONYSET"})
> {
> > > > >>>     require
> > $ENV{"SLONYSET"};
> > > > >>> }
> > > > >>>
> > > > >>> 1;
> > > > >>>
> > > > >>> vi /etc/conf.d/slony1
> > > > >>> USER=postgres
> > > > >>> CLUSTER=bijayant
> > > > >>> DBUSER=bijayant
> > > > >>> DBNAME=bijayant
> > > > >>> DBHOST=192.168.99.23
> > > > >>>
> > LOGFILE=/var/lib/postgresql/data/slony1.log
> > > > >>> LOGLEVEL=4 
> > > > >>>
> > > > >>> When i start the slony1
> > /etc/init.d/slony1
> > > > start on
> > > > >>>      
> > > > >> both the machine it generates lots
> of
> > logs with
> > > > errors line
> > > > >> like
> > > > >>    
> > > > >>> 2008-06-30 15:54:55 IST DEBUG2
> > slon_retry()
> > > > from
> > > > >>>      
> > > > >> pid=19007
> > > > >>    
> > > > >>> 2008-06-30 15:54:55 IST DEBUG1
> slon:
> > retry
> > > > requested
> > > > >>> 2008-06-30 15:54:55 IST DEBUG2
> slon:
> > notify
> > > > worker
> > > > >>>      
> > > > >> process to shutdown
> > > > >>    
> > > > >>> 2008-06-30 15:54:55 IST DEBUG2
> slon:
> > child
> > > > terminated
> > > > >>>      
> > > > >> status: 0; pid: 19007, current
> worker
> > pid: 19007
> > > > >>    
> > > > >>> 2008-06-30 15:54:55 IST DEBUG1
> slon:
> > restart
> > > > of worker
> > > > >>> 2008-06-30 15:54:55 IST CONFIG
> main:
> > slon
> > > > version
> > > > >>>      
> > > > >> 1.2.10 starting up
> > > > >>    
> > > > >>> 2008-06-30 15:54:55 IST DEBUG2
> slon:
> > watchdog
> > > > process
> > > > >>>      
> > > > >> started
> > > > >>    
> > > > >>> 2008-06-30 15:54:55 IST DEBUG2
> slon:
> > watchdog
> > > > ready -
> > > > >>>      
> > > > >> pid = 19005
> > > > >>    
> > > > >>> 2008-06-30 15:54:55 IST DEBUG2
> slon:
> > worker
> > > > process
> > > > >>>      
> > > > >> created - pid = 19034
> > > > >>    
> > > > >>> 2008-06-30 15:54:55 IST ERROR 
> > cannot get
> > > > >>>      
> > > > >> sl_local_node_id - ERROR:  schema
> > > > "_bijayant"
> > > > >> does not exist
> > > > >>    
> > > > >>> 2008-06-30 15:54:55 IST FATAL 
> main:
> > Node is
> > > > not
> > > > >>>      
> > > > >> initialized properly - sleep 10s
> > > > >>    
> > > > >>> I am sure that i am not
> > understanding some
> > > > basic
> > > > >>>      
> > > > >> things about the slony1. Can
> anybody
> > help me to
> > > > understand
> > > > >> the logic, i will be very helpful
> for
> > you all.
> > > > Please tell
> > > > >> me what i am doing wrong here, what
> > should i do. I
> > > > have
> > > > >> read the documentation at the
> website
> > but not able
> > > > to
> > > > >> understand fully. Please help me
> out.
> > > > >>    
> > > > >>> Thanks & Regards,
> > > > >>> Bijayant Kumar
> > > > >>>
> > > > >>> Send instant messages to your
> online
> > friends
> > > > >>>      
> > > > >> http://uk.messenger.yahoo.com 
> > > > >>    
> > > > >>>
> > > >
> _______________________________________________
> > > > >>> Slony1-general mailing list
> > > > >>> Slony1-general@lists.slony.info
> > > > >>>
> > > > >>>      
> > > > >>
> > > >
> >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > > > >>    
> > > > >>>  
> > > > >>>      
> > > > >>
> > _______________________________________________
> > > > >> Slony1-general mailing list
> > > > >> Slony1-general@lists.slony.info
> > > > >>
> > > >
> >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > > > >>    
> > > > >
> > > > > Send instant messages to your online
> friends
> > > > http://uk.messenger.yahoo.com 
> > > > >  
> > > > 
> > > >
> _______________________________________________
> > > > Slony1-general mailing list
> > > > Slony1-general@lists.slony.info
> > > >
> >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > > 
> > > Send instant messages to your online friends
> > http://uk.messenger.yahoo.com
> > > _______________________________________________
> > > Slony1-general mailing list
> > > Slony1-general@lists.slony.info
> > >
> >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > 
> > 
> > 
> >      
> >
> __________________________________________________________
> > Not happy with your email address?.
> > Get the one you really want - millions of new email
> > addresses available now at Yahoo!
> > http://uk.docs.yahoo.com/ymail/new.html
> 
> Send instant messages to your online friends
> http://uk.messenger.yahoo.com
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

Send instant messages to your online friends http://uk.messenger.yahoo.com 
From glynastill at yahoo.co.uk  Tue Jul  1 05:56:00 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue Jul  1 05:56:05 2008
Subject: [Slony1-general] Node is not initialized properly
Message-ID: <566046.29997.qm@web25807.mail.ukl.yahoo.com>

Check what slons are running against each database. You should have a slon running for each machine, so if you are running one slon on each server you should have the conninfo parameter configured in slon.conf, or be passing it at the command line. A ps -ax should show what server the slons are acting on.



----- Original Message ----
> From: bijayant kumar <bijayant4u@yahoo.com>
> To: Martin Eriksson <m.eriksson@albourne.com>; Glyn Astill <glynastill@yahoo.co.uk>
> Cc: slony1-general@lists.slony.info
> Sent: Tuesday, 1 July, 2008 1:45:11 PM
> Subject: Re: [Slony1-general] Node is not initialized properly
> 
> Hello to list,
> 
> My this problem is solved now. I have re-installed postgresql and slony1 and the 
> problem gone. Now i have another problem. When i run slony on Master 
> Server(192.168.99.23), i can see in the logs
> 
> 2008-07-01 18:09:50 IST CONFIG enableNode: no_id=2
> 2008-07-01 18:09:50 IST DEBUG1 remoteWorkerThread_2: thread starts
> 2008-07-01 18:09:50 IST DEBUG1 remoteListenThread_2: thread starts
> 2008-07-01 18:09:50 IST DEBUG1 main: running scheduler mainloop
> 2008-07-01 18:09:50 IST DEBUG1 cleanupThread: thread starts
> 2008-07-01 18:09:50 IST DEBUG1 syncThread: thread starts
> 2008-07-01 18:09:50 IST DEBUG1 remoteListenThread_2: connected to 
> 'host=192.168.99.134 dbname=bijayant user=bijayant port=5432 password=bijayant'
> 
> But when on second node that is on Slave server(192.168.99.134), i run slony i 
> got
> 
> 2008-07-01 18:03:07 IST FATAL  Do you already have a slon running against this 
> node?
> 2008-07-01 18:03:07 IST FATAL  Or perhaps a residual idle backend connection 
> from a dead slon?
> 2008-07-01 18:03:07 IST DEBUG2 slon_abort() from pid=7871
> 2008-07-01 18:03:07 IST DEBUG1 slon: shutdown requested
> 2008-07-01 18:03:07 IST DEBUG2 slon: notify worker process to shutdown
> 2008-07-01 18:03:27 IST DEBUG1 slon: child termination timeout - kill child
> 2008-07-01 18:03:27 IST DEBUG2 slon: child terminated status: 9; pid: 7871, 
> current worker pid: 7871
> 2008-07-01 18:03:27 IST DEBUG1 slon: done
> 2008-07-01 18:03:27 IST DEBUG2 slon: remove pid file
> 2008-07-01 18:03:27 IST DEBUG2 slon: exit(0)
> 
> 
> I have a exact same configuration(like xerox copy) on both the server. Is it 
> creating the problem? Should i have to change the 
> parameters(/etc/slon_tools.conf) according to Master/Slave.
> 
> Please help me, i think i am very near to replicate my first ever database.
> 
> Bijayant Kumar
> 
> 
> --- On Tue, 1/7/08, bijayant kumar wrote:
> 
> > From: bijayant kumar 
> > Subject: Re: [Slony1-general] Node is not initialized properly
> > To: "Martin Eriksson" , "Glyn Astill" 
> 
> > Cc: slony1-general@lists.slony.info
> > Date: Tuesday, 1 July, 2008, 3:05 PM
> > Hi,
> > The user "bijayant" is a super user. I have
> > created this user like
> > 
> > postgres@bijayant ~ $ createuser bijayant
> > Shall the new user be allowed to create databases? (y/n) y
> > Shall the new user be allowed to create more new users?
> > (y/n) y
> > CREATE USER
> > 
> > It means that bijayant is a super user right? Or i am doing
> > some silly thing. If i will use user as pgsql than what to
> > use for password.
> > 
> > bijayant ~ # pg_config --libdir
> > /usr/lib
> > 
> > bijayant ~ # pg_config --pkglibdir
> > /usr/lib/postgresql
> > 
> > In the Master Database server the xxid.so file is in 
> > /usr/local/pgsql/lib/xxid.so. I have copied to this file to
> > /usr/lib/postgresql/ and /usr/lib/ also. But no luck.
> > 
> > On the Slave Server, its in /usr/lib64/postgresql/xxid.so.
> > 
> > Is it creating the problem. How to resolve this problem.
> > I have installed postgres and slony1 by the emerge utility
> > of gentoo.
> > 
> > Thanks & Regards,
> > 
> > Bijayant Kumar
> > 
> > 
> > --- On Tue, 1/7/08, Glyn Astill
> > wrote:
> > 
> > > From: Glyn Astill 
> > > Subject: Re: [Slony1-general] Node is not initialized
> > properly
> > > To: bijayant4u@yahoo.com, "Martin Eriksson"
> > 
> > > Cc: slony1-general@lists.slony.info
> > > Date: Tuesday, 1 July, 2008, 2:22 PM
> > > I'd be checking to see where xxid.so was on the
> > system,
> > > and if I had an older version lurking somewhere.
> > > 
> > > I'd also make sure bijayant was a database
> > superuser.
> > > 
> > > 
> > > 
> > > ----- Original Message ----
> > > > From: bijayant kumar 
> > > > To: Martin Eriksson
> > 
> > > > Cc: slony1-general@lists.slony.info
> > > > Sent: Tuesday, 1 July, 2008 8:00:19 AM
> > > > Subject: Re: [Slony1-general] Node is not
> > initialized
> > > properly
> > > > 
> > > > Thanks to all for the reply. Now i think that i
> > am
> > > coming closer to run slonik. 
> > > > When i did as suggested by all
> > > > 
> > > > bijayant ~ # slonik_init_cluster --config
> > > /etc/slon_tools.conf | slonik
> > > > 
> > > > :6: PGRES_FATAL_ERROR load
> > '$libdir/xxid';  -
> > > ERROR:  could not access 
> > > > file "$libdir/xxid": No such file or
> > > directory
> > > > 
> > > > :6: Error: the extension for the xxid data type
> > cannot
> > > be loaded in 
> > > > database 'host=192.168.99.23 dbname=bijayant
> > > user=bijayant port=5432 
> > > > password=bijayant'
> > > > 
> > > > :6: ERROR: no admin conninfo for node 134598992
> > > > 
> > > > The first two lines of the error i couldnot
> > > understand.
> > > > The "admin conninfo" error, i think
> > this
> > > parameter should be present in conf 
> > > > file, right? But in my case what should be there,
> > as i
> > > am already giving the 
> > > > username and password to connect to the
> > postgresql
> > > database.
> > > > 
> > > > In this problem thread one gentleman has asked me
> > to
> > > check whether "_bijayant" 
> > > > schema is created or not? I am sorry to ask but i
> > > really dont know about this. I 
> > > > have only created a user,database and table in
> > the
> > > postgresql database nothing 
> > > > else. When we create database, schema is also
> > created
> > > automatically, right?
> > > > 
> > > > Please suggest me what should i do next. Sorry
> > but i
> > > am very new to slonik and 
> > > > database. I am trying hard to understand the
> > concept
> > > > 
> > > > Thanks & Regards,
> > > > 
> > > > Bijayant Kumar
> > > > 
> > > > 
> > > > --- On Mon, 30/6/08, Martin Eriksson wrote:
> > > > 
> > > > > From: Martin Eriksson 
> > > > > Subject: Re: [Slony1-general] Node is not
> > > initialized properly
> > > > > To: 
> > > > > Cc: slony1-general@lists.slony.info
> > > > > Date: Monday, 30 June, 2008, 8:34 PM
> > > > > If you only run it like that, it only prints
> > what
> > > it will
> > > > > execute, to 
> > > > > actually execute this you need to | it to
> > the
> > > slonik app..
> > > > > 
> > > > > eg. /data/pgsql/slony/slonik_init_cluster
> > > --config
> > > > > | 
> > > > > /data/pgsql/bin/slonik
> > > > > 
> > > > > all scripts starting with
> > "slonik_"
> > > doesn't
> > > > > actually do anything 
> > > > > themself, its just a way to format a command
> > > correctly for
> > > > > the slonik 
> > > > > parser, which actually does the work..
> > > > > 
> > > > > 
> > > > > 
> > > > > bijayant kumar wrote:
> > > > > > Thanks for the reply. I executed the
> > command
> > > > > "slonik_init_cluster".It gives the
> > > output like
> > > > > >
> > > > > > # INIT CLUSTER
> > > > > > cluster name = bijayant;
> > > > > >  node 1 admin
> > > conninfo='host=192.168.99.23
> > > > > dbname=bijayant user=bijayant port=5432
> > > > > password=bijayant';
> > > > > >  node 2 admin
> > > conninfo='host=192.168.99.134
> > > > > dbname=bijayant user=bijayant port=5432
> > > > > password=bijayant';
> > > > > >   init cluster (id = 1, comment =
> > 'Node
> > > 1 -
> > > > > bijayant@192.168.99.23');
> > > > > >
> > > > > > # STORE NODE
> > > > > >   store node (id = 2, event node = 1,
> > > comment =
> > > > > 'Node 2 - bijayant@192.168.99.134');
> > > > > >   echo 'Set up replication
> > nodes';
> > > > > >
> > > > > > # STORE PATH
> > > > > >   echo 'Next: configure paths for
> > each
> > > > > node/origin';
> > > > > >   store path (server = 1, client = 2,
> > > conninfo =
> > > > > 'host=192.168.99.23 dbname=bijayant
> > > user=bijayant
> > > > > port=5432 password=bijayant');
> > > > > >   store path (server = 2, client = 1,
> > > conninfo =
> > > > > 'host=192.168.99.134 dbname=bijayant
> > > user=bijayant
> > > > > port=5432 password=bijayant');
> > > > > >   echo 'Replication nodes
> > prepared';
> > > > > >   echo 'Please start a slon
> > replication
> > > daemon for
> > > > > each node';
> > > > > >
> > > > > > After that i run the slonik daemon and
> > got
> > > the error
> > > > > mentioned.
> > > > > >
> > > > > >
> > > > > > Bijayant Kumar
> > > > > >
> > > > > >
> > > > > > --- On Mon, 30/6/08, Martin Eriksson
> > > > > wrote:
> > > > > >
> > > > > >  
> > > > > >> From: Martin Eriksson
> > > > > 
> > > > > >> Subject: Re: [Slony1-general] Node
> > is
> > > not
> > > > > initialized properly
> > > > > >> To: 
> > > > > >> Cc: slony1-general@lists.slony.info
> > > > > >> Date: Monday, 30 June, 2008, 7:18
> > PM
> > > > > >> Very basic, but saw no mention of
> > it in
> > > the
> > > > > e-mail,
> > > > > >>
> > > > > >> I assume you ran the
> > > > > "slonik_init_cluster" before
> > > > > >> trying to start the 
> > > > > >> slon daemons? as that does the
> > slony
> > > setup on each
> > > > > of the
> > > > > >> dbs
> > > > > >>
> > > > > >>
> > > > > >>
> > > > > >> bijayant kumar wrote:
> > > > > >>    
> > > > > >>> Hello list,
> > > > > >>>
> > > > > >>> I am a very new user of slony1
> > so
> > > please
> > > > > forgive me if
> > > > > >>>      
> > > > > >> i am asking very stupid question. 
> > > > > >>    
> > > > > >>> I have installed Postgresql and
> > > Slony1 on two
> > > > > gentoo
> > > > > >>>      
> > > > > >> machine. Postgresql is working fine
> > no
> > > problem at
> > > > > all. I
> > > > > >> want to use Slony1 to replicate the
> > two
> > > databases
> > > > > across
> > > > > >> the systems. To understand the
> > slony1
> > > concept i
> > > > > made a 
> > > > > >>    
> > > > > >>> test database with one single
> > table
> > > and only
> > > > > one entry
> > > > > >>>      
> > > > > >> into it. But when i start slony1 i
> > get
> > > error like
> > > > > >>    
> > > > > >>> 2008-06-30 15:54:55 IST ERROR 
> > > cannot get
> > > > > >>>      
> > > > > >> sl_local_node_id - ERROR:  schema
> > > > > "_bijayant"
> > > > > >> does not exist
> > > > > >>    
> > > > > >>> 2008-06-30 15:54:55 IST FATAL 
> > main:
> > > Node is
> > > > > not
> > > > > >>>      
> > > > > >> initialized properly - sleep 10s
> > > > > >>    
> > > > > >>> Now i am giving here my
> > > configuration details.
> > > > > >>>
> > > > > >>> /* Master Server */
> > > > > >>> IP Address 192.168.99.23
> > > > > >>> Database name bijayant
> > > > > >>> Table name kavach
> > > > > >>>
> > > > > >>> bijayant=# select * from
> > kavach;
> > > > > >>>  id |   name   | designation | 
> > 
> > > address
> > > > > >>>
> > > ----+----------+-------------+-------------
> > > > > >>>   1 | Bijayant | consultant  |
> > > Lakkasandra
> > > > > >>> (1 row)
> > > > > >>>
> > > > > >>>
> > > > > >>> vi /etc/slon_tools.conf
> > > > > >>>
> > > > > >>> if
> > ($ENV{"SLONYNODES"}) {
> > > > > >>>     require
> > > $ENV{"SLONYNODES"};
> > > > > >>> } else {
> > > > > >>>     $CLUSTER_NAME =
> > > 'bijayant';
> > > > > >>>     $LOGDIR =
> > > '/var/log/slony';
> > > > > >>>     $MASTERNODE = 1;
> > > > > >>>     add_node(node     => 1,
> > > > > >>>              host     =>
> > > > > '192.168.99.23',
> > > > > >>>              dbname   =>
> > > > > 'bijayant',
> > > > > >>>              port     =>
> > 5432,
> > > > > >>>              user     =>
> > > > > 'bijayant',
> > > > > >>>              password =>
> > > > > 'bijayant');
> > > > > >>>
> > > > > >>>     add_node(node     => 2,
> > > > > >>>              host     =>
> > > > > '192.168.99.134',
> > > > > >>>              dbname   =>
> > > > > 'bijayant',
> > > > > >>>              port     =>
> > 5432,
> > > > > >>>              user     =>
> > > > > 'bijayant',
> > > > > >>>              password =>
> > > > > 'bijayant',
> > > > > >>>              parent => 1
> > > > > >>>              );
> > > > > >>> }
> > > > > >>>
> > > > > >>> $SLONY_SETS = {
> > > > > >>>                
> > "set1"
> > > => {
> > > > > >>>                          
> > > "set_id"
> > > > > => 1,
> > > > > >>>                          
> > > > > "pkeyedtables"
> > > > > >>>      
> > > > > >> => [
> > > > > >>    
> > > > > >>>                          
> > > > > 'public.kavach',
> > > > > >>>                            ],
> > > > > >>>         },
> > > > > >>> if ($ENV{"SLONYSET"})
> > {
> > > > > >>>     require
> > > $ENV{"SLONYSET"};
> > > > > >>> }
> > > > > >>>
> > > > > >>> 1;
> > > > > >>>
> > > > > >>> vi /etc/conf.d/slony1
> > > > > >>> USER=postgres
> > > > > >>> CLUSTER=bijayant
> > > > > >>> DBUSER=bijayant
> > > > > >>> DBNAME=bijayant
> > > > > >>> DBHOST=192.168.99.23
> > > > > >>>
> > > LOGFILE=/var/lib/postgresql/data/slony1.log
> > > > > >>> LOGLEVEL=4
> > > > > >>>
> > > > > >>> /* On the Slave Server */
> > > > > >>>
> > > > > >>> IP Address 192.168.99.134
> > > > > >>> Database name bijayant
> > > > > >>> Table name kavach
> > > > > >>>
> > > > > >>> vi /etc/slon_tools.conf
> > > > > >>>
> > > > > >>> if
> > ($ENV{"SLONYNODES"}) {
> > > > > >>>     require
> > > $ENV{"SLONYNODES"};
> > > > > >>> } else {
> > > > > >>>     $CLUSTER_NAME =
> > > 'bijayant';
> > > > > >>>     $LOGDIR =
> > > '/var/log/slony';
> > > > > >>>     # SYNC check interval (slon
> > -s
> > > option)
> > > > > >>>     # $SYNC_CHECK_INTERVAL =
> > 1000;
> > > > > >>>     $MASTERNODE = 1;
> > > > > >>>     add_node(node     => 1,
> > > > > >>>              host     =>
> > > > > '192.168.99.23',
> > > > > >>>              dbname   =>
> > > > > 'bijayant',
> > > > > >>>              port     =>
> > 5432,
> > > > > >>>              user     =>
> > > > > 'bijayant',
> > > > > >>>              password =>
> > > > > 'bijayant');
> > > > > >>>
> > > > > >>>     add_node(node     => 2,
> > > > > >>>              host     =>
> > > > > '192.168.99.134',
> > > > > >>>              dbname   =>
> > > > > 'bijayant',
> > > > > >>>              port     =>
> > 5432,
> > > > > >>>              user     =>
> > > > > 'bijayant',
> > > > > >>>              password =>
> > > > > 'bijayant',
> > > > > >>>              parent => 1
> > > > > >>>              );
> > > > > >>> }
> > > > > >>> $SLONY_SETS = {
> > > > > >>>     "set1" => {
> > > > > >>>              
> > "set_id"
> > > => 1,
> > > > > >>>                
> > > "pkeyedtables" =>
> > > > > [
> > > > > >>>                                
> >  
> > > > > >>>      
> > > > > >> 'public.kavach',
> > > > > >>    
> > > > > >>>                                
> >   ],
> > > > > >>>               },
> > > > > >>> };
> > > > > >>>
> > > > > >>> if ($ENV{"SLONYSET"})
> > {
> > > > > >>>     require
> > > $ENV{"SLONYSET"};
> > > > > >>> }
> > > > > >>>
> > > > > >>> 1;
> > > > > >>>
> > > > > >>> vi /etc/conf.d/slony1
> > > > > >>> USER=postgres
> > > > > >>> CLUSTER=bijayant
> > > > > >>> DBUSER=bijayant
> > > > > >>> DBNAME=bijayant
> > > > > >>> DBHOST=192.168.99.23
> > > > > >>>
> > > LOGFILE=/var/lib/postgresql/data/slony1.log
> > > > > >>> LOGLEVEL=4 
> > > > > >>>
> > > > > >>> When i start the slony1
> > > /etc/init.d/slony1
> > > > > start on
> > > > > >>>      
> > > > > >> both the machine it generates lots
> > of
> > > logs with
> > > > > errors line
> > > > > >> like
> > > > > >>    
> > > > > >>> 2008-06-30 15:54:55 IST DEBUG2
> > > slon_retry()
> > > > > from
> > > > > >>>      
> > > > > >> pid=19007
> > > > > >>    
> > > > > >>> 2008-06-30 15:54:55 IST DEBUG1
> > slon:
> > > retry
> > > > > requested
> > > > > >>> 2008-06-30 15:54:55 IST DEBUG2
> > slon:
> > > notify
> > > > > worker
> > > > > >>>      
> > > > > >> process to shutdown
> > > > > >>    
> > > > > >>> 2008-06-30 15:54:55 IST DEBUG2
> > slon:
> > > child
> > > > > terminated
> > > > > >>>      
> > > > > >> status: 0; pid: 19007, current
> > worker
> > > pid: 19007
> > > > > >>    
> > > > > >>> 2008-06-30 15:54:55 IST DEBUG1
> > slon:
> > > restart
> > > > > of worker
> > > > > >>> 2008-06-30 15:54:55 IST CONFIG
> > main:
> > > slon
> > > > > version
> > > > > >>>      
> > > > > >> 1.2.10 starting up
> > > > > >>    
> > > > > >>> 2008-06-30 15:54:55 IST DEBUG2
> > slon:
> > > watchdog
> > > > > process
> > > > > >>>      
> > > > > >> started
> > > > > >>    
> > > > > >>> 2008-06-30 15:54:55 IST DEBUG2
> > slon:
> > > watchdog
> > > > > ready -
> > > > > >>>      
> > > > > >> pid = 19005
> > > > > >>    
> > > > > >>> 2008-06-30 15:54:55 IST DEBUG2
> > slon:
> > > worker
> > > > > process
> > > > > >>>      
> > > > > >> created - pid = 19034
> > > > > >>    
> > > > > >>> 2008-06-30 15:54:55 IST ERROR 
> > > cannot get
> > > > > >>>      
> > > > > >> sl_local_node_id - ERROR:  schema
> > > > > "_bijayant"
> > > > > >> does not exist
> > > > > >>    
> > > > > >>> 2008-06-30 15:54:55 IST FATAL 
> > main:
> > > Node is
> > > > > not
> > > > > >>>      
> > > > > >> initialized properly - sleep 10s
> > > > > >>    
> > > > > >>> I am sure that i am not
> > > understanding some
> > > > > basic
> > > > > >>>      
> > > > > >> things about the slony1. Can
> > anybody
> > > help me to
> > > > > understand
> > > > > >> the logic, i will be very helpful
> > for
> > > you all.
> > > > > Please tell
> > > > > >> me what i am doing wrong here, what
> > > should i do. I
> > > > > have
> > > > > >> read the documentation at the
> > website
> > > but not able
> > > > > to
> > > > > >> understand fully. Please help me
> > out.
> > > > > >>    
> > > > > >>> Thanks & Regards,
> > > > > >>> Bijayant Kumar
> > > > > >>>
> > > > > >>> Send instant messages to your
> > online
> > > friends
> > > > > >>>      
> > > > > >> http://uk.messenger.yahoo.com 
> > > > > >>    
> > > > > >>>
> > > > >
> > _______________________________________________
> > > > > >>> Slony1-general mailing list
> > > > > >>> Slony1-general@lists.slony.info
> > > > > >>>
> > > > > >>>      
> > > > > >>
> > > > >
> > >
> > http://lists.slony.info/mailman/listinfo/slony1-general
> > > > > >>    
> > > > > >>>  
> > > > > >>>      
> > > > > >>
> > > _______________________________________________
> > > > > >> Slony1-general mailing list
> > > > > >> Slony1-general@lists.slony.info
> > > > > >>
> > > > >
> > >
> > http://lists.slony.info/mailman/listinfo/slony1-general
> > > > > >>    
> > > > > >
> > > > > > Send instant messages to your online
> > friends
> > > > > http://uk.messenger.yahoo.com 
> > > > > >  
> > > > > 
> > > > >
> > _______________________________________________
> > > > > Slony1-general mailing list
> > > > > Slony1-general@lists.slony.info
> > > > >
> > >
> > http://lists.slony.info/mailman/listinfo/slony1-general
> > > > 
> > > > Send instant messages to your online friends
> > > http://uk.messenger.yahoo.com
> > > > _______________________________________________
> > > > Slony1-general mailing list
> > > > Slony1-general@lists.slony.info
> > > >
> > >
> > http://lists.slony.info/mailman/listinfo/slony1-general
> > > 
> > > 
> > > 
> > >      
> > >
> > __________________________________________________________
> > > Not happy with your email address?.
> > > Get the one you really want - millions of new email
> > > addresses available now at Yahoo!
> > > http://uk.docs.yahoo.com/ymail/new.html
> > 
> > Send instant messages to your online friends
> > http://uk.messenger.yahoo.com
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general@lists.slony.info
> > http://lists.slony.info/mailman/listinfo/slony1-general
> 
> Send instant messages to your online friends http://uk.messenger.yahoo.com



      __________________________________________________________
Not happy with your email address?.
Get the one you really want - millions of new email addresses available now at Yahoo! http://uk.docs.yahoo.com/ymail/new.html
From bijayant4u at yahoo.com  Tue Jul  1 06:08:37 2008
From: bijayant4u at yahoo.com (bijayant kumar)
Date: Tue Jul  1 06:08:41 2008
Subject: [Slony1-general] Node is not initialized properly
In-Reply-To: <566046.29997.qm@web25807.mail.ukl.yahoo.com>
Message-ID: <330171.93690.qm@web32704.mail.mud.yahoo.com>

On the Master Server (192.168.99.23)

bijayant ~ # ps aux | grep slon

root      3956  0.0  0.1   2116   692 pts/2    S    18:27   0:00 /usr/bin/slon -s 1000 -d2 bijayant host=192.168.99.23 dbname=bijayant user=bijayant port=5432 password=bijayant

root      3961  0.0  0.1  51424   864 pts/2    Sl   18:27   0:00 
/usr/bin/slon -s 1000 -d2 bijayant host=192.168.99.23 dbname=bijayant user=bijayant port=5432 password=bijayant

root      3968  0.0  0.4   3612  2188 pts/2    S    18:27   0:00 /usr/local/bin/perl /usr/bin/slon_watchdog --config=/etc/slon_tools.conf node1 30

root      3998  0.0  0.1   2120   692 pts/2    S    18:28   0:00 /usr/bin/slon -s 1000 -d2 bijayant host=192.168.99.134 dbname=bijayant user=bijayant port=5432 password=bijayant

root      4006  0.0  0.1  51428   868 pts/2    Sl   18:28   0:00 /usr/bin/slon -s 1000 -d2 bijayant host=192.168.99.134 dbname=bijayant 
user=bijayant port=5432 password=bijayant

root      4008  0.0  0.4   3616  2192 pts/2    S    18:28   0:00 /usr/local/bin/perl /usr/bin/slon_watchdog --config=/etc/slon_tools.conf node2 30

On server vi /etc/slon_tools.conf
if ($ENV{"SLONYNODES"}) {
    require $ENV{"SLONYNODES"};
} else {
    $CLUSTER_NAME = 'bijayant';
    $LOGDIR = '/var/log/slony';
    $MASTERNODE = 1;
    add_node(node     => 1,
             host     => '192.168.99.23',
             dbname   => 'bijayant',
             port     => 5432,
             user     => 'bijayant',
             password => 'bijayant');

    add_node(node     => 2,
             host     => '192.168.99.134',
             dbname   => 'bijayant',
             port     => 5432,
             user     => 'bijayant',
             password => 'bijayant',
             parent => 1
             );
}

$SLONY_SETS = {
    # A unique name for the set
    "set1" => {
        # The set_id, also unique
        "set_id" => 1,
        # "origin" => 1,
        # foldCase => 0,
#       "table_id"    => 1,
#       "sequence_id" => 1,
       "pkeyedtables" => [
                           'public.kavach',
                           ],
        },
};

if ($ENV{"SLONYSET"}) {
    require $ENV{"SLONYSET"};
}

1;

vi /etc/conf.d/slony1
USER=postgres
CLUSTER=bijayant
DBUSER=bijayant
DBNAME=bijayant
DBHOST=192.168.99.23
LOGFILE=/var/lib/postgresql/data/slony1.log
LOGLEVEL=1


And on the Slave Server (192.168.99.134) the file is exactly same.

Bijayant Kumar


--- On Tue, 1/7/08, Glyn Astill <glynastill@yahoo.co.uk> wrote:

> From: Glyn Astill <glynastill@yahoo.co.uk>
> Subject: Re: [Slony1-general] Node is not initialized properly
> To: bijayant4u@yahoo.com, "Martin Eriksson" <m.eriksson@albourne.com>
> Cc: slony1-general@lists.slony.info
> Date: Tuesday, 1 July, 2008, 6:26 PM
> Check what slons are running against each database. You
> should have a slon running for each machine, so if you are
> running one slon on each server you should have the
> conninfo parameter configured in slon.conf, or be passing
> it at the command line. A ps -ax should show what server
> the slons are acting on.
> 
> 
> 
> ----- Original Message ----
> > From: bijayant kumar <bijayant4u@yahoo.com>
> > To: Martin Eriksson <m.eriksson@albourne.com>;
> Glyn Astill <glynastill@yahoo.co.uk>
> > Cc: slony1-general@lists.slony.info
> > Sent: Tuesday, 1 July, 2008 1:45:11 PM
> > Subject: Re: [Slony1-general] Node is not initialized
> properly
> > 
> > Hello to list,
> > 
> > My this problem is solved now. I have re-installed
> postgresql and slony1 and the 
> > problem gone. Now i have another problem. When i run
> slony on Master 
> > Server(192.168.99.23), i can see in the logs
> > 
> > 2008-07-01 18:09:50 IST CONFIG enableNode: no_id=2
> > 2008-07-01 18:09:50 IST DEBUG1 remoteWorkerThread_2:
> thread starts
> > 2008-07-01 18:09:50 IST DEBUG1 remoteListenThread_2:
> thread starts
> > 2008-07-01 18:09:50 IST DEBUG1 main: running scheduler
> mainloop
> > 2008-07-01 18:09:50 IST DEBUG1 cleanupThread: thread
> starts
> > 2008-07-01 18:09:50 IST DEBUG1 syncThread: thread
> starts
> > 2008-07-01 18:09:50 IST DEBUG1 remoteListenThread_2:
> connected to 
> > 'host=192.168.99.134 dbname=bijayant user=bijayant
> port=5432 password=bijayant'
> > 
> > But when on second node that is on Slave
> server(192.168.99.134), i run slony i 
> > got
> > 
> > 2008-07-01 18:03:07 IST FATAL  Do you already have a
> slon running against this 
> > node?
> > 2008-07-01 18:03:07 IST FATAL  Or perhaps a residual
> idle backend connection 
> > from a dead slon?
> > 2008-07-01 18:03:07 IST DEBUG2 slon_abort() from
> pid=7871
> > 2008-07-01 18:03:07 IST DEBUG1 slon: shutdown
> requested
> > 2008-07-01 18:03:07 IST DEBUG2 slon: notify worker
> process to shutdown
> > 2008-07-01 18:03:27 IST DEBUG1 slon: child termination
> timeout - kill child
> > 2008-07-01 18:03:27 IST DEBUG2 slon: child terminated
> status: 9; pid: 7871, 
> > current worker pid: 7871
> > 2008-07-01 18:03:27 IST DEBUG1 slon: done
> > 2008-07-01 18:03:27 IST DEBUG2 slon: remove pid file
> > 2008-07-01 18:03:27 IST DEBUG2 slon: exit(0)
> > 
> > 
> > I have a exact same configuration(like xerox copy) on
> both the server. Is it 
> > creating the problem? Should i have to change the 
> > parameters(/etc/slon_tools.conf) according to
> Master/Slave.
> > 
> > Please help me, i think i am very near to replicate my
> first ever database.
> > 
> > Bijayant Kumar
> > 
> > 
> > --- On Tue, 1/7/08, bijayant kumar wrote:
> > 
> > > From: bijayant kumar 
> > > Subject: Re: [Slony1-general] Node is not
> initialized properly
> > > To: "Martin Eriksson" , "Glyn
> Astill" 
> > 
> > > Cc: slony1-general@lists.slony.info
> > > Date: Tuesday, 1 July, 2008, 3:05 PM
> > > Hi,
> > > The user "bijayant" is a super user. I
> have
> > > created this user like
> > > 
> > > postgres@bijayant ~ $ createuser bijayant
> > > Shall the new user be allowed to create
> databases? (y/n) y
> > > Shall the new user be allowed to create more new
> users?
> > > (y/n) y
> > > CREATE USER
> > > 
> > > It means that bijayant is a super user right? Or
> i am doing
> > > some silly thing. If i will use user as pgsql
> than what to
> > > use for password.
> > > 
> > > bijayant ~ # pg_config --libdir
> > > /usr/lib
> > > 
> > > bijayant ~ # pg_config --pkglibdir
> > > /usr/lib/postgresql
> > > 
> > > In the Master Database server the xxid.so file is
> in 
> > > /usr/local/pgsql/lib/xxid.so. I have copied to
> this file to
> > > /usr/lib/postgresql/ and /usr/lib/ also. But no
> luck.
> > > 
> > > On the Slave Server, its in
> /usr/lib64/postgresql/xxid.so.
> > > 
> > > Is it creating the problem. How to resolve this
> problem.
> > > I have installed postgres and slony1 by the
> emerge utility
> > > of gentoo.
> > > 
> > > Thanks & Regards,
> > > 
> > > Bijayant Kumar
> > > 
> > > 
> > > --- On Tue, 1/7/08, Glyn Astill
> > > wrote:
> > > 
> > > > From: Glyn Astill 
> > > > Subject: Re: [Slony1-general] Node is not
> initialized
> > > properly
> > > > To: bijayant4u@yahoo.com, "Martin
> Eriksson"
> > > 
> > > > Cc: slony1-general@lists.slony.info
> > > > Date: Tuesday, 1 July, 2008, 2:22 PM
> > > > I'd be checking to see where xxid.so was
> on the
> > > system,
> > > > and if I had an older version lurking
> somewhere.
> > > > 
> > > > I'd also make sure bijayant was a
> database
> > > superuser.
> > > > 
> > > > 
> > > > 
> > > > ----- Original Message ----
> > > > > From: bijayant kumar 
> > > > > To: Martin Eriksson
> > > 
> > > > > Cc: slony1-general@lists.slony.info
> > > > > Sent: Tuesday, 1 July, 2008 8:00:19 AM
> > > > > Subject: Re: [Slony1-general] Node is
> not
> > > initialized
> > > > properly
> > > > > 
> > > > > Thanks to all for the reply. Now i
> think that i
> > > am
> > > > coming closer to run slonik. 
> > > > > When i did as suggested by all
> > > > > 
> > > > > bijayant ~ # slonik_init_cluster
> --config
> > > > /etc/slon_tools.conf | slonik
> > > > > 
> > > > > :6: PGRES_FATAL_ERROR load
> > > '$libdir/xxid';  -
> > > > ERROR:  could not access 
> > > > > file "$libdir/xxid": No such
> file or
> > > > directory
> > > > > 
> > > > > :6: Error: the extension for the xxid
> data type
> > > cannot
> > > > be loaded in 
> > > > > database 'host=192.168.99.23
> dbname=bijayant
> > > > user=bijayant port=5432 
> > > > > password=bijayant'
> > > > > 
> > > > > :6: ERROR: no admin conninfo for node
> 134598992
> > > > > 
> > > > > The first two lines of the error i
> couldnot
> > > > understand.
> > > > > The "admin conninfo" error, i
> think
> > > this
> > > > parameter should be present in conf 
> > > > > file, right? But in my case what should
> be there,
> > > as i
> > > > am already giving the 
> > > > > username and password to connect to the
> > > postgresql
> > > > database.
> > > > > 
> > > > > In this problem thread one gentleman
> has asked me
> > > to
> > > > check whether "_bijayant" 
> > > > > schema is created or not? I am sorry to
> ask but i
> > > > really dont know about this. I 
> > > > > have only created a user,database and
> table in
> > > the
> > > > postgresql database nothing 
> > > > > else. When we create database, schema
> is also
> > > created
> > > > automatically, right?
> > > > > 
> > > > > Please suggest me what should i do
> next. Sorry
> > > but i
> > > > am very new to slonik and 
> > > > > database. I am trying hard to
> understand the
> > > concept
> > > > > 
> > > > > Thanks & Regards,
> > > > > 
> > > > > Bijayant Kumar
> > > > > 
> > > > > 
> > > > > --- On Mon, 30/6/08, Martin Eriksson
> wrote:
> > > > > 
> > > > > > From: Martin Eriksson 
> > > > > > Subject: Re: [Slony1-general] Node
> is not
> > > > initialized properly
> > > > > > To: 
> > > > > > Cc:
> slony1-general@lists.slony.info
> > > > > > Date: Monday, 30 June, 2008, 8:34
> PM
> > > > > > If you only run it like that, it
> only prints
> > > what
> > > > it will
> > > > > > execute, to 
> > > > > > actually execute this you need to
> | it to
> > > the
> > > > slonik app..
> > > > > > 
> > > > > > eg.
> /data/pgsql/slony/slonik_init_cluster
> > > > --config
> > > > > > | 
> > > > > > /data/pgsql/bin/slonik
> > > > > > 
> > > > > > all scripts starting with
> > > "slonik_"
> > > > doesn't
> > > > > > actually do anything 
> > > > > > themself, its just a way to format
> a command
> > > > correctly for
> > > > > > the slonik 
> > > > > > parser, which actually does the
> work..
> > > > > > 
> > > > > > 
> > > > > > 
> > > > > > bijayant kumar wrote:
> > > > > > > Thanks for the reply. I
> executed the
> > > command
> > > > > > "slonik_init_cluster".It
> gives the
> > > > output like
> > > > > > >
> > > > > > > # INIT CLUSTER
> > > > > > > cluster name = bijayant;
> > > > > > >  node 1 admin
> > > > conninfo='host=192.168.99.23
> > > > > > dbname=bijayant user=bijayant
> port=5432
> > > > > > password=bijayant';
> > > > > > >  node 2 admin
> > > > conninfo='host=192.168.99.134
> > > > > > dbname=bijayant user=bijayant
> port=5432
> > > > > > password=bijayant';
> > > > > > >   init cluster (id = 1,
> comment =
> > > 'Node
> > > > 1 -
> > > > > > bijayant@192.168.99.23');
> > > > > > >
> > > > > > > # STORE NODE
> > > > > > >   store node (id = 2, event
> node = 1,
> > > > comment =
> > > > > > 'Node 2 -
> bijayant@192.168.99.134');
> > > > > > >   echo 'Set up
> replication
> > > nodes';
> > > > > > >
> > > > > > > # STORE PATH
> > > > > > >   echo 'Next: configure
> paths for
> > > each
> > > > > > node/origin';
> > > > > > >   store path (server = 1,
> client = 2,
> > > > conninfo =
> > > > > > 'host=192.168.99.23
> dbname=bijayant
> > > > user=bijayant
> > > > > > port=5432 password=bijayant');
> > > > > > >   store path (server = 2,
> client = 1,
> > > > conninfo =
> > > > > > 'host=192.168.99.134
> dbname=bijayant
> > > > user=bijayant
> > > > > > port=5432 password=bijayant');
> > > > > > >   echo 'Replication nodes
> > > prepared';
> > > > > > >   echo 'Please start a
> slon
> > > replication
> > > > daemon for
> > > > > > each node';
> > > > > > >
> > > > > > > After that i run the slonik
> daemon and
> > > got
> > > > the error
> > > > > > mentioned.
> > > > > > >
> > > > > > >
> > > > > > > Bijayant Kumar
> > > > > > >
> > > > > > >
> > > > > > > --- On Mon, 30/6/08, Martin
> Eriksson
> > > > > > wrote:
> > > > > > >
> > > > > > >  
> > > > > > >> From: Martin Eriksson
> > > > > > 
> > > > > > >> Subject: Re:
> [Slony1-general] Node
> > > is
> > > > not
> > > > > > initialized properly
> > > > > > >> To: 
> > > > > > >> Cc:
> slony1-general@lists.slony.info
> > > > > > >> Date: Monday, 30 June,
> 2008, 7:18
> > > PM
> > > > > > >> Very basic, but saw no
> mention of
> > > it in
> > > > the
> > > > > > e-mail,
> > > > > > >>
> > > > > > >> I assume you ran the
> > > > > > "slonik_init_cluster"
> before
> > > > > > >> trying to start the 
> > > > > > >> slon daemons? as that
> does the
> > > slony
> > > > setup on each
> > > > > > of the
> > > > > > >> dbs
> > > > > > >>
> > > > > > >>
> > > > > > >>
> > > > > > >> bijayant kumar wrote:
> > > > > > >>    
> > > > > > >>> Hello list,
> > > > > > >>>
> > > > > > >>> I am a very new user
> of slony1
> > > so
> > > > please
> > > > > > forgive me if
> > > > > > >>>      
> > > > > > >> i am asking very stupid
> question. 
> > > > > > >>    
> > > > > > >>> I have installed
> Postgresql and
> > > > Slony1 on two
> > > > > > gentoo
> > > > > > >>>      
> > > > > > >> machine. Postgresql is
> working fine
> > > no
> > > > problem at
> > > > > > all. I
> > > > > > >> want to use Slony1 to
> replicate the
> > > two
> > > > databases
> > > > > > across
> > > > > > >> the systems. To
> understand the
> > > slony1
> > > > concept i
> > > > > > made a 
> > > > > > >>    
> > > > > > >>> test database with
> one single
> > > table
> > > > and only
> > > > > > one entry
> > > > > > >>>      
> > > > > > >> into it. But when i start
> slony1 i
> > > get
> > > > error like
> > > > > > >>    
> > > > > > >>> 2008-06-30 15:54:55
> IST ERROR 
> > > > cannot get
> > > > > > >>>      
> > > > > > >> sl_local_node_id - ERROR:
>  schema
> > > > > > "_bijayant"
> > > > > > >> does not exist
> > > > > > >>    
> > > > > > >>> 2008-06-30 15:54:55
> IST FATAL 
> > > main:
> > > > Node is
> > > > > > not
> > > > > > >>>      
> > > > > > >> initialized properly -
> sleep 10s
> > > > > > >>    
> > > > > > >>> Now i am giving here
> my
> > > > configuration details.
> > > > > > >>>
> > > > > > >>> /* Master Server */
> > > > > > >>> IP Address
> 192.168.99.23
> > > > > > >>> Database name
> bijayant
> > > > > > >>> Table name kavach
> > > > > > >>>
> > > > > > >>> bijayant=# select *
> from
> > > kavach;
> > > > > > >>>  id |   name   |
> designation | 
> > > 
> > > > address
> > > > > > >>>
> > > > ----+----------+-------------+-------------
> > > > > > >>>   1 | Bijayant |
> consultant  |
> > > > Lakkasandra
> > > > > > >>> (1 row)
> > > > > > >>>
> > > > > > >>>
> > > > > > >>> vi
> /etc/slon_tools.conf
> > > > > > >>>
> > > > > > >>> if
> > > ($ENV{"SLONYNODES"}) {
> > > > > > >>>     require
> > > > $ENV{"SLONYNODES"};
> > > > > > >>> } else {
> > > > > > >>>     $CLUSTER_NAME =
> > > > 'bijayant';
> > > > > > >>>     $LOGDIR =
> > > > '/var/log/slony';
> > > > > > >>>     $MASTERNODE = 1;
> > > > > > >>>     add_node(node    
> => 1,
> > > > > > >>>              host    
> =>
> > > > > > '192.168.99.23',
> > > > > > >>>              dbname  
> =>
> > > > > > 'bijayant',
> > > > > > >>>              port    
> =>
> > > 5432,
> > > > > > >>>              user    
> =>
> > > > > > 'bijayant',
> > > > > > >>>              password
> =>
> > > > > > 'bijayant');
> > > > > > >>>
> > > > > > >>>     add_node(node    
> => 2,
> > > > > > >>>              host    
> =>
> > > > > > '192.168.99.134',
> > > > > > >>>              dbname  
> =>
> > > > > > 'bijayant',
> > > > > > >>>              port    
> =>
> > > 5432,
> > > > > > >>>              user    
> =>
> > > > > > 'bijayant',
> > > > > > >>>              password
> =>
> > > > > > 'bijayant',
> > > > > > >>>              parent
> => 1
> > > > > > >>>              );
> > > > > > >>> }
> > > > > > >>>
> > > > > > >>> $SLONY_SETS = {
> > > > > > >>>                
> > > "set1"
> > > > => {
> > > > > > >>>                      
>    
> > > > "set_id"
> > > > > > => 1,
> > > > > > >>>                      
>    
> > > > > > "pkeyedtables"
> > > > > > >>>      
> > > > > > >> => [
> > > > > > >>    
> > > > > > >>>                      
>    
> > > > > > 'public.kavach',
> > > > > > >>>                      
>      ],
> > > > > > >>>         },
> > > > > > >>> if
> ($ENV{"SLONYSET"})
> > > {
> > > > > > >>>     require
> > > > $ENV{"SLONYSET"};
> > > > > > >>> }
> > > > > > >>>
> > > > > > >>> 1;
> > > > > > >>>
> > > > > > >>> vi /etc/conf.d/slony1
> > > > > > >>> USER=postgres
> > > > > > >>> CLUSTER=bijayant
> > > > > > >>> DBUSER=bijayant
> > > > > > >>> DBNAME=bijayant
> > > > > > >>> DBHOST=192.168.99.23
> > > > > > >>>
> > > > LOGFILE=/var/lib/postgresql/data/slony1.log
> > > > > > >>> LOGLEVEL=4
> > > > > > >>>
> > > > > > >>> /* On the Slave
> Server */
> > > > > > >>>
> > > > > > >>> IP Address
> 192.168.99.134
> > > > > > >>> Database name
> bijayant
> > > > > > >>> Table name kavach
> > > > > > >>>
> > > > > > >>> vi
> /etc/slon_tools.conf
> > > > > > >>>
> > > > > > >>> if
> > > ($ENV{"SLONYNODES"}) {
> > > > > > >>>     require
> > > > $ENV{"SLONYNODES"};
> > > > > > >>> } else {
> > > > > > >>>     $CLUSTER_NAME =
> > > > 'bijayant';
> > > > > > >>>     $LOGDIR =
> > > > '/var/log/slony';
> > > > > > >>>     # SYNC check
> interval (slon
> > > -s
> > > > option)
> > > > > > >>>     #
> $SYNC_CHECK_INTERVAL =
> > > 1000;
> > > > > > >>>     $MASTERNODE = 1;
> > > > > > >>>     add_node(node    
> => 1,
> > > > > > >>>              host    
> =>
> > > > > > '192.168.99.23',
> > > > > > >>>              dbname  
> =>
> > > > > > 'bijayant',
> > > > > > >>>              port    
> =>
> > > 5432,
> > > > > > >>>              user    
> =>
> > > > > > 'bijayant',
> > > > > > >>>              password
> =>
> > > > > > 'bijayant');
> > > > > > >>>
> > > > > > >>>     add_node(node    
> => 2,
> > > > > > >>>              host    
> =>
> > > > > > '192.168.99.134',
> > > > > > >>>              dbname  
> =>
> > > > > > 'bijayant',
> > > > > > >>>              port    
> =>
> > > 5432,
> > > > > > >>>              user    
> =>
> > > > > > 'bijayant',
> > > > > > >>>              password
> =>
> > > > > > 'bijayant',
> > > > > > >>>              parent
> => 1
> > > > > > >>>              );
> > > > > > >>> }
> > > > > > >>> $SLONY_SETS = {
> > > > > > >>>     "set1"
> => {
> > > > > > >>>              
> > > "set_id"
> > > > => 1,
> > > > > > >>>                
> > > > "pkeyedtables" =>
> > > > > > [
> > > > > > >>>                      
>          
> > >  
> > > > > > >>>      
> > > > > > >> 'public.kavach',
> > > > > > >>    
> > > > > > >>>                      
>          
> > >   ],
> > > > > > >>>               },
> > > > > > >>> };
> > > > > > >>>
> > > > > > >>> if
> ($ENV{"SLONYSET"})
> > > {
> > > > > > >>>     require
> > > > $ENV{"SLONYSET"};
> > > > > > >>> }
> > > > > > >>>
> > > > > > >>> 1;
> > > > > > >>>
> > > > > > >>> vi /etc/conf.d/slony1
> > > > > > >>> USER=postgres
> > > > > > >>> CLUSTER=bijayant
> > > > > > >>> DBUSER=bijayant
> > > > > > >>> DBNAME=bijayant
> > > > > > >>> DBHOST=192.168.99.23
> > > > > > >>>
> > > > LOGFILE=/var/lib/postgresql/data/slony1.log
> > > > > > >>> LOGLEVEL=4 
> > > > > > >>>
> > > > > > >>> When i start the
> slony1
> > > > /etc/init.d/slony1
> > > > > > start on
> > > > > > >>>      
> > > > > > >> both the machine it
> generates lots
> > > of
> > > > logs with
> > > > > > errors line
> > > > > > >> like
> > > > > > >>    
> > > > > > >>> 2008-06-30 15:54:55
> IST DEBUG2
> > > > slon_retry()
> > > > > > from
> > > > > > >>>      
> > > > > > >> pid=19007
> > > > > > >>    
> > > > > > >>> 2008-06-30 15:54:55
> IST DEBUG1
> > > slon:
> > > > retry
> > > > > > requested
> > > > > > >>> 2008-06-30 15:54:55
> IST DEBUG2
> > > slon:
> > > > notify
> > > > > > worker
> > > > > > >>>      
> > > > > > >> process to shutdown
> > > > > > >>    
> > > > > > >>> 2008-06-30 15:54:55
> IST DEBUG2
> > > slon:
> > > > child
> > > > > > terminated
> > > > > > >>>      
> > > > > > >> status: 0; pid: 19007,
> current
> > > worker
> > > > pid: 19007
> > > > > > >>    
> > > > > > >>> 2008-06-30 15:54:55
> IST DEBUG1
> > > slon:
> > > > restart
> > > > > > of worker
> > > > > > >>> 2008-06-30 15:54:55
> IST CONFIG
> > > main:
> > > > slon
> > > > > > version
> > > > > > >>>      
> > > > > > >> 1.2.10 starting up
> > > > > > >>    
> > > > > > >>> 2008-06-30 15:54:55
> IST DEBUG2
> > > slon:
> > > > watchdog
> > > > > > process
> > > > > > >>>      
> > > > > > >> started
> > > > > > >>    
> > > > > > >>> 2008-06-30 15:54:55
> IST DEBUG2
> > > slon:
> > > > watchdog
> > > > > > ready -
> > > > > > >>>      
> > > > > > >> pid = 19005
> > > > > > >>    
> > > > > > >>> 2008-06-30 15:54:55
> IST DEBUG2
> > > slon:
> > > > worker
> > > > > > process
> > > > > > >>>      
> > > > > > >> created - pid = 19034
> > > > > > >>    
> > > > > > >>> 2008-06-30 15:54:55
> IST ERROR 
> > > > cannot get
> > > > > > >>>      
> > > > > > >> sl_local_node_id - ERROR:
>  schema
> > > > > > "_bijayant"
> > > > > > >> does not exist
> > > > > > >>    
> > > > > > >>> 2008-06-30 15:54:55
> IST FATAL 
> > > main:
> > > > Node is
> > > > > > not
> > > > > > >>>      
> > > > > > >> initialized properly -
> sleep 10s
> > > > > > >>    
> > > > > > >>> I am sure that i am
> not
> > > > understanding some
> > > > > > basic
> > > > > > >>>      
> > > > > > >> things about the slony1.
> Can
> > > anybody
> > > > help me to
> > > > > > understand
> > > > > > >> the logic, i will be very
> helpful
> > > for
> > > > you all.
> > > > > > Please tell
> > > > > > >> me what i am doing wrong
> here, what
> > > > should i do. I
> > > > > > have
> > > > > > >> read the documentation at
> the
> > > website
> > > > but not able
> > > > > > to
> > > > > > >> understand fully. Please
> help me
> > > out.
> > > > > > >>    
> > > > > > >>> Thanks & Regards,
> > > > > > >>> Bijayant Kumar
> > > > > > >>>
> > > > > > >>> Send instant messages
> to your
> > > online
> > > > friends
> > > > > > >>>      
> > > > > > >>
> http://uk.messenger.yahoo.com 
> > > > > > >>    
> > > > > > >>>
> > > > > >
> > > _______________________________________________
> > > > > > >>> Slony1-general
> mailing list
> > > > > > >>>
> Slony1-general@lists.slony.info
> > > > > > >>>
> > > > > > >>>      
> > > > > > >>
> > > > > >
> > > >
> > >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > > > > > >>    
> > > > > > >>>  
> > > > > > >>>      
> > > > > > >>
> > > >
> _______________________________________________
> > > > > > >> Slony1-general mailing
> list
> > > > > > >>
> Slony1-general@lists.slony.info
> > > > > > >>
> > > > > >
> > > >
> > >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > > > > > >>    
> > > > > > >
> > > > > > > Send instant messages to your
> online
> > > friends
> > > > > > http://uk.messenger.yahoo.com 
> > > > > > >  
> > > > > > 
> > > > > >
> > > _______________________________________________
> > > > > > Slony1-general mailing list
> > > > > > Slony1-general@lists.slony.info
> > > > > >
> > > >
> > >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > > > > 
> > > > > Send instant messages to your online
> friends
> > > > http://uk.messenger.yahoo.com
> > > > >
> _______________________________________________
> > > > > Slony1-general mailing list
> > > > > Slony1-general@lists.slony.info
> > > > >
> > > >
> > >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > > > 
> > > > 
> > > > 
> > > >      
> > > >
> > >
> __________________________________________________________
> > > > Not happy with your email address?.
> > > > Get the one you really want - millions of
> new email
> > > > addresses available now at Yahoo!
> > > > http://uk.docs.yahoo.com/ymail/new.html
> > > 
> > > Send instant messages to your online friends
> > > http://uk.messenger.yahoo.com
> > > _______________________________________________
> > > Slony1-general mailing list
> > > Slony1-general@lists.slony.info
> > >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > 
> > Send instant messages to your online friends
> http://uk.messenger.yahoo.com
> 
> 
> 
>      
> __________________________________________________________
> Not happy with your email address?.
> Get the one you really want - millions of new email
> addresses available now at Yahoo!
> http://uk.docs.yahoo.com/ymail/new.html

Send instant messages to your online friends http://uk.messenger.yahoo.com 
From glynastill at yahoo.co.uk  Tue Jul  1 06:56:57 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue Jul  1 06:57:04 2008
Subject: [Slony1-general] Node is not initialized properly
Message-ID: <146643.93138.qm@web25807.mail.ukl.yahoo.com>

Looks like you are running slons for both servers on each server.

You only need 1 slon per server.

So on the master (192.168.99.23) just run the slon for the master, and
on the slave (192.168.99.134) just run the slon for the slave.

Or ofcourse you can run both slons on the master and none on the slave,
both on the slave and none on the master, or both on a totally separate
machine...


----- Original Message ----
> From: bijayant kumar <bijayant4u@yahoo.com>
> To: Martin Eriksson <m.eriksson@albourne.com>; Glyn Astill <glynastill@yahoo.co.uk>
> Cc: slony1-general@lists.slony.info
> Sent: Tuesday, 1 July, 2008 2:08:37 PM
> Subject: Re: [Slony1-general] Node is not initialized properly
> 
> On the Master Server (192.168.99.23)
> 
> bijayant ~ # ps aux | grep slon
> 
> root      3956  0.0  0.1   2116   692 pts/2    S    18:27   0:00 /usr/bin/slon 
> -s 1000 -d2 bijayant host=192.168.99.23 dbname=bijayant user=bijayant port=5432 
> password=bijayant
> 
> root      3961  0.0  0.1  51424   864 pts/2    Sl   18:27   0:00 
> /usr/bin/slon -s 1000 -d2 bijayant host=192.168.99.23 dbname=bijayant 
> user=bijayant port=5432 password=bijayant
> 
> root      3968  0.0  0.4   3612  2188 pts/2    S    18:27   0:00 
> /usr/local/bin/perl /usr/bin/slon_watchdog --config=/etc/slon_tools.conf node1 
> 30
> 
> root      3998  0.0  0.1   2120   692 pts/2    S    18:28   0:00 /usr/bin/slon 
> -s 1000 -d2 bijayant host=192.168.99.134 dbname=bijayant user=bijayant port=5432 
> password=bijayant
> 
> root      4006  0.0  0.1  51428   868 pts/2    Sl   18:28   0:00 /usr/bin/slon 
> -s 1000 -d2 bijayant host=192.168.99.134 dbname=bijayant 
> user=bijayant port=5432 password=bijayant
> 
> root      4008  0.0  0.4   3616  2192 pts/2    S    18:28   0:00 
> /usr/local/bin/perl /usr/bin/slon_watchdog --config=/etc/slon_tools.conf node2 
> 30
> 
> On server vi /etc/slon_tools.conf
> if ($ENV{"SLONYNODES"}) {
>     require $ENV{"SLONYNODES"};
> } else {
>     $CLUSTER_NAME = 'bijayant';
>     $LOGDIR = '/var/log/slony';
>     $MASTERNODE = 1;
>     add_node(node     => 1,
>              host     => '192.168.99.23',
>              dbname   => 'bijayant',
>              port     => 5432,
>              user     => 'bijayant',
>              password => 'bijayant');
> 
>     add_node(node     => 2,
>              host     => '192.168.99.134',
>              dbname   => 'bijayant',
>              port     => 5432,
>              user     => 'bijayant',
>              password => 'bijayant',
>              parent => 1
>              );
> }
> 
> $SLONY_SETS = {
>     # A unique name for the set
>     "set1" => {
>         # The set_id, also unique
>         "set_id" => 1,
>         # "origin" => 1,
>         # foldCase => 0,
> #       "table_id"    => 1,
> #       "sequence_id" => 1,
>        "pkeyedtables" => [
>                            'public.kavach',
>                            ],
>         },
> };
> 
> if ($ENV{"SLONYSET"}) {
>     require $ENV{"SLONYSET"};
> }
> 
> 1;
> 
> vi /etc/conf.d/slony1
> USER=postgres
> CLUSTER=bijayant
> DBUSER=bijayant
> DBNAME=bijayant
> DBHOST=192.168.99.23
> LOGFILE=/var/lib/postgresql/data/slony1.log
> LOGLEVEL=1
> 
> 
> And on the Slave Server (192.168.99.134) the file is exactly same.
> 
> Bijayant Kumar
> 
> 
> --- On Tue, 1/7/08, Glyn Astill wrote:
> 
> > From: Glyn Astill 
> > Subject: Re: [Slony1-general] Node is not initialized properly
> > To: bijayant4u@yahoo.com, "Martin Eriksson" 
> > Cc: slony1-general@lists.slony.info
> > Date: Tuesday, 1 July, 2008, 6:26 PM
> > Check what slons are running against each database. You
> > should have a slon running for each machine, so if you are
> > running one slon on each server you should have the
> > conninfo parameter configured in slon.conf, or be passing
> > it at the command line. A ps -ax should show what server
> > the slons are acting on.
> > 
> > 
> > 
> > ----- Original Message ----
> > > From: bijayant kumar 
> > > To: Martin Eriksson ;
> > Glyn Astill 
> > > Cc: slony1-general@lists.slony.info
> > > Sent: Tuesday, 1 July, 2008 1:45:11 PM
> > > Subject: Re: [Slony1-general] Node is not initialized
> > properly
> > > 
> > > Hello to list,
> > > 
> > > My this problem is solved now. I have re-installed
> > postgresql and slony1 and the 
> > > problem gone. Now i have another problem. When i run
> > slony on Master 
> > > Server(192.168.99.23), i can see in the logs
> > > 
> > > 2008-07-01 18:09:50 IST CONFIG enableNode: no_id=2
> > > 2008-07-01 18:09:50 IST DEBUG1 remoteWorkerThread_2:
> > thread starts
> > > 2008-07-01 18:09:50 IST DEBUG1 remoteListenThread_2:
> > thread starts
> > > 2008-07-01 18:09:50 IST DEBUG1 main: running scheduler
> > mainloop
> > > 2008-07-01 18:09:50 IST DEBUG1 cleanupThread: thread
> > starts
> > > 2008-07-01 18:09:50 IST DEBUG1 syncThread: thread
> > starts
> > > 2008-07-01 18:09:50 IST DEBUG1 remoteListenThread_2:
> > connected to 
> > > 'host=192.168.99.134 dbname=bijayant user=bijayant
> > port=5432 password=bijayant'
> > > 
> > > But when on second node that is on Slave
> > server(192.168.99.134), i run slony i 
> > > got
> > > 
> > > 2008-07-01 18:03:07 IST FATAL  Do you already have a
> > slon running against this 
> > > node?
> > > 2008-07-01 18:03:07 IST FATAL  Or perhaps a residual
> > idle backend connection 
> > > from a dead slon?
> > > 2008-07-01 18:03:07 IST DEBUG2 slon_abort() from
> > pid=7871
> > > 2008-07-01 18:03:07 IST DEBUG1 slon: shutdown
> > requested
> > > 2008-07-01 18:03:07 IST DEBUG2 slon: notify worker
> > process to shutdown
> > > 2008-07-01 18:03:27 IST DEBUG1 slon: child termination
> > timeout - kill child
> > > 2008-07-01 18:03:27 IST DEBUG2 slon: child terminated
> > status: 9; pid: 7871, 
> > > current worker pid: 7871
> > > 2008-07-01 18:03:27 IST DEBUG1 slon: done
> > > 2008-07-01 18:03:27 IST DEBUG2 slon: remove pid file
> > > 2008-07-01 18:03:27 IST DEBUG2 slon: exit(0)
> > > 
> > > 
> > > I have a exact same configuration(like xerox copy) on
> > both the server. Is it 
> > > creating the problem? Should i have to change the 
> > > parameters(/etc/slon_tools.conf) according to
> > Master/Slave.
> > > 
> > > Please help me, i think i am very near to replicate my
> > first ever database.
> > > 
> > > Bijayant Kumar
> > > 
> > > 
> > > --- On Tue, 1/7/08, bijayant kumar wrote:
> > > 
> > > > From: bijayant kumar 
> > > > Subject: Re: [Slony1-general] Node is not
> > initialized properly
> > > > To: "Martin Eriksson" , "Glyn
> > Astill" 
> > > 
> > > > Cc: slony1-general@lists.slony.info
> > > > Date: Tuesday, 1 July, 2008, 3:05 PM
> > > > Hi,
> > > > The user "bijayant" is a super user. I
> > have
> > > > created this user like
> > > > 
> > > > postgres@bijayant ~ $ createuser bijayant
> > > > Shall the new user be allowed to create
> > databases? (y/n) y
> > > > Shall the new user be allowed to create more new
> > users?
> > > > (y/n) y
> > > > CREATE USER
> > > > 
> > > > It means that bijayant is a super user right? Or
> > i am doing
> > > > some silly thing. If i will use user as pgsql
> > than what to
> > > > use for password.
> > > > 
> > > > bijayant ~ # pg_config --libdir
> > > > /usr/lib
> > > > 
> > > > bijayant ~ # pg_config --pkglibdir
> > > > /usr/lib/postgresql
> > > > 
> > > > In the Master Database server the xxid.so file is
> > in 
> > > > /usr/local/pgsql/lib/xxid.so. I have copied to
> > this file to
> > > > /usr/lib/postgresql/ and /usr/lib/ also. But no
> > luck.
> > > > 
> > > > On the Slave Server, its in
> > /usr/lib64/postgresql/xxid.so.
> > > > 
> > > > Is it creating the problem. How to resolve this
> > problem.
> > > > I have installed postgres and slony1 by the
> > emerge utility
> > > > of gentoo.
> > > > 
> > > > Thanks & Regards,
> > > > 
> > > > Bijayant Kumar
> > > > 
> > > > 
> > > > --- On Tue, 1/7/08, Glyn Astill
> > > > wrote:
> > > > 
> > > > > From: Glyn Astill 
> > > > > Subject: Re: [Slony1-general] Node is not
> > initialized
> > > > properly
> > > > > To: bijayant4u@yahoo.com, "Martin
> > Eriksson"
> > > > 
> > > > > Cc: slony1-general@lists.slony.info
> > > > > Date: Tuesday, 1 July, 2008, 2:22 PM
> > > > > I'd be checking to see where xxid.so was
> > on the
> > > > system,
> > > > > and if I had an older version lurking
> > somewhere.
> > > > > 
> > > > > I'd also make sure bijayant was a
> > database
> > > > superuser.
> > > > > 
> > > > > 
> > > > > 
> > > > > ----- Original Message ----
> > > > > > From: bijayant kumar 
> > > > > > To: Martin Eriksson
> > > > 
> > > > > > Cc: slony1-general@lists.slony.info
> > > > > > Sent: Tuesday, 1 July, 2008 8:00:19 AM
> > > > > > Subject: Re: [Slony1-general] Node is
> > not
> > > > initialized
> > > > > properly
> > > > > > 
> > > > > > Thanks to all for the reply. Now i
> > think that i
> > > > am
> > > > > coming closer to run slonik. 
> > > > > > When i did as suggested by all
> > > > > > 
> > > > > > bijayant ~ # slonik_init_cluster
> > --config
> > > > > /etc/slon_tools.conf | slonik
> > > > > > 
> > > > > > :6: PGRES_FATAL_ERROR load
> > > > '$libdir/xxid';  -
> > > > > ERROR:  could not access 
> > > > > > file "$libdir/xxid": No such
> > file or
> > > > > directory
> > > > > > 
> > > > > > :6: Error: the extension for the xxid
> > data type
> > > > cannot
> > > > > be loaded in 
> > > > > > database 'host=192.168.99.23
> > dbname=bijayant
> > > > > user=bijayant port=5432 
> > > > > > password=bijayant'
> > > > > > 
> > > > > > :6: ERROR: no admin conninfo for node
> > 134598992
> > > > > > 
> > > > > > The first two lines of the error i
> > couldnot
> > > > > understand.
> > > > > > The "admin conninfo" error, i
> > think
> > > > this
> > > > > parameter should be present in conf 
> > > > > > file, right? But in my case what should
> > be there,
> > > > as i
> > > > > am already giving the 
> > > > > > username and password to connect to the
> > > > postgresql
> > > > > database.
> > > > > > 
> > > > > > In this problem thread one gentleman
> > has asked me
> > > > to
> > > > > check whether "_bijayant" 
> > > > > > schema is created or not? I am sorry to
> > ask but i
> > > > > really dont know about this. I 
> > > > > > have only created a user,database and
> > table in
> > > > the
> > > > > postgresql database nothing 
> > > > > > else. When we create database, schema
> > is also
> > > > created
> > > > > automatically, right?
> > > > > > 
> > > > > > Please suggest me what should i do
> > next. Sorry
> > > > but i
> > > > > am very new to slonik and 
> > > > > > database. I am trying hard to
> > understand the
> > > > concept
> > > > > > 
> > > > > > Thanks & Regards,
> > > > > > 
> > > > > > Bijayant Kumar
> > > > > > 
> > > > > > 
> > > > > > --- On Mon, 30/6/08, Martin Eriksson
> > wrote:
> > > > > > 
> > > > > > > From: Martin Eriksson 
> > > > > > > Subject: Re: [Slony1-general] Node
> > is not
> > > > > initialized properly
> > > > > > > To: 
> > > > > > > Cc:
> > slony1-general@lists.slony.info
> > > > > > > Date: Monday, 30 June, 2008, 8:34
> > PM
> > > > > > > If you only run it like that, it
> > only prints
> > > > what
> > > > > it will
> > > > > > > execute, to 
> > > > > > > actually execute this you need to
> > | it to
> > > > the
> > > > > slonik app..
> > > > > > > 
> > > > > > > eg.
> > /data/pgsql/slony/slonik_init_cluster
> > > > > --config
> > > > > > > | 
> > > > > > > /data/pgsql/bin/slonik
> > > > > > > 
> > > > > > > all scripts starting with
> > > > "slonik_"
> > > > > doesn't
> > > > > > > actually do anything 
> > > > > > > themself, its just a way to format
> > a command
> > > > > correctly for
> > > > > > > the slonik 
> > > > > > > parser, which actually does the
> > work..
> > > > > > > 
> > > > > > > 
> > > > > > > 
> > > > > > > bijayant kumar wrote:
> > > > > > > > Thanks for the reply. I
> > executed the
> > > > command
> > > > > > > "slonik_init_cluster".It
> > gives the
> > > > > output like
> > > > > > > >
> > > > > > > > # INIT CLUSTER
> > > > > > > > cluster name = bijayant;
> > > > > > > >  node 1 admin
> > > > > conninfo='host=192.168.99.23
> > > > > > > dbname=bijayant user=bijayant
> > port=5432
> > > > > > > password=bijayant';
> > > > > > > >  node 2 admin
> > > > > conninfo='host=192.168.99.134
> > > > > > > dbname=bijayant user=bijayant
> > port=5432
> > > > > > > password=bijayant';
> > > > > > > >   init cluster (id = 1,
> > comment =
> > > > 'Node
> > > > > 1 -
> > > > > > > bijayant@192.168.99.23');
> > > > > > > >
> > > > > > > > # STORE NODE
> > > > > > > >   store node (id = 2, event
> > node = 1,
> > > > > comment =
> > > > > > > 'Node 2 -
> > bijayant@192.168.99.134');
> > > > > > > >   echo 'Set up
> > replication
> > > > nodes';
> > > > > > > >
> > > > > > > > # STORE PATH
> > > > > > > >   echo 'Next: configure
> > paths for
> > > > each
> > > > > > > node/origin';
> > > > > > > >   store path (server = 1,
> > client = 2,
> > > > > conninfo =
> > > > > > > 'host=192.168.99.23
> > dbname=bijayant
> > > > > user=bijayant
> > > > > > > port=5432 password=bijayant');
> > > > > > > >   store path (server = 2,
> > client = 1,
> > > > > conninfo =
> > > > > > > 'host=192.168.99.134
> > dbname=bijayant
> > > > > user=bijayant
> > > > > > > port=5432 password=bijayant');
> > > > > > > >   echo 'Replication nodes
> > > > prepared';
> > > > > > > >   echo 'Please start a
> > slon
> > > > replication
> > > > > daemon for
> > > > > > > each node';
> > > > > > > >
> > > > > > > > After that i run the slonik
> > daemon and
> > > > got
> > > > > the error
> > > > > > > mentioned.
> > > > > > > >
> > > > > > > >
> > > > > > > > Bijayant Kumar
> > > > > > > >
> > > > > > > >
> > > > > > > > --- On Mon, 30/6/08, Martin
> > Eriksson
> > > > > > > wrote:
> > > > > > > >
> > > > > > > >  
> > > > > > > >> From: Martin Eriksson
> > > > > > > 
> > > > > > > >> Subject: Re:
> > [Slony1-general] Node
> > > > is
> > > > > not
> > > > > > > initialized properly
> > > > > > > >> To: 
> > > > > > > >> Cc:
> > slony1-general@lists.slony.info
> > > > > > > >> Date: Monday, 30 June,
> > 2008, 7:18
> > > > PM
> > > > > > > >> Very basic, but saw no
> > mention of
> > > > it in
> > > > > the
> > > > > > > e-mail,
> > > > > > > >>
> > > > > > > >> I assume you ran the
> > > > > > > "slonik_init_cluster"
> > before
> > > > > > > >> trying to start the 
> > > > > > > >> slon daemons? as that
> > does the
> > > > slony
> > > > > setup on each
> > > > > > > of the
> > > > > > > >> dbs
> > > > > > > >>
> > > > > > > >>
> > > > > > > >>
> > > > > > > >> bijayant kumar wrote:
> > > > > > > >>    
> > > > > > > >>> Hello list,
> > > > > > > >>>
> > > > > > > >>> I am a very new user
> > of slony1
> > > > so
> > > > > please
> > > > > > > forgive me if
> > > > > > > >>>      
> > > > > > > >> i am asking very stupid
> > question. 
> > > > > > > >>    
> > > > > > > >>> I have installed
> > Postgresql and
> > > > > Slony1 on two
> > > > > > > gentoo
> > > > > > > >>>      
> > > > > > > >> machine. Postgresql is
> > working fine
> > > > no
> > > > > problem at
> > > > > > > all. I
> > > > > > > >> want to use Slony1 to
> > replicate the
> > > > two
> > > > > databases
> > > > > > > across
> > > > > > > >> the systems. To
> > understand the
> > > > slony1
> > > > > concept i
> > > > > > > made a 
> > > > > > > >>    
> > > > > > > >>> test database with
> > one single
> > > > table
> > > > > and only
> > > > > > > one entry
> > > > > > > >>>      
> > > > > > > >> into it. But when i start
> > slony1 i
> > > > get
> > > > > error like
> > > > > > > >>    
> > > > > > > >>> 2008-06-30 15:54:55
> > IST ERROR 
> > > > > cannot get
> > > > > > > >>>      
> > > > > > > >> sl_local_node_id - ERROR:
> >  schema
> > > > > > > "_bijayant"
> > > > > > > >> does not exist
> > > > > > > >>    
> > > > > > > >>> 2008-06-30 15:54:55
> > IST FATAL 
> > > > main:
> > > > > Node is
> > > > > > > not
> > > > > > > >>>      
> > > > > > > >> initialized properly -
> > sleep 10s
> > > > > > > >>    
> > > > > > > >>> Now i am giving here
> > my
> > > > > configuration details.
> > > > > > > >>>
> > > > > > > >>> /* Master Server */
> > > > > > > >>> IP Address
> > 192.168.99.23
> > > > > > > >>> Database name
> > bijayant
> > > > > > > >>> Table name kavach
> > > > > > > >>>
> > > > > > > >>> bijayant=# select *
> > from
> > > > kavach;
> > > > > > > >>>  id |   name   |
> > designation | 
> > > > 
> > > > > address
> > > > > > > >>>
> > > > > ----+----------+-------------+-------------
> > > > > > > >>>   1 | Bijayant |
> > consultant  |
> > > > > Lakkasandra
> > > > > > > >>> (1 row)
> > > > > > > >>>
> > > > > > > >>>
> > > > > > > >>> vi
> > /etc/slon_tools.conf
> > > > > > > >>>
> > > > > > > >>> if
> > > > ($ENV{"SLONYNODES"}) {
> > > > > > > >>>     require
> > > > > $ENV{"SLONYNODES"};
> > > > > > > >>> } else {
> > > > > > > >>>     $CLUSTER_NAME =
> > > > > 'bijayant';
> > > > > > > >>>     $LOGDIR =
> > > > > '/var/log/slony';
> > > > > > > >>>     $MASTERNODE = 1;
> > > > > > > >>>     add_node(node    
> > => 1,
> > > > > > > >>>              host    
> > =>
> > > > > > > '192.168.99.23',
> > > > > > > >>>              dbname  
> > =>
> > > > > > > 'bijayant',
> > > > > > > >>>              port    
> > =>
> > > > 5432,
> > > > > > > >>>              user    
> > =>
> > > > > > > 'bijayant',
> > > > > > > >>>              password
> > =>
> > > > > > > 'bijayant');
> > > > > > > >>>
> > > > > > > >>>     add_node(node    
> > => 2,
> > > > > > > >>>              host    
> > =>
> > > > > > > '192.168.99.134',
> > > > > > > >>>              dbname  
> > =>
> > > > > > > 'bijayant',
> > > > > > > >>>              port    
> > =>
> > > > 5432,
> > > > > > > >>>              user    
> > =>
> > > > > > > 'bijayant',
> > > > > > > >>>              password
> > =>
> > > > > > > 'bijayant',
> > > > > > > >>>              parent
> > => 1
> > > > > > > >>>              );
> > > > > > > >>> }
> > > > > > > >>>
> > > > > > > >>> $SLONY_SETS = {
> > > > > > > >>>                
> > > > "set1"
> > > > > => {
> > > > > > > >>>                      
> >    
> > > > > "set_id"
> > > > > > > => 1,
> > > > > > > >>>                      
> >    
> > > > > > > "pkeyedtables"
> > > > > > > >>>      
> > > > > > > >> => [
> > > > > > > >>    
> > > > > > > >>>                      
> >    
> > > > > > > 'public.kavach',
> > > > > > > >>>                      
> >      ],
> > > > > > > >>>         },
> > > > > > > >>> if
> > ($ENV{"SLONYSET"})
> > > > {
> > > > > > > >>>     require
> > > > > $ENV{"SLONYSET"};
> > > > > > > >>> }
> > > > > > > >>>
> > > > > > > >>> 1;
> > > > > > > >>>
> > > > > > > >>> vi /etc/conf.d/slony1
> > > > > > > >>> USER=postgres
> > > > > > > >>> CLUSTER=bijayant
> > > > > > > >>> DBUSER=bijayant
> > > > > > > >>> DBNAME=bijayant
> > > > > > > >>> DBHOST=192.168.99.23
> > > > > > > >>>
> > > > > LOGFILE=/var/lib/postgresql/data/slony1.log
> > > > > > > >>> LOGLEVEL=4
> > > > > > > >>>
> > > > > > > >>> /* On the Slave
> > Server */
> > > > > > > >>>
> > > > > > > >>> IP Address
> > 192.168.99.134
> > > > > > > >>> Database name
> > bijayant
> > > > > > > >>> Table name kavach
> > > > > > > >>>
> > > > > > > >>> vi
> > /etc/slon_tools.conf
> > > > > > > >>>
> > > > > > > >>> if
> > > > ($ENV{"SLONYNODES"}) {
> > > > > > > >>>     require
> > > > > $ENV{"SLONYNODES"};
> > > > > > > >>> } else {
> > > > > > > >>>     $CLUSTER_NAME =
> > > > > 'bijayant';
> > > > > > > >>>     $LOGDIR =
> > > > > '/var/log/slony';
> > > > > > > >>>     # SYNC check
> > interval (slon
> > > > -s
> > > > > option)
> > > > > > > >>>     #
> > $SYNC_CHECK_INTERVAL =
> > > > 1000;
> > > > > > > >>>     $MASTERNODE = 1;
> > > > > > > >>>     add_node(node    
> > => 1,
> > > > > > > >>>              host    
> > =>
> > > > > > > '192.168.99.23',
> > > > > > > >>>              dbname  
> > =>
> > > > > > > 'bijayant',
> > > > > > > >>>              port    
> > =>
> > > > 5432,
> > > > > > > >>>              user    
> > =>
> > > > > > > 'bijayant',
> > > > > > > >>>              password
> > =>
> > > > > > > 'bijayant');
> > > > > > > >>>
> > > > > > > >>>     add_node(node    
> > => 2,
> > > > > > > >>>              host    
> > =>
> > > > > > > '192.168.99.134',
> > > > > > > >>>              dbname  
> > =>
> > > > > > > 'bijayant',
> > > > > > > >>>              port    
> > =>
> > > > 5432,
> > > > > > > >>>              user    
> > =>
> > > > > > > 'bijayant',
> > > > > > > >>>              password
> > =>
> > > > > > > 'bijayant',
> > > > > > > >>>              parent
> > => 1
> > > > > > > >>>              );
> > > > > > > >>> }
> > > > > > > >>> $SLONY_SETS = {
> > > > > > > >>>     "set1"
> > => {
> > > > > > > >>>              
> > > > "set_id"
> > > > > => 1,
> > > > > > > >>>                
> > > > > "pkeyedtables" =>
> > > > > > > [
> > > > > > > >>>                      
> >          
> > > >  
> > > > > > > >>>      
> > > > > > > >> 'public.kavach',
> > > > > > > >>    
> > > > > > > >>>                      
> >          
> > > >   ],
> > > > > > > >>>               },
> > > > > > > >>> };
> > > > > > > >>>
> > > > > > > >>> if
> > ($ENV{"SLONYSET"})
> > > > {
> > > > > > > >>>     require
> > > > > $ENV{"SLONYSET"};
> > > > > > > >>> }
> > > > > > > >>>
> > > > > > > >>> 1;
> > > > > > > >>>
> > > > > > > >>> vi /etc/conf.d/slony1
> > > > > > > >>> USER=postgres
> > > > > > > >>> CLUSTER=bijayant
> > > > > > > >>> DBUSER=bijayant
> > > > > > > >>> DBNAME=bijayant
> > > > > > > >>> DBHOST=192.168.99.23
> > > > > > > >>>
> > > > > LOGFILE=/var/lib/postgresql/data/slony1.log
> > > > > > > >>> LOGLEVEL=4 
> > > > > > > >>>
> > > > > > > >>> When i start the
> > slony1
> > > > > /etc/init.d/slony1
> > > > > > > start on
> > > > > > > >>>      
> > > > > > > >> both the machine it
> > generates lots
> > > > of
> > > > > logs with
> > > > > > > errors line
> > > > > > > >> like
> > > > > > > >>    
> > > > > > > >>> 2008-06-30 15:54:55
> > IST DEBUG2
> > > > > slon_retry()
> > > > > > > from
> > > > > > > >>>      
> > > > > > > >> pid=19007
> > > > > > > >>    
> > > > > > > >>> 2008-06-30 15:54:55
> > IST DEBUG1
> > > > slon:
> > > > > retry
> > > > > > > requested
> > > > > > > >>> 2008-06-30 15:54:55
> > IST DEBUG2
> > > > slon:
> > > > > notify
> > > > > > > worker
> > > > > > > >>>      
> > > > > > > >> process to shutdown
> > > > > > > >>    
> > > > > > > >>> 2008-06-30 15:54:55
> > IST DEBUG2
> > > > slon:
> > > > > child
> > > > > > > terminated
> > > > > > > >>>      
> > > > > > > >> status: 0; pid: 19007,
> > current
> > > > worker
> > > > > pid: 19007
> > > > > > > >>    
> > > > > > > >>> 2008-06-30 15:54:55
> > IST DEBUG1
> > > > slon:
> > > > > restart
> > > > > > > of worker
> > > > > > > >>> 2008-06-30 15:54:55
> > IST CONFIG
> > > > main:
> > > > > slon
> > > > > > > version
> > > > > > > >>>      
> > > > > > > >> 1.2.10 starting up
> > > > > > > >>    
> > > > > > > >>> 2008-06-30 15:54:55
> > IST DEBUG2
> > > > slon:
> > > > > watchdog
> > > > > > > process
> > > > > > > >>>      
> > > > > > > >> started
> > > > > > > >>    
> > > > > > > >>> 2008-06-30 15:54:55
> > IST DEBUG2
> > > > slon:
> > > > > watchdog
> > > > > > > ready -
> > > > > > > >>>      
> > > > > > > >> pid = 19005
> > > > > > > >>    
> > > > > > > >>> 2008-06-30 15:54:55
> > IST DEBUG2
> > > > slon:
> > > > > worker
> > > > > > > process
> > > > > > > >>>      
> > > > > > > >> created - pid = 19034
> > > > > > > >>    
> > > > > > > >>> 2008-06-30 15:54:55
> > IST ERROR 
> > > > > cannot get
> > > > > > > >>>      
> > > > > > > >> sl_local_node_id - ERROR:
> >  schema
> > > > > > > "_bijayant"
> > > > > > > >> does not exist
> > > > > > > >>    
> > > > > > > >>> 2008-06-30 15:54:55
> > IST FATAL 
> > > > main:
> > > > > Node is
> > > > > > > not
> > > > > > > >>>      
> > > > > > > >> initialized properly -
> > sleep 10s
> > > > > > > >>    
> > > > > > > >>> I am sure that i am
> > not
> > > > > understanding some
> > > > > > > basic
> > > > > > > >>>      
> > > > > > > >> things about the slony1.
> > Can
> > > > anybody
> > > > > help me to
> > > > > > > understand
> > > > > > > >> the logic, i will be very
> > helpful
> > > > for
> > > > > you all.
> > > > > > > Please tell
> > > > > > > >> me what i am doing wrong
> > here, what
> > > > > should i do. I
> > > > > > > have
> > > > > > > >> read the documentation at
> > the
> > > > website
> > > > > but not able
> > > > > > > to
> > > > > > > >> understand fully. Please
> > help me
> > > > out.
> > > > > > > >>    
> > > > > > > >>> Thanks & Regards,
> > > > > > > >>> Bijayant Kumar
> > > > > > > >>>
> > > > > > > >>> Send instant messages
> > to your
> > > > online
> > > > > friends
> > > > > > > >>>      
> > > > > > > >>
> > http://uk.messenger.yahoo.com 
> > > > > > > >>    
> > > > > > > >>>
> > > > > > >
> > > > _______________________________________________
> > > > > > > >>> Slony1-general
> > mailing list
> > > > > > > >>>
> > Slony1-general@lists.slony.info
> > > > > > > >>>
> > > > > > > >>>      
> > > > > > > >>
> > > > > > >
> > > > >
> > > >
> > http://lists.slony.info/mailman/listinfo/slony1-general
> > > > > > > >>    
> > > > > > > >>>  
> > > > > > > >>>      
> > > > > > > >>
> > > > >
> > _______________________________________________
> > > > > > > >> Slony1-general mailing
> > list
> > > > > > > >>
> > Slony1-general@lists.slony.info
> > > > > > > >>
> > > > > > >
> > > > >
> > > >
> > http://lists.slony.info/mailman/listinfo/slony1-general
> > > > > > > >>    
> > > > > > > >
> > > > > > > > Send instant messages to your
> > online
> > > > friends
> > > > > > > http://uk.messenger.yahoo.com 
> > > > > > > >  
> > > > > > > 
> > > > > > >
> > > > _______________________________________________
> > > > > > > Slony1-general mailing list
> > > > > > > Slony1-general@lists.slony.info
> > > > > > >
> > > > >
> > > >
> > http://lists.slony.info/mailman/listinfo/slony1-general
> > > > > > 
> > > > > > Send instant messages to your online
> > friends
> > > > > http://uk.messenger.yahoo.com
> > > > > >
> > _______________________________________________
> > > > > > Slony1-general mailing list
> > > > > > Slony1-general@lists.slony.info
> > > > > >
> > > > >
> > > >
> > http://lists.slony.info/mailman/listinfo/slony1-general
> > > > > 
> > > > > 
> > > > > 
> > > > >      
> > > > >
> > > >
> > __________________________________________________________
> > > > > Not happy with your email address?.
> > > > > Get the one you really want - millions of
> > new email
> > > > > addresses available now at Yahoo!
> > > > > http://uk.docs.yahoo.com/ymail/new.html
> > > > 
> > > > Send instant messages to your online friends
> > > > http://uk.messenger.yahoo.com
> > > > _______________________________________________
> > > > Slony1-general mailing list
> > > > Slony1-general@lists.slony.info
> > > >
> > http://lists.slony.info/mailman/listinfo/slony1-general
> > > 
> > > Send instant messages to your online friends
> > http://uk.messenger.yahoo.com
> > 
> > 
> > 
> >      
> > __________________________________________________________
> > Not happy with your email address?.
> > Get the one you really want - millions of new email
> > addresses available now at Yahoo!
> > http://uk.docs.yahoo.com/ymail/new.html
> 
> Send instant messages to your online friends http://uk.messenger.yahoo.com



      __________________________________________________________
Not happy with your email address?.
Get the one you really want - millions of new email addresses available now at Yahoo! http://uk.docs.yahoo.com/ymail/new.html
From shahaf at redfin.com  Tue Jul  1 08:27:32 2008
From: shahaf at redfin.com (Shahaf Abileah)
Date: Tue Jul  1 08:27:45 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from locking
	alltables
In-Reply-To: <486A0070.2060600@echo.fr>
References: <486A0070.2060600@echo.fr>
Message-ID: <082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>

You can change your configuration such that the tables that you're
looking to alter are in their own set.  That will still cause them to be
locked, but at least it won't cause the other ones to be locked.

Also, in certain situations you might be able to run the alter command
on each of the slaves and then on the master.  E.g. if you're adding a
new column that allows null values, then it might be possible to alter
the slaves even while rows are being inserted into the master.  You will
still be taking a lock on the table while altering it, but at least
you're not locking multiple tables at a time across multiple machines.
Not sure if this is recommended practice.

--S

-----Original Message-----
From: slony1-general-bounces@lists.slony.info
[mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Cyril
SCETBON
Sent: Tuesday, July 01, 2008 3:01 AM
To: slony1-general@lists.slony.info
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from locking
alltables

Hi,

We got a cluster configuration with something like 500 tables. Sometimes

we have to add columns to tables. Today we're using execute script which

locks all tables and not only the tables that belong to the set 
specified in this command. We have a lot of DML statement and these 
locks are really disturbing our application.

Are there other possibilities of using EXECUTE SCRIPT ? Is there a bad 
and a good use of this command ?

Thanks
-- 
Cyril SCETBON
_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


From scetbon at echo.fr  Tue Jul  1 09:03:44 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Tue Jul  1 09:03:51 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from locking
	alltables
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
References: <486A0070.2060600@echo.fr>
	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
Message-ID: <486A5560.3080507@echo.fr>



Shahaf Abileah wrote:
> You can change your configuration such that the tables that you're
> looking to alter are in their own set.  That will still cause them to be
> locked, but at least it won't cause the other ones to be locked.
>   
that won't be the case, cause in version > 1.1 all replicated tables are 
locked.
> Also, in certain situations you might be able to run the alter command
> on each of the slaves and then on the master.  E.g. if you're adding a
> new column that allows null values, then it might be possible to alter
> the slaves even while rows are being inserted into the master.  You will
> still be taking a lock on the table while altering it, but at least
> you're not locking multiple tables at a time across multiple machines.
> Not sure if this is recommended practice.
>   
We had to use this method but it's not working with new columns. Slony 
Triggers are based on the number of columns, see the third argument of 
the "_clustername_logtrigger" triggers added on your tables.
> --S
>
> -----Original Message-----
> From: slony1-general-bounces@lists.slony.info
> [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Cyril
> SCETBON
> Sent: Tuesday, July 01, 2008 3:01 AM
> To: slony1-general@lists.slony.info
> Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from locking
> alltables
>
> Hi,
>
> We got a cluster configuration with something like 500 tables. Sometimes
>
> we have to add columns to tables. Today we're using execute script which
>
> locks all tables and not only the tables that belong to the set 
> specified in this command. We have a lot of DML statement and these 
> locks are really disturbing our application.
>
> Are there other possibilities of using EXECUTE SCRIPT ? Is there a bad 
> and a good use of this command ?
>
> Thanks
>   

-- 
Cyril SCETBON
From troy at troywolf.com  Tue Jul  1 12:55:32 2008
From: troy at troywolf.com (Troy Wolf)
Date: Tue Jul  1 12:55:43 2008
Subject: [Slony1-general] RE: how to prevent EXECUTE SCRIPT from locking
Message-ID: <e0d7c3f50807011255k2da89ba0mc179238296ec863a@mail.gmail.com>

I don't think your advice is correct, Shahaf. Obviously if I'm wrong,
I hope somebody will correct me.

See this Note from the documentation--especially the last phrase.
From: http://www.slony.info/documentation/ddlchanges.html
"Note:  Actually, as of version 1.1.5 and later, this is NOT TRUE. The
danger of someone making DDL changes that crosses replication sets
seems sufficiently palpable that slon has been changed to lock ALL
replicated tables, whether they are in the specified replication set
or not."

And even more surprising (disturbing?) is that Slony waits on locks
that have NOTHING to do with any of the replicated objects. For
example, if you have 11 tables in your database. 10 tables are in a
slony set. The 11th table is not replicated and has zero foreign-keys.
It is for all practical purposes, a "stand-alone" table. Yet Slony
will not be able to do things such as DDL changes (execute_script) or
subscribe if another process has a lock on that 11th table.

I have a patch to the code that is supposed to alleviate this problem.
It was sent to me by another list member. I've not yet had time to
review the code or test it.

I truly wish I understood more about this aspect of Slony.

> Message: 1
> Date: Tue, 1 Jul 2008 08:27:32 -0700
> From: Shahaf Abileah <shahaf@redfin.com>
> Subject: RE: [Slony1-general] how to prevent EXECUTE SCRIPT from
>        locking alltables
> To: "Cyril SCETBON" <scetbon@echo.fr>,
>        <slony1-general@lists.slony.info>
> Message-ID: <082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
> Content-Type: text/plain; charset="us-ascii"
>
> You can change your configuration such that the tables that you're
> looking to alter are in their own set.  That will still cause them to be
> locked, but at least it won't cause the other ones to be locked.
>
> Also, in certain situations you might be able to run the alter command
> on each of the slaves and then on the master.  E.g. if you're adding a
> new column that allows null values, then it might be possible to alter
> the slaves even while rows are being inserted into the master.  You will
> still be taking a lock on the table while altering it, but at least
> you're not locking multiple tables at a time across multiple machines.
> Not sure if this is recommended practice.
>
> --S
>
> -----Original Message-----
> From: slony1-general-bounces@lists.slony.info
> [mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Cyril
> SCETBON
> Sent: Tuesday, July 01, 2008 3:01 AM
> To: slony1-general@lists.slony.info
> Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from locking
> alltables
>
> Hi,
>
> We got a cluster configuration with something like 500 tables. Sometimes
>
> we have to add columns to tables. Today we're using execute script which
>
> locks all tables and not only the tables that belong to the set
> specified in this command. We have a lot of DML statement and these
> locks are really disturbing our application.
>
> Are there other possibilities of using EXECUTE SCRIPT ? Is there a bad
> and a good use of this command ?
>
> Thanks
> --
> Cyril SCETBON
From depesz at depesz.com  Wed Jul  2 01:31:40 2008
From: depesz at depesz.com (hubert depesz lubaczewski)
Date: Wed Jul  2 01:32:08 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from locking
	alltables
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
References: <486A0070.2060600@echo.fr>
	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
Message-ID: <20080702083140.GA12941@depesz.com>

On Tue, Jul 01, 2008 at 08:27:32AM -0700, Shahaf Abileah wrote:
> Also, in certain situations you might be able to run the alter command
> on each of the slaves and then on the master.  E.g. if you're adding a
> new column that allows null values, then it might be possible to alter
> the slaves even while rows are being inserted into the master.  You will
> still be taking a lock on the table while altering it, but at least
> you're not locking multiple tables at a time across multiple machines.
> Not sure if this is recommended practice.

it's not recommended, but it works.
you add column on slave, then run this on master:
UPDATE pg_trigger SET tgargs=substring(tgargs for octet_length(tgargs)-1)||E'v\\000' where tgname = 'TRIGGER_NAME';
and then add the column on master.

best regards,

depesz

From scetbon at echo.fr  Wed Jul  2 05:23:49 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Wed Jul  2 05:23:54 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from
	locking	alltables
In-Reply-To: <20080702083140.GA12941@depesz.com>
References: <486A0070.2060600@echo.fr>
	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
	<20080702083140.GA12941@depesz.com>
Message-ID: <486B7355.2010307@echo.fr>

there's no trigger on the pg_trigger table. I think you have to do it 
also on each node.

hubert depesz lubaczewski wrote:
> On Tue, Jul 01, 2008 at 08:27:32AM -0700, Shahaf Abileah wrote:
>   
>> Also, in certain situations you might be able to run the alter command
>> on each of the slaves and then on the master.  E.g. if you're adding a
>> new column that allows null values, then it might be possible to alter
>> the slaves even while rows are being inserted into the master.  You will
>> still be taking a lock on the table while altering it, but at least
>> you're not locking multiple tables at a time across multiple machines.
>> Not sure if this is recommended practice.
>>     
>
> it's not recommended, but it works.
> you add column on slave, then run this on master:
> UPDATE pg_trigger SET tgargs=substring(tgargs for octet_length(tgargs)-1)||E'v\\000' where tgname = 'TRIGGER_NAME';
> and then add the column on master.
>
> best regards,
>
> depesz
>
>   

-- 
Cyril SCETBON - Ing?nieur bases de donn?es
AUSY pour France T?l?com - OPF/PORTAILS/DOP/HEBEX

T?l : +33 (0)4 97 12 87 60
Jabber : cscetbon@jabber.org
France Telecom - Orange
790 Avenue du Docteur Maurice Donat 
B?timent Marco Polo C2 - Bureau 202
06250 Mougins
France

***********************************
Ce message et toutes les pieces jointes (ci-apres le 'message') sont
confidentiels et etablis a l'intention exclusive de ses destinataires.
Toute utilisation ou diffusion non autorisee est interdite.
Tout message electronique est susceptible d'alteration. Le Groupe France
Telecom decline toute responsabilite au titre de ce message s'il a ete
altere, deforme ou falsifie.
Si vous n'etes pas destinataire de ce message, merci de le detruire
immediatement et d'avertir l'expediteur.
***********************************
This message and any attachments (the 'message') are confidential and
intended solely for the addressees.
Any unauthorised use or dissemination is prohibited.
Messages are susceptible to alteration. France Telecom Group shall not be
liable for the message if altered, changed or falsified.
If you are not recipient of this message, please cancel it immediately and
inform the sender.
************************************

From depesz at depesz.com  Wed Jul  2 05:27:31 2008
From: depesz at depesz.com (hubert depesz lubaczewski)
Date: Wed Jul  2 05:27:38 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from locking
	alltables
In-Reply-To: <486B7355.2010307@echo.fr>
References: <486A0070.2060600@echo.fr>
	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
	<20080702083140.GA12941@depesz.com> <486B7355.2010307@echo.fr>
Message-ID: <20080702122731.GA29320@depesz.com>

On Wed, Jul 02, 2008 at 02:23:49PM +0200, Cyril SCETBON wrote:
> there's no trigger on the pg_trigger table. I think you have to do it  
> also on each node.

trigger is not on pg_trigger table, but on table to which you add
column.

depesz
From scetbon at echo.fr  Wed Jul  2 05:42:59 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Wed Jul  2 05:43:03 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from
	locking	alltables
In-Reply-To: <20080702122731.GA29320@depesz.com>
References: <486A0070.2060600@echo.fr>
	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
	<20080702083140.GA12941@depesz.com> <486B7355.2010307@echo.fr>
	<20080702122731.GA29320@depesz.com>
Message-ID: <486B77D3.1040601@echo.fr>



hubert depesz lubaczewski wrote:
> On Wed, Jul 02, 2008 at 02:23:49PM +0200, Cyril SCETBON wrote:
>   
>> there's no trigger on the pg_trigger table. I think you have to do it  
>> also on each node.
>>     
>
> trigger is not on pg_trigger table, but on table to which you add
> column.
>   
yes that's why I'm saying that the update pg_trigger won't be sent to 
other node.

-- 
Cyril SCETBON
From depesz at depesz.com  Wed Jul  2 05:51:08 2008
From: depesz at depesz.com (hubert depesz lubaczewski)
Date: Wed Jul  2 05:51:14 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from locking
	alltables
In-Reply-To: <486B77D3.1040601@echo.fr>
References: <486A0070.2060600@echo.fr>
	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
	<20080702083140.GA12941@depesz.com> <486B7355.2010307@echo.fr>
	<20080702122731.GA29320@depesz.com> <486B77D3.1040601@echo.fr>
Message-ID: <20080702125108.GA30475@depesz.com>

On Wed, Jul 02, 2008 at 02:42:59PM +0200, Cyril SCETBON wrote:
> yes that's why I'm saying that the update pg_trigger won't be sent to  
> other node.

yes, but it doesn't have to. trigger on slave node is different anyway.

depesz
From victor.aluko at gmail.com  Wed Jul  2 06:08:22 2008
From: victor.aluko at gmail.com (ajcity)
Date: Wed Jul  2 06:08:26 2008
Subject: [Slony1-general] Upgrading Postgresql 8.3.1 to 8.3.3: Any issues
	with Slony1-1.2.13
Message-ID: <18234291.post@talk.nabble.com>


  Hi all,
  I have a master node running PGSQL 8.3.1 and Slony 1.2.13  RHEL 3 and 2
nodes with the same PGSQL and Slony versions on OpenSuSE 10.3
  I wanna upgrade the Postgres to 8.3.3 on all 3 nodes, how do I do this
without having to drop my replication clusters and are there any known
issues with between Slony 1.2.13 and Postgresql 8.3.3?
  Do I also have to upgrade the Slony cos I wanna wait till Slony 2.0 is
fully released before doing this?
  
  Thanks for you help in advance.

   Victor
-- 
View this message in context: http://www.nabble.com/Upgrading-Postgresql-8.3.1-to-8.3.3%3A-Any-issues-with-Slony1-1.2.13-tp18234291p18234291.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From ajs at crankycanuck.ca  Wed Jul  2 06:44:31 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Jul  2 06:44:53 2008
Subject: [Slony1-general] RE: how to prevent EXECUTE SCRIPT from locking
In-Reply-To: <e0d7c3f50807011255k2da89ba0mc179238296ec863a@mail.gmail.com>
References: <e0d7c3f50807011255k2da89ba0mc179238296ec863a@mail.gmail.com>
Message-ID: <20080702134431.GB32287@crankycanuck.ca>

On Tue, Jul 01, 2008 at 02:55:32PM -0500, Troy Wolf wrote:
> 
> And even more surprising (disturbing?) is that Slony waits on locks
> that have NOTHING to do with any of the replicated objects. For

Yes.

> I have a patch to the code that is supposed to alleviate this problem.
> It was sent to me by another list member. I've not yet had time to
> review the code or test it.

You could actually go back through the CVS logs and revert the change -- as
I recall, it was all done in one change, but I'm relying on memory here.
 
> I truly wish I understood more about this aspect of Slony.

This wasn't a completely uncontroversial change; I opposed it, for instance,
exactly because I thought it made DDL much harder to do.  But the reason it
was adopted was because there was no mechanism to enforce that all relations
in a foreign key relationship had to be in the same set. (You couldn't make
that a stricture, because if you added a new table with a new FK, you have
to do that in a separate set.) Because of this, we had persistent problems
with people making changes that locked only one set, and didn't lock enough
sets.  This caused inconsistencies which Slony would later notice, and that
would cause the replication to halt at the point of the DDL.  The original
mechanism for this was, "Be careful, and do manual locks if need be." That
didn't work, because we rapidly discovered that altogether too many people
have no clue how their schema is designed, and want to use Slony while
remaining ignorant.

Since we always envisioned a DDL change to require an application outage
anyway, the developers decided that it would be acceptable just to lock
everything in order to perform the DDL.

The flat truth is that you have to take an application outage to perform DDL
on a Slony system.  Anyone claiming differently is thinking about the
academic operation of PostgreSQL, not the actual operation of the system
where locks can be held on a table for a surprising length of time.  (8.3 is
quite a bit better than previous releases if you're using autovacuum, note.)

An improvement that really would help would be a locking system that
abandoned its lock if it couldn't get it.  We don't have that yet, but 8.3
again has the ability to offer such a feature.

One other thing: if you really understand the bare metal functions Slony is
using, there actually _is_ a way to do this for just one table.  If you
spend a great deal of time with the manual, you can figure it out.  I don't
feel comfortable posting the instructions, because it's terribly dangerous. 
You can see a sort-of example in a (broken) script I posted to the list
about a year ago for doing bulk loads on many nodes.

A 

From ajs at crankycanuck.ca  Wed Jul  2 06:49:39 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Jul  2 06:49:48 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from locking
	alltables
In-Reply-To: <082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
References: <486A0070.2060600@echo.fr>
	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
Message-ID: <20080702134939.GC32287@crankycanuck.ca>

On Tue, Jul 01, 2008 at 08:27:32AM -0700, Shahaf Abileah wrote:
> Also, in certain situations you might be able to run the alter command
> on each of the slaves and then on the master.  E.g. if you're adding a
> new column that allows null values, then it might be possible to alter
> the slaves even while rows are being inserted into the master.  

No, you can't do that.  It will hopelessly break things.  Don't do it.

A

From ajs at crankycanuck.ca  Wed Jul  2 06:50:46 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Jul  2 06:50:56 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from locking
	alltables
In-Reply-To: <20080702134939.GC32287@crankycanuck.ca>
References: <486A0070.2060600@echo.fr>
	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
	<20080702134939.GC32287@crankycanuck.ca>
Message-ID: <20080702135046.GD32287@crankycanuck.ca>

On Wed, Jul 02, 2008 at 09:49:39AM -0400, Andrew Sullivan wrote:
> 
> No, you can't do that.  It will hopelessly break things.  Don't do it.

Well, I should say, "Don't do that unless you know how to fix it with the
bare-metal functions."  There is a way, but you really want to know what
you're doing. 

A
From ajs at crankycanuck.ca  Wed Jul  2 06:51:25 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Jul  2 06:51:35 2008
Subject: [Slony1-general] fail over failed
In-Reply-To: <d4f444290807010250s3b5a238ay8735a4a59441e04a@mail.gmail.com>
References: <d4f444290807010250s3b5a238ay8735a4a59441e04a@mail.gmail.com>
Message-ID: <20080702135125.GE32287@crankycanuck.ca>

On Tue, Jul 01, 2008 at 11:50:29AM +0200, lio bod wrote:
> 
> Subscription for set 1 is not active? My replication seems ok. At least it
> replicates...
> So i don't understand the message
> What could be wrong on this fail over?

What does sl_set say?

A
From nagy at ecircle-ag.com  Wed Jul  2 06:51:30 2008
From: nagy at ecircle-ag.com (Csaba Nagy)
Date: Wed Jul  2 06:51:37 2008
Subject: [Slony1-general] RE: how to prevent EXECUTE SCRIPT from locking
In-Reply-To: <20080702134431.GB32287@crankycanuck.ca>
References: <e0d7c3f50807011255k2da89ba0mc179238296ec863a@mail.gmail.com>
	<20080702134431.GB32287@crankycanuck.ca>
Message-ID: <1215006690.2834.17.camel@PCD12478>

On Wed, 2008-07-02 at 09:44 -0400, Andrew Sullivan wrote:
> One other thing: if you really understand the bare metal functions Slony is
> using, there actually _is_ a way to do this for just one table.

If there would be a slony script to add columns to just 1 table
reliably, and do all the necessary things transparently to the user, I
would argue that 90% of the use cases of EXECUTE SCRIPT would be
covered. Is that a scenario which can be covered reliably with a script
and only minimal locking involved ?

Cheers,
Csaba.


From ajs at crankycanuck.ca  Wed Jul  2 06:58:24 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Jul  2 06:58:34 2008
Subject: [Slony1-general] RE: how to prevent EXECUTE SCRIPT from locking
In-Reply-To: <1215006690.2834.17.camel@PCD12478>
References: <e0d7c3f50807011255k2da89ba0mc179238296ec863a@mail.gmail.com>
	<20080702134431.GB32287@crankycanuck.ca>
	<1215006690.2834.17.camel@PCD12478>
Message-ID: <20080702135824.GF32287@crankycanuck.ca>

On Wed, Jul 02, 2008 at 03:51:30PM +0200, Csaba Nagy wrote:
> 
> If there would be a slony script to add columns to just 1 table
> reliably, and do all the necessary things transparently to the user, I
> would argue that 90% of the use cases of EXECUTE SCRIPT would be
> covered. Is that a scenario which can be covered reliably with a script
> and only minimal locking involved ?

"Trasparently to the user?"  No.  The special case that I had was for doing
a bulk load, but it'd work for adding a column too.  The problem is that you
need to be _absolutely sure_ that nothing is touching that table in the
meantime.  One way to do it is to lock that table on all nodes.  It's a
complicated but not difficult bit of logic.  I think it's possible to write
a bespoke script for each occasion.  It's probably even possible (I haven't
tried, but I think I know how to do it) to write a small reusable bit of
code that would do this for all the nodes without the really heavy-duty lock
that Slony takes.

BUT (and that's in 21-point bold in red with the <blink> tag), it's not
safe as a general-purpose tool, for exactly the reason that the lock Slony
actually takes got escalated to every table.  If you have foreign keys or
something, and this script doesn't lock all of them at the same time, you
can easily deadlock and leave yourself in a serious mess.  

So I recommend against such a script.  Slony does it the way it does because
of safety: it's trying to protect you, and this is the only reliable way to
do it.  If you can be sure you have a case where you don't need that
protection, then by careful use of the bare metal functions (which is how
Slony does its work, after all) you can do this with less protection.

A
From nagy at ecircle-ag.com  Wed Jul  2 07:14:51 2008
From: nagy at ecircle-ag.com (Csaba Nagy)
Date: Wed Jul  2 07:14:57 2008
Subject: [Slony1-general] RE: how to prevent EXECUTE SCRIPT from locking
In-Reply-To: <20080702135824.GF32287@crankycanuck.ca>
References: <e0d7c3f50807011255k2da89ba0mc179238296ec863a@mail.gmail.com>
	<20080702134431.GB32287@crankycanuck.ca>
	<1215006690.2834.17.camel@PCD12478>
	<20080702135824.GF32287@crankycanuck.ca>
Message-ID: <1215008091.2834.26.camel@PCD12478>

On Wed, 2008-07-02 at 09:58 -0400, Andrew Sullivan wrote:
> So I recommend against such a script.  Slony does it the way it does because
> of safety: it's trying to protect you, and this is the only reliable way to
> do it.  If you can be sure you have a case where you don't need that
> protection, then by careful use of the bare metal functions (which is how
> Slony does its work, after all) you can do this with less protection.

But that's a lot messier and riskier than having a script with a few
clear instructions in what circumstances it can work, and maybe a few
checks to enforce those circumstances are met (e.g. it must be a way to
figure out in a programmatic way if there are any FKs to/from that
table). It would also be relatively easy to enforce that the SQL to be
applied is just 1 statement which only touches 1 table...

The alternatives for me are:

 - keep watching my DBs for the moment I can sneak in a full DB lock
without breaking too many things (we managed to grow ourselves to the
point we have a pretty round the clock operation which is really always
busy);

 - study the bare metal functions you're talking about and cross my
fingers it wont kill my DB;

Both of these alternatives are very uncomfortable to me. Any script
which gets some testing would be a lot safer and welcome...

Cheers,
Csaba.


From scetbon at echo.fr  Wed Jul  2 08:07:30 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Wed Jul  2 08:07:37 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from
	locking	alltables
In-Reply-To: <20080702125108.GA30475@depesz.com>
References: <486A0070.2060600@echo.fr>	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>	<20080702083140.GA12941@depesz.com>
	<486B7355.2010307@echo.fr>	<20080702122731.GA29320@depesz.com>
	<486B77D3.1040601@echo.fr> <20080702125108.GA30475@depesz.com>
Message-ID: <486B99B2.1090900@echo.fr>



hubert depesz lubaczewski wrote:
> On Wed, Jul 02, 2008 at 02:42:59PM +0200, Cyril SCETBON wrote:
>   
>> yes that's why I'm saying that the update pg_trigger won't be sent to  
>> other node.
>>     
>
> yes, but it doesn't have to. trigger on slave node is different anyway.
>   
you're right ! Did you test SWITCH after this modification ? no issue ?
> depesz
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>   

-- 
Cyril SCETBON - Ing?nieur bases de donn?es
AUSY pour France T?l?com - OPF/PORTAILS/DOP/HEBEX

T?l : +33 (0)4 97 12 87 60
Jabber : cscetbon@jabber.org
France Telecom - Orange
790 Avenue du Docteur Maurice Donat 
B?timent Marco Polo C2 - Bureau 202
06250 Mougins
France

***********************************
Ce message et toutes les pieces jointes (ci-apres le 'message') sont
confidentiels et etablis a l'intention exclusive de ses destinataires.
Toute utilisation ou diffusion non autorisee est interdite.
Tout message electronique est susceptible d'alteration. Le Groupe France
Telecom decline toute responsabilite au titre de ce message s'il a ete
altere, deforme ou falsifie.
Si vous n'etes pas destinataire de ce message, merci de le detruire
immediatement et d'avertir l'expediteur.
***********************************
This message and any attachments (the 'message') are confidential and
intended solely for the addressees.
Any unauthorised use or dissemination is prohibited.
Messages are susceptible to alteration. France Telecom Group shall not be
liable for the message if altered, changed or falsified.
If you are not recipient of this message, please cancel it immediately and
inform the sender.
************************************

From depesz at depesz.com  Wed Jul  2 08:14:11 2008
From: depesz at depesz.com (hubert depesz lubaczewski)
Date: Wed Jul  2 08:14:19 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from locking
	alltables
In-Reply-To: <486B99B2.1090900@echo.fr>
References: <486A0070.2060600@echo.fr>
	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
	<20080702083140.GA12941@depesz.com> <486B7355.2010307@echo.fr>
	<20080702122731.GA29320@depesz.com> <486B77D3.1040601@echo.fr>
	<20080702125108.GA30475@depesz.com> <486B99B2.1090900@echo.fr>
Message-ID: <20080702151411.GA7233@depesz.com>

On Wed, Jul 02, 2008 at 05:07:30PM +0200, Cyril SCETBON wrote:
> you're right ! Did you test SWITCH after this modification ? no issue ?

no, i haven't. this method was given me on this list some time ago, and
i used it, but didn't need to switch, so it is untested. for me. but the
test should be pretty simple.

depesz

From stephane.schildknecht at postgresqlfr.org  Wed Jul  2 08:21:16 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Wed Jul  2 08:21:28 2008
Subject: [Slony1-general] Upgrading Postgresql 8.3.1 to 8.3.3: Any issues
	with Slony1-1.2.13
In-Reply-To: <18234291.post@talk.nabble.com>
References: <18234291.post@talk.nabble.com>
Message-ID: <486B9CEC.1080409@postgresqlfr.org>

ajcity a ?crit :
>   Hi all,
>   I have a master node running PGSQL 8.3.1 and Slony 1.2.13  RHEL 3 and 2
> nodes with the same PGSQL and Slony versions on OpenSuSE 10.3
>   I wanna upgrade the Postgres to 8.3.3 on all 3 nodes, how do I do this
> without having to drop my replication clusters and are there any known
> issues with between Slony 1.2.13 and Postgresql 8.3.3?
>   Do I also have to upgrade the Slony cos I wanna wait till Slony 2.0 is
> fully released before doing this?
>   
>   Thanks for you help in advance.
> 
>    Victor

To upgrade PG, you just have to install the new version on every server and
restart the postmaster. That's all.

The replication won't be impacted.

You could upgrade Slony, also. In order to do that, you have to compile it on
every node, stop the replication, upgrade stored procedures with the "update
functions" directive, and the restart slon. That's it. You can have a look at
that page : http://slony.info/documentation/slonyupgrade.html

Best regards,
-- 
St?phane SCHILDKNECHT
Pr?sident de PostgreSQLFr
T?l. 09 53 69 97 12
http://www.postgresql.fr

From ajs at crankycanuck.ca  Wed Jul  2 08:37:09 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Jul  2 08:37:40 2008
Subject: [Slony1-general] RE: how to prevent EXECUTE SCRIPT from locking
In-Reply-To: <1215008091.2834.26.camel@PCD12478>
References: <e0d7c3f50807011255k2da89ba0mc179238296ec863a@mail.gmail.com>
	<20080702134431.GB32287@crankycanuck.ca>
	<1215006690.2834.17.camel@PCD12478>
	<20080702135824.GF32287@crankycanuck.ca>
	<1215008091.2834.26.camel@PCD12478>
Message-ID: <20080702153709.GH32287@crankycanuck.ca>

On Wed, Jul 02, 2008 at 04:14:51PM +0200, Csaba Nagy wrote:
> 
> But that's a lot messier and riskier than having a script with a few
> clear instructions in what circumstances it can work, and maybe a few
> checks to enforce those circumstances are met (e.g. it must be a way to
> figure out in a programmatic way if there are any FKs to/from that
> table). It would also be relatively easy to enforce that the SQL to be
> applied is just 1 statement which only touches 1 table...

That's the arrangement we used to have, and it didn't work.  It's how the
lock got as heavy duty as it is.

>  - keep watching my DBs for the moment I can sneak in a full DB lock
> without breaking too many things (we managed to grow ourselves to the
> point we have a pretty round the clock operation which is really always
> busy);

If you can't schedule 10 minutes of downtime for a schema change, then
Slony's the wrong tool for your job.  Sorry.  It can't provide five-nines.
 
>  - study the bare metal functions you're talking about and cross my
> fingers it wont kill my DB;
> 
> Both of these alternatives are very uncomfortable to me. Any script
> which gets some testing would be a lot safer and welcome...

The script will get more testing for your environment if you write it for
your environment.  A general-purpose script isn't ever going to get enough
testing.  And you don't have to cross your fingers: those functions _get
called all the time_.  They're what Slony uses. 

You really have two choices, but they're not the ones you listed:

1.  Live with the Slony-provided limitation.

2.  Become expert in Slony, and then write something that reduces the locks
taken while still behaving safely in your environment.

If you want all of fast, cheap, and good, you're not going to get it.  Sorry
:-(

A
From victor.aluko at gmail.com  Wed Jul  2 08:45:26 2008
From: victor.aluko at gmail.com (ajcity)
Date: Wed Jul  2 08:45:34 2008
Subject: [Slony1-general] Upgrading Postgresql 8.3.1 to 8.3.3: Any
	issues with Slony1-1.2.13
In-Reply-To: <486B9CEC.1080409@postgresqlfr.org>
References: <18234291.post@talk.nabble.com> <486B9CEC.1080409@postgresqlfr.org>
Message-ID: <18240617.post@talk.nabble.com>




&quot;St?phane A. Schildknecht&quot; wrote:
> 
> ajcity a ?crit :
>>   Hi all,
>>   I have a master node running PGSQL 8.3.1 and Slony 1.2.13  RHEL 3 and 2
>> nodes with the same PGSQL and Slony versions on OpenSuSE 10.3
>>   I wanna upgrade the Postgres to 8.3.3 on all 3 nodes, how do I do this
>> without having to drop my replication clusters and are there any known
>> issues with between Slony 1.2.13 and Postgresql 8.3.3?
>>   Do I also have to upgrade the Slony cos I wanna wait till Slony 2.0 is
>> fully released before doing this?
>>   
>>   Thanks for you help in advance.
>> 
>>    Victor
> 
> To upgrade PG, you just have to install the new version on every server
> and
> restart the postmaster. That's all.
> 
> The replication won't be impacted.
> 
> You could upgrade Slony, also. In order to do that, you have to compile it
> on
> every node, stop the replication, upgrade stored procedures with the
> "update
> functions" directive, and the restart slon. That's it. You can have a look
> at
> that page : http://slony.info/documentation/slonyupgrade.html
> 
> Best regards,
> -- 
> St?phane SCHILDKNECHT
> Pr?sident de PostgreSQLFr
> T?l. 09 53 69 97 12
> http://www.postgresql.fr
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

  Thanks for your quick response and making it straight to the point.
  I have one more question:
    For the nodes running OpenSuSE, I installed the Postgresql from the
tarball and in order to upgrade the Postgresql I have to change pgsql folder
name which might affect the Slony installation.
Do I have to re-install Slony in the new pgsql folder or do I just copy the
'slon' and 'slonik' command executables?
   
  Thanks in advance for your response.

  Victor

-- 
View this message in context: http://www.nabble.com/Upgrading-Postgresql-8.3.1-to-8.3.3%3A-Any-issues-with-Slony1-1.2.13-tp18234291p18240617.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nagy at ecircle-ag.com  Wed Jul  2 09:02:56 2008
From: nagy at ecircle-ag.com (Csaba Nagy)
Date: Wed Jul  2 09:03:05 2008
Subject: [Slony1-general] RE: how to prevent EXECUTE SCRIPT from locking
In-Reply-To: <20080702153709.GH32287@crankycanuck.ca>
References: <e0d7c3f50807011255k2da89ba0mc179238296ec863a@mail.gmail.com>
	<20080702134431.GB32287@crankycanuck.ca>
	<1215006690.2834.17.camel@PCD12478>
	<20080702135824.GF32287@crankycanuck.ca>
	<1215008091.2834.26.camel@PCD12478>
	<20080702153709.GH32287@crankycanuck.ca>
Message-ID: <1215014577.2834.39.camel@PCD12478>

On Wed, 2008-07-02 at 11:37 -0400, Andrew Sullivan wrote:
> If you can't schedule 10 minutes of downtime for a schema change, then
> Slony's the wrong tool for your job.  Sorry.  It can't provide five-nines.

I could live with that if I would be convinced that it is the case,
which I'm not... it must be a way to achieve a lower contention schema
update scenario which is generic enough.

> The script will get more testing for your environment if you write it for
> your environment.  A general-purpose script isn't ever going to get enough
> testing.  And you don't have to cross your fingers: those functions _get
> called all the time_.  They're what Slony uses. 

It's not about the reliability of the slony bare metal functions, it's
all about the reliability of _my_ judgment when applying them... all the
security is coming from experience, and I would like to avoid as much as
possible experimenting on production DBs. Learning on test DBs has only
limited value, in my experience there will always be a small but
significant detail which makes the test machine work and the production
one break.

> You really have two choices, but they're not the ones you listed:
> 
> 1.  Live with the Slony-provided limitation.

If it is a real limitation, I will have to live with it, but as I said
I'm not convinced. There must be a way to do it...

> 2.  Become expert in Slony, and then write something that reduces the locks
> taken while still behaving safely in your environment.
> 
> If you want all of fast, cheap, and good, you're not going to get it.  Sorry
> :-(

Well I want the best open source I can get. And I'm not shy of doing it
myself, it's just slony is an incredibly complicated beast, and I just
can't trust any home-grown solution enough to just use it and hope that
I didn't overlook anything... that's why I would hope a generic tool
which is reviewed by people who know why it is wrong if it is.

In this very moment I don't have the time to research this, but in the
long run I definitely will find the time to look at it as it is the most
problematic issue with slony for us. And there must be a way to fix
it...

Cheers,
Csaba.




From dane at greatschools.net  Wed Jul  2 10:27:41 2008
From: dane at greatschools.net (Dane Miller)
Date: Wed Jul  2 10:28:19 2008
Subject: [Slony1-general] SUBSCRIBE SET/WAIT FOR EVENT documentation problems
In-Reply-To: <e0d7c3f50806260626m271c96ceg9dadcb99cd21fa6d@mail.gmail.com>
References: <20080625143347.9D34A290D7B@main.slony.info>
	<e0d7c3f50806250840g7d4a534dq1a551121c1c7b65f@mail.gmail.com>
	<88daf38c0806250855x16d09893t2c4fceb37fe0fa4a@mail.gmail.com>
	<e0d7c3f50806251021t510f8793h6d50fa52ec6acb36@mail.gmail.com>
	<60bq1p2y6n.fsf@dba2.int.libertyrms.com>
	<88daf38c0806251220h1ccf2419jecd1f239229db29a@mail.gmail.com>
	<e0d7c3f50806260626m271c96ceg9dadcb99cd21fa6d@mail.gmail.com>
Message-ID: <1215019661.15000.38.camel@danedesk.greatschools.net>

Hi Slony Friends,

Like many of you, I followed the recent thread about SUBSCRIBE SET/WAIT
FOR EVENT (labeled with the unhelpful subject line "Slony1-general
Digest, Vol 16, Issue 27").  Excerpted here...

On Thu, 2008-06-26 at 08:26 -0500, Troy Wolf wrote:
> Alexander, I don't know if this is the missing magic, but compare your
> code to the the code Chris Browne posted:
> 
> On Wed, Jun 25, 2008 at 2:20 PM, Alexander Staubo <alex@bengler.no>
> wrote:
> >> SUBSCRIBE SET (ID=1, PROVIDER=1, RECEIVER=2);
> >> WAIT FOR EVENT (ORIGIN=2, CONFIRMED = 1);
> >> SYNC(ID = 1);
> >> WAIT FOR EVENT (ORIGIN=1, CONFIRMED=2);
> 
> Chris Browne:
> subscribe set (id=1, provider=1, receiver=2, forward=yes);
> sync(id=1);    # Submits a SYNC event
> wait for event (origin=all, confirmed=2, wait on=1);   # Waits for
> that event to be confirmed on node #2
> 
> First notice that the WAIT FOR EVENT after the SUBSCRIBE is useless.
> It tells you nothing. Second, notice the "origin=all" and "wait on=1"
> in the WAIT FOR EVENT after the SYNC. Perhaps these are important
> ingredients? 

IMO, two very important pieces of information resulted from this thread,
that I think are worth highlighting:

   1) The correct usage of WAIT FOR EVENT is...
       WAIT FOR EVENT (origin=all, confirmed=2, wait on=1)

   2) SYNC and WAIT FOR EVENT must happen in the same slonik request.

Note that #1 is wrong in the documentation here
http://www.slony.info/documentation/addthings.html.  Perhaps this is
already fixed in the documentation in CVS... who knows? Why is this not
a wiki, I'd have fixed it myself?

Perhaps this is a good time to ask for clarification on WAIT FOR EVENT
usage.  What are the proper values for w, x, y, z given the following:
     SUBSCRIBE SET (ID = 999, PROVIDER = 10, RECEIVER = 20);
     SYNC (ID=w);
     WAIT FOR EVENT (ORIGIN = x, CONFIRMED = y, WAIT FOR = z);

Furthermore, is there a relationship between these values that always
holds true?  For example, w = x = z.

Perhaps also a good time to bring up the topic of a slony community wiki
to keep documentation fresh and spread the load among slony community
members.  Lots of smart people on this list would like to give back.
Let us.

Dane
-- 
Dane Miller
Systems Administrator
Greatschools, Inc
http://www.greatschools.net

From ajs at crankycanuck.ca  Wed Jul  2 11:30:14 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Jul  2 11:30:44 2008
Subject: [Slony1-general] SUBSCRIBE SET/WAIT FOR EVENT documentation
	problems
In-Reply-To: <1215019661.15000.38.camel@danedesk.greatschools.net>
References: <20080625143347.9D34A290D7B@main.slony.info>
	<e0d7c3f50806250840g7d4a534dq1a551121c1c7b65f@mail.gmail.com>
	<88daf38c0806250855x16d09893t2c4fceb37fe0fa4a@mail.gmail.com>
	<e0d7c3f50806251021t510f8793h6d50fa52ec6acb36@mail.gmail.com>
	<60bq1p2y6n.fsf@dba2.int.libertyrms.com>
	<88daf38c0806251220h1ccf2419jecd1f239229db29a@mail.gmail.com>
	<e0d7c3f50806260626m271c96ceg9dadcb99cd21fa6d@mail.gmail.com>
	<1215019661.15000.38.camel@danedesk.greatschools.net>
Message-ID: <20080702183014.GI32287@crankycanuck.ca>

On Wed, Jul 02, 2008 at 10:27:41AM -0700, Dane Miller wrote:
> IMO, two very important pieces of information resulted from this thread,
> that I think are worth highlighting:
> 
>    1) The correct usage of WAIT FOR EVENT is...
>        WAIT FOR EVENT (origin=all, confirmed=2, wait on=1)

No.  That's _a_ correct use.  There are other cases where you just want to
know, "Did that get over there?"
 
> Perhaps also a good time to bring up the topic of a slony community wiki
> to keep documentation fresh and spread the load among slony community
> members.  Lots of smart people on this list would like to give back.
> Let us.

Please, not a wiki.  Documentation in wikis is, in my experience, always
incomplete, or subtly wrong, or badly organized, or all of the above.  If
you have changes to the docs, send a patch.

A

From jameshtucker at gmail.com  Wed Jul  2 18:40:14 2008
From: jameshtucker at gmail.com (James Tucker)
Date: Wed Jul  2 18:40:32 2008
Subject: [Slony1-general] Execute script when subscriber lagging
Message-ID: <b10d372b0807021840q473466d1vc262528cbd7dab99@mail.gmail.com>

Hi All,

Just wanting to confirm there is no issue running an execute script when a
subscriber is lagging the provider.  Does the DDL just get put in the sync
queue and is executed at the same point in the replication stream as it was
on the master ?  Or does it prioritise the DDL ?

Cheers
James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080703/=
487b0526/attachment.htm
From Ow.Mun.Heng at wdc.com  Wed Jul  2 19:22:36 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Wed Jul  2 19:44:15 2008
Subject: [Slony1-general] Execute script when subscriber lagging
In-Reply-To: <b10d372b0807021840q473466d1vc262528cbd7dab99@mail.gmail.com>
References: <b10d372b0807021840q473466d1vc262528cbd7dab99@mail.gmail.com>
Message-ID: <1215051756.19389.6.camel@neuromancer.home.net>

On Thu, 2008-07-03 at 11:40 +1000, James Tucker wrote:
> Just wanting to confirm there is no issue running an execute script
> when a subscriber is lagging the provider.  Does the DDL just get put
> in the sync queue and is executed at the same point in the replication
> stream as it was on the master ?  Or does it prioritise the DDL ?

Just to share my 2 bits worth.

My understanding is that it will get into a sync queue and it needs to
make sure that everything is in sync before the DDL will execute.

In my experience, in my environment, what I have experienced is that I
need to make sure that

1) syncs are up-to-date
2) no connections to the master DB (I kill off pgpool especially
culprit)

then I will execute the DDL execute script

#!/bin/sh

#
# Create a new column in a table in the replicated set
#
echo "Creating new column in the whatever table"
slonik <<_EOF_
cluster name = hmxmms_my_cluster;
node 1 admin conninfo = 'dbname=hmxmms host=xxx user=postgres
port=5432';
node 2 admin conninfo = 'dbname=hmxmms host=yyy user=postgres
port=5432';

execute script ( SET ID = 6, FILENAME =
'/home/pg_operator/slony/slony_add_column_20080624.sql', EVENT NODE =
1);
_EOF_


This is the hardest and most tedious script which I need to drop down to
a SSH connection to execute. For whatever reason, pgadmin doesn't have
this functionality.

From bijayant4u at yahoo.com  Thu Jul  3 00:25:36 2008
From: bijayant4u at yahoo.com (bijayant kumar)
Date: Thu Jul  3 00:25:53 2008
Subject: [Slony1-general] Node is not initialized properly
In-Reply-To: <146643.93138.qm@web25807.mail.ukl.yahoo.com>
Message-ID: <38240.23396.qm@web32703.mail.mud.yahoo.com>

Hi,

Now i run the slon on the Master Server(192.168.99.23) only by slon_start 1 and slon_start2. It started without any error and in logs i am seeing

tail -f /var/log/slony/slony1/node1/bijayant-2008-07-03_12:43:28.log

2008-07-03 12:49:36 IST DEBUG2 remoteWorkerThread_2: forward confirm 1,414 received by 2
2008-07-03 12:49:40 IST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 415
2008-07-03 12:49:42 IST DEBUG2 localListenThread: Received event 1,415 SYNC
2008-07-03 12:49:47 IST DEBUG2 remoteListenThread_2: queue event 2,363 SYNC

2008-07-03 12:49:47 IST DEBUG2 remoteWorkerThread_2: Received event 2,363 SYNC
2008-07-03 12:49:47 IST DEBUG2 remoteWorkerThread_2: SYNC 363 processing
2008-07-03 12:49:47 IST DEBUG2 remoteWorkerThread_2: no sets need syncing for this event

and 
tail -f /var/log/slony/slony1/node2/bijayant-2008-07-03_12:45:20.log

2008-07-03 12:51:37 IST DEBUG2 remoteWorkerThread_1: forward confirm 2,374 received by 1
2008-07-03 12:51:43 IST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 375
2008-07-03 12:51:44 IST DEBUG2 remoteListenThread_1: queue event 1,427 SYNC
2008-07-03 12:51:44 IST DEBUG2 remoteWorkerThread_1: Received event 1,427 SYNC
2008-07-03 12:51:44 IST DEBUG2 remoteWorkerThread_1: SYNC 427 processing
2008-07-03 12:51:44 IST DEBUG2 remoteWorkerThread_1: no sets need syncing for this event
2008-07-03 12:51:45 IST DEBUG2 localListenThread: Received event 2,375 SYNC

Nothing is happening in the database, no datas are getting copied also. But when i did tcpdump on the Slave Server, i can see the packets are going to and fro.

tcpdump -ni eth1 host 192.168.99.23 and port 5432

12:45:49.225540 IP 192.168.99.23.49820 > 192.168.99.134.5432: . ack 3955 win 1996 <nop,nop,timestamp 2359500 457346>
12:45:49.225585 IP 192.168.99.23.49820 > 192.168.99.134.5432: P 4312:4339(27) ack 3955 win 1996 <nop,nop,timestamp 2359500 457346>

Please guide me, i think i am very close.

Thanks & Regards,

Bijayant Kumar


--- On Tue, 1/7/08, Glyn Astill <glynastill@yahoo.co.uk> wrote:

> From: Glyn Astill <glynastill@yahoo.co.uk>
> Subject: Re: [Slony1-general] Node is not initialized properly
> To: bijayant4u@yahoo.com, "Martin Eriksson" <m.eriksson@albourne.com>
> Cc: slony1-general@lists.slony.info
> Date: Tuesday, 1 July, 2008, 7:26 PM
> Looks like you are running slons for both servers on each
> server.
> 
> You only need 1 slon per server.
> 
> So on the master (192.168.99.23) just run the slon for the
> master, and
> on the slave (192.168.99.134) just run the slon for the
> slave.
> 
> Or ofcourse you can run both slons on the master and none
> on the slave,
> both on the slave and none on the master, or both on a
> totally separate
> machine...
> 
> 
> ----- Original Message ----
> > From: bijayant kumar <bijayant4u@yahoo.com>
> > To: Martin Eriksson <m.eriksson@albourne.com>;
> Glyn Astill <glynastill@yahoo.co.uk>
> > Cc: slony1-general@lists.slony.info
> > Sent: Tuesday, 1 July, 2008 2:08:37 PM
> > Subject: Re: [Slony1-general] Node is not initialized
> properly
> > 
> > On the Master Server (192.168.99.23)
> > 
> > bijayant ~ # ps aux | grep slon
> > 
> > root      3956  0.0  0.1   2116   692 pts/2    S   
> 18:27   0:00 /usr/bin/slon 
> > -s 1000 -d2 bijayant host=192.168.99.23
> dbname=bijayant user=bijayant port=5432 
> > password=bijayant
> > 
> > root      3961  0.0  0.1  51424   864 pts/2    Sl  
> 18:27   0:00 
> > /usr/bin/slon -s 1000 -d2 bijayant host=192.168.99.23
> dbname=bijayant 
> > user=bijayant port=5432 password=bijayant
> > 
> > root      3968  0.0  0.4   3612  2188 pts/2    S   
> 18:27   0:00 
> > /usr/local/bin/perl /usr/bin/slon_watchdog
> --config=/etc/slon_tools.conf node1 
> > 30
> > 
> > root      3998  0.0  0.1   2120   692 pts/2    S   
> 18:28   0:00 /usr/bin/slon 
> > -s 1000 -d2 bijayant host=192.168.99.134
> dbname=bijayant user=bijayant port=5432 
> > password=bijayant
> > 
> > root      4006  0.0  0.1  51428   868 pts/2    Sl  
> 18:28   0:00 /usr/bin/slon 
> > -s 1000 -d2 bijayant host=192.168.99.134
> dbname=bijayant 
> > user=bijayant port=5432 password=bijayant
> > 
> > root      4008  0.0  0.4   3616  2192 pts/2    S   
> 18:28   0:00 
> > /usr/local/bin/perl /usr/bin/slon_watchdog
> --config=/etc/slon_tools.conf node2 
> > 30
> > 
> > On server vi /etc/slon_tools.conf
> > if ($ENV{"SLONYNODES"}) {
> >     require $ENV{"SLONYNODES"};
> > } else {
> >     $CLUSTER_NAME = 'bijayant';
> >     $LOGDIR = '/var/log/slony';
> >     $MASTERNODE = 1;
> >     add_node(node     => 1,
> >              host     => '192.168.99.23',
> >              dbname   => 'bijayant',
> >              port     => 5432,
> >              user     => 'bijayant',
> >              password => 'bijayant');
> > 
> >     add_node(node     => 2,
> >              host     => '192.168.99.134',
> >              dbname   => 'bijayant',
> >              port     => 5432,
> >              user     => 'bijayant',
> >              password => 'bijayant',
> >              parent => 1
> >              );
> > }
> > 
> > $SLONY_SETS = {
> >     # A unique name for the set
> >     "set1" => {
> >         # The set_id, also unique
> >         "set_id" => 1,
> >         # "origin" => 1,
> >         # foldCase => 0,
> > #       "table_id"    => 1,
> > #       "sequence_id" => 1,
> >        "pkeyedtables" => [
> >                            'public.kavach',
> >                            ],
> >         },
> > };
> > 
> > if ($ENV{"SLONYSET"}) {
> >     require $ENV{"SLONYSET"};
> > }
> > 
> > 1;
> > 
> > vi /etc/conf.d/slony1
> > USER=postgres
> > CLUSTER=bijayant
> > DBUSER=bijayant
> > DBNAME=bijayant
> > DBHOST=192.168.99.23
> > LOGFILE=/var/lib/postgresql/data/slony1.log
> > LOGLEVEL=1
> > 
> > 
> > And on the Slave Server (192.168.99.134) the file is
> exactly same.
> > 
> > Bijayant Kumar
> > 
> > 
> > --- On Tue, 1/7/08, Glyn Astill wrote:
> > 
> > > From: Glyn Astill 
> > > Subject: Re: [Slony1-general] Node is not
> initialized properly
> > > To: bijayant4u@yahoo.com, "Martin
> Eriksson" 
> > > Cc: slony1-general@lists.slony.info
> > > Date: Tuesday, 1 July, 2008, 6:26 PM
> > > Check what slons are running against each
> database. You
> > > should have a slon running for each machine, so
> if you are
> > > running one slon on each server you should have
> the
> > > conninfo parameter configured in slon.conf, or be
> passing
> > > it at the command line. A ps -ax should show what
> server
> > > the slons are acting on.
> > > 
> > > 
> > > 
> > > ----- Original Message ----
> > > > From: bijayant kumar 
> > > > To: Martin Eriksson ;
> > > Glyn Astill 
> > > > Cc: slony1-general@lists.slony.info
> > > > Sent: Tuesday, 1 July, 2008 1:45:11 PM
> > > > Subject: Re: [Slony1-general] Node is not
> initialized
> > > properly
> > > > 
> > > > Hello to list,
> > > > 
> > > > My this problem is solved now. I have
> re-installed
> > > postgresql and slony1 and the 
> > > > problem gone. Now i have another problem.
> When i run
> > > slony on Master 
> > > > Server(192.168.99.23), i can see in the logs
> > > > 
> > > > 2008-07-01 18:09:50 IST CONFIG enableNode:
> no_id=2
> > > > 2008-07-01 18:09:50 IST DEBUG1
> remoteWorkerThread_2:
> > > thread starts
> > > > 2008-07-01 18:09:50 IST DEBUG1
> remoteListenThread_2:
> > > thread starts
> > > > 2008-07-01 18:09:50 IST DEBUG1 main: running
> scheduler
> > > mainloop
> > > > 2008-07-01 18:09:50 IST DEBUG1
> cleanupThread: thread
> > > starts
> > > > 2008-07-01 18:09:50 IST DEBUG1 syncThread:
> thread
> > > starts
> > > > 2008-07-01 18:09:50 IST DEBUG1
> remoteListenThread_2:
> > > connected to 
> > > > 'host=192.168.99.134 dbname=bijayant
> user=bijayant
> > > port=5432 password=bijayant'
> > > > 
> > > > But when on second node that is on Slave
> > > server(192.168.99.134), i run slony i 
> > > > got
> > > > 
> > > > 2008-07-01 18:03:07 IST FATAL  Do you
> already have a
> > > slon running against this 
> > > > node?
> > > > 2008-07-01 18:03:07 IST FATAL  Or perhaps a
> residual
> > > idle backend connection 
> > > > from a dead slon?
> > > > 2008-07-01 18:03:07 IST DEBUG2 slon_abort()
> from
> > > pid=7871
> > > > 2008-07-01 18:03:07 IST DEBUG1 slon:
> shutdown
> > > requested
> > > > 2008-07-01 18:03:07 IST DEBUG2 slon: notify
> worker
> > > process to shutdown
> > > > 2008-07-01 18:03:27 IST DEBUG1 slon: child
> termination
> > > timeout - kill child
> > > > 2008-07-01 18:03:27 IST DEBUG2 slon: child
> terminated
> > > status: 9; pid: 7871, 
> > > > current worker pid: 7871
> > > > 2008-07-01 18:03:27 IST DEBUG1 slon: done
> > > > 2008-07-01 18:03:27 IST DEBUG2 slon: remove
> pid file
> > > > 2008-07-01 18:03:27 IST DEBUG2 slon: exit(0)
> > > > 
> > > > 
> > > > I have a exact same configuration(like xerox
> copy) on
> > > both the server. Is it 
> > > > creating the problem? Should i have to
> change the 
> > > > parameters(/etc/slon_tools.conf) according
> to
> > > Master/Slave.
> > > > 
> > > > Please help me, i think i am very near to
> replicate my
> > > first ever database.
> > > > 
> > > > Bijayant Kumar
> > > > 
> > > > 
> > > > --- On Tue, 1/7/08, bijayant kumar wrote:
> > > > 
> > > > > From: bijayant kumar 
> > > > > Subject: Re: [Slony1-general] Node is
> not
> > > initialized properly
> > > > > To: "Martin Eriksson" ,
> "Glyn
> > > Astill" 
> > > > 
> > > > > Cc: slony1-general@lists.slony.info
> > > > > Date: Tuesday, 1 July, 2008, 3:05 PM
> > > > > Hi,
> > > > > The user "bijayant" is a
> super user. I
> > > have
> > > > > created this user like
> > > > > 
> > > > > postgres@bijayant ~ $ createuser
> bijayant
> > > > > Shall the new user be allowed to create
> > > databases? (y/n) y
> > > > > Shall the new user be allowed to create
> more new
> > > users?
> > > > > (y/n) y
> > > > > CREATE USER
> > > > > 
> > > > > It means that bijayant is a super user
> right? Or
> > > i am doing
> > > > > some silly thing. If i will use user as
> pgsql
> > > than what to
> > > > > use for password.
> > > > > 
> > > > > bijayant ~ # pg_config --libdir
> > > > > /usr/lib
> > > > > 
> > > > > bijayant ~ # pg_config --pkglibdir
> > > > > /usr/lib/postgresql
> > > > > 
> > > > > In the Master Database server the
> xxid.so file is
> > > in 
> > > > > /usr/local/pgsql/lib/xxid.so. I have
> copied to
> > > this file to
> > > > > /usr/lib/postgresql/ and /usr/lib/
> also. But no
> > > luck.
> > > > > 
> > > > > On the Slave Server, its in
> > > /usr/lib64/postgresql/xxid.so.
> > > > > 
> > > > > Is it creating the problem. How to
> resolve this
> > > problem.
> > > > > I have installed postgres and slony1 by
> the
> > > emerge utility
> > > > > of gentoo.
> > > > > 
> > > > > Thanks & Regards,
> > > > > 
> > > > > Bijayant Kumar
> > > > > 
> > > > > 
> > > > > --- On Tue, 1/7/08, Glyn Astill
> > > > > wrote:
> > > > > 
> > > > > > From: Glyn Astill 
> > > > > > Subject: Re: [Slony1-general] Node
> is not
> > > initialized
> > > > > properly
> > > > > > To: bijayant4u@yahoo.com,
> "Martin
> > > Eriksson"
> > > > > 
> > > > > > Cc:
> slony1-general@lists.slony.info
> > > > > > Date: Tuesday, 1 July, 2008, 2:22
> PM
> > > > > > I'd be checking to see where
> xxid.so was
> > > on the
> > > > > system,
> > > > > > and if I had an older version
> lurking
> > > somewhere.
> > > > > > 
> > > > > > I'd also make sure bijayant
> was a
> > > database
> > > > > superuser.
> > > > > > 
> > > > > > 
> > > > > > 
> > > > > > ----- Original Message ----
> > > > > > > From: bijayant kumar 
> > > > > > > To: Martin Eriksson
> > > > > 
> > > > > > > Cc:
> slony1-general@lists.slony.info
> > > > > > > Sent: Tuesday, 1 July, 2008
> 8:00:19 AM
> > > > > > > Subject: Re: [Slony1-general]
> Node is
> > > not
> > > > > initialized
> > > > > > properly
> > > > > > > 
> > > > > > > Thanks to all for the reply.
> Now i
> > > think that i
> > > > > am
> > > > > > coming closer to run slonik. 
> > > > > > > When i did as suggested by
> all
> > > > > > > 
> > > > > > > bijayant ~ #
> slonik_init_cluster
> > > --config
> > > > > > /etc/slon_tools.conf | slonik
> > > > > > > 
> > > > > > > :6: PGRES_FATAL_ERROR load
> > > > > '$libdir/xxid';  -
> > > > > > ERROR:  could not access 
> > > > > > > file
> "$libdir/xxid": No such
> > > file or
> > > > > > directory
> > > > > > > 
> > > > > > > :6: Error: the extension for
> the xxid
> > > data type
> > > > > cannot
> > > > > > be loaded in 
> > > > > > > database
> 'host=192.168.99.23
> > > dbname=bijayant
> > > > > > user=bijayant port=5432 
> > > > > > > password=bijayant'
> > > > > > > 
> > > > > > > :6: ERROR: no admin conninfo
> for node
> > > 134598992
> > > > > > > 
> > > > > > > The first two lines of the
> error i
> > > couldnot
> > > > > > understand.
> > > > > > > The "admin
> conninfo" error, i
> > > think
> > > > > this
> > > > > > parameter should be present in
> conf 
> > > > > > > file, right? But in my case
> what should
> > > be there,
> > > > > as i
> > > > > > am already giving the 
> > > > > > > username and password to
> connect to the
> > > > > postgresql
> > > > > > database.
> > > > > > > 
> > > > > > > In this problem thread one
> gentleman
> > > has asked me
> > > > > to
> > > > > > check whether
> "_bijayant" 
> > > > > > > schema is created or not? I
> am sorry to
> > > ask but i
> > > > > > really dont know about this. I 
> > > > > > > have only created a
> user,database and
> > > table in
> > > > > the
> > > > > > postgresql database nothing 
> > > > > > > else. When we create
> database, schema
> > > is also
> > > > > created
> > > > > > automatically, right?
> > > > > > > 
> > > > > > > Please suggest me what should
> i do
> > > next. Sorry
> > > > > but i
> > > > > > am very new to slonik and 
> > > > > > > database. I am trying hard to
> > > understand the
> > > > > concept
> > > > > > > 
> > > > > > > Thanks & Regards,
> > > > > > > 
> > > > > > > Bijayant Kumar
> > > > > > > 
> > > > > > > 
> > > > > > > --- On Mon, 30/6/08, Martin
> Eriksson
> > > wrote:
> > > > > > > 
> > > > > > > > From: Martin Eriksson 
> > > > > > > > Subject: Re:
> [Slony1-general] Node
> > > is not
> > > > > > initialized properly
> > > > > > > > To: 
> > > > > > > > Cc:
> > > slony1-general@lists.slony.info
> > > > > > > > Date: Monday, 30 June,
> 2008, 8:34
> > > PM
> > > > > > > > If you only run it like
> that, it
> > > only prints
> > > > > what
> > > > > > it will
> > > > > > > > execute, to 
> > > > > > > > actually execute this
> you need to
> > > | it to
> > > > > the
> > > > > > slonik app..
> > > > > > > > 
> > > > > > > > eg.
> > > /data/pgsql/slony/slonik_init_cluster
> > > > > > --config
> > > > > > > > | 
> > > > > > > > /data/pgsql/bin/slonik
> > > > > > > > 
> > > > > > > > all scripts starting
> with
> > > > > "slonik_"
> > > > > > doesn't
> > > > > > > > actually do anything 
> > > > > > > > themself, its just a way
> to format
> > > a command
> > > > > > correctly for
> > > > > > > > the slonik 
> > > > > > > > parser, which actually
> does the
> > > work..
> > > > > > > > 
> > > > > > > > 
> > > > > > > > 
> > > > > > > > bijayant kumar wrote:
> > > > > > > > > Thanks for the
> reply. I
> > > executed the
> > > > > command
> > > > > > > >
> "slonik_init_cluster".It
> > > gives the
> > > > > > output like
> > > > > > > > >
> > > > > > > > > # INIT CLUSTER
> > > > > > > > > cluster name =
> bijayant;
> > > > > > > > >  node 1 admin
> > > > > > conninfo='host=192.168.99.23
> > > > > > > > dbname=bijayant
> user=bijayant
> > > port=5432
> > > > > > > > password=bijayant';
> > > > > > > > >  node 2 admin
> > > > > > conninfo='host=192.168.99.134
> > > > > > > > dbname=bijayant
> user=bijayant
> > > port=5432
> > > > > > > > password=bijayant';
> > > > > > > > >   init cluster (id
> = 1,
> > > comment =
> > > > > 'Node
> > > > > > 1 -
> > > > > > > >
> bijayant@192.168.99.23');
> > > > > > > > >
> > > > > > > > > # STORE NODE
> > > > > > > > >   store node (id =
> 2, event
> > > node = 1,
> > > > > > comment =
> > > > > > > > 'Node 2 -
> > > bijayant@192.168.99.134');
> > > > > > > > >   echo 'Set up
> > > replication
> > > > > nodes';
> > > > > > > > >
> > > > > > > > > # STORE PATH
> > > > > > > > >   echo 'Next:
> configure
> > > paths for
> > > > > each
> > > > > > > > node/origin';
> > > > > > > > >   store path
> (server = 1,
> > > client = 2,
> > > > > > conninfo =
> > > > > > > > 'host=192.168.99.23
> > > dbname=bijayant
> > > > > > user=bijayant
> > > > > > > > port=5432
> password=bijayant');
> > > > > > > > >   store path
> (server = 2,
> > > client = 1,
> > > > > > conninfo =
> > > > > > > > 'host=192.168.99.134
> > > dbname=bijayant
> > > > > > user=bijayant
> > > > > > > > port=5432
> password=bijayant');
> > > > > > > > >   echo
> 'Replication nodes
> > > > > prepared';
> > > > > > > > >   echo 'Please
> start a
> > > slon
> > > > > replication
> > > > > > daemon for
> > > > > > > > each node';
> > > > > > > > >
> > > > > > > > > After that i run
> the slonik
> > > daemon and
> > > > > got
> > > > > > the error
> > > > > > > > mentioned.
> > > > > > > > >
> > > > > > > > >
> > > > > > > > > Bijayant Kumar
> > > > > > > > >
> > > > > > > > >
> > > > > > > > > --- On Mon,
> 30/6/08, Martin
> > > Eriksson
> > > > > > > > wrote:
> > > > > > > > >
> > > > > > > > >  
> > > > > > > > >> From: Martin
> Eriksson
> > > > > > > > 
> > > > > > > > >> Subject: Re:
> > > [Slony1-general] Node
> > > > > is
> > > > > > not
> > > > > > > > initialized properly
> > > > > > > > >> To: 
> > > > > > > > >> Cc:
> > > slony1-general@lists.slony.info
> > > > > > > > >> Date: Monday,
> 30 June,
> > > 2008, 7:18
> > > > > PM
> > > > > > > > >> Very basic, but
> saw no
> > > mention of
> > > > > it in
> > > > > > the
> > > > > > > > e-mail,
> > > > > > > > >>
> > > > > > > > >> I assume you
> ran the
> > > > > > > >
> "slonik_init_cluster"
> > > before
> > > > > > > > >> trying to start
> the 
> > > > > > > > >> slon daemons?
> as that
> > > does the
> > > > > slony
> > > > > > setup on each
> > > > > > > > of the
> > > > > > > > >> dbs
> > > > > > > > >>
> > > > > > > > >>
> > > > > > > > >>
> > > > > > > > >> bijayant kumar
> wrote:
> > > > > > > > >>    
> > > > > > > > >>> Hello list,
> > > > > > > > >>>
> > > > > > > > >>> I am a very
> new user
> > > of slony1
> > > > > so
> > > > > > please
> > > > > > > > forgive me if
> > > > > > > > >>>      
> > > > > > > > >> i am asking
> very stupid
> > > question. 
> > > > > > > > >>    
> > > > > > > > >>> I have
> installed
> > > Postgresql and
> > > > > > Slony1 on two
> > > > > > > > gentoo
> > > > > > > > >>>      
> > > > > > > > >> machine.
> Postgresql is
> > > working fine
> > > > > no
> > > > > > problem at
> > > > > > > > all. I
> > > > > > > > >> want to use
> Slony1 to
> > > replicate the
> > > > > two
> > > > > > databases
> > > > > > > > across
> > > > > > > > >> the systems. To
> > > understand the
> > > > > slony1
> > > > > > concept i
> > > > > > > > made a 
> > > > > > > > >>    
> > > > > > > > >>> test
> database with
> > > one single
> > > > > table
> > > > > > and only
> > > > > > > > one entry
> > > > > > > > >>>      
> > > > > > > > >> into it. But
> when i start
> > > slony1 i
> > > > > get
> > > > > > error like
> > > > > > > > >>    
> > > > > > > > >>> 2008-06-30
> 15:54:55
> > > IST ERROR 
> > > > > > cannot get
> > > > > > > > >>>      
> > > > > > > > >>
> sl_local_node_id - ERROR:
> > >  schema
> > > > > > > > "_bijayant"
> > > > > > > > >> does not exist
> > > > > > > > >>    
> > > > > > > > >>> 2008-06-30
> 15:54:55
> > > IST FATAL 
> > > > > main:
> > > > > > Node is
> > > > > > > > not
> > > > > > > > >>>      
> > > > > > > > >> initialized
> properly -
> > > sleep 10s
> > > > > > > > >>    
> > > > > > > > >>> Now i am
> giving here
> > > my
> > > > > > configuration details.
> > > > > > > > >>>
> > > > > > > > >>> /* Master
> Server */
> > > > > > > > >>> IP Address
> > > 192.168.99.23
> > > > > > > > >>> Database
> name
> > > bijayant
> > > > > > > > >>> Table name
> kavach
> > > > > > > > >>>
> > > > > > > > >>> bijayant=#
> select *
> > > from
> > > > > kavach;
> > > > > > > > >>>  id |  
> name   |
> > > designation | 
> > > > > 
> > > > > > address
> > > > > > > > >>>
> > > > > >
> ----+----------+-------------+-------------
> > > > > > > > >>>   1 |
> Bijayant |
> > > consultant  |
> > > > > > Lakkasandra
> > > > > > > > >>> (1 row)
> > > > > > > > >>>
> > > > > > > > >>>
> > > > > > > > >>> vi
> > > /etc/slon_tools.conf
> > > > > > > > >>>
> > > > > > > > >>> if
> > > > > ($ENV{"SLONYNODES"}) {
> > > > > > > > >>>     require
> > > > > > $ENV{"SLONYNODES"};
> > > > > > > > >>> } else {
> > > > > > > > >>>    
> $CLUSTER_NAME =
> > > > > > 'bijayant';
> > > > > > > > >>>     $LOGDIR
> =
> > > > > > '/var/log/slony';
> > > > > > > > >>>    
> $MASTERNODE = 1;
> > > > > > > > >>>    
> add_node(node    
> > > => 1,
> > > > > > > > >>>            
>  host    
> > > =>
> > > > > > > > '192.168.99.23',
> > > > > > > > >>>            
>  dbname  
> > > =>
> > > > > > > > 'bijayant',
> > > > > > > > >>>            
>  port    
> > > =>
> > > > > 5432,
> > > > > > > > >>>            
>  user    
> > > =>
> > > > > > > > 'bijayant',
> > > > > > > > >>>            
>  password
> > > =>
> > > > > > > > 'bijayant');
> > > > > > > > >>>
> > > > > > > > >>>    
> add_node(node    
> > > => 2,
> > > > > > > > >>>            
>  host    
> > > =>
> > > > > > > >
> '192.168.99.134',
> > > > > > > > >>>            
>  dbname  
> > > =>
> > > > > > > > 'bijayant',
> > > > > > > > >>>            
>  port    
> > > =>
> > > > > 5432,
> > > > > > > > >>>            
>  user    
> > > =>
> > > > > > > > 'bijayant',
> > > > > > > > >>>            
>  password
> > > =>
> > > > > > > > 'bijayant',
> > > > > > > > >>>            
>  parent
> > > => 1
> > > > > > > > >>>            
>  );
> > > > > > > > >>> }
> > > > > > > > >>>
> > > > > > > > >>> $SLONY_SETS
> = {
> > > > > > > > >>>            
>    
> > > > > "set1"
> > > > > > => {
> > > > > > > > >>>            
>          
> > >    
> > > > > > "set_id"
> > > > > > > > => 1,
> > > > > > > > >>>            
>          
> > >    
> > > > > > > > "pkeyedtables"
> > > > > > > > >>>      
> > > > > > > > >> => [
> > > > > > > > >>    
> > > > > > > > >>>            
>          
> > >    
> > > > > > > > 'public.kavach',
> > > > > > > > >>>            
>          
> > >      ],
> > > > > > > > >>>         },
> > > > > > > > >>> if
> > > ($ENV{"SLONYSET"})
> > > > > {
> > > > > > > > >>>     require
> > > > > > $ENV{"SLONYSET"};
> > > > > > > > >>> }
> > > > > > > > >>>
> > > > > > > > >>> 1;
> > > > > > > > >>>
> > > > > > > > >>> vi
> /etc/conf.d/slony1
> > > > > > > > >>>
> USER=postgres
> > > > > > > > >>>
> CLUSTER=bijayant
> > > > > > > > >>>
> DBUSER=bijayant
> > > > > > > > >>>
> DBNAME=bijayant
> > > > > > > > >>>
> DBHOST=192.168.99.23
> > > > > > > > >>>
> > > > > >
> LOGFILE=/var/lib/postgresql/data/slony1.log
> > > > > > > > >>> LOGLEVEL=4
> > > > > > > > >>>
> > > > > > > > >>> /* On the
> Slave
> > > Server */
> > > > > > > > >>>
> > > > > > > > >>> IP Address
> > > 192.168.99.134
> > > > > > > > >>> Database
> name
> > > bijayant
> > > > > > > > >>> Table name
> kavach
> > > > > > > > >>>
> > > > > > > > >>> vi
> > > /etc/slon_tools.conf
> > > > > > > > >>>
> > > > > > > > >>> if
> > > > > ($ENV{"SLONYNODES"}) {
> > > > > > > > >>>     require
> > > > > > $ENV{"SLONYNODES"};
> > > > > > > > >>> } else {
> > > > > > > > >>>    
> $CLUSTER_NAME =
> > > > > > 'bijayant';
> > > > > > > > >>>     $LOGDIR
> =
> > > > > > '/var/log/slony';
> > > > > > > > >>>     # SYNC
> check
> > > interval (slon
> > > > > -s
> > > > > > option)
> > > > > > > > >>>     #
> > > $SYNC_CHECK_INTERVAL =
> > > > > 1000;
> > > > > > > > >>>    
> $MASTERNODE = 1;
> > > > > > > > >>>    
> add_node(node    
> > > => 1,
> > > > > > > > >>>            
>  host    
> > > =>
> > > > > > > > '192.168.99.23',
> > > > > > > > >>>            
>  dbname  
> > > =>
> > > > > > > > 'bijayant',
> > > > > > > > >>>            
>  port    
> > > =>
> > > > > 5432,
> > > > > > > > >>>            
>  user    
> > > =>
> > > > > > > > 'bijayant',
> > > > > > > > >>>            
>  password
> > > =>
> > > > > > > > 'bijayant');
> > > > > > > > >>>
> > > > > > > > >>>    
> add_node(node    
> > > => 2,
> > > > > > > > >>>            
>  host    
> > > =>
> > > > > > > >
> '192.168.99.134',
> > > > > > > > >>>            
>  dbname  
> > > =>
> > > > > > > > 'bijayant',
> > > > > > > > >>>            
>  port    
> > > =>
> > > > > 5432,
> > > > > > > > >>>            
>  user    
> > > =>
> > > > > > > > 'bijayant',
> > > > > > > > >>>            
>  password
> > > =>
> > > > > > > > 'bijayant',
> > > > > > > > >>>            
>  parent
> > > => 1
> > > > > > > > >>>            
>  );
> > > > > > > > >>> }
> > > > > > > > >>> $SLONY_SETS
> = {
> > > > > > > > >>>    
> "set1"
> > > => {
> > > > > > > > >>>            
>  
> > > > > "set_id"
> > > > > > => 1,
> > > > > > > > >>>            
>    
> > > > > > "pkeyedtables" =>
> > > > > > > > [
> > > > > > > > >>>            
>          
> > >          
> > > > >  
> > > > > > > > >>>      
> > > > > > > > >>
> 'public.kavach',
> > > > > > > > >>    
> > > > > > > > >>>            
>          
> > >          
> > > > >   ],
> > > > > > > > >>>            
>   },
> > > > > > > > >>> };
> > > > > > > > >>>
> > > > > > > > >>> if
> > > ($ENV{"SLONYSET"})
> > > > > {
> > > > > > > > >>>     require
> > > > > > $ENV{"SLONYSET"};
> > > > > > > > >>> }
> > > > > > > > >>>
> > > > > > > > >>> 1;
> > > > > > > > >>>
> > > > > > > > >>> vi
> /etc/conf.d/slony1
> > > > > > > > >>>
> USER=postgres
> > > > > > > > >>>
> CLUSTER=bijayant
> > > > > > > > >>>
> DBUSER=bijayant
> > > > > > > > >>>
> DBNAME=bijayant
> > > > > > > > >>>
> DBHOST=192.168.99.23
> > > > > > > > >>>
> > > > > >
> LOGFILE=/var/lib/postgresql/data/slony1.log
> > > > > > > > >>> LOGLEVEL=4 
> > > > > > > > >>>
> > > > > > > > >>> When i
> start the
> > > slony1
> > > > > > /etc/init.d/slony1
> > > > > > > > start on
> > > > > > > > >>>      
> > > > > > > > >> both the
> machine it
> > > generates lots
> > > > > of
> > > > > > logs with
> > > > > > > > errors line
> > > > > > > > >> like
> > > > > > > > >>    
> > > > > > > > >>> 2008-06-30
> 15:54:55
> > > IST DEBUG2
> > > > > > slon_retry()
> > > > > > > > from
> > > > > > > > >>>      
> > > > > > > > >> pid=19007
> > > > > > > > >>    
> > > > > > > > >>> 2008-06-30
> 15:54:55
> > > IST DEBUG1
> > > > > slon:
> > > > > > retry
> > > > > > > > requested
> > > > > > > > >>> 2008-06-30
> 15:54:55
> > > IST DEBUG2
> > > > > slon:
> > > > > > notify
> > > > > > > > worker
> > > > > > > > >>>      
> > > > > > > > >> process to
> shutdown
> > > > > > > > >>    
> > > > > > > > >>> 2008-06-30
> 15:54:55
> > > IST DEBUG2
> > > > > slon:
> > > > > > child
> > > > > > > > terminated
> > > > > > > > >>>      
> > > > > > > > >> status: 0; pid:
> 19007,
> > > current
> > > > > worker
> > > > > > pid: 19007
> > > > > > > > >>    
> > > > > > > > >>> 2008-06-30
> 15:54:55
> > > IST DEBUG1
> > > > > slon:
> > > > > > restart
> > > > > > > > of worker
> > > > > > > > >>> 2008-06-30
> 15:54:55
> > > IST CONFIG
> > > > > main:
> > > > > > slon
> > > > > > > > version
> > > > > > > > >>>      
> > > > > > > > >> 1.2.10 starting
> up
> > > > > > > > >>    
> > > > > > > > >>> 2008-06-30
> 15:54:55
> > > IST DEBUG2
> > > > > slon:
> > > > > > watchdog
> > > > > > > > process
> > > > > > > > >>>      
> > > > > > > > >> started
> > > > > > > > >>    
> > > > > > > > >>> 2008-06-30
> 15:54:55
> > > IST DEBUG2
> > > > > slon:
> > > > > > watchdog
> > > > > > > > ready -
> > > > > > > > >>>      
> > > > > > > > >> pid = 19005
> > > > > > > > >>    
> > > > > > > > >>> 2008-06-30
> 15:54:55
> > > IST DEBUG2
> > > > > slon:
> > > > > > worker
> > > > > > > > process
> > > > > > > > >>>      
> > > > > > > > >> created - pid =
> 19034
> > > > > > > > >>    
> > > > > > > > >>> 2008-06-30
> 15:54:55
> > > IST ERROR 
> > > > > > cannot get
> > > > > > > > >>>      
> > > > > > > > >>
> sl_local_node_id - ERROR:
> > >  schema
> > > > > > > > "_bijayant"
> > > > > > > > >> does not exist
> > > > > > > > >>    
> > > > > > > > >>> 2008-06-30
> 15:54:55
> > > IST FATAL 
> > > > > main:
> > > > > > Node is
> > > > > > > > not
> > > > > > > > >>>      
> > > > > > > > >> initialized
> properly -
> > > sleep 10s
> > > > > > > > >>    
> > > > > > > > >>> I am sure
> that i am
> > > not
> > > > > > understanding some
> > > > > > > > basic
> > > > > > > > >>>      
> > > > > > > > >> things about
> the slony1.
> > > Can
> > > > > anybody
> > > > > > help me to
> > > > > > > > understand
> > > > > > > > >> the logic, i
> will be very
> > > helpful
> > > > > for
> > > > > > you all.
> > > > > > > > Please tell
> > > > > > > > >> me what i am
> doing wrong
> > > here, what
> > > > > > should i do. I
> > > > > > > > have
> > > > > > > > >> read the
> documentation at
> > > the
> > > > > website
> > > > > > but not able
> > > > > > > > to
> > > > > > > > >> understand
> fully. Please
> > > help me
> > > > > out.
> > > > > > > > >>    
> > > > > > > > >>> Thanks
> & Regards,
> > > > > > > > >>> Bijayant
> Kumar
> > > > > > > > >>>
> > > > > > > > >>> Send
> instant messages
> > > to your
> > > > > online
> > > > > > friends
> > > > > > > > >>>      
> > > > > > > > >>
> > > http://uk.messenger.yahoo.com 
> > > > > > > > >>    
> > > > > > > > >>>
> > > > > > > >
> > > > >
> _______________________________________________
> > > > > > > > >>>
> Slony1-general
> > > mailing list
> > > > > > > > >>>
> > > Slony1-general@lists.slony.info
> > > > > > > > >>>
> > > > > > > > >>>      
> > > > > > > > >>
> > > > > > > >
> > > > > >
> > > > >
> > >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > > > > > > > >>    
> > > > > > > > >>>  
> > > > > > > > >>>      
> > > > > > > > >>
> > > > > >
> > > _______________________________________________
> > > > > > > > >> Slony1-general
> mailing
> > > list
> > > > > > > > >>
> > > Slony1-general@lists.slony.info
> > > > > > > > >>
> > > > > > > >
> > > > > >
> > > > >
> > >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > > > > > > > >>    
> > > > > > > > >
> > > > > > > > > Send instant
> messages to your
> > > online
> > > > > friends
> > > > > > > >
> http://uk.messenger.yahoo.com 
> > > > > > > > >  
> > > > > > > > 
> > > > > > > >
> > > > >
> _______________________________________________
> > > > > > > > Slony1-general mailing
> list
> > > > > > > >
> Slony1-general@lists.slony.info
> > > > > > > >
> > > > > >
> > > > >
> > >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > > > > > > 
> > > > > > > Send instant messages to your
> online
> > > friends
> > > > > > http://uk.messenger.yahoo.com
> > > > > > >
> > > _______________________________________________
> > > > > > > Slony1-general mailing list
> > > > > > >
> Slony1-general@lists.slony.info
> > > > > > >
> > > > > >
> > > > >
> > >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > > > > > 
> > > > > > 
> > > > > > 
> > > > > >      
> > > > > >
> > > > >
> > >
> __________________________________________________________
> > > > > > Not happy with your email
> address?.
> > > > > > Get the one you really want -
> millions of
> > > new email
> > > > > > addresses available now at Yahoo!
> > > > > >
> http://uk.docs.yahoo.com/ymail/new.html
> > > > > 
> > > > > Send instant messages to your online
> friends
> > > > > http://uk.messenger.yahoo.com
> > > > >
> _______________________________________________
> > > > > Slony1-general mailing list
> > > > > Slony1-general@lists.slony.info
> > > > >
> > >
> http://lists.slony.info/mailman/listinfo/slony1-general
> > > > 
> > > > Send instant messages to your online friends
> > > http://uk.messenger.yahoo.com
> > > 
> > > 
> > > 
> > >      
> > >
> __________________________________________________________
> > > Not happy with your email address?.
> > > Get the one you really want - millions of new
> email
> > > addresses available now at Yahoo!
> > > http://uk.docs.yahoo.com/ymail/new.html
> > 
> > Send instant messages to your online friends
> http://uk.messenger.yahoo.com
> 
> 
> 
>      
> __________________________________________________________
> Not happy with your email address?.
> Get the one you really want - millions of new email
> addresses available now at Yahoo!
> http://uk.docs.yahoo.com/ymail/new.html

Send instant messages to your online friends http://uk.messenger.yahoo.com 
From liobod.slony at gmail.com  Thu Jul  3 00:55:39 2008
From: liobod.slony at gmail.com (lio bod)
Date: Thu Jul  3 00:56:04 2008
Subject: [Slony1-general] fail over failed
In-Reply-To: <20080702135125.GE32287@crankycanuck.ca>
References: <d4f444290807010250s3b5a238ay8735a4a59441e04a@mail.gmail.com>
	<20080702135125.GE32287@crankycanuck.ca>
Message-ID: <d4f444290807030055h7ead7aa5mb35684973f3c5f46@mail.gmail.com>

Don't remember. it's a while now.
I could test it again but i have no time for that by now. sorry.
May a big load on postgres (numerous instance of databases, other slony
cluster) may cause side effects
such this one?
I already faced the famous "WARN   remoteWorkerThread_1: transactions
earlier than XID XXXXX are still in progress" on my loaded postgres. And of
course no idea what is XXXXX ...

Note since i'm testing my slonik scripts on my slony cluster on a brand new
postgres, fail over and subscriptions are working properly.

Whoever logged any other experience on side effects due to load on postgres
is welcome..


2008/7/2, Andrew Sullivan <ajs@crankycanuck.ca>:
>
> On Tue, Jul 01, 2008 at 11:50:29AM +0200, lio bod wrote:
> >
> > Subscription for set 1 is not active? My replication seems ok. At least
> it
> > replicates...
> > So i don't understand the message
> > What could be wrong on this fail over?
>
> What does sl_set say?
>
> A
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080703/=
cd23e7de/attachment.htm
From liobod.slony at gmail.com  Thu Jul  3 05:10:56 2008
From: liobod.slony at gmail.com (lio bod)
Date: Thu Jul  3 05:11:00 2008
Subject: [Slony1-general] Adding A Node To Replication after fail over
Message-ID: <d4f444290807030510v486a1adage760f1d3a60d237@mail.gmail.com>

Hello world,

I' trying to adding a node to replication after fail over following 12.4
from http://www.slony.info/documentation/addthings.html

I've got only a master(node1)/save(node2) pair and i fail over promoting my
slave to my new master

Once my slons processes are killed, i perform the following steps

- i drop my old database as node1
- i dump my new promoted slony1_extract_schema.sh from my new master.
- i restore a new fresh database from the previous dump (with no slony
"cruft")
- i re-create my old node 1 using store node (cool : it creates the slony
cluster schema by the way)
  store node (id=3D1, comment=3D'slony_cluster node 1', event node=3D2);
- i restart my slons processes
- i store my path on both sides using :
  STORE PATH (SERVER=3D1, CLIENT=3D2, CONNINFO=3D'dbname=3Dbop_v203_replic
host=3Dkpncavern user=3Dpgslony port=3D5432');
  STORE PATH (SERVER=3D2, CLIENT=3D1, CONNINFO=3D'dbname=3Dbop_v203_replic
host=3Dkpnigloo user=3Dpgslony port=3D5432');
- i resubscribe node 2 to node 1:
  subscribe set (id=3D1, provider=3D2, receiver=3D1, forward=3Dyes);

And ... no replication occures

What's wrong (empty sl_table, sl_sequence on new node 1? ...)?
What step is missing

rgds,

May warning or Notice logs gives a clue?
The only things i see in in my logs are (sonce i restart slons) :

slon1 :
2008-07-03 11:27:28 CEST [26359] CONFIG main: slon version 1.2.12 starting
up
2008-07-03 11:27:28 CEST [26361] CONFIG main: local node id =3D 1
2008-07-03 11:27:28 CEST [26361] CONFIG main: launching sched_start_mainloop
2008-07-03 11:27:28 CEST [26361] CONFIG main: loading current cluster
configuration
2008-07-03 11:27:28 CEST [26361] CONFIG storeNode: no_id=3D2
no_comment=3D'bop_slony_cluster subscriber node 2'
2008-07-03 11:27:28 CEST [26361] CONFIG storeSet: set_id=3D1 set_origin=3D2
set_comment=3D'slony_cluster Tables and Sequences'
2008-07-03 11:27:28 CEST [26361] WARN   remoteWorker_wakeup: node 2 - no
worker thread2008-07-03 11:27:28 CEST [26359] CONFIG main: slon version
1.2.12 starting up
2008-07-03 11:27:28 CEST [26361] CONFIG main: local node id =3D 1
2008-07-03 11:27:28 CEST [26361] CONFIG main: launching sched_start_mainloop
2008-07-03 11:27:28 CEST [26361] CONFIG main: loading current cluster
configuration
2008-07-03 11:27:28 CEST [26361] CONFIG storeNode: no_id=3D2
no_comment=3D'slony_cluster subscriber node 2'
2008-07-03 11:27:28 CEST [26361] CONFIG storeSet: set_id=3D1 set_origin=3D2
set_comment=3D'slony_cluster Tables and Sequences'
2008-07-03 11:27:28 CEST [26361] WARN   remoteWorker_wakeup: node 2 - no
worker thread
2008-07-03 11:27:28 CEST [26361] CONFIG main: configuration complete -
starting threads
2008-07-03 11:27:28 CEST [26361] DEBUG1 localListenThread: thread starts
2008-07-03 11:27:28 CEST [26361] CONFIG enableNode: no_id=3D2
2008-07-03 11:27:28 CEST [26361] DEBUG1 main: running scheduler mainloop
2008-07-03 11:27:28 CEST [26361] DEBUG1 remoteWorkerThread_2: thread starts
2008-07-03 11:27:28 CEST [26361] DEBUG1 cleanupThread: thread starts
2008-07-03 11:27:28 CEST [26361] DEBUG1 syncThread: thread starts
2008-07-03 11:33:10 CEST [26361] CONFIG storePath: pa_server=3D2 pa_client=
=3D1
pa_conninfo=3D"dbname=3Dmydb host=3Dhost1 user=3Dpgslony port=3D5432" pa_co=
nnretry=3D10
2008-07-03 11:33:10 CEST [26361] CONFIG storeListen: li_origin=3D2
li_receiver=3D1 li_provider=3D2
2008-07-03 11:33:10 CEST [26361] DEBUG1 remoteListenThread_2: thread starts
2008-07-03 11:33:10 CEST [26361] DEBUG1 remoteListenThread_2: connected to
'dbname=3Dmydb host=3Dhost2 user=3Dpgslony port=3D5432'
2008-07-03 11:33:10 CEST [26361] CONFIG storeListen: li_origin=3D2
li_receiver=3D1 li_provider=3D2
2008-07-03 11:33:10 CEST [26361] CONFIG storeListen: li_origin=3D2
li_receiver=3D1 li_provider=3D2
2008-07-03 11:33:10 CEST [26361] CONFIG storeListen: li_origin=3D2
li_receiver=3D1 li_provider=3D2
2008-07-03 11:38:35 CEST [26361] DEBUG1 cleanupThread:    0.030 seconds for
cleanupEvent()
2008-07-03 11:38:35 CEST [26361] DEBUG1 cleanupThread:    0.020 seconds for
delete logs
2008-07-03 11:47:53 CEST [26361] CONFIG storeSubscribe: sub_set=3D1
sub_provider=3D2 sub_forward=3D't'
2008-07-03 11:47:53 CEST [26361] CONFIG storeListen: li_origin=3D2
li_receiver=3D1 li_provider=3D2
2008-07-03 11:47:53 CEST [26361] DEBUG1 copy_set 1
2008-07-03 11:47:53 CEST [26361] DEBUG1 remoteWorkerThread_2: connected to
provider DB
2008-07-03 11:47:53 CEST [26361] DEBUG1 remoteWorkerThread_2: disconnected
from provider DB
2008-07-03 11:47:53 CEST [26361] DEBUG1 copy_set 1 done in 0.029 seconds
2008-07-03 11:47:53 CEST [26361] CONFIG enableSubscription: sub_set=3D1
2008-07-03 11:47:53 CEST [26361] CONFIG storeListen: li_origin=3D2
li_receiver=3D1 li_provider=3D2
2008-07-03 11:47:53 CEST [26361] DEBUG1 remoteWorkerThread_2: helper thread
for provider 2 created
2008-07-03 11:47:55 CEST [26361] DEBUG1 remoteWorkerThread_2: connected to
data provider 2 on 'dbname=3Dmydb host=3Dhost2 user=3Dpgslony port=3D5432'
2008-07-03 11:49:57 CEST [26361] DEBUG1 cleanupThread:    0.026 seconds for
cleanupEvent()
2008-07-03 11:49:57 CEST [26361] DEBUG1 cleanupThread:    0.019 seconds for
delete logs
../..

slon2 :
2008-07-03 11:27:28 CEST [26386] CONFIG main: slon version 1.2.12 starting
up
2008-07-03 11:27:28 CEST [26388] CONFIG main: local node id =3D 2
2008-07-03 11:27:28 CEST [26388] CONFIG main: launching sched_start_mainloop
2008-07-03 11:27:28 CEST [26388] CONFIG main: loading current cluster
configuration
2008-07-03 11:27:28 CEST [26388] CONFIG storeNode: no_id=3D1
no_comment=3D'slony_cluster node 1'
2008-07-03 11:27:28 CEST [26388] CONFIG storeSet: set_id=3D1 set_origin=3D2
set_comment=3D'slony_cluster Tables and Sequences'
2008-07-03 11:27:28 CEST [26388] CONFIG main: configuration complete -
starting threads
2008-07-03 11:27:28 CEST [26388] DEBUG1 localListenThread: thread starts
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=3D25965
2008-07-03 11:27:28 CEST [26388] CONFIG enableNode: no_id=3D1
2008-07-03 11:27:28 CEST [26388] DEBUG1 main: running scheduler mainloop
2008-07-03 11:27:28 CEST [26388] DEBUG1 remoteWorkerThread_1: thread starts
2008-07-03 11:27:28 CEST [26388] DEBUG1 cleanupThread: thread starts
2008-07-03 11:27:28 CEST [26388] DEBUG1 syncThread: thread starts
2008-07-03 11:33:06 CEST [26388] CONFIG storePath: pa_server=3D1 pa_client=
=3D2
pa_conninfo=3D"dbname=3Dmydb host=3Dhost1 user=3Dpgslony port=3D5432" pa_co=
nnretry=3D10
2008-07-03 11:33:06 CEST [26388] CONFIG storeListen: li_origin=3D1
li_receiver=3D2 li_provider=3D1
2008-07-03 11:33:06 CEST [26388] DEBUG1 remoteListenThread_1: thread starts
2008-07-03 11:33:06 CEST [26388] DEBUG1 remoteListenThread_1: connected to
'dbname=3Dmydb host=3Dhost1 user=3Dpgslony port=3D5432'
2008-07-03 11:33:06 CEST [26388] CONFIG storeListen: li_origin=3D1
li_receiver=3D2 li_provider=3D1
2008-07-03 11:38:35 CEST [26388] DEBUG1 cleanupThread:    0.030 seconds for
cleanupEvent()
2008-07-03 11:38:35 CEST [26388] DEBUG1 cleanupThread:    0.032 seconds for
delete logs
2008-07-03 11:47:54 CEST [26388] CONFIG storeListen: li_origin=3D1
li_receiver=3D2 li_provider=3D1
2008-07-03 11:47:54 CEST [26388] CONFIG storeListen: li_origin=3D1
li_receiver=3D2 li_provider=3D1
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=3D29851
CONTEXT:  PL/pgSQL function "cleanupevent" line 77 at perform
2008-07-03 11:49:57 CEST [26388] DEBUG1 cleanupThread:    0.025 seconds for
cleanupEvent()
2008-07-03 11:49:57 CEST [26388] DEBUG1 cleanupThread:    0.019 seconds for
delete logs
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080703/=
96c168f3/attachment.htm
From psheats at pbpost.com  Thu Jul  3 08:38:11 2008
From: psheats at pbpost.com (Peter Sheats)
Date: Thu Jul  3 08:38:47 2008
Subject: [Slony1-general] Repeating logging...
Message-ID: <C4926AA3.71D9%psheats@pbpost.com>

I have noticed that Slony is logging the same thing over  and over.  You can
see the logs here: http://dpaste.com/60428/

Also, it seems that Slony is replicating new records but not updates.  Could
these be related?  Are those logs harmless or are they telling me something.

Node 6 is the master, Node 8 is the slave... These logs are on 8.

Peter
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080703/3e6f0be3/attachment.htm
From ahodgson at simkin.ca  Thu Jul  3 09:00:34 2008
From: ahodgson at simkin.ca (Alan Hodgson)
Date: Thu Jul  3 09:00:44 2008
Subject: [Slony1-general] Repeating logging...
In-Reply-To: <C4926AA3.71D9%psheats@pbpost.com>
References: <C4926AA3.71D9%psheats@pbpost.com>
Message-ID: <200807030900.34483@hal.medialogik.com>

On Thursday 03 July 2008, "Peter Sheats" <psheats@pbpost.com> wrote:
> I have noticed that Slony is logging the same thing over  and over.  You
> can see the logs here: http://dpaste.com/60428/
>
> Also, it seems that Slony is replicating new records but not updates. 
> Could these be related?  Are those logs harmless or are they telling me
> something.
>

2008-07-02 15:54:29 EDT INFO   completed DDL script - run 
ddlScript_complete_int()
2008-07-02 15:54:31 EDT ERROR  
remoteWorkerThread_6: "notify "_dbname_replication_Event"; insert 
into "_dbname_replication".sl_event     (ev_origin, ev_seqno, ev_timestamp,      
ev_minxid, ev_maxxid, ev_xip, ev_type , ev_data1, ev_data2, ev_data3    ) 
values ('6', '14405', '2008-06-06 
14:43:59.361661', '5559800', '5559800', '', 'DDL_SCRIPT', '1', 'update 
features_address set address = ''* * CONFIDENTIAL RECORD * *'' where 
raw_address like ''* * CONFIDENTIAL%'';
    ', '-1'); insert into "_dbname_replication".sl_confirm        
(con_origin, con_received, con_seqno, con_timestamp)    values (6, 
8, '14405', now()); commit transaction;" PGRES_FATAL_ERROR ERROR:  could 
not find trigger 1649923
2008-07-02 15:54:31 EDT INFO   remoteListenThread_6: disconnecting 
from 'host=host6 dbname=dbname user=slony port=5432 password=********'


This DDL update is failing (could not find trigger 1649923). Have any tables 
on the slave been modified directly, not using EXECUTE SCRIPT?


-- 
Alan
From psheats at pbpost.com  Thu Jul  3 11:51:10 2008
From: psheats at pbpost.com (Peter Sheats)
Date: Thu Jul  3 11:51:45 2008
Subject: [Slony1-general] Repeating logging...
In-Reply-To: <20080703160043.D90B4290EA5@main.slony.info>
Message-ID: <C49297DE.71E1%psheats@pbpost.com>


> 2008-07-02 15:54:29 EDT INFO   completed DDL script - run
> ddlScript_complete_int()
> 2008-07-02 15:54:31 EDT ERROR
> remoteWorkerThread_6: "notify "_dbname_replication_Event"; insert
> into "_dbname_replication".sl_event     (ev_origin, ev_seqno, ev_timestamp,
> ev_minxid, ev_maxxid, ev_xip, ev_type , ev_data1, ev_data2, ev_data3    )
> values ('6', '14405', '2008-06-06
> 14:43:59.361661', '5559800', '5559800', '', 'DDL_SCRIPT', '1', 'update
> features_address set address = ''* * CONFIDENTIAL RECORD * *'' where
> raw_address like ''* * CONFIDENTIAL%'';
>     ', '-1'); insert into "_dbname_replication".sl_confirm
> (con_origin, con_received, con_seqno, con_timestamp)    values (6,
> 8, '14405', now()); commit transaction;" PGRES_FATAL_ERROR ERROR:  could
> not find trigger 1649923
> 2008-07-02 15:54:31 EDT INFO   remoteListenThread_6: disconnecting
> from 'host=host6 dbname=dbname user=slony port=5432 password=********'
> 
> 
> This DDL update is failing (could not find trigger 1649923). Have any tables
> on the slave been modified directly, not using EXECUTE SCRIPT?
> 


Possibly, I'm sure we probably did something wrong... But we do know to use
execute script.  Guess I will just start again.  There's no way to repair
things like this is there?

The execute script is only for schema changes, right?  Would running
something like the following through execute script mess anything up?

update features_address set address = '* * CONFIDENTIAL RECORD * *' where
raw_address like '* * CONFIDENTIAL%';
    

Thanks,

Peter

From msteben at autorevenue.com  Thu Jul  3 12:39:45 2008
From: msteben at autorevenue.com (Mark Steben)
Date: Thu Jul  3 12:39:56 2008
Subject: [Slony1-general] execute script doesn't work
In-Reply-To: <60r6aizlxe.fsf@dba2.int.libertyrms.com>
Message-ID: <004a01c8dd44$8bd55df0$14010a0a@dei26g028575>

Thanks Chris - that worked


Mark Steben? Database Administrator? @utoRevenue?
        480 Pleasant Street, Suite B200 ?  Lee, Ma 01238
                        413.243.4800  ' ? 413.243.4809 ?


The problem here is that every slonik script needs to start with the
"preamble" portion, that is, it must start with:

a) Declaration of the cluster
  CLUSTER NAME = something;

b) Declarations of how slonik can reach the nodes.

   If the command is accessing node #1, then it must at least have...

  NODE 1 ADMIN CONNINFO = 'dsn for accessing node 1, maybe dbname=mydb host=host1 port=5432';

Any script that doesn't begin with "CLUSTER NAME" (or a reference to a
file *containing* "CLUSTER NAME") will error out in this fashion.
-- 
(reverse (concatenate 'string "moc.enworbbc" "@" "enworbbc"))
http://linuxfinances.info/info/rdbms.html
"There is   nothing in the world  more  helpless and irresponsible and
depraved than a man in the depths of an ether binge."
-- Dr. Hunter S. Thompson

From cbbrowne at ca.afilias.info  Thu Jul  3 12:43:05 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Jul  3 12:43:16 2008
Subject: [Slony1-general] Execute script when subscriber lagging
In-Reply-To: <b10d372b0807021840q473466d1vc262528cbd7dab99@mail.gmail.com>
	(James Tucker's message of "Thu, 3 Jul 2008 11:40:14 +1000")
References: <b10d372b0807021840q473466d1vc262528cbd7dab99@mail.gmail.com>
Message-ID: <87wsk2zrcm.fsf@dba2.int.libertyrms.com>

"James Tucker" <jameshtucker@gmail.com> writes:
> Just wanting to confirm there is no issue running an execute script
> when a subscriber is lagging the provider.? Does the DDL just get
> put in the sync queue and is executed at the same point in the
> replication stream as it was on the master ?? Or does it prioritise
> the DDL ?

Correct, it is indeed executed at a "common controlled point within
the replication transaction stream".
   <http://linuxfinances.info/info/stmtddlscript.html>
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From jameshtucker at gmail.com  Thu Jul  3 17:04:13 2008
From: jameshtucker at gmail.com (James Tucker)
Date: Thu Jul  3 17:04:30 2008
Subject: [Slony1-general] manual deletion from sl_log tables
Message-ID: <b10d372b0807031704y76d5da10v1aa4d86bf375d6be@mail.gmail.com>

Hi All,

One of our applications has had an issue and queued up a massive amount of
updates to a table containing huge binary columns.  We don't actually care
about any of these updates and have removed everything from the table on the
master, however the slave still has a large number of events queued up to
process.  Because of a few constraints it simply can't keep up and is just
falling further and further behind.

Can I simply remove the offending updates from the sl_log table on the
master ?

Cheers,
JT.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080704/=
318aad07/attachment.htm
From cbbrowne at ca.afilias.info  Fri Jul  4 09:42:08 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Jul  4 09:42:16 2008
Subject: [Slony1-general] manual deletion from sl_log tables
In-Reply-To: <b10d372b0807031704y76d5da10v1aa4d86bf375d6be@mail.gmail.com>
	(James Tucker's message of "Fri, 4 Jul 2008 10:04:13 +1000")
References: <b10d372b0807031704y76d5da10v1aa4d86bf375d6be@mail.gmail.com>
Message-ID: <87lk0hzjmn.fsf@dba2.int.libertyrms.com>

"James Tucker" <jameshtucker@gmail.com> writes:
> Hi All,
> One of our applications has had an issue and queued up a massive amount of updates to a table containing huge binary columns.? We don't actually
> care about any of these updates and have removed everything from the table on the master, however the slave still has a large number of events
> queued up to process.? Because of a few constraints it simply can't keep up and is just falling further and further behind.
> Can I simply remove the offending updates from the sl_log table on the master ?

There is something of "rocket surgery" to this, but yes, you could
identify and remove offending updates from the sl_log_? table on the
master.

I'd want to carefully identify the table, and be really certain that I
had it right before getting to a COMMIT statement :-).

] begin;
] select count(*) from _my_schema.sl_log_1 where log_tableid = 42;
[make sure that reported an apropos number of tuples for the table I think is #42...]
] delete from _my_schema.sl_log_1 where log_tableid = 42;
[make sure that reports something consistent with what is expected...]
] commit;
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/slony.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From ehl22_12 at live.com  Sat Jul  5 03:38:14 2008
From: ehl22_12 at live.com (Ehl Le)
Date: Sat Jul  5 03:38:44 2008
Subject: [Slony1-general] (no subject)
Message-ID: <BLU107-W539CC7146F49E7CD1DA0C59E9A0@phx.gbl>


Hello,


I'm pretty annoyed now...  The last days I tried to set up slony to replicate the database on to slave from one master - that works very fine now.

But the problem is that slony requires from the slave to be able to connect to the master, something I don't want, since the slave is not entirely sealed, there are database-users on the slave-server, I do not want slony to store login-information in it's database on the slave nor do I want to allow access *from* a slave *to the master* (pg_hba) ...is that access really required?

All I want is that slony replicates one database on the slave, nothing more. That login-information stored in the slave's slony-database also contains the password to the master-server.  I just can't use trust on the master-server for auth, since as I said, there are also other users on the slave which could connect to the master without supplying a password then.


Regards, 
ehl22
_________________________________________________________________
Explore the seven wonders of the world
http://search.msn.com/results.aspx?q=7+wonders+world&mkt=en-US&form=QBRE
From troy at troywolf.com  Sat Jul  5 12:23:14 2008
From: troy at troywolf.com (Troy Wolf)
Date: Sat Jul  5 12:23:26 2008
Subject: [Slony1-general] Re: Slony Logshipping
Message-ID: <e0d7c3f50807051223s204b2ee1n7ab2b5209619d314@mail.gmail.com>

You should look at the Log Shipping option with Slony I:
http://www.slony.info/documentation/logshipping.html

It is perfect for situations where your subscriber is "untrusted" or
for using over slower WAN links, etc. I am not a Log Shipping expert,
but I just setup a log shipping subscriber and it is working very
well.

Some caveats are that you need at least one normal subscriber because
a normal subscriber is what creates the log shipping files. As I
understand, if you only have one master and you want one log shipping
subscriber, you'd have to setup a third node just to be a trusted
subscriber so it can create the log shipping files. This is fine for
me because I have a master and a trusted subscriber then I have a
third node replicating using log shipping. My reasons for using log
shipping were #1) a slower WAN link is involved, and #2) I do not want
this 3rd node to have any impact on the main node and the main
subscriber. That is, if replication communication breaks on the 3rd
node, I don't want that keeping #1 and #2 from replicating.

Important note: at least one other maillist user and I have found that
the slony_logshipper program that comes with Slony simply does not
work reliably. We have not had any response from the developer(s)
regarding it. Thus, we (he) developed our own shell scripts to do the
job. If you get to a point where you want to try log shipping
replication, I can send you a copy of the 2 shell scripts--one moves
the files over and the other applies them to your subscriber node.

The documentation for log shipping is a bit sparse. It took me about 2
days of tinkering and testing (in between putting other fires out) to
get it working. Now that it is working, it seems to be a good,
reliable solution. I do wonder why every SYNC event is replicated via
log shipping, but admittedly I don't understand all the features of
log shipping either. Whether you have any changes to replicate or not,
log shipping files are being constantly produced--thousands per day.
Without knowing better, I think I'd rather just have files created
when there are changes to apply.

Enjoy!

> Date: Sat, 5 Jul 2008 10:38:14 +0000
> From: Ehl Le <ehl22_12@live.com>
> Subject: [Slony1-general] (no subject)
>
> All I want is that slony replicates one database on the slave, nothing more. That login-information stored in the slave's slony-database also contains the password to the master-server.  I just can't use trust on the master-server for auth, since as I said, there are also other users on the slave which could connect to the master without supplying a password then.
>
>
> Regards,
> ehl22
> _________________________________________________________________
> Explore the seven wonders of the world
> http://search.msn.com/results.aspx?q=7+wonders+world&mkt=en-US&form=QBRE
>
> ------------------------------
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
> End of Slony1-general Digest, Vol 17, Issue 12
> **********************************************
>
From Ow.Mun.Heng at wdc.com  Mon Jul  7 01:15:24 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Mon Jul  7 01:15:50 2008
Subject: [Slony1-general] DDL (altering current column types)
Message-ID: <1215418524.4952.22.camel@neuromancer.home.net>

I just want to be sure and ask the list before I plunge into this.

I have a bunch of tables which I need to change a column type from
varchar(5) to varchar().
These tables are ~30millions deep and running the DDL on a NON-Slony
database takes ~2-4 hours (each table).

I want to know if I can do the DDL 

alter table foo alter type foo_column type character varying

individually on each slave/master table or I have to do it via slony via
EXECUTE script?


From scetbon at echo.fr  Mon Jul  7 02:59:36 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Mon Jul  7 03:00:04 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from
	locking	alltables
In-Reply-To: <20080702151411.GA7233@depesz.com>
References: <486A0070.2060600@echo.fr>	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>	<20080702083140.GA12941@depesz.com>
	<486B7355.2010307@echo.fr>	<20080702122731.GA29320@depesz.com>
	<486B77D3.1040601@echo.fr>	<20080702125108.GA30475@depesz.com>
	<486B99B2.1090900@echo.fr> <20080702151411.GA7233@depesz.com>
Message-ID: <4871E908.4090309@echo.fr>



hubert depesz lubaczewski wrote:
> On Wed, Jul 02, 2008 at 05:07:30PM +0200, Cyril SCETBON wrote:
>   
>> you're right ! Did you test SWITCH after this modification ? no issue ?
>>     
>
> no, i haven't. this method was given me on this list some time ago, and
> i used it, but didn't need to switch, so it is untested. for me. but the
> test should be pretty simple.
>   
ok. Thanks
> depesz
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>   

-- 
Cyril SCETBON - Ing?nieur bases de donn?es
AUSY pour France T?l?com - OPF/PORTAILS/DOP/HEBEX

T?l : +33 (0)4 97 12 87 60
Jabber : cscetbon@jabber.org
France Telecom - Orange
790 Avenue du Docteur Maurice Donat 
B?timent Marco Polo C2 - Bureau 202
06250 Mougins
France

***********************************
Ce message et toutes les pieces jointes (ci-apres le 'message') sont
confidentiels et etablis a l'intention exclusive de ses destinataires.
Toute utilisation ou diffusion non autorisee est interdite.
Tout message electronique est susceptible d'alteration. Le Groupe France
Telecom decline toute responsabilite au titre de ce message s'il a ete
altere, deforme ou falsifie.
Si vous n'etes pas destinataire de ce message, merci de le detruire
immediatement et d'avertir l'expediteur.
***********************************
This message and any attachments (the 'message') are confidential and
intended solely for the addressees.
Any unauthorised use or dissemination is prohibited.
Messages are susceptible to alteration. France Telecom Group shall not be
liable for the message if altered, changed or falsified.
If you are not recipient of this message, please cancel it immediately and
inform the sender.
************************************

From jc at oxado.com  Mon Jul  7 03:15:52 2008
From: jc at oxado.com (Jacques Caron)
Date: Mon Jul  7 03:16:56 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from
	locking alltables
In-Reply-To: <20080702083140.GA12941@depesz.com>
References: <486A0070.2060600@echo.fr>
	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
	<20080702083140.GA12941@depesz.com>
Message-ID: <20080707101625.C523D12C700E@zeus.directinfos.com>

Hi,

One VERY IMPORTANT point that got lost somewhere since my original 
post on the topic: the columns added MUST NOT have ANY constraints or 
defaults. Otherwise there are good chances it WILL break your 
replication, especially if there are updates/inserts into the table 
at the same time. Really think twice (or probably more times) about 
using that method with any constraint or default, and don't do it 
unless you understand how Slony works internally.

Also it only works for adding a column. Don't try to do the same for 
deleting a column. And changing a column's type, constraints, etc. is 
in most cases not possible with this method either.

In short, unless you are ADDING one column WITHOUT any constraints or 
defaults, use EXECUTE SCRIPT.

Jacques.

At 10:31 02/07/2008, hubert depesz lubaczewski wrote:

>On Tue, Jul 01, 2008 at 08:27:32AM -0700, Shahaf Abileah wrote:
> > Also, in certain situations you might be able to run the alter command
> > on each of the slaves and then on the master.  E.g. if you're adding a
> > new column that allows null values, then it might be possible to alter
> > the slaves even while rows are being inserted into the master.  You will
> > still be taking a lock on the table while altering it, but at least
> > you're not locking multiple tables at a time across multiple machines.
> > Not sure if this is recommended practice.
>
>it's not recommended, but it works.
>you add column on slave, then run this on master:
>UPDATE pg_trigger SET tgargs=substring(tgargs for 
>octet_length(tgargs)-1)||E'v\\000' where tgname = 'TRIGGER_NAME';
>and then add the column on master.
>
>best regards,
>
>depesz
>
>_______________________________________________
>Slony1-general mailing list
>Slony1-general@lists.slony.info
>http://lists.slony.info/mailman/listinfo/slony1-general

From ajs at crankycanuck.ca  Mon Jul  7 05:07:18 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon Jul  7 05:07:42 2008
Subject: [Slony1-general] (no subject)
In-Reply-To: <BLU107-W539CC7146F49E7CD1DA0C59E9A0@phx.gbl>
References: <BLU107-W539CC7146F49E7CD1DA0C59E9A0@phx.gbl>
Message-ID: <20080707120718.GA30643@crankycanuck.ca>

On Sat, Jul 05, 2008 at 10:38:14AM +0000, Ehl Le wrote:
> 
> I'm pretty annoyed now...  The last days I tried to set up slony to
> replicate the database on to slave from one master - that works very fine
> now.
> 
> But the problem is that slony requires from the slave to be able to
> connect to the master, something I don't want, since the slave is not
> entirely sealed, there are database-users on the slave-server, I do not
> want slony to store login-information in it's database on the slave nor do
> I want to allow access *from* a slave *to the master* (pg_hba) ...is that
> access really required?

Yes.  I expect you would have been less annoyed if you'd read the
documentation more carefully, because this is quite clearly noted there as a
requirement.  Sorry.

If you added an intermediate box that you could trust, you could use
logshipping to the box you don't trust.  That's one-way.

A
From scetbon at echo.fr  Mon Jul  7 05:31:38 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Mon Jul  7 05:31:42 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT from	locking
	alltables
In-Reply-To: <20080707101625.C523D12C700E@zeus.directinfos.com>
References: <486A0070.2060600@echo.fr>	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>	<20080702083140.GA12941@depesz.com>
	<20080707101625.C523D12C700E@zeus.directinfos.com>
Message-ID: <48720CAA.5000405@echo.fr>



Jacques Caron wrote:
> Hi,
>
> One VERY IMPORTANT point that got lost somewhere since my original 
> post on the topic: the columns added MUST NOT have ANY constraints or 
> defaults. Otherwise there are good chances it WILL break your 
> replication, especially if there are updates/inserts into the table at 
> the same time. Really think twice (or probably more times) about using 
> that method with any constraint or default, and don't do it unless you 
> understand how Slony works internally.
>
> Also it only works for adding a column. Don't try to do the same for 
> deleting a column. And changing a column's type, constraints, etc. is 
> in most cases not possible with this method either.
What the difference between adding and removing a column ? (in the case, 
there's no constraint)
>
> In short, unless you are ADDING one column WITHOUT any constraints or 
> defaults, use EXECUTE SCRIPT.
>
> Jacques.
>
> At 10:31 02/07/2008, hubert depesz lubaczewski wrote:
>
>> On Tue, Jul 01, 2008 at 08:27:32AM -0700, Shahaf Abileah wrote:
>> > Also, in certain situations you might be able to run the alter command
>> > on each of the slaves and then on the master.  E.g. if you're adding a
>> > new column that allows null values, then it might be possible to alter
>> > the slaves even while rows are being inserted into the master.  You 
>> will
>> > still be taking a lock on the table while altering it, but at least
>> > you're not locking multiple tables at a time across multiple machines.
>> > Not sure if this is recommended practice.
>>
>> it's not recommended, but it works.
>> you add column on slave, then run this on master:
>> UPDATE pg_trigger SET tgargs=substring(tgargs for 
>> octet_length(tgargs)-1)||E'v\\000' where tgname = 'TRIGGER_NAME';
>> and then add the column on master.
>>
>> best regards,
>>
>> depesz
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

-- 
Cyril SCETBON - Ing?nieur bases de donn?es
AUSY pour France T?l?com - OPF/PORTAILS/DOP/HEBEX

T?l : +33 (0)4 97 12 87 60
Jabber : cscetbon@jabber.org
France Telecom - Orange
790 Avenue du Docteur Maurice Donat 
B?timent Marco Polo C2 - Bureau 202
06250 Mougins
France

***********************************
Ce message et toutes les pieces jointes (ci-apres le 'message') sont
confidentiels et etablis a l'intention exclusive de ses destinataires.
Toute utilisation ou diffusion non autorisee est interdite.
Tout message electronique est susceptible d'alteration. Le Groupe France
Telecom decline toute responsabilite au titre de ce message s'il a ete
altere, deforme ou falsifie.
Si vous n'etes pas destinataire de ce message, merci de le detruire
immediatement et d'avertir l'expediteur.
***********************************
This message and any attachments (the 'message') are confidential and
intended solely for the addressees.
Any unauthorised use or dissemination is prohibited.
Messages are susceptible to alteration. France Telecom Group shall not be
liable for the message if altered, changed or falsified.
If you are not recipient of this message, please cancel it immediately and
inform the sender.
************************************

From jc at oxado.com  Mon Jul  7 06:31:58 2008
From: jc at oxado.com (Jacques Caron)
Date: Mon Jul  7 06:32:34 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT
	from	locking alltables
In-Reply-To: <48720CAA.5000405@echo.fr>
References: <486A0070.2060600@echo.fr>
	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
	<20080702083140.GA12941@depesz.com>
	<20080707101625.C523D12C700E@zeus.directinfos.com>
	<48720CAA.5000405@echo.fr>
Message-ID: <20080707133229.97F7B12C7005@zeus.directinfos.com>

At 14:31 07/07/2008, Cyril SCETBON wrote:
>What the difference between adding and removing a column ? (in the 
>case, there's no constraint)

Adding a column without using EXECUTE SCRIPT works because Slony logs 
the column names and builds statements with those names. For 
instance, if you have a table with two columns a and b, it will use 
statements of the form INSERT INTO table (a,b) VALUES(1,2) on the 
slaves. So if you add the column on the destination(s) first, those 
statements will work, and the new column will just have its default 
value (that's one of the reasons you don't want to have a constraint 
on that new column: the constraint is likely to break).

If you delete a column, then you should do it on the master first, 
and the destinations afterwards (actually, only once any 
inserts/updates on the table generated prior to the column deletion 
on the master have been processed on all nodes), otherwise you'll get 
statements referencing the columns that don't exist anymore. There 
should be no constraint on the column to delete either (otherwise you 
may end up with Slony inserting default values on the slave that 
don't match the constraint). You may also have to modify the trigger 
argument (removing one "v" in the right place), though that's not 
necessary if your primary key columns are the first ones. Of course 
you shouldn't delete a column that is part of the primary key (or 
change the primary key or anything like that).

In reality, there are a lot of things you can actually do without 
EXECUTE SCRIPT if there is no activity on the table you're modifying 
(and possibly any other tables that reference them etc.). The only 
problem being that you actually have to *think* (carefully) about the 
consequences of any changes, so as pointed out by Andrew, EXECUTE 
SCRIPT has to take such extensive locks because it can't know if the 
DDL you're submitting is "safe" to execute with minimal locking or not.

In short, EXECUTE SCRIPT will attempt very strongly not to let you 
shoot yourself in the foot, while doing DDL changes directly on the 
different nodes is an ICBM targeted at your foot if you don't know 
what you are doing (and even if you think you do)! :-)

Jacques.

From scetbon at echo.fr  Mon Jul  7 08:15:04 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Mon Jul  7 08:15:10 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT	from	locking
	alltables
In-Reply-To: <20080707133229.97F7B12C7005@zeus.directinfos.com>
References: <486A0070.2060600@echo.fr>	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>	<20080702083140.GA12941@depesz.com>	<20080707101625.C523D12C700E@zeus.directinfos.com>	<48720CAA.5000405@echo.fr>
	<20080707133229.97F7B12C7005@zeus.directinfos.com>
Message-ID: <487232F8.9030407@echo.fr>



Jacques Caron wrote:
> At 14:31 07/07/2008, Cyril SCETBON wrote:
>> What the difference between adding and removing a column ? (in the 
>> case, there's no constraint)
>
> Adding a column without using EXECUTE SCRIPT works because Slony logs 
> the column names and builds statements with those names. For instance, 
> if you have a table with two columns a and b, it will use statements 
> of the form INSERT INTO table (a,b) VALUES(1,2) on the slaves. So if 
> you add the column on the destination(s) first, those statements will 
> work, and the new column will just have its default value (that's one 
> of the reasons you don't want to have a constraint on that new column: 
> the constraint is likely to break).
>
> If you delete a column, then you should do it on the master first, and 
> the destinations afterwards (actually, only once any inserts/updates 
> on the table generated prior to the column deletion on the master have 
> been processed on all nodes),
OK
> otherwise you'll get statements referencing the columns that don't 
> exist anymore. There should be no constraint on the column to delete 
> either (otherwise you may end up with Slony inserting default values 
> on the slave that don't match the constraint). You may also have to 
> modify the trigger argument (removing one "v" in the right place), 
> though that's not necessary if your primary key columns are the first 
> ones.
it seems that it was not working without doing this action (adding the 
v) even if our primary key is the first column.
> Of course you shouldn't delete a column that is part of the primary 
> key (or change the primary key or anything like that).
that's not the case.
>
> In reality, there are a lot of things you can actually do without 
> EXECUTE SCRIPT if there is no activity on the table you're modifying 
> (and possibly any other tables that reference them etc.). The only 
> problem being that you actually have to *think* (carefully) about the 
> consequences of any changes, so as pointed out by Andrew, EXECUTE 
> SCRIPT has to take such extensive locks because it can't know if the 
> DDL you're submitting is "safe" to execute with minimal locking or not.
that's why I've initiated this thread. Cause We get a lot of DML orders 
and everything was locked during the execute script. We cannot use the 
EXECUTE SCRIPT as long as it works that way.
>
> In short, EXECUTE SCRIPT will attempt very strongly not to let you 
> shoot yourself in the foot, 
yes
> while doing DDL changes directly on the different nodes is an ICBM 
> targeted at your foot if you don't know what you are doing (and even 
> if you think you do)! :-)
We'll take care and try to know what we do and to think that the things 
that we do are good :-)
>
> Jacques.
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

-- 
Cyril SCETBON
From jc at oxado.com  Mon Jul  7 09:01:31 2008
From: jc at oxado.com (Jacques Caron)
Date: Mon Jul  7 09:01:55 2008
Subject: [Slony1-general] how to prevent EXECUTE
	SCRIPT	from	locking alltables
In-Reply-To: <487232F8.9030407@echo.fr>
References: <486A0070.2060600@echo.fr>
	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
	<20080702083140.GA12941@depesz.com>
	<20080707101625.C523D12C700E@zeus.directinfos.com>
	<48720CAA.5000405@echo.fr>
	<20080707133229.97F7B12C7005@zeus.directinfos.com>
	<487232F8.9030407@echo.fr>
Message-ID: <20080707160151.3F28D12C7004@zeus.directinfos.com>

At 17:15 07/07/2008, Cyril SCETBON wrote:
>it seems that it was not working without doing this action (adding 
>the v) even if our primary key is the first column.

Yes, the current implementation of Slony needs a letter for each 
column telling it whether it's a key or a value. If you don't have 
enough letters in the argument, at best it will be missing columns in 
the update, at worst it will segfault. I sent a patch a while ago 
that allows you to add columns without having to update the trigger 
arguments, you should be able to find in the archives.

Jacques.

From ajs at crankycanuck.ca  Mon Jul  7 11:45:47 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon Jul  7 11:46:14 2008
Subject: [Slony1-general] DDL (altering current column types)
In-Reply-To: <1215418524.4952.22.camel@neuromancer.home.net>
References: <1215418524.4952.22.camel@neuromancer.home.net>
Message-ID: <20080707184547.GD31669@crankycanuck.ca>

On Mon, Jul 07, 2008 at 04:15:24PM +0800, Ow Mun Heng wrote:
> individually on each slave/master table or I have to do it via slony via
> EXECUTE script?

You have to do it via execute script.

From scetbon at echo.fr  Tue Jul  8 01:07:56 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Tue Jul  8 01:08:28 2008
Subject: [Slony1-general] how to prevent EXECUTE	SCRIPT	from	locking
	alltables
In-Reply-To: <20080707160151.3F28D12C7004@zeus.directinfos.com>
References: <486A0070.2060600@echo.fr>	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>	<20080702083140.GA12941@depesz.com>	<20080707101625.C523D12C700E@zeus.directinfos.com>	<48720CAA.5000405@echo.fr>	<20080707133229.97F7B12C7005@zeus.directinfos.com>	<487232F8.9030407@echo.fr>
	<20080707160151.3F28D12C7004@zeus.directinfos.com>
Message-ID: <4873205C.9000706@echo.fr>



Jacques Caron wrote:
> At 17:15 07/07/2008, Cyril SCETBON wrote:
>> it seems that it was not working without doing this action (adding 
>> the v) even if our primary key is the first column.
>
> Yes, the current implementation of Slony needs a letter for each 
> column telling it whether it's a key or a value. If you don't have 
> enough letters in the argument, at best it will be missing columns in 
> the update, at worst it will segfault. I sent a patch a while ago that 
> allows you to add columns without having to update the trigger 
> arguments, you should be able to find in the archives.
found http://archive.netbsd.se/?ml=slony1-general&a=2007-11&t=5735520

You said it saves a few cycles. Did you test/benchmarck it ? We have a 
lot of updates if it can help it would be great :-)

Regards
>
> Jacques.
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

-- 
Cyril SCETBON
From rafael.domiciano at gmail.com  Tue Jul  8 06:27:23 2008
From: rafael.domiciano at gmail.com (Rafael Domiciano)
Date: Tue Jul  8 06:27:27 2008
Subject: [Slony1-general] Drop Table
Message-ID: <3a0028490807080627t700893f4ud39168a88b707996@mail.gmail.com>

Hi guys,

I need to drop a table that is replicated. This table there is no data, and
it's begin dropped because there is no more use.
I would like to know how can I drop the table? If a only drop the table in
the master, will Slony break into error?
Or I need to stop Slony, drop the set correspondent, drop the table, create
the set and then start the Slony?

Best Regards,
Rafael Domiciano
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080708/=
f28b6141/attachment.htm
From wmoran at collaborativefusion.com  Tue Jul  8 06:46:43 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Tue Jul  8 06:47:49 2008
Subject: [Slony1-general] Drop Table
In-Reply-To: <3a0028490807080627t700893f4ud39168a88b707996@mail.gmail.com>
References: <3a0028490807080627t700893f4ud39168a88b707996@mail.gmail.com>
Message-ID: <20080708094643.bfc87d97.wmoran@collaborativefusion.com>

In response to "Rafael Domiciano" <rafael.domiciano@gmail.com>:

> Hi guys,
> 
> I need to drop a table that is replicated. This table there is no data, and
> it's begin dropped because there is no more use.
> I would like to know how can I drop the table? If a only drop the table in
> the master, will Slony break into error?

Yes, it will.

> Or I need to stop Slony, drop the set correspondent, drop the table, create
> the set and then start the Slony?

You can drop the table from replication first, which is a single Slony
action:
http://slony.info/documentation/stmtsetdroptable.html


Once it's no longer replicated, you can go in to each replica and
issue an SQL DROP TABLE to remove it.  There's no need to stop Slony
or reconfigure replication.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From glynastill at yahoo.co.uk  Tue Jul  8 06:48:02 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue Jul  8 06:48:06 2008
Subject: [Slony1-general] Drop Table
Message-ID: <143931.42444.qm@web25803.mail.ukl.yahoo.com>

WW91IG5lZWQgdG8gZG8gYSBTRVQgRFJPUCBUQUJMRSB2aWEgc2xvbmlrLCBhbmQgdGhlbiByZW1v
dmUgdGhlIHRhYmxlIGZyb20gZWFjaCBkYXRhYmFzZSB2aWEgdGhlIHVzdWFsIGRyb3AgdGFibGUu
CgpBZmFpayBpZiB5b3UgdHJ5IGFuZCBkcm9wIHRoZSB0YWJsZSB3aGlsc3QgaXRzIGluIHJlcGxp
Y2F0aW9uIHlvdSdsbCBlbmQgdXAgd2l0aCBhICJjYWNoZSBsb29rdXAgZmFpbGVkIGZvciByZWxh
dGlvbiIgZXJyb3Igb3Igc29tZXRoaW5nIHNpbWlsYXIuCgoKCgotLS0tLSBPcmlnaW5hbCBNZXNz
YWdlIC0tLS0KRnJvbTogUmFmYWVsIERvbWljaWFubyA8cmFmYWVsLmRvbWljaWFub0BnbWFpbC5j
b20+ClRvOiBzbG9ueTEtZ2VuZXJhbEBsaXN0cy5zbG9ueS5pbmZvClNlbnQ6IFR1ZXNkYXksIDgg
SnVseSwgMjAwOCAyOjI3OjIzIFBNClN1YmplY3Q6IFtTbG9ueTEtZ2VuZXJhbF0gRHJvcCBUYWJs
ZQoKSGkgZ3V5cywKCkkgbmVlZCB0byBkcm9wIGEgdGFibGUgdGhhdCBpcyByZXBsaWNhdGVkLiBU
aGlzIHRhYmxlIHRoZXJlIGlzIG5vIGRhdGEsIGFuZCBpdCdzIGJlZ2luIGRyb3BwZWQgYmVjYXVz
ZSB0aGVyZSBpcyBubyBtb3JlIHVzZS4KSSB3b3VsZCBsaWtlIHRvIGtub3cgaG93IGNhbiBJIGRy
b3AgdGhlIHRhYmxlPyBJZiBhIG9ubHkgZHJvcCB0aGUgdGFibGUgaW4gdGhlIG1hc3Rlciwgd2ls
bCBTbG9ueSBicmVhayBpbnRvIGVycm9yPwpPciBJIG5lZWQgdG8gc3RvcCBTbG9ueSwgZHJvcCB0
aGUgc2V0IGNvcnJlc3BvbmRlbnQsIGRyb3AgdGhlIHRhYmxlLCBjcmVhdGUgdGhlIHNldCBhbmQg
dGhlbiBzdGFydCB0aGUgU2xvbnk/CgpCZXN0IFJlZ2FyZHMsClJhZmFlbCBEb21pY2lhbm8KCgoK
ICAgICAgX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19f
X19fX19fXwpOb3QgaGFwcHkgd2l0aCB5b3VyIGVtYWlsIGFkZHJlc3M/LgpHZXQgdGhlIG9uZSB5
b3UgcmVhbGx5IHdhbnQgLSBtaWxsaW9ucyBvZiBuZXcgZW1haWwgYWRkcmVzc2VzIGF2YWlsYWJs
ZSBub3cgYXQgWWFob28hIGh0dHA6Ly91ay5kb2NzLnlhaG9vLmNvbS95bWFpbC9uZXcuaHRtbAot
LS0tLS0tLS0tLS0tLSBuZXh0IHBhcnQgLS0tLS0tLS0tLS0tLS0KQW4gSFRNTCBhdHRhY2htZW50
IHdhcyBzY3J1YmJlZC4uLgpVUkw6IGh0dHA6Ly9saXN0cy5zbG9ueS5pbmZvL3BpcGVybWFpbC9z
bG9ueTEtZ2VuZXJhbC9hdHRhY2htZW50cy8yMDA4MDcwOC8zYWI0NGI4ZS9hdHRhY2htZW50Lmh0
bQo=
From Ow.Mun.Heng at wdc.com  Tue Jul  8 20:11:37 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Tue Jul  8 20:11:56 2008
Subject: [Slony1-general] DDL (altering current column types)
In-Reply-To: <20080707184547.GD31669@crankycanuck.ca>
References: <1215418524.4952.22.camel@neuromancer.home.net>
	<20080707184547.GD31669@crankycanuck.ca>
Message-ID: <1215573097.22595.4.camel@neuromancer.home.net>

On Mon, 2008-07-07 at 14:45 -0400, Andrew Sullivan wrote:
> On Mon, Jul 07, 2008 at 04:15:24PM +0800, Ow Mun Heng wrote:
> > individually on each slave/master table or I have to do it via slony via
> > EXECUTE script?
> 
> You have to do it via execute script.

There is absolutely no other method? (how bout a second opinion just to
re-affirm the inevitable)
I really dread doing it via slony as this is really take a _long_ time
and tables will be locked.

Thanks.
From victor.aluko at gmail.com  Wed Jul  9 03:56:15 2008
From: victor.aluko at gmail.com (ajcity)
Date: Wed Jul  9 03:56:42 2008
Subject: [Slony1-general] Slony daemon running but not replicating data
Message-ID: <18358913.post@talk.nabble.com>


  Hi all,
  Am having a strange problem and I need help. I have a replication setup
with one master and 2 slave nodes. Replication was working fine until a week
ago when I upgraded the version of the Postgresql on all the nodes from
8.3.1 to 8.3.3 and since I installed the postgresql on the slve nodes
through tarball, I reinstalled the slony to the same version 1.2.13
  The problem is that thelon daemons are connection and receiving sync
events (both on the master and slave nodes) but no new data is coming into
the slave nodes. The slon daemon is fetching 0 log rows but there is always
data being given out in the sl_log_2 table of the master node.
  Any ideas as to what might be wrong or what I should do?
  Thanks in advance for your help.

  Victor
-- 
View this message in context: http://www.nabble.com/Slony-daemon-running-but-not-replicating-data-tp18358913p18358913.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From Ow.Mun.Heng at wdc.com  Wed Jul  9 18:42:28 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Wed Jul  9 18:42:47 2008
Subject: FYI Re: [Slony1-general] DDL (altering current column types)
In-Reply-To: <1215573097.22595.4.camel@neuromancer.home.net>
References: <1215418524.4952.22.camel@neuromancer.home.net>
	<20080707184547.GD31669@crankycanuck.ca>
	<1215573097.22595.4.camel@neuromancer.home.net>
Message-ID: <1215654148.32232.10.camel@neuromancer.home.net>

On Wed, 2008-07-09 at 11:11 +0800, Ow Mun Heng wrote:
> On Mon, 2008-07-07 at 14:45 -0400, Andrew Sullivan wrote:
> > On Mon, Jul 07, 2008 at 04:15:24PM +0800, Ow Mun Heng wrote:
> > > individually on each slave/master table or I have to do it via slony via
> > > EXECUTE script?
> > 
> > You have to do it via execute script.
> 
> There is absolutely no other method? (how bout a second opinion just to
> re-affirm the inevitable)
> I really dread doing it via slony as this is really take a _long_ time
> and tables will be locked.

[Just for the list to Digest as a good FYI to watch out for in case you
happen to want to do the same thing]

History:
I took the plunge and did it via execute script on the master.
The DDL completed fine on the master, but nothing is happening on the
slave. Nothing is moving and there has not been any updates to any of
the other replicated tables.

In the master's slong I keep seeing

[log]
2008-07-10 09:19:57 MYT DEBUG2 remoteListenThread_1: queue event
1,1442800 SYNC
2008-07-10 09:19:57 MYT DEBUG2 remoteWorker_event: ignore new events due
to shutdown
2008-07-10 09:19:57 MYT DEBUG2 remoteListenThread_1: queue event
1,1442801 SYNC
2008-07-10 09:19:57 MYT DEBUG2 remoteWorker_event: ignore new events due
to shutdown


restarting the slon daemon and putting the logs somewhere I see?
2008-07-10 09:20:49 MYT INFO   prepared for DDL script
2008-07-10 09:20:49 MYT CONFIG remoteWorkerThread_1: DDL request with 2
statements
2008-07-10 09:20:49 MYT CONFIG remoteWorkerThread_1: DDL Statement 0:
[alter table xmms.d_trh_dlu_rfpe

alter media_dcm type character varying,
alter head_dcm type character varying,
alter preamp_dcm type character varying;]
2008-07-10 09:20:49 MYT ERROR  DDL Statement failed - PGRES_FATAL_ERROR
2008-07-10 09:20:49 MYT DEBUG2 slon_retry() from pid=14545
2008-07-10 09:20:49 MYT DEBUG1 slon: retry requested
2008-07-10 09:20:49 MYT DEBUG2 slon: notify worker process to shutdown
2008-07-10 09:20:49 MYT DEBUG1 syncThread: thread done
2008-07-10 09:20:49 MYT DEBUG1 cleanupThread: thread done
2008-07-10 09:20:49 MYT DEBUG1 main: scheduler mainloop returned
2008-07-10 09:20:49 MYT DEBUG2 main: wait for remote threads
2008-07-10 09:20:49 MYT DEBUG2 sched_wakeup_node(): no_id=1 (0 threads +
worker signaled)
2008-07-10 09:20:49 MYT DEBUG1 localListenThread: thread done
2008-07-10 09:20:49 MYT DEBUG2 remoteListenThread_1: queue event
1,1435029 SYNC
2008-07-10 09:20:49 MYT DEBUG2 remoteWorker_event: ignore new events due
to shutdown
2008-07-10 09:20:49 MYT DEBUG2 remoteListenThread_1: queue event
1,1435030 SYNC
[/log]

I checked the usual problems, is it due to path issues etc. meaning, the
log-ged in slony user is having a different path (eg: show search_path;)

Nope.

Then I checked to see if the DDL will run on the slave as is. 

[execute via pgadmin]
begin;
alter table xx alter yyy type sss
[/xecute via pgadmin]

errors present in pgadmin

[log]
ERROR:  cannot alter type of a column used by a view or rule
DETAIL:  rule _RETURN on view xmms.v_dlu_rfpe_raw depends on column
"media_dcm"
********** Error **********
ERROR: cannot alter type of a column used by a view or rule
SQL state: 0A000
Detail: rule _RETURN on view xmms.v_dlu_rfpe_raw depends on column
"media_dcm"
?[/log]

I _did_ remember to remove the view from the master (not via execute
script though).
So, that's just FYI.

BTW, at the risk of sounding like a broken record, this _really_ can't
be executed independent of slony? That would be the best case I would
say.






From ajs at crankycanuck.ca  Thu Jul 10 14:15:26 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Jul 10 14:16:01 2008
Subject: [Slony1-general] DDL (altering current column types)
In-Reply-To: <1215573097.22595.4.camel@neuromancer.home.net>
References: <1215418524.4952.22.camel@neuromancer.home.net>
	<20080707184547.GD31669@crankycanuck.ca>
	<1215573097.22595.4.camel@neuromancer.home.net>
Message-ID: <20080710211526.GA5704@crankycanuck.ca>

On Wed, Jul 09, 2008 at 11:11:37AM +0800, Ow Mun Heng wrote:
> 
> There is absolutely no other method? (how bout a second opinion just to
> re-affirm the inevitable)

See the other discussion about doing things without execute script.  If you
really know what you are doing, you can do this without the heavy weight
locks, but it's still really tricky, prone to race conditions, and otherwise
dangerous.  If it weren't dangerous, Slony wouldn't treat it so strictly.

From ajs at crankycanuck.ca  Thu Jul 10 14:29:15 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Jul 10 14:29:34 2008
Subject: FYI Re: [Slony1-general] DDL (altering current column types)
In-Reply-To: <1215654148.32232.10.camel@neuromancer.home.net>
References: <1215418524.4952.22.camel@neuromancer.home.net>
	<20080707184547.GD31669@crankycanuck.ca>
	<1215573097.22595.4.camel@neuromancer.home.net>
	<1215654148.32232.10.camel@neuromancer.home.net>
Message-ID: <20080710212914.GB5704@crankycanuck.ca>

On Thu, Jul 10, 2008 at 09:42:28AM +0800, Ow Mun Heng wrote:
> Then I checked to see if the DDL will run on the slave as is. 

If you check the Slony admin manual, it actually suggests that you do this
on the target machines, wrapped in BEGIN;?;ROLLBACK.  This is because it's
very easy with execute script to run into the problem you just did.  See
section 15.2.

> BTW, at the risk of sounding like a broken record, this _really_ can't
> be executed independent of slony? That would be the best case I would
> say.

Here's why it has to be done with Slony's knowledge: suppose you make a
change to the schema on the data origin at time _t_, and a change to the
schema on a replica at time _t_+2.  Now, suppose you have a transaction that
gets replicated at _t_+1.  The origin has the schema change, and Slony
doesn't know that the schema on the replica isn't ready for the data.  So
it will try to replicate that data, and things will break.

So that's why you need to have it done by Slony.  

A
From scetbon at echo.fr  Fri Jul 11 06:52:14 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Fri Jul 11 06:52:24 2008
Subject: [Slony1-general] Slow Replication issue
Message-ID: <4877658E.4050205@echo.fr>

Hi,

I've got a lot of updates (~1000 w/s) on the master and on the nearest 
slave (like others) I've got a big replication lag. It seems that 
fetching lines from the cursor is not taking much time, but processing 
events is not well managed.

Here you can find an extract of the log that shows that the proposed 
size for grouping is often just 3 and not greater :

http://pastebin.com/f2a2974a

The slon parameters used are : -g 1000 -o 0

Any idea to speed up the processing ?


-- 
Cyril SCETBON
From cbbrowne at ca.afilias.info  Fri Jul 11 08:00:22 2008
From: cbbrowne at ca.afilias.info (chris)
Date: Fri Jul 11 08:00:32 2008
Subject: [Slony1-general] Slow Replication issue
In-Reply-To: <4877658E.4050205@echo.fr> (Cyril SCETBON's message of "Fri, 11
	Jul 2008 15:52:14 +0200")
References: <4877658E.4050205@echo.fr>
Message-ID: <87od5431q1.fsf@dba2.int.libertyrms.com>

Cyril SCETBON <scetbon@echo.fr> writes:
> I've got a lot of updates (~1000 w/s) on the master and on the nearest
> slave (like others) I've got a big replication lag. It seems that
> fetching lines from the cursor is not taking much time, but processing
> events is not well managed.
>
> Here you can find an extract of the log that shows that the proposed
> size for grouping is often just 3 and not greater :
>
> http://pastebin.com/f2a2974a
>
> The slon parameters used are : -g 1000 -o 0
>
> Any idea to speed up the processing ?

There's something a bit confusing in the logs; it's not affecting how
the grouping is actually working.  The "just 3" looks to be when it's
evaluating sync grouping for events coming from *other* nodes than the
provider, which is pretty much irrelevant since those syncs don't lead
to any actual work.

Looks to me like the processing of data from node #1 is working out
about as can be expected on a node that is processing a lot of data.
There may be relevant/material improvements to handling of this in
2.0; I don't think there's anything to be done in terms of
configuration.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From JanWieck at Yahoo.com  Fri Jul 11 08:16:00 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Fri Jul 11 08:16:29 2008
Subject: [Slony1-general] Slow Replication issue
In-Reply-To: <87od5431q1.fsf@dba2.int.libertyrms.com>
References: <4877658E.4050205@echo.fr> <87od5431q1.fsf@dba2.int.libertyrms.com>
Message-ID: <48777930.704@Yahoo.com>

On 7/11/2008 11:00 AM, chris wrote:
> Cyril SCETBON <scetbon@echo.fr> writes:
>> I've got a lot of updates (~1000 w/s) on the master and on the nearest
>> slave (like others) I've got a big replication lag. It seems that
>> fetching lines from the cursor is not taking much time, but processing
>> events is not well managed.
>>
>> Here you can find an extract of the log that shows that the proposed
>> size for grouping is often just 3 and not greater :
>>
>> http://pastebin.com/f2a2974a
>>
>> The slon parameters used are : -g 1000 -o 0
>>
>> Any idea to speed up the processing ?
> 
> There's something a bit confusing in the logs; it's not affecting how
> the grouping is actually working.  The "just 3" looks to be when it's
> evaluating sync grouping for events coming from *other* nodes than the
> provider, which is pretty much irrelevant since those syncs don't lead
> to any actual work.

Correct. The 3 or 4 group size is for non origins. The current group 
size for node 1, which seems to be the only origin in the system, is 
actually 255.

> 
> Looks to me like the processing of data from node #1 is working out
> about as can be expected on a node that is processing a lot of data.
> There may be relevant/material improvements to handling of this in
> 2.0; I don't think there's anything to be done in terms of
> configuration.

I seem to remember that Slony had some problems with large numbers of 
sets. Although it seems that in this particular case it only accounts 
for 0.3 out of 135 seconds to completely process 255 sync events.

Anyhow, I counted 31 sets with a total of 513 tables which have a pretty 
strange pattern. Up to set 29, all the odd set id's have 17 tables while 
the even set id's have 18 tables. Is this one of those misdesigned 
applications that creates a separate set of tables per user or the like?


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From scetbon at echo.fr  Fri Jul 11 08:17:25 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Fri Jul 11 08:17:32 2008
Subject: [Slony1-general] Slow Replication issue
In-Reply-To: <87od5431q1.fsf@dba2.int.libertyrms.com>
References: <4877658E.4050205@echo.fr> <87od5431q1.fsf@dba2.int.libertyrms.com>
Message-ID: <48777985.2000805@echo.fr>



chris wrote:
> Cyril SCETBON <scetbon@echo.fr> writes:
>   
>> I've got a lot of updates (~1000 w/s) on the master and on the nearest
>> slave (like others) I've got a big replication lag. It seems that
>> fetching lines from the cursor is not taking much time, but processing
>> events is not well managed.
>>
>> Here you can find an extract of the log that shows that the proposed
>> size for grouping is often just 3 and not greater :
>>
>> http://pastebin.com/f2a2974a
>>
>> The slon parameters used are : -g 1000 -o 0
>>
>> Any idea to speed up the processing ?
>>     
>
> There's something a bit confusing in the logs; it's not affecting how
> the grouping is actually working.  The "just 3" looks to be when it's
> evaluating sync grouping for events coming from *other* nodes than the
> provider, which is pretty much irrelevant since those syncs don't lead
> to any actual work.
>   
you mean it does not consume time ?
> Looks to me like the processing of data from node #1 is working out
> about as can be expected on a node that is processing a lot of data.
>   
grouping does not mean using bigger transactions to apply changes ?
> There may be relevant/material improvements to handling of this in
> 2.0; 
ok, but I cannot use it (postgresql 8.2)
> I don't think there's anything to be done in terms of
> configuration.
>   
For your information I'm using version 1.2.10.

-- 
Cyril SCETBON
From rafael.domiciano at gmail.com  Fri Jul 11 09:18:38 2008
From: rafael.domiciano at gmail.com (Rafael Domiciano)
Date: Fri Jul 11 09:18:46 2008
Subject: [Slony1-general] [SLONY] Fail-over
Message-ID: <3a0028490807110918h3810e86ev726abd4a7e752425@mail.gmail.com>

Hi folks,

I have a doubt about fail-over, and the documentation doesn't answered me
correctly.
So... how does the Fail-Over works? How to go back? And how about the events
in the new server?
I have another doubt, if my master have go down, how does i use the slave?
I am asking these because i'm configuring a cascade slony: 50 -> 22 -> 02
The 22 machine is going to be a read-only server to minimize the operations
in the master (50)
And the machine is goint to be the cascade server.

Thnks,

Rafael Domiciano
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080711/=
b7a5c7d4/attachment.htm
From scetbon at echo.fr  Fri Jul 11 10:20:05 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Fri Jul 11 10:20:36 2008
Subject: [Slony1-general] Slow Replication issue
In-Reply-To: <48777930.704@Yahoo.com>
References: <4877658E.4050205@echo.fr> <87od5431q1.fsf@dba2.int.libertyrms.com>
	<48777930.704@Yahoo.com>
Message-ID: <48779645.50301@echo.fr>



Jan Wieck wrote:
> On 7/11/2008 11:00 AM, chris wrote:
>> Cyril SCETBON <scetbon@echo.fr> writes:
>>> I've got a lot of updates (~1000 w/s) on the master and on the nearest
>>> slave (like others) I've got a big replication lag. It seems that
>>> fetching lines from the cursor is not taking much time, but processing
>>> events is not well managed.
>>>
>>> Here you can find an extract of the log that shows that the proposed
>>> size for grouping is often just 3 and not greater :
>>>
>>> http://pastebin.com/f2a2974a
>>>
>>> The slon parameters used are : -g 1000 -o 0
>>>
>>> Any idea to speed up the processing ?
>>
>> There's something a bit confusing in the logs; it's not affecting how
>> the grouping is actually working.  The "just 3" looks to be when it's
>> evaluating sync grouping for events coming from *other* nodes than the
>> provider, which is pretty much irrelevant since those syncs don't lead
>> to any actual work.
>
> Correct. The 3 or 4 group size is for non origins. The current group 
> size for node 1, which seems to be the only origin in the system, is 
> actually 255.
>
>>
>> Looks to me like the processing of data from node #1 is working out
>> about as can be expected on a node that is processing a lot of data.
>> There may be relevant/material improvements to handling of this in
>> 2.0; I don't think there's anything to be done in terms of
>> configuration.
>
> I seem to remember that Slony had some problems with large numbers of 
> sets. Although it seems that in this particular case it only accounts 
> for 0.3 out of 135 seconds to completely process 255 sync events.
>
> Anyhow, I counted 31 sets with a total of 513 tables which have a 
> pretty strange pattern. Up to set 29, all the odd set id's have 17 
> tables while the even set id's have 18 tables. Is this one of those 
> misdesigned applications that creates a separate set of tables per 
> user or the like?
we got 30 sets (1 is for an heartbeat table) to be able to spread them 
over up to three/four differents  masters if needed (one at this time). 
The number of tables is a manual partitionning (lot of rows and 
performance issues)

Is does not seem to hurt performance as you said, but do you see 
anything else ?
>
>
> Jan
>

-- 
Cyril SCETBON
From scetbon at echo.fr  Fri Jul 11 10:32:56 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Fri Jul 11 10:33:06 2008
Subject: [Slony1-general] Slow Replication issue
In-Reply-To: <48779645.50301@echo.fr>
References: <4877658E.4050205@echo.fr>
	<87od5431q1.fsf@dba2.int.libertyrms.com>	<48777930.704@Yahoo.com>
	<48779645.50301@echo.fr>
Message-ID: <48779948.6090809@echo.fr>



Cyril SCETBON wrote:
>
>
> Jan Wieck wrote:
>> On 7/11/2008 11:00 AM, chris wrote:
>>> Cyril SCETBON <scetbon@echo.fr> writes:
>>>> I've got a lot of updates (~1000 w/s) on the master and on the nearest
>>>> slave (like others) I've got a big replication lag. It seems that
>>>> fetching lines from the cursor is not taking much time, but processing
>>>> events is not well managed.
>>>>
>>>> Here you can find an extract of the log that shows that the proposed
>>>> size for grouping is often just 3 and not greater :
>>>>
>>>> http://pastebin.com/f2a2974a
>>>>
>>>> The slon parameters used are : -g 1000 -o 0
>>>>
>>>> Any idea to speed up the processing ?
>>>
>>> There's something a bit confusing in the logs; it's not affecting how
>>> the grouping is actually working.  The "just 3" looks to be when it's
>>> evaluating sync grouping for events coming from *other* nodes than the
>>> provider, which is pretty much irrelevant since those syncs don't lead
>>> to any actual work.
>>
>> Correct. The 3 or 4 group size is for non origins. The current group 
>> size for node 1, which seems to be the only origin in the system, is 
>> actually 255.
>>
>>>
>>> Looks to me like the processing of data from node #1 is working out
>>> about as can be expected on a node that is processing a lot of data.
>>> There may be relevant/material improvements to handling of this in
>>> 2.0; I don't think there's anything to be done in terms of
>>> configuration.
>>
>> I seem to remember that Slony had some problems with large numbers of 
>> sets. Although it seems that in this particular case it only accounts 
>> for 0.3 out of 135 seconds to completely process 255 sync events.
>>
>> Anyhow, I counted 31 sets with a total of 513 tables which have a 
>> pretty strange pattern. Up to set 29, all the odd set id's have 17 
>> tables while the even set id's have 18 tables. Is this one of those 
>> misdesigned applications that creates a separate set of tables per 
>> user or the like?
> we got 30 sets (1 is for an heartbeat table) to be able to spread them 
> over up to three/four differents  masters if needed (one at this 
> time). The number of tables is a manual partitionning (lot of rows and 
> performance issues)
>
> Is does not seem to hurt performance as you said, but do you see 
> anything else ?
If I check the different grouping size I see :

count      size

    1              255
    1              763
    2                  7
290                  3

If I understand, you both mean that 301.017 seconds is the duration of 
the processing of events from node 1 which size is 255 and not the 290 
groups of 3 events ?

>>
>>
>> Jan
>>
>

-- 
Cyril SCETBON
From cbbrowne at ca.afilias.info  Fri Jul 11 11:06:50 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Jul 11 11:07:01 2008
Subject: [Slony1-general] Slow Replication issue
In-Reply-To: <48779948.6090809@echo.fr> (Cyril SCETBON's message of "Fri, 11
	Jul 2008 19:32:56 +0200")
References: <4877658E.4050205@echo.fr>
	<87od5431q1.fsf@dba2.int.libertyrms.com> <48777930.704@Yahoo.com>
	<48779645.50301@echo.fr> <48779948.6090809@echo.fr>
Message-ID: <87fxqg2t39.fsf@dba2.int.libertyrms.com>

Cyril SCETBON <scetbon@echo.fr> writes:
>> Is does not seem to hurt performance as you said, but do you see
>> anything else ?
> If I check the different grouping size I see :
>
> count      size
>
>    1              255
>    1              763
>    2                  7
> 290                  3
>
> If I understand, you both mean that 301.017 seconds is the duration of
> the processing of events from node 1 which size is 255 and not the 290
> groups of 3 events ?

Right, 301.017s is the duration of the processing of events from node
1.

It's not reporting any duration information for the other nodes in
that it's not doing the work of:

 - searching for the scope of the SYNC group,
 - opening a cursor to pull the relevant log data for that scope, and
 - applying the changes to the subscriber.

The processing of the 290 groups of 3 events is taking place
concurrently with the Real Work, in separate threads, so it's not
consuming any time that's worth measuring.

Over lunch, Jan and I had a chat about this; it looks like we don't
report quite comprehensive enough information in the logs to make it
easy to interpret what parts of SYNC processing are consuming what
time.

The "straw man" idea we came up with is to do a much better breakdown
of the time, in particlar, to record:

 - time spent in pqexec() against the provider, broken down into...
    - time spent processing what transactions are part of the SYNC group
    - time spent processing the LOG cursor
 - time spent in pqexec() against the subscriber (the I/U/D phase)
 - numbers of pqexecs()
    against provider
    against subscriber
 - possibly, the number of times we grab timestamps

This could be costly to record (since it will call gettimeofday()
quite a lot of times), so we'd add in an option as to whether to do
this analysis or not.

This would put more useful information in the logs, specifically, it
would make it easier to see where the bottleneck really lies.

For instance, if the log indicates that 297s are being spent in
pqexec() on the provider, and only 4s on pqexec() on the subscriber,
that makes it pretty that there's some problem with the queries
searching for data to replicate.  

Reverse the figures, and it becomes clear that, for some reason, the
subscriber is very slow in accepting updates.

In either case, that is helpful in suggesting where to look for
improvements.  To be sure, we'd not focus on the place with 4s; we'd
focus on the 297s!

Improvements welcome; I'll probably put off starting to implement
anything until next week.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From scetbon at echo.fr  Fri Jul 11 11:50:58 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Fri Jul 11 11:51:09 2008
Subject: [Slony1-general] Slow Replication issue
In-Reply-To: <87fxqg2t39.fsf@dba2.int.libertyrms.com>
References: <4877658E.4050205@echo.fr>	<87od5431q1.fsf@dba2.int.libertyrms.com>
	<48777930.704@Yahoo.com>	<48779645.50301@echo.fr>
	<48779948.6090809@echo.fr> <87fxqg2t39.fsf@dba2.int.libertyrms.com>
Message-ID: <4877AB92.9020102@echo.fr>



Christopher Browne wrote:
> Cyril SCETBON <scetbon@echo.fr> writes:
>   
>>> Is does not seem to hurt performance as you said, but do you see
>>> anything else ?
>>>       
>> If I check the different grouping size I see :
>>
>> count      size
>>
>>    1              255
>>    1              763
>>    2                  7
>> 290                  3
>>
>> If I understand, you both mean that 301.017 seconds is the duration of
>> the processing of events from node 1 which size is 255 and not the 290
>> groups of 3 events ?
>>     
>
> Right, 301.017s is the duration of the processing of events from node
> 1.
>
> It's not reporting any duration information for the other nodes in
> that it's not doing the work of:
>
>  - searching for the scope of the SYNC group,
>  - opening a cursor to pull the relevant log data for that scope, and
>  - applying the changes to the subscriber.
>
> The processing of the 290 groups of 3 events is taking place
> concurrently with the Real Work, in separate threads, so it's not
> consuming any time that's worth measuring.
>
> Over lunch, Jan and I had a chat about this; it looks like we don't
> report quite comprehensive enough information in the logs to make it
> easy to interpret what parts of SYNC processing are consuming what
> time.
>
> The "straw man" idea we came up with is to do a much better breakdown
> of the time, in particlar, to record:
>
>  - time spent in pqexec() against the provider, broken down into...
>     - time spent processing what transactions are part of the SYNC group
>     - time spent processing the LOG cursor
>  - time spent in pqexec() against the subscriber (the I/U/D phase)
>  - numbers of pqexecs()
>     against provider
>     against subscriber
>  - possibly, the number of times we grab timestamps
>
> This could be costly to record (since it will call gettimeofday()
> quite a lot of times), so we'd add in an option as to whether to do
> this analysis or not.
>
> This would put more useful information in the logs, specifically, it
> would make it easier to see where the bottleneck really lies.
>
> For instance, if the log indicates that 297s are being spent in
> pqexec() on the provider, and only 4s on pqexec() on the subscriber,
> that makes it pretty that there's some problem with the queries
> searching for data to replicate.  
>
> Reverse the figures, and it becomes clear that, for some reason, the
> subscriber is very slow in accepting updates.
>   
I think that would be really a great improvement for log analysis
> In either case, that is helpful in suggesting where to look for
> improvements.  To be sure, we'd not focus on the place with 4s; we'd
> focus on the 297s!
>
> Improvements welcome; I'll probably put off starting to implement
> anything until next week.
>   
For now, I'll try to enable the log_duration and log_statement 
parameters in postgresql.conf file to investiguate.

Thanks


-- 
Cyril SCETBON
From cbbrowne at ca.afilias.info  Fri Jul 11 12:39:41 2008
From: cbbrowne at ca.afilias.info (chris)
Date: Fri Jul 11 12:39:53 2008
Subject: [Slony1-general] Slow Replication issue
In-Reply-To: <87fxqg2t39.fsf@dba2.int.libertyrms.com> (Christopher Browne's
	message of "Fri, 11 Jul 2008 14:06:50 -0400")
References: <4877658E.4050205@echo.fr>
	<87od5431q1.fsf@dba2.int.libertyrms.com> <48777930.704@Yahoo.com>
	<48779645.50301@echo.fr> <48779948.6090809@echo.fr>
	<87fxqg2t39.fsf@dba2.int.libertyrms.com>
Message-ID: <87abgo2osi.fsf@dba2.int.libertyrms.com>

Christopher Browne <cbbrowne@ca.afilias.info> writes:
> Over lunch, Jan and I had a chat about this; it looks like we don't
> report quite comprehensive enough information in the logs to make it
> easy to interpret what parts of SYNC processing are consuming what
> time.
>
> The "straw man" idea we came up with is to do a much better breakdown
> of the time, in particlar, to record:
>
>  - time spent in pqexec() against the provider, broken down into...
>     - time spent processing what transactions are part of the SYNC group
>     - time spent processing the LOG cursor
>  - time spent in pqexec() against the subscriber (the I/U/D phase)
>  - numbers of pqexecs()
>     against provider
>     against subscriber
>  - possibly, the number of times we grab timestamps

And, let us augment this with the number of "large tuple" fetches...
That will be really cheap from gettimeofday() perspective, but gives
us a good idea of how much flow interruption takes place.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From ibrahim.harrani at gmail.com  Sun Jul 13 07:59:20 2008
From: ibrahim.harrani at gmail.com (Ibrahim Harrani)
Date: Sun Jul 13 07:59:27 2008
Subject: [Slony1-general] ERROR: Slony-I: alterTableRestore(): Table with id
	54 not found
Message-ID: <530068a0807130759h4f9af7e3y5081c3417947168d@mail.gmail.com>

Hi,

One of my colleague restored a master pgsql database which is
replicated with slony-1 without dumping oid.  (pg_dump is not issued
with -o option)

So, all the table's oid (tab_reloid)  in my_cluster.st_table is wrong!
When I tried to drop a table or uninstall the node, I get the
following error message:

<stdin>:5: PGRES_FATAL_ERROR select "_mycluster".setDropTable(54);  -
ERROR:  Slony-I: alterTableRestore(): Table with id 54 not found


Here is the some select output on master db:

cargomax=# SELECT tab_reloid  from _gokboracluster.sl_table where tab_id=54;
 tab_reloid
------------
      21428


cargomax=# SELECT oid FROM pg_class WHERE relname = 'mytable';
   oid
---------
 5577619

There are more than 3000 tables in the slony sl_table.  What is the
best solution to solve this problem?

Thanks in advance
From Ow.Mun.Heng at wdc.com  Sun Jul 13 18:55:39 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Sun Jul 13 18:56:00 2008
Subject: FYI Re: [Slony1-general] DDL (altering current column types)
In-Reply-To: <20080710212914.GB5704@crankycanuck.ca>
References: <1215418524.4952.22.camel@neuromancer.home.net>
	<20080707184547.GD31669@crankycanuck.ca>
	<1215573097.22595.4.camel@neuromancer.home.net>
	<1215654148.32232.10.camel@neuromancer.home.net>
	<20080710212914.GB5704@crankycanuck.ca>
Message-ID: <1216000539.22806.18.camel@neuromancer.home.net>

On Thu, 2008-07-10 at 17:29 -0400, Andrew Sullivan wrote:
> On Thu, Jul 10, 2008 at 09:42:28AM +0800, Ow Mun Heng wrote:
> > Then I checked to see if the DDL will run on the slave as is. 
> 
> If you check the Slony admin manual, it actually suggests that you do this
> on the target machines, wrapped in BEGIN;?;ROLLBACK.  This is because it's
> very easy with execute script to run into the problem you just did.  See
> section 15.2.

The issue with the begin/rollback line isn't that it's extra work, it's
just extra time wasted to test it out. (well, better safe than sorry in
most instances)

> > BTW, at the risk of sounding like a broken record, this _really_ can't
> > be executed independent of slony? That would be the best case I would
> > say.
> 
> Here's why it has to be done with Slony's knowledge: suppose you make a
> change to the schema on the data origin at time _t_, and a change to the
> schema on a replica at time _t_+2.  Now, suppose you have a transaction that
> gets replicated at _t_+1.  The origin has the schema change, and Slony
> doesn't know that the schema on the replica isn't ready for the data.  So
> it will try to replicate that data, and things will break.
> 

This I wholly understand the implication. But again, it depends on the
situation, and in this situation, the initial population of the end
tables are via some loading mechanism which at the very beginning is
still only permitting input of varchar(4), hence anything beyond will
still be varchar(4) no matter what the update time is on master or
slave.

If I'm not mistaken, there would be no breakage as long as I don't mess
with the loading mechanism to permit input of varchar(5) data.

Anyway, I've found a way to do this independent of slony which would
perhaps interest readers here. (but whether it will break or otherwise,
it remains to be seen. It's a calculated risk I'm taking)

?
[quote ?Mario Weilguni from postgresql perform mailing list]
Example:
{OLDLEN} = 4
{NEWLEN} = 60

update pg_attribute
   set atttypmod={NEWLEN}+4
 where attname='the-name-of-the-column'
   and attrelid=(select oid from pg_class where 
relname='the-name-of-the-table')
   and atttypmod={OLDLEN}+4;

[/quote]

This is a 2 sec / table change. I change all the end-result tables
before I change the loading table to permit varchar(5) to go into the
loading table.


RAW (varchar(5)) -> loading table (varchar(4)) -> end table (varchar(4))

becomes
?
RAW (varchar(5)) -> loading table (varchar(4)) -> end table (varchar(5))

becomes

?RAW (varchar(5)) -> loading table (varchar(5)) -> end table
(varchar(5))

DONE.


From glynastill at yahoo.co.uk  Mon Jul 14 09:36:27 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Mon Jul 14 09:36:37 2008
Subject: [Slony1-general] ERROR: Slony-I: alterTableRestore(): Table with
	id 54 not found
Message-ID: <622744.6555.qm@web25808.mail.ukl.yahoo.com>

Hi,

AFAIK you cannot dump out, then restore a schema with slony in place.

Slony uses object OIDs (as you've seen in alterTableRestore()) and even with the -o flag you cannot dump them, you can only dump the OIDs for row data, thus when you reload all your data the slony schema is junk.

The only way to do it is to drop cascade the slony schema and setup the slony cluster again by starting from scratch or adding it as a new node to an existing cluster.

Put me straight if I'm wrong here people.
Glyn



----- Original Message ----
> From: Ibrahim Harrani <ibrahim.harrani@gmail.com>
> To: slony1-general@lists.slony.info
> Sent: Sunday, 13 July, 2008 3:59:20 PM
> Subject: [Slony1-general] ERROR: Slony-I: alterTableRestore(): Table with id 54 not found
> 
> Hi,
> 
> One of my colleague restored a master pgsql database which is
> replicated with slony-1 without dumping oid.  (pg_dump is not issued
> with -o option)
> 
> So, all the table's oid (tab_reloid)  in my_cluster.st_table is wrong!
> When I tried to drop a table or uninstall the node, I get the
> following error message:
> 
> :5: PGRES_FATAL_ERROR select "_mycluster".setDropTable(54);  -
> ERROR:  Slony-I: alterTableRestore(): Table with id 54 not found
> 
> 
> Here is the some select output on master db:
> 
> cargomax=# SELECT tab_reloid  from _gokboracluster.sl_table where tab_id=54;
> tab_reloid
> ------------
>       21428
> 
> 
> cargomax=# SELECT oid FROM pg_class WHERE relname = 'mytable';
>    oid
> ---------
> 5577619
> 
> There are more than 3000 tables in the slony sl_table.  What is the
> best solution to solve this problem?
> 
> Thanks in advance
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general



      __________________________________________________________
Not happy with your email address?.
Get the one you really want - millions of new email addresses available now at Yahoo! http://uk.docs.yahoo.com/ymail/new.html
From cbbrowne at ca.afilias.info  Mon Jul 14 09:53:36 2008
From: cbbrowne at ca.afilias.info (chris)
Date: Mon Jul 14 09:53:45 2008
Subject: [Slony1-general] ERROR: Slony-I: alterTableRestore(): Table with
	id 54 not found
In-Reply-To: <622744.6555.qm@web25808.mail.ukl.yahoo.com> (Glyn Astill's
	message of "Mon, 14 Jul 2008 16:36:27 +0000 (GMT)")
References: <622744.6555.qm@web25808.mail.ukl.yahoo.com>
Message-ID: <87abgk1k6n.fsf@dba2.int.libertyrms.com>

Glyn Astill <glynastill@yahoo.co.uk> writes:
> AFAIK you cannot dump out, then restore a schema with slony in place.
>
> Slony uses object OIDs (as you've seen in alterTableRestore()) and even with the -o flag you cannot dump them, you can only dump the OIDs for row data, thus when you reload all your data the slony schema is junk.
>
> The only way to do it is to drop cascade the slony schema and setup the slony cluster again by starting from scratch or adding it as a new node to an existing cluster.
>
> Put me straight if I'm wrong here people.

There is a Slonik command intended to do this sort of repair...
  <http://linuxfinances.info/info/stmtrepairconfig.html>
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From glynastill at yahoo.co.uk  Mon Jul 14 10:10:16 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Mon Jul 14 10:10:26 2008
Subject: [Slony1-general] ERROR: Slony-I: alterTableRestore(): Table with
	id 54 not found
Message-ID: <364242.14491.qm@web25807.mail.ukl.yahoo.com>

> Glyn Astill writes:
> > AFAIK you cannot dump out, then restore a schema with slony in place.
> >
> > Slony uses object OIDs (as you've seen in alterTableRestore()) and even with 
> the -o flag you cannot dump them, you can only dump the OIDs for row data, thus 
> when you reload all your data the slony schema is junk.
> >
> > The only way to do it is to drop cascade the slony schema and setup the slony 
> cluster again by starting from scratch or adding it as a new node to an existing 
> cluster.
> >
> > Put me straight if I'm wrong here people.
> >
> From: chris <cbbrowne@ca.afilias.info>
> 
> There is a Slonik command intended to do this sort of repair...
>   

Wow, makes me wonder how I missed that before!!



      __________________________________________________________
Not happy with your email address?.
Get the one you really want - millions of new email addresses available now at Yahoo! http://uk.docs.yahoo.com/ymail/new.html
From ibrahim.harrani at gmail.com  Mon Jul 14 14:03:40 2008
From: ibrahim.harrani at gmail.com (Ibrahim Harrani)
Date: Mon Jul 14 14:04:41 2008
Subject: [Slony1-general] ERROR: Slony-I: alterTableRestore(): Table with
	id 54 not found
In-Reply-To: <87abgk1k6n.fsf@dba2.int.libertyrms.com>
References: <622744.6555.qm@web25808.mail.ukl.yahoo.com>
	<87abgk1k6n.fsf@dba2.int.libertyrms.com>
Message-ID: <530068a0807141403t2788f22bke6ae402b46e9bd70@mail.gmail.com>

Hi Chris,

I repaired the oid with REPAIR CONFIG on the master db(pg_dump was
issued on master)

I dropped the slave node with DROP NODE (ID =2);
But I can't UNINSTALL slave NODE  (UNINSTALL NODE (ID =2)

When I issue the commands (UNINSTALL NODE (ID =2) or the following sql
statements,
I always get "Table with id 54 not found"; But I believe that the
table is exist in the system and sl_table. What could be wrong in my
setup?

Thanks.

# SELECT _mycluster.uninstallNode();
Slony-I: alterTableRestore(): Table with id 54 not found
# SELECT _mycluster.setdroptable_int(54);
ERROR:  Slony-I: alterTableRestore(): Table with id 54 not found
# SELECT _mycluster.alterTableRestore(54);
ERROR:  Slony-I: alterTableRestore(): Table with id 54 not found

mydb=# SELECT * from _myusercluster.sl_table where tab_id=54;
 tab_id | tab_reloid | tab_relname | tab_nspname | tab_set |
tab_idxname  | tab_altered |       tab_comment
--------+------------+-------------+-------------+---------+--------------+-------------+-------------------------
     54 |     143265 | mytable   | myuser     |       2 | pk_mytable |
t           | Table myuser.mytable
(1 row)


mydb=# SELECT * from pg_catalog.pg_tables where tablename='mytable'
and tableowner='myuser';
 schemaname | tablename | tableowner | tablespace | hasindexes |
hasrules | hastriggers
------------+-----------+------------+------------+------------+----------+-------------
 myuser    | mytable | myuser    |            | t          | f        | t
(1 row)



On Mon, Jul 14, 2008 at 7:53 PM, chris <cbbrowne@ca.afilias.info> wrote:
> Glyn Astill <glynastill@yahoo.co.uk> writes:
>> AFAIK you cannot dump out, then restore a schema with slony in place.
>>
>> Slony uses object OIDs (as you've seen in alterTableRestore()) and even with the -o flag you cannot dump them, you can only dump the OIDs for row data, thus when you reload all your data the slony schema is junk.
>>
>> The only way to do it is to drop cascade the slony schema and setup the slony cluster again by starting from scratch or adding it as a new node to an existing cluster.
>>
>> Put me straight if I'm wrong here people.
>
> There is a Slonik command intended to do this sort of repair...
>  <http://linuxfinances.info/info/stmtrepairconfig.html>
> --
> select 'cbbrowne' || '@' || 'linuxfinances.info';
> http://cbbrowne.com/info/lsf.html
> Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
> feature exposed pipes.  While they add to the  gloomy atmosphere, they
> are good  conductors of vibrations and  a lot of  prisoners know Morse
> code." <http://www.eviloverlord.com/>
>
From cbbrowne at ca.afilias.info  Mon Jul 14 14:35:55 2008
From: cbbrowne at ca.afilias.info (chris)
Date: Mon Jul 14 14:36:12 2008
Subject: [Slony1-general] ERROR: Slony-I: alterTableRestore(): Table with
	id 54 not found
In-Reply-To: <530068a0807141403t2788f22bke6ae402b46e9bd70@mail.gmail.com>
	(Ibrahim Harrani's message of "Tue, 15 Jul 2008 00:03:40 +0300")
References: <622744.6555.qm@web25808.mail.ukl.yahoo.com>
	<87abgk1k6n.fsf@dba2.int.libertyrms.com>
	<530068a0807141403t2788f22bke6ae402b46e9bd70@mail.gmail.com>
Message-ID: <87k5foywqs.fsf@dba2.int.libertyrms.com>

"Ibrahim Harrani" <ibrahim.harrani@gmail.com> writes:
> Hi Chris,
>
> I repaired the oid with REPAIR CONFIG on the master db(pg_dump was
> issued on master)
>
> I dropped the slave node with DROP NODE (ID =2);
> But I can't UNINSTALL slave NODE  (UNINSTALL NODE (ID =2)
>
> When I issue the commands (UNINSTALL NODE (ID =2) or the following sql
> statements,
> I always get "Table with id 54 not found"; But I believe that the
> table is exist in the system and sl_table. What could be wrong in my
> setup?
>
> Thanks.
>
> # SELECT _mycluster.uninstallNode();
> Slony-I: alterTableRestore(): Table with id 54 not found
> # SELECT _mycluster.setdroptable_int(54);
> ERROR:  Slony-I: alterTableRestore(): Table with id 54 not found
> # SELECT _mycluster.alterTableRestore(54);
> ERROR:  Slony-I: alterTableRestore(): Table with id 54 not found
>
> mydb=# SELECT * from _myusercluster.sl_table where tab_id=54;
>  tab_id | tab_reloid | tab_relname | tab_nspname | tab_set |
> tab_idxname  | tab_altered |       tab_comment
> --------+------------+-------------+-------------+---------+--------------+-------------+-------------------------
>      54 |     143265 | mytable   | myuser     |       2 | pk_mytable |
> t           | Table myuser.mytable
> (1 row)
>
>
> mydb=# SELECT * from pg_catalog.pg_tables where tablename='mytable'
> and tableowner='myuser';
>  schemaname | tablename | tableowner | tablespace | hasindexes |
> hasrules | hastriggers
> ------------+-----------+------------+------------+------------+----------+-------------
>  myuser    | mytable | myuser    |            | t          | f        | t
> (1 row)

Well, here's the code fragment that is complaining:
	-- ----
	-- Get the sl_table row and the current tables origin. Check
	-- that the table currently IS in altered state.
	-- ----
	select T.tab_reloid, T.tab_set, T.tab_altered,
			S.set_origin, PGX.indexrelid,
			@NAMESPACE@.slon_quote_brute(PGN.nspname) || ''.'' ||
			@NAMESPACE@.slon_quote_brute(PGC.relname) as tab_fqname
			into v_tab_row
			from @NAMESPACE@.sl_table T, @NAMESPACE@.sl_set S,
				"pg_catalog".pg_class PGC, "pg_catalog".pg_namespace PGN,
				"pg_catalog".pg_index PGX, "pg_catalog".pg_class PGXC
			where T.tab_id = p_tab_id
				and T.tab_set = S.set_id
				and T.tab_reloid = PGC.oid
				and PGC.relnamespace = PGN.oid
				and PGX.indrelid = T.tab_reloid
				and PGX.indexrelid = PGXC.oid
				and PGXC.relname = T.tab_idxname
				for update;
	if not found then
		raise exception ''Slony-I: alterTableRestore(): Table with id % not found'', p_tab_id;
	end if;

You can compare results via running the following query:

	select T.tab_reloid, T.tab_set, T.tab_altered,
			S.set_origin, PGX.indexrelid,
			from _myusercluster.sl_table T, _myusercluster.sl_set S,
				"pg_catalog".pg_class PGC, "pg_catalog".pg_namespace PGN,
				"pg_catalog".pg_index PGX, "pg_catalog".pg_class PGXC
			where T.tab_id = 54
				and T.tab_set = S.set_id
				and T.tab_reloid = PGC.oid
				and PGC.relnamespace = PGN.oid
				and PGX.indrelid = T.tab_reloid
				and PGX.indexrelid = PGXC.oid
				and PGXC.relname = T.tab_idxname;

Breaking that down into individual queries agaisnt individual tables:

We already have the query against sl_table:

> mydb=# SELECT * from _myusercluster.sl_table where tab_id=54;
>  tab_id | tab_reloid | tab_relname | tab_nspname | tab_set |
> tab_idxname  | tab_altered |       tab_comment
> --------+------------+-------------+-------------+---------+--------------+-------------+-------------------------
>      54 |     143265 | mytable   | myuser     |       2 | pk_mytable |
> t           | Table myuser.mytable
> (1 row)

Find the table:

   select * from pg_catalog.pg_class where oid = 143265;

Find the index:

   select * from pg_catalog.pg_index I, pg_catalog.pg_class IC 
     where indrelid = 143265 and IC.oid = I.indexrelid and
       IC.relname = 'pk_mytable';

Part of the query has to do with namespace matching; I wouldn't expect
that to be at issue.

I expect you'll find that one or another of those queries doesn't find
any tuples, and that should indicate something about why table 54
wasn't found.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From ibrahim.harrani at gmail.com  Mon Jul 14 15:02:56 2008
From: ibrahim.harrani at gmail.com (Ibrahim Harrani)
Date: Mon Jul 14 15:03:13 2008
Subject: [Slony1-general] ERROR: Slony-I: alterTableRestore(): Table with
	id 54 not found
In-Reply-To: <87k5foywqs.fsf@dba2.int.libertyrms.com>
References: <622744.6555.qm@web25808.mail.ukl.yahoo.com>
	<87abgk1k6n.fsf@dba2.int.libertyrms.com>
	<530068a0807141403t2788f22bke6ae402b46e9bd70@mail.gmail.com>
	<87k5foywqs.fsf@dba2.int.libertyrms.com>
Message-ID: <530068a0807141502y4a9fbebdt378b10ea33dfbb24@mail.gmail.com>

Hi Chris,

Thanks for your prompt reply.

I think, I found where is the problem. It seems that someone has been
changed the index name for this table (from pk_mytable to
mytable_pkey), so second query returns no result.

mydb=#  select * from pg_catalog.pg_index I, pg_catalog.pg_class IC
     where indrelid = 143265 and IC.oid = I.indexrelid and
       IC.relname = 'pk_mytable';
 indexrelid | indrelid | indnatts | indisunique | indisprimary |
indisclustered | indisvalid | indkey | indclass | indexprs | indpred |
relname | relnamespace | reltype | relowner | relam | relfilenode |
reltablespace | relpages | reltuples | reltoastrelid | reltoastidxid |
relhasindex | relisshared | relkind | relnatts | relchecks |
reltriggers | relukeys | relfkeys | relrefs | relhasoids | relhaspkey
| relhasrules | relhassubclass | relfrozenxid | relacl | reloptions
------------+----------+----------+-------------+--------------+----------------+------------+--------+----------+----------+---------+---------+--------------+---------+----------+-------+-------------+---------------+----------+-----------+---------------+---------------+-------------+-------------+---------+----------+-----------+-------------+----------+----------+---------+------------+------------+-------------+----------------+--------------+--------+------------
(0 rows)


# \d mytable;
   Tabe "mydb.mytable"

--------+-----------+-----------
 dt     | date      | not null
 firmid | integer   | not null
Indexes
    "mytable_pkey" PRIMARY KEY, btree (dt, firmid)
    "fki_" btree (firmid)

#	SELECT tab_idxname FROM _mycluster.sl_table where tab_id=54;
 tab_idxname
--------------
 pk_mytable

It seems that the primary key names on the master are also changed!
How can I fix this changed primary keys?

It seems that everything has been messed up!
I would like to drop slave node totally also the master if it is necessary?

Thanks

On Tue, Jul 15, 2008 at 12:35 AM, chris <cbbrowne@ca.afilias.info> wrote:
> "Ibrahim Harrani" <ibrahim.harrani@gmail.com> writes:
>> Hi Chris,
>>
>> I repaired the oid with REPAIR CONFIG on the master db(pg_dump was
>> issued on master)
>>
>> I dropped the slave node with DROP NODE (ID =2);
>> But I can't UNINSTALL slave NODE  (UNINSTALL NODE (ID =2)
>>
>> When I issue the commands (UNINSTALL NODE (ID =2) or the following sql
>> statements,
>> I always get "Table with id 54 not found"; But I believe that the
>> table is exist in the system and sl_table. What could be wrong in my
>> setup?
>>
>> Thanks.
>>
>> # SELECT _mycluster.uninstallNode();
>> Slony-I: alterTableRestore(): Table with id 54 not found
>> # SELECT _mycluster.setdroptable_int(54);
>> ERROR:  Slony-I: alterTableRestore(): Table with id 54 not found
>> # SELECT _mycluster.alterTableRestore(54);
>> ERROR:  Slony-I: alterTableRestore(): Table with id 54 not found
>>
>> mydb=# SELECT * from _myusercluster.sl_table where tab_id=54;
>>  tab_id | tab_reloid | tab_relname | tab_nspname | tab_set |
>> tab_idxname  | tab_altered |       tab_comment
>> --------+------------+-------------+-------------+---------+--------------+-------------+-------------------------
>>      54 |     143265 | mytable   | myuser     |       2 | pk_mytable |
>> t           | Table myuser.mytable
>> (1 row)
>>
>>
>> mydb=# SELECT * from pg_catalog.pg_tables where tablename='mytable'
>> and tableowner='myuser';
>>  schemaname | tablename | tableowner | tablespace | hasindexes |
>> hasrules | hastriggers
>> ------------+-----------+------------+------------+------------+----------+-------------
>>  myuser    | mytable | myuser    |            | t          | f        | t
>> (1 row)
>
> Well, here's the code fragment that is complaining:
>        -- ----
>        -- Get the sl_table row and the current tables origin. Check
>        -- that the table currently IS in altered state.
>        -- ----
>        select T.tab_reloid, T.tab_set, T.tab_altered,
>                        S.set_origin, PGX.indexrelid,
>                        @NAMESPACE@.slon_quote_brute(PGN.nspname) || ''.'' ||
>                        @NAMESPACE@.slon_quote_brute(PGC.relname) as tab_fqname
>                        into v_tab_row
>                        from @NAMESPACE@.sl_table T, @NAMESPACE@.sl_set S,
>                                "pg_catalog".pg_class PGC, "pg_catalog".pg_namespace PGN,
>                                "pg_catalog".pg_index PGX, "pg_catalog".pg_class PGXC
>                        where T.tab_id = p_tab_id
>                                and T.tab_set = S.set_id
>                                and T.tab_reloid = PGC.oid
>                                and PGC.relnamespace = PGN.oid
>                                and PGX.indrelid = T.tab_reloid
>                                and PGX.indexrelid = PGXC.oid
>                                and PGXC.relname = T.tab_idxname
>                                for update;
>        if not found then
>                raise exception ''Slony-I: alterTableRestore(): Table with id % not found'', p_tab_id;
>        end if;
>
> You can compare results via running the following query:
>
>        select T.tab_reloid, T.tab_set, T.tab_altered,
>                        S.set_origin, PGX.indexrelid,
>                        from _myusercluster.sl_table T, _myusercluster.sl_set S,
>                                "pg_catalog".pg_class PGC, "pg_catalog".pg_namespace PGN,
>                                "pg_catalog".pg_index PGX, "pg_catalog".pg_class PGXC
>                        where T.tab_id = 54
>                                and T.tab_set = S.set_id
>                                and T.tab_reloid = PGC.oid
>                                and PGC.relnamespace = PGN.oid
>                                and PGX.indrelid = T.tab_reloid
>                                and PGX.indexrelid = PGXC.oid
>                                and PGXC.relname = T.tab_idxname;
>
> Breaking that down into individual queries agaisnt individual tables:
>
> We already have the query against sl_table:
>
>> mydb=# SELECT * from _myusercluster.sl_table where tab_id=54;
>>  tab_id | tab_reloid | tab_relname | tab_nspname | tab_set |
>> tab_idxname  | tab_altered |       tab_comment
>> --------+------------+-------------+-------------+---------+--------------+-------------+-------------------------
>>      54 |     143265 | mytable   | myuser     |       2 | pk_mytable |
>> t           | Table myuser.mytable
>> (1 row)
>
> Find the table:
>
>   select * from pg_catalog.pg_class where oid = 143265;
>
> Find the index:
>
>   select * from pg_catalog.pg_index I, pg_catalog.pg_class IC
>     where indrelid = 143265 and IC.oid = I.indexrelid and
>       IC.relname = 'pk_mytable';
>
> Part of the query has to do with namespace matching; I wouldn't expect
> that to be at issue.
>
> I expect you'll find that one or another of those queries doesn't find
> any tuples, and that should indicate something about why table 54
> wasn't found.
> --
> select 'cbbrowne' || '@' || 'linuxfinances.info';
> http://cbbrowne.com/info/lsf.html
> Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
> feature exposed pipes.  While they add to the  gloomy atmosphere, they
> are good  conductors of vibrations and  a lot of  prisoners know Morse
> code." <http://www.eviloverlord.com/>
>
From rafael.domiciano at gmail.com  Tue Jul 15 07:17:27 2008
From: rafael.domiciano at gmail.com (Rafael Domiciano)
Date: Tue Jul 15 07:17:34 2008
Subject: [Slony1-general] Schema, Triggers and FK
Message-ID: <3a0028490807150717k78a30980w33d0f49408733322@mail.gmail.com>

Hi guys,

People, I have the following situation: I used Slony to replicate some
tables to a slave machine (13). This machine (13) now substitute the master
machine, I had done a change of hardware.
I had 2 schemas in the 13 machine, and 1 of them continues there. But the
second I couldn't drop. When I tryed to drop ocurred the error: "table_pkey
is a index".
So, for people use the base, I dropped the triggers denying access, but 2
triggers doesn't drop, I don't know why.
Now I discovered that if I don't drop the schema all my triggers and FK
doesn't work, so I need a way to do it :S

Best Regards,

Rafael Domiciano
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080715/=
7e404d89/attachment.htm
From threshar at torgo.978.org  Tue Jul 15 07:46:45 2008
From: threshar at torgo.978.org (Jeff)
Date: Tue Jul 15 07:47:29 2008
Subject: [Slony1-general] Identify backlog in sl_log_1
Message-ID: <FD5378DF-498D-48BC-AC24-74E914BE3242@torgo.978.org>

So, we've had a problem recently where Slony would get itself into  
trouble because sl_log would grow and grow and of course, our beloved  
readers would back up scanning the data.

The slave machines were mostly idle, replication was mostly in sync  
(although when the backlog got high it would lag a bit - but we're  
talking 5-10 minutes) and events were being produced and confirmed.   
So why was the cleanup thread not giving any love?

There were no long running txns and no idle txns.

So after learning the guts of slony a bit I figured it out (also due  
to another problem that arose where I caught pg_dump with its hand in  
the cookie jar - our db has grown big enough that pg_dump isn't really  
useful anymore, it holds locks on those tables for far too long which  
was also causing backlog (logswitch_finish taking the exclusive lock  
on the config table) - that problem is already documented from what I  
see).

Anyway, I still had the issue of it growing.  so here's the rundown of  
me hunting it down. I'd wager if I haven't been woken up at 3am the  
last few nights I would have come across this sooner.

The cleanup thread kills all but the latest confirmed event and all  
events
previous to that latest confirm.  Then we get the lowest xid from the  
events
we have left and nuke everything before that.  The lowest xid is the  
lowest
xid that was running when the event occured.  So, lets do some digging.

First, what is our lowest xid in the events?

somedb@[local] PRODUCTION # select * from _replication.sl_event order  
by ev_minxid asc limit 1;
  ev_origin | ev_seqno |        ev_timestamp        | ev_minxid  |  
ev_maxxid   
| 
                                                                                         ev_xip 
                                                                                          | 
  ev_type | ev_data1 | ev_data2 | ev_data3 | ev_data4 | ev_data5 |  
ev_data6 | ev_data7 | ev_data8
-----------+----------+----------------------------+------------ 
+------------ 
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+----------+----------+----------+----------+----------+----------+----------+----------
          1 | 27348789 | 2008-07-15 09:53:22.322241 | 2803277293 |  
2804440182 |  
'2803277293 
','2803468364 
','2803476694 
','2804440169 
','2803463327 
','2803481337 
','2804440181 
','2803482209 
','2804392536 
','2803495014','2804288504','2803461584','2803492317','2803435238' |  
SYNC    |          |          |          |          |           
|          |          |


our earliest event has an minxid of 2803277293.

so, lets see what is in the log: (we're using log 2 right now)

somedb@[local] PRODUCTION # select count(*) from _replication.sl_log_2  
where log_xid < '2803277293';
  count
-------
      0
(1 row)

hmmm nothing earlier than that. what about after?

somedb@[local] PRODUCTION # select count(*) from _replication.sl_log_2  
where log_xid > '2803277293';
  count
--------
  374009
(1 row)


thats a good amount of backlog in there.

so what is this txn? lets dig it up.  an open txn will always have at  
least
one lock, so we can use pg_locks which gets us the pid:

somedb@[local] PRODUCTION # select distinct pid from pg_locks where  
transaction = '2803277293';
  pid
------
  8207
(1 row)

ok. pid 8207 is being naughty - now we can look it up in  
pg_stat_activity
somedb@[local] PRODUCTION # select * from pg_stat_activity where  
procpid = 8207;
    datid   |  datname  | procpid | usesysid  |  usename   
|                                    
current_query                                    | waiting |           
query_start          |         backend_start         | client_addr |  
client_port
-----------+-----------+---------+-----------+----------- 
+ 
------------------------------------------------------------------------------------+ 
---------+------------------------------- 
+-------------------------------+-------------+-------------
  447595047 | xxxx |    8207 | 447595046 | xxxx | [some query] |  
f       | 2008-07-15 10:03:55.408807-04 | 2008-07-14  
20:30:01.099457-04 | somebox    |       38380
(1 row)


now, you may be able to identify it via the query that is running, but  
if not,
there is hope.

(further proof). we have the client port as 38380 on somebox


[root@somebox ~]# /usr/sbin/lsof -n  | grep 38380
pgpool    13561          root    9u     IPv4            
14878546                   TCP 1.2.3.4:38380->xxxxx:postgres  
(ESTABLISHED)

we run pgpool, so the app is going through there. If you were direct  
you'd
see whatever process it was.

[root@somebox ~]# lsof  -n -p 13561 | grep PGSQL

look for latest socket:
pgpool  13561 root    8u  unix 0xffff81004f066100         14879580 / 
tmp/.s.PGSQL.5432

next, search for 1 less than that 580 number, which should be our client

[root@somebox ~]# /usr/sbin/lsof -n | grep 14879579
badapp 13573     xxxxxx    4u     unix 0xffff81004f067180               
14879579 socket

there you have it. now you can add more commits to badapp or see wtf  
it is doing or kill it.
Then at your next cleanup event it should be able to nuke the logs.  
(if you had a major backup you may want to start a logswtich by  
calling logswitch_start() so you can get a truncate in there to clear  
out the dead.  Or you could vac full it, but that tis a silly idea).

--
Jeff Trout <jeff@jefftrout.com>
http://www.stuarthamm.net/
http://www.dellsmartexitin.com/



From cbbrowne at ca.afilias.info  Tue Jul 15 09:58:32 2008
From: cbbrowne at ca.afilias.info (cbbrowne)
Date: Tue Jul 15 09:58:41 2008
Subject: [Slony1-general] Identify backlog in sl_log_1
In-Reply-To: <FD5378DF-498D-48BC-AC24-74E914BE3242@torgo.978.org>
References: <FD5378DF-498D-48BC-AC24-74E914BE3242@torgo.978.org>
Message-ID: <487CD738.9040506@ca.afilias.info>

Jeff wrote:
> there you have it. now you can add more commits to badapp or see wtf 
> it is doing or kill it.
> Then at your next cleanup event it should be able to nuke the logs. 
> (if you had a major backup you may want to start a logswtich by 
> calling logswitch_start() so you can get a truncate in there to clear 
> out the dead.  Or you could vac full it, but that tis a silly idea).
Worth observing...

In version 2.0, we no longer *ever* do DELETE against either of the log 
tables; we instead truncate the unused table when there's no data left 
that's accessible.

That should mean that there's no need ever to vacuum these tables.

I'll see if some of this could go into the admin docs.

-- 
(format nil "~S@~S" "cbbrowne" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)

From cbbrowne at ca.afilias.info  Tue Jul 15 15:35:42 2008
From: cbbrowne at ca.afilias.info (chris)
Date: Tue Jul 15 15:35:56 2008
Subject: [Slony1-general] Added in tools/start_slon.sh
Message-ID: <878ww2zsg1.fsf@dba2.int.libertyrms.com>

I have added in an rc.d-style script to start up a slon.  It's not
(yet) properly documented, but I have integrated it into the
regression tests, so it'll become well tested soon enough, now being a
dependancy for *all* runs of the tests.

Its use is as follows:

 - You either edit the first few lines to set SLON_BIN_PATH,
   SLON_CONF, and SLON_LOG, to indicate where to find these
   interesting values, or

 - Set those values in your environment.

   (I would somewhat prefer to use hardcoded values, but that would
   make regression tests much harder...)

Note that it *requires* use of "slon.conf" files to manage the
configuration, and further requires that you specify, in that file, a
place to stow the slon's PID file.

Then, you can run:

$ start_slon.sh start
to start the slon

$ start_slon.sh stop
to (surprise!) stop the slon

$ start_slon.sh status
to get info about it; it reports back the values of SLON_CONF,
SLON_BIN_PATH, and reports whether it found the slon running.

I'd appreciate if people running on non-Linux platforms could take a
quick peek at this from a portability perspective; I tried to make
sure this was portable, but ps often seems troublesome :-(.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From Felix.LINHART at frequentis.com  Wed Jul 16 09:45:15 2008
From: Felix.LINHART at frequentis.com (LINHART Felix)
Date: Wed Jul 16 09:45:28 2008
Subject: [Slony1-general] sl_nodelock messages
Message-ID: <CAB1644F1C94A24BAC68B05D0F97746302578E0B@VIECLEX01.frequentis.frq>

Hi Everyone,

 

I have just noticed that my postgres logfile receives continuously messages
from slony listed below:

 

2008-07-16 12:37:50 CEST NOTICE:  Slony-I: cleanup stale sl_nodelock entry
for pid=5810

2008-07-16 12:37:50 CEST CONTEXT:  SQL statement "SELECT
"_slony1".cleanupNodelock()"

        PL/pgSQL function "cleanupevent" line 77 at PERFORM

2008-07-16 12:51:35 CEST LOG:  could not receive data from client: Connection
timed out

2008-07-16 12:51:35 CEST LOG:  unexpected EOF on client connection

2008-07-16 13:00:44 CEST NOTICE:  Slony-I: cleanup stale sl_nodelock entry
for pid=30480

2008-07-16 13:00:44 CEST CONTEXT:  SQL statement "SELECT
"_slony1".cleanupNodelock()"

        PL/pgSQL function "cleanupevent" line 77 at PERFORM

2008-07-16 13:08:52 CEST LOG:  could not receive data from client: Connection
timed out

2008-07-16 13:08:52 CEST LOG:  unexpected EOF on client connection

2008-07-16 13:11:45 CEST NOTICE:  Slony-I: cleanup stale sl_nodelock entry
for pid=29970

2008-07-16 13:11:45 CEST CONTEXT:  SQL statement "SELECT
"_slony1".cleanupNodelock()"

        PL/pgSQL function "cleanupevent" line 77 at PERFORM

2008-07-16 13:13:48 CEST LOG:  could not receive data from client: Connection
timed out

2008-07-16 13:13:48 CEST LOG:  unexpected EOF on client connection

2008-07-16 13:23:25 CEST NOTICE:  Slony-I: cleanup stale sl_nodelock entry
for pid=14475

2008-07-16 13:23:25 CEST CONTEXT:  SQL statement "SELECT
"_slony1".cleanupNodelock()"

        PL/pgSQL function "cleanupevent" line 77 at PERFORM

2008-07-16 13:24:33 CEST LOG:  could not receive data from client: Connection
timed out

2008-07-16 13:24:33 CEST LOG:  unexpected EOF on client connection

2008-07-16 13:32:48 CEST LOG:  could not receive data from client: Connection
timed out

2008-07-16 13:32:48 CEST LOG:  unexpected EOF on client connection

2008-07-16 13:34:07 CEST NOTICE:  Slony-I: cleanup stale sl_nodelock entry
for pid=21685

2008-07-16 13:34:07 CEST CONTEXT:  SQL statement "SELECT
"_slony1".cleanupNodelock()"

        PL/pgSQL function "cleanupevent" line 77 at PERFORM

2008-07-16 13:34:07 CEST NOTICE:  Slony-I: cleanup stale sl_nodelock entry
for pid=28842

2008-07-16 13:34:07 CEST CONTEXT:  SQL statement "SELECT
"_slony1".cleanupNodelock()"

        PL/pgSQL function "cleanupevent" line 77 at PERFORM

2008-07-16 15:02:01 CEST LOG:  incomplete startup packet

2008-07-16 16:22:04 CEST LOG:  could not receive data from client: Connection
timed out

2008-07-16 16:22:04 CEST LOG:  unexpected EOF on client connection

2008-07-16 16:24:36 CEST NOTICE:  Slony-I: cleanup stale sl_nodelock entry
for pid=1779

2008-07-16 16:24:36 CEST CONTEXT:  SQL statement "SELECT
"_slony1".cleanupNodelock()"

        PL/pgSQL function "cleanupevent" line 77 at PERFORM

 

What does that message mean? Is there something wrong?

 

Thx

Best regards

Felix

 

 

 

Slony logfile:

 

2008-07-16 12:37:50 CEST DEBUG2 calc sync size - last time: 1 last length:
2494 ideal: 24 proposed size: 3

2008-07-16 12:37:50 CEST DEBUG2 remoteWorkerThread_3: SYNC 6967 processing

2008-07-16 12:37:50 CEST DEBUG2 remoteWorkerThread_3: no sets need syncing
for this event

2008-07-16 12:37:50 CEST DEBUG2 remoteWorkerThread_7: forward confirm 3,6967
received by 7

2008-07-16 12:37:50 CEST DEBUG2 remoteWorkerThread_7: forward confirm 6,6969
received by 4

2008-07-16 12:37:50 CEST DEBUG2 syncThread: new sl_action_seq 1376802 - SYNC
14077

2008-07-16 12:37:57 CEST DEBUG2 remoteListenThread_2: LISTEN

2008-07-16 12:37:57 CEST DEBUG2 remoteWorkerThread_2: forward confirm 1,14076
received by 5

2008-07-16 12:37:57 CEST DEBUG2 remoteWorkerThread_2: forward confirm 1,14076
received by 4

2008-07-16 12:37:57 CEST DEBUG2 remoteWorkerThread_2: forward confirm 3,6967
received by 2

2008-07-16 12:37:57 CEST DEBUG2 remoteWorkerThread_2: forward confirm 6,6969
received by 9

2008-07-16 12:37:57 CEST DEBUG2 remoteWorkerThread_2: forward confirm 3,6967
received by 5

2008-07-16 12:37:57 CEST DEBUG2 remoteWorkerThread_2: forward confirm 3,6967
received by 9

2008-07-16 12:37:58 CEST DEBUG2 remoteListenThread_8: queue event 8,6970 SYNC

2008-07-16 12:37:58 CEST DEBUG2 remoteListenThread_8: UNLISTEN

2008-07-16 12:37:58 CEST DEBUG2 remoteWorkerThread_8: Received event 8,6970
SYNC

2008-07-16 12:37:58 CEST DEBUG2 calc sync size - last time: 1 last length:
8671 ideal: 6 proposed size: 3

2008-07-16 12:37:58 CEST DEBUG2 remoteWorkerThread_8: SYNC 6970 processing

2008-07-16 12:37:58 CEST DEBUG2 remoteWorkerThread_8: no sets need syncing
for this event

2008-07-16 12:37:58 CEST DEBUG2 remoteWorkerThread_8: forward confirm 1,14077
received by 6

2008-07-16 12:37:58 CEST DEBUG2 remoteWorkerThread_8: forward confirm 3,6967
received by 8

2008-07-16 12:37:58 CEST DEBUG2 remoteWorkerThread_8: forward confirm 3,6967
received by 6

2008-07-16 12:37:58 CEST DEBUG2 remoteWorkerThread_8: forward confirm 1,14077
received by 8

2008-07-16 12:37:58 CEST DEBUG2 remoteWorkerThread_8: forward confirm 1,14077
received by 9

2008-07-16 12:37:58 CEST DEBUG2 remoteWorkerThread_8: forward confirm 8,6970
received by 6

2008-07-16 12:37:58 CEST DEBUG2 remoteWorkerThread_8: forward confirm 3,6967
received by 4

2008-07-16 12:38:00 CEST DEBUG2 remoteListenThread_2: LISTEN

2008-07-16 12:38:00 CEST DEBUG2 remoteWorkerThread_2: forward confirm 8,6970
received by 2

2008-07-16 12:38:00 CEST DEBUG2 remoteWorkerThread_2: forward confirm 1,14077
received by 2

2008-07-16 12:38:00 CEST DEBUG2 localListenThread: Received event 1,14077
SYNC

2008-07-16 12:38:02 CEST DEBUG2 remoteListenThread_9: queue event 9,6980 SYNC

2008-07-16 12:38:02 CEST DEBUG2 remoteListenThread_9: UNLISTEN

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_9: Received event 9,6980
SYNC

2008-07-16 12:38:02 CEST DEBUG2 calc sync size - last time: 1 last length:
3519 ideal: 17 proposed size: 3

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_9: SYNC 6980 processing

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_9: no sets need syncing
for this event

2008-07-16 12:38:02 CEST DEBUG2 remoteListenThread_2: queue event 5,6967 SYNC

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_5: Received event 5,6967
SYNC

2008-07-16 12:38:02 CEST DEBUG2 calc sync size - last time: 1 last length: 49
ideal: 1224 proposed size: 3

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_5: SYNC 6967 processing

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_5: no sets need syncing
for this event

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_2: forward confirm 5,6967
received by 2

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_2: forward confirm 5,6967
received by 4

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_2: forward confirm 8,6970
received by 4

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_2: forward confirm 1,14077
received by 3

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_2: forward confirm 8,6970
received by 5

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_2: forward confirm 8,6970
received by 7

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_2: forward confirm 1,14076
received by 7

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_2: forward confirm 8,6970
received by 3

2008-07-16 12:38:02 CEST DEBUG2 remoteListenThread_5: LISTEN

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_5: forward confirm 9,6980
received by 3

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_5: forward confirm 9,6980
received by 6

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_5: forward confirm 5,6967
received by 6

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_5: forward confirm 5,6967
received by 3

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_5: forward confirm 9,6980
received by 7

2008-07-16 12:38:02 CEST DEBUG2 remoteWorkerThread_5: forward confirm 9,6980
received by 5

.

.

.

.

.

2008-07-16 12:39:37 CEST DEBUG2 remoteListenThread_5: UNLISTEN

2008-07-16 12:39:37 CEST DEBUG2 remoteWorkerThread_4: Received event 4,6973
SYNC

2008-07-16 12:39:37 CEST DEBUG2 calc sync size - last time: 1 last length:
4517 ideal: 13 proposed size: 3

2008-07-16 12:39:37 CEST DEBUG2 remoteWorkerThread_4: SYNC 6973 processing

2008-07-16 12:39:37 CEST DEBUG2 remoteWorkerThread_4: no sets need syncing
for this event

2008-07-16 12:39:37 CEST DEBUG2 remoteWorkerThread_5: forward confirm 4,6973
received by 5

2008-07-16 12:39:37 CEST DEBUG2 remoteWorkerThread_5: forward confirm 1,14084
received by 8

2008-07-16 12:39:37 CEST DEBUG2 remoteWorkerThread_5: forward confirm 1,14083
received by 7

NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=24788

CONTEXT:  SQL statement "SELECT  "_slony1".cleanupNodelock()"

PL/pgSQL function "cleanupevent" line 77 at PERFORM

NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=19225

CONTEXT:  SQL statement "SELECT  "_slony1".cleanupNodelock()"

PL/pgSQL function "cleanupevent" line 77 at PERFORM

NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=25418

CONTEXT:  SQL statement "SELECT  "_slony1".cleanupNodelock()"

PL/pgSQL function "cleanupevent" line 77 at PERFORM

NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=19821

CONTEXT:  SQL statement "SELECT  "_slony1".cleanupNodelock()"

PL/pgSQL function "cleanupevent" line 77 at PERFORM

NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=26081

CONTEXT:  SQL statement "SELECT  "_slony1".cleanupNodelock()"

PL/pgSQL function "cleanupevent" line 77 at PERFORM

NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=23227

CONTEXT:  SQL statement "SELECT  "_slony1".cleanupNodelock()"

PL/pgSQL function "cleanupevent" line 77 at PERFORM

NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=9719

CONTEXT:  SQL statement "SELECT  "_slony1".cleanupNodelock()"

PL/pgSQL function "cleanupevent" line 77 at PERFORM

2008-07-16 12:39:42 CEST DEBUG1 cleanupThread:    0.035 seconds for
cleanupEvent()

2008-07-16 12:39:42 CEST DEBUG1 cleanupThread:    0.048 seconds for delete
logs

2008-07-16 12:39:43 CEST DEBUG2 remoteListenThread_2: queue event 2,6974 SYNC

2008-07-16 12:39:43 CEST DEBUG2 remoteWorkerThread_2: Received event 2,6974
SYNC

2008-07-16 12:39:43 CEST DEBUG2 calc sync size - last time: 1 last length:
6151 ideal: 9 proposed size: 3

2008-07-16 12:39:43 CEST DEBUG2 remoteWorkerThread_2: SYNC 6974 processing

2008-07-16 12:39:43 CEST DEBUG2 remoteWorkerThread_2: no sets need syncing
for this event

2008-07-16 12:39:43 CEST DEBUG2 remoteWorkerThread_2: forward confirm 2,6974
received by 9

2008-07-16 12:39:43 CEST DEBUG2 remoteWorkerThread_2: forward confirm 4,6973
received by 6

2008-07-16 12:39:43 CEST DEBUG2 remoteWorkerThread_2: forward confirm 4,6973
received by 8

2008-07-16 12:39:43 CEST DEBUG2 remoteWorkerThread_2: forward confirm 4,6973
received by 7

2008-07-16 12:39:43 CEST DEBUG2 remoteWorkerThread_2: forward confirm 4,6973
received by 9

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080716/3e4b2212/attachment-0001.htm
From quinn_jones at pobox.com  Wed Jul 16 09:48:02 2008
From: quinn_jones at pobox.com (Quinn Jones)
Date: Wed Jul 16 09:48:11 2008
Subject: [Slony1-general] Failover with unresponsive slaves
Message-ID: <b49a4abf0807160948r33e34fffqdc2649662644b8a0@mail.gmail.com>

Hello,

I've been lurking on the list for a while, but now have a problem that I
haven't seen:  I tried to failover a node, but a slave was also unresponsive
and slonik errored out after timing out (so the failover didn't happen).

Here's our set-up: We have a database replicated to three slave nodes and a
total of three sites, like this
site 1: db1 (master) and db2
site 2: db3
site 3: db4

Our problem started when site 1 went away completely and abruptly (so db1
and db2 were out of commission).  Our plan called for failing the database
over to db3.  When I tried to failover, though, slonik timed out with the
message 'could not connect to server: Connection timed out.  Is the server
running on host "x.x.x.x" and accepting TCP/IP connections on port 5432?'.
The ip address was db2, so seeing that there is a logical problem to solve I
tried dropping the downed slave node first.  This timed out as well, and the
slave was not dropped.

While trying to figure out an intelligent next step, short of dropping
replication entirely and just using db3 stand-alone (and rebuilding the
cluster from scratch later) site1 mostly came back up.  We lucked out and in
the end saved some time by not being able to fail over the way we wanted,
though we did lose an unknown number of sales because we were effectively
down.

How do we drop a non-responsive slave, or force the failover to ignore it?
This is a situation that shouldn't come up frequently for us, but it could
and this was rather troublesome.  I understand why failover would want to
communicate with every other server, but there must be a way to step over
other dead servers to get a functional cluster (I just haven't found it
yet).  Also, shouldn't dropping a slave node happen whether the node can be
seen or not?

Quinn
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080716/=
f5b56b49/attachment.htm
From cbbrowne at ca.afilias.info  Wed Jul 16 09:52:05 2008
From: cbbrowne at ca.afilias.info (chris)
Date: Wed Jul 16 09:52:15 2008
Subject: [Slony1-general] sl_nodelock messages
In-Reply-To: <CAB1644F1C94A24BAC68B05D0F97746302578E0B@VIECLEX01.frequentis.frq>
	(LINHART Felix's message of "Wed, 16 Jul 2008 18:45:15 +0200")
References: <CAB1644F1C94A24BAC68B05D0F97746302578E0B@VIECLEX01.frequentis.frq>
Message-ID: <87k5flydoq.fsf@dba2.int.libertyrms.com>

"LINHART Felix" <Felix.LINHART@frequentis.com> writes:
> I have just noticed that my postgres logfile receives continuously messages from slony listed below::p>
>
> :p>?
>
> 2008-07-16 12:37:50 CEST NOTICE:? Slony-I: cleanup stale sl_nodelock entry for pid=5810:p>
>
> 2008-07-16 12:37:50 CEST CONTEXT:? SQL statement "SELECT? "_slony1".cleanupNodelock()":p>
>
> ??????? PL/pgSQL function "cleanupevent" line 77 at PERFORM:p>
>
> 2008-07-16 12:51:35 CEST LOG:? could not receive data from client: Connection timed out:p>
>
> 2008-07-16 12:51:35 CEST LOG:? unexpected EOF on client connection:p>

Since these are "NOTICE" log records, it does not indicate any error
as far as Slony-I is concerned.

When the slon falls over, and another starts up, the new one has to
clean up after the old one.  It's not a real big deal, though it
suggests that you have some sort of network problem or something that
is causing slon processes to fall over so they need to be restarted.

That falls outside what Slony-I is, itself, doing.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From Felix.LINHART at frequentis.com  Wed Jul 16 10:02:39 2008
From: Felix.LINHART at frequentis.com (LINHART Felix)
Date: Wed Jul 16 10:02:52 2008
Subject: [Slony1-general] sl_nodelock messages
In-Reply-To: <87k5flydoq.fsf@dba2.int.libertyrms.com>
References: <CAB1644F1C94A24BAC68B05D0F97746302578E0B@VIECLEX01.frequentis.frq>
	<87k5flydoq.fsf@dba2.int.libertyrms.com>
Message-ID: <CAB1644F1C94A24BAC68B05D0F97746302578E0E@VIECLEX01.frequentis.frq>

I have to db clusters on 1 server and 2 other ha-clusters (also with 2 db
clusters) spread over the network.
Since I have these sl_nodelock messages also in my 2 local postgres logs I
don't think that this is caused by a network problem.

Can I somehow enable more logging to analyse this in more detail?

Thx
felix

-----Original Message-----
From: chris [mailto:cbbrowne@ca.afilias.info] 
Sent: Mittwoch, 16. Juli 2008 18:52
To: LINHART Felix
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] sl_nodelock messages

"LINHART Felix" <Felix.LINHART@frequentis.com> writes:
> I have just noticed that my postgres logfile receives continuously 
> messages from slony listed below::p>
>
> :p>
>
> 2008-07-16 12:37:50 CEST NOTICE:? Slony-I: cleanup stale sl_nodelock 
> entry for pid=5810:p>
>
> 2008-07-16 12:37:50 CEST CONTEXT:? SQL statement "SELECT? 
> "_slony1".cleanupNodelock()":p>
>
> ??????? PL/pgSQL function "cleanupevent" line 77 at PERFORM:p>
>
> 2008-07-16 12:51:35 CEST LOG:? could not receive data from client: 
> Connection timed out:p>
>
> 2008-07-16 12:51:35 CEST LOG:? unexpected EOF on client connection:p>

Since these are "NOTICE" log records, it does not indicate any error as far
as Slony-I is concerned.

When the slon falls over, and another starts up, the new one has to clean up
after the old one.  It's not a real big deal, though it suggests that you
have some sort of network problem or something that is causing slon processes
to fall over so they need to be restarted.

That falls outside what Slony-I is, itself, doing.
--
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html Rules  of the  Evil Overlord  #145. "My
dungeon cell  decor  will not feature exposed pipes.  While they add to the
gloomy atmosphere, they are good  conductors of vibrations and  a lot of
prisoners know Morse code." <http://www.eviloverlord.com/>.
From Felix.LINHART at frequentis.com  Wed Jul 16 12:17:33 2008
From: Felix.LINHART at frequentis.com (LINHART Felix)
Date: Wed Jul 16 12:17:47 2008
Subject: [Slony1-general] sl_nodelock messages
In-Reply-To: <CAB1644F1C94A24BAC68B05D0F97746302578E0E@VIECLEX01.frequentis.frq>
References: <CAB1644F1C94A24BAC68B05D0F97746302578E0B@VIECLEX01.frequentis.frq><87k5flydoq.fsf@dba2.int.libertyrms.com>
	<CAB1644F1C94A24BAC68B05D0F97746302578E0E@VIECLEX01.frequentis.frq>
Message-ID: <CAB1644F1C94A24BAC68B05D0F97746302578E20@VIECLEX01.frequentis.frq>

Hi all,

I have found out something new regarding my sl_nodelock messages. The
following error occurred and also my slony pids for some nodes where
different after this error message.

2008-07-16 20:15:07 CEST DEBUG2 remoteListenThread_2: LISTEN
2008-07-16 20:15:07 CEST DEBUG2 remoteWorkerThread_4: forward confirm 4,7884
received by 6
2008-07-16 20:15:07 CEST DEBUG2 remoteWorkerThread_4: forward confirm 4,7884
received by 7
2008-07-16 20:15:07 CEST DEBUG2 remoteWorkerThread_4: forward confirm 5,7881
received by 9
2008-07-16 20:15:07 CEST DEBUG2 remoteWorkerThread_2: forward confirm 5,7881
received by 3
2008-07-16 20:15:07 CEST DEBUG2 remoteWorkerThread_2: forward confirm 4,7884
received by 5
2008-07-16 20:15:07 CEST DEBUG2 remoteWorkerThread_2: forward confirm 4,7884
received by 9
2008-07-16 20:15:07 CEST DEBUG2 remoteWorkerThread_3: forward confirm 4,7884
received by 3
2008-07-16 20:15:07 CEST DEBUG2 remoteWorkerThread_3: forward confirm 4,7884
received by 8
2008-07-16 20:15:07 CEST DEBUG2 remoteWorkerThread_3: forward confirm 1,15904
received by 3
2008-07-16 20:15:07 CEST DEBUG2 remoteListenThread_2: queue event 7,7890 SYNC
2008-07-16 20:15:07 CEST DEBUG2 remoteListenThread_2: UNLISTEN
2008-07-16 20:15:07 CEST DEBUG2 remoteWorkerThread_7: Received event 7,7890
SYNC
2008-07-16 20:15:07 CEST DEBUG2 calc sync size - last time: 1 last length:
292 ideal: 205 proposed size: 3
2008-07-16 20:15:07 CEST DEBUG2 remoteWorkerThread_7: SYNC 7890 processing
2008-07-16 20:15:07 CEST DEBUG2 remoteWorkerThread_7: no sets need syncing
for this event
2008-07-16 20:15:07 CEST DEBUG2 remoteWorkerThread_2: forward confirm 7,7890
received by 3
2008-07-16 20:15:07 CEST DEBUG2 remoteWorkerThread_2: forward confirm 7,7890
received by 2
2008-07-16 20:15:07 CEST DEBUG2 remoteWorkerThread_2: forward confirm 7,7890
received by 4
2008-07-16 20:15:07 CEST DEBUG2 remoteWorkerThread_2: forward confirm 7,7890
received by 8
2008-07-16 20:15:13 CEST ERROR  remoteListenThread_8: "select ev_origin,
ev_seqno, ev_timestamp,        ev_minxid, ev_maxxid, ev_xip,        ev_type,
ev_data1, ev_data2,        ev_data3, ev_data4,        ev_data5, ev_data6,
ev_data7, ev_data8 from "_slony1".sl_event e where (e.ev_origin = '7' and
e.ev_seqno > '7890') or (e.ev_origin = '8' and e.ev_seqno > '7885') or
(e.ev_origin = '2' and e.ev_seqno > '7884') or (e.ev_origin = '3' and
e.ev_seqno > '7880') or (e.ev_origin = '4' and e.ev_seqno > '7884') or
(e.ev_origin = '5' and e.ev_seqno > '7881') or (e.ev_origin = '6' and
e.ev_seqno > '7885') or (e.ev_origin = '9' and e.ev_seqno > '7895') order by
e.ev_origin, e.ev_seqno" - server closed the connection unexpectedly
        This probably means the server terminated abnormally
        before or while processing the request.
2008-07-16 20:15:22 CEST DEBUG2 localListenThread: Received event 1,15904
SYNC
2008-07-16 20:15:22 CEST DEBUG2 syncThread: new sl_action_seq 1561499 - SYNC
15905
2008-07-16 20:15:22 CEST DEBUG2 remoteListenThread_7: queue event 8,7886 SYNC
2008-07-16 20:15:22 CEST DEBUG2 remoteListenThread_7: queue event 9,7896 SYNC
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_8: Received event 8,7886
SYNC
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_9: Received event 9,7896
SYNC
2008-07-16 20:15:22 CEST DEBUG2 calc sync size - last time: 1 last length:
14536 ideal: 4 proposed size: 3
2008-07-16 20:15:22 CEST DEBUG2 calc sync size - last time: 1 last length:
14536 ideal: 4 proposed size: 3
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_8: SYNC 7886 processing
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_8: no sets need syncing
for this event
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_9: SYNC 7896 processing
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_9: no sets need syncing
for this event
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_7: forward confirm 9,7896
received by 8
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_7: forward confirm 9,7896
received by 3
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_7: forward confirm 9,7896
received by 6
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_7: forward confirm 7,7890
received by 6
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_7: forward confirm 9,7896
received by 7
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_7: forward confirm 9,7896
received by 5
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_7: forward confirm 7,7890
received by 9
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_7: forward confirm 7,7890
received by 5
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_7: forward confirm 9,7896
received by 4
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_7: forward confirm 1,15904
received by 6
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_7: forward confirm 8,7886
received by 7
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_7: forward confirm 1,15904
received by 8
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_7: forward confirm 1,15904
received by 7
2008-07-16 20:15:22 CEST DEBUG2 remoteWorkerThread_7: forward confirm 1,15904
received by 4
2008-07-16 20:15:22 CEST DEBUG2 remoteListenThread_2: queue event 2,7885 SYNC
2008-07-16 20:15:22 CEST DEBUG2 remoteListenThread_2: queue event 3,7881 SYNC

The above messages where logged on my slony masternode

db=# SELECT * from _slony1.sl_set;
 set_id | set_origin | set_locked |   set_comment
--------+------------+------------+------------------
      1 |          1 |            | Set 1 for slony1
      2 |          1 |            | Set 2 for slony1
      3 |          1 |            | Set 3 for slony1
      4 |          1 |            | Set 4 for slony1
(4 rows)

db=# SELECT * from _slony1.sl_status;
 st_origin | st_received | st_last_event |     st_last_event_ts      |
st_last_received |    st_last_received_ts     | st_last_received_event_ts  |
st_lag_num_events |   st_lag_time
-----------+-------------+---------------+---------------------------+-------
-----------+----------------------------+----------------------------+-------
------------+-----------------
         1 |           7 |         16050 | 2008-07-16 20:51:40.34712 |
16049 | 2008-07-16 20:51:47.979432 | 2008-07-16 20:51:25.327307 |
1 | 00:00:17.634575
         1 |           5 |         16050 | 2008-07-16 20:51:40.34712 |
16048 | 2008-07-16 20:51:39.676025 | 2008-07-16 20:51:10.299876 |
2 | 00:00:32.662006
         1 |           9 |         16050 | 2008-07-16 20:51:40.34712 |
16049 | 2008-07-16 20:51:42.70644  | 2008-07-16 20:51:25.327307 |
1 | 00:00:17.634575
         1 |           6 |         16050 | 2008-07-16 20:51:40.34712 |
16049 | 2008-07-16 20:51:37.104608 | 2008-07-16 20:51:25.327307 |
1 | 00:00:17.634575
         1 |           3 |         16050 | 2008-07-16 20:51:40.34712 |
16048 | 2008-07-16 20:51:36.692369 | 2008-07-16 20:51:10.299876 |
2 | 00:00:32.662006
         1 |           8 |         16050 | 2008-07-16 20:51:40.34712 |
16049 | 2008-07-16 20:51:47.674594 | 2008-07-16 20:51:25.327307 |
1 | 00:00:17.634575
         1 |           4 |         16050 | 2008-07-16 20:51:40.34712 |
16049 | 2008-07-16 20:51:37.567119 | 2008-07-16 20:51:25.327307 |
1 | 00:00:17.634575
         1 |           2 |         16050 | 2008-07-16 20:51:40.34712 |
16048 | 2008-07-16 20:51:30.982579 | 2008-07-16 20:51:10.299876 |
2 | 00:00:32.662006
(8 rows)

db=#


Can someone please explain me the error message from above? 

Thx
Br
Felix


-----Original Message-----
From: slony1-general-bounces@lists.slony.info
[mailto:slony1-general-bounces@lists.slony.info] On Behalf Of LINHART Felix
Sent: Mittwoch, 16. Juli 2008 19:03
To: chris
Cc: slony1-general@lists.slony.info
Subject: RE: [Slony1-general] sl_nodelock messages

I have to db clusters on 1 server and 2 other ha-clusters (also with 2 db
clusters) spread over the network.
Since I have these sl_nodelock messages also in my 2 local postgres logs I
don't think that this is caused by a network problem.

Can I somehow enable more logging to analyse this in more detail?

Thx
felix

-----Original Message-----
From: chris [mailto:cbbrowne@ca.afilias.info]
Sent: Mittwoch, 16. Juli 2008 18:52
To: LINHART Felix
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] sl_nodelock messages

"LINHART Felix" <Felix.LINHART@frequentis.com> writes:
> I have just noticed that my postgres logfile receives continuously 
> messages from slony listed below::p>
>
> :p>
>
> 2008-07-16 12:37:50 CEST NOTICE:? Slony-I: cleanup stale sl_nodelock 
> entry for pid=5810:p>
>
> 2008-07-16 12:37:50 CEST CONTEXT:? SQL statement "SELECT 
> "_slony1".cleanupNodelock()":p>
>
> ??????? PL/pgSQL function "cleanupevent" line 77 at PERFORM:p>
>
> 2008-07-16 12:51:35 CEST LOG:? could not receive data from client: 
> Connection timed out:p>
>
> 2008-07-16 12:51:35 CEST LOG:? unexpected EOF on client connection:p>

Since these are "NOTICE" log records, it does not indicate any error as far
as Slony-I is concerned.

When the slon falls over, and another starts up, the new one has to clean up
after the old one.  It's not a real big deal, though it suggests that you
have some sort of network problem or something that is causing slon processes
to fall over so they need to be restarted.

That falls outside what Slony-I is, itself, doing.
--
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html Rules  of the  Evil Overlord  #145. "My
dungeon cell  decor  will not feature exposed pipes.  While they add to the
gloomy atmosphere, they are good  conductors of vibrations and  a lot of
prisoners know Morse code." <http://www.eviloverlord.com/>.
_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general
From jennifer.spencer at stanford.edu  Wed Jul 16 14:05:28 2008
From: jennifer.spencer at stanford.edu (Jennifer Spencer)
Date: Wed Jul 16 14:05:42 2008
Subject: [Slony1-general] To upgrade or not to upgrade?
Message-ID: <487E6298.5040809@stanford.edu>

Hi -
I am hoping you all can help me decide whether it's worth it to upgrade to Slony 2.0.

Right now, we are using 1.2.12, and could update to 1.2.14 instead.  This is still at a beta stage of 
rollout, so I have options.  The real workhorse of our installation is planned to be log shipping.

Does v. 2.0 offer significant improvements in log-shipping that would make it the obvious choice over 
1.2.14?

I have scripts that add and drop tables from the replication all the way through to the log ship 
slaves, and they use "wait for event" and other slon commands.  I don't know if I would have to 
re-write those to get them working on 2.0.

Thank you.
From cbbrowne at ca.afilias.info  Thu Jul 17 15:54:28 2008
From: cbbrowne at ca.afilias.info (chris)
Date: Thu Jul 17 15:54:45 2008
Subject: [Slony1-general] Breaking down query time
In-Reply-To: <87abgo2osi.fsf@dba2.int.libertyrms.com> (chris's message of
	"Fri, 11 Jul 2008 15:39:41 -0400")
References: <4877658E.4050205@echo.fr>
	<87od5431q1.fsf@dba2.int.libertyrms.com> <48777930.704@Yahoo.com>
	<48779645.50301@echo.fr> <48779948.6090809@echo.fr>
	<87fxqg2t39.fsf@dba2.int.libertyrms.com>
	<87abgo2osi.fsf@dba2.int.libertyrms.com>
Message-ID: <87zlogunob.fsf_-_@dba2.int.libertyrms.com>

chris <cbbrowne@ca.afilias.info> writes:
> Christopher Browne <cbbrowne@ca.afilias.info> writes:
>> Over lunch, Jan and I had a chat about this; it looks like we don't
>> report quite comprehensive enough information in the logs to make it
>> easy to interpret what parts of SYNC processing are consuming what
>> time.
>>
>> The "straw man" idea we came up with is to do a much better breakdown
>> of the time, in particlar, to record:
>>
>>  - time spent in pqexec() against the provider, broken down into...
>>     - time spent processing what transactions are part of the SYNC group
>>     - time spent processing the LOG cursor
>>  - time spent in pqexec() against the subscriber (the I/U/D phase)
>>  - numbers of pqexecs()
>>     against provider
>>     against subscriber
>>  - possibly, the number of times we grab timestamps
>
> And, let us augment this with the number of "large tuple" fetches...
> That will be really cheap from gettimeofday() perspective, but gives
> us a good idea of how much flow interruption takes place.

To better explain the above; at present, we wind up needing to do
quite a lot of very deep guesstimating in order to infer where
performance bottlenecks may lie.

If we report these various values, namely:
 - How many pqexecs, and
 - How long those pqexecs took

  a) Against the data provider, which tells us how expensive it was to PULL
     the sync,

  b) Against the subscriber, which tells us how expensive it was to load the
     data in, there, and

  c) For large tuples that break up the usual "do 100 rows at a time" behaviour

That may be expected to allow people to MUCH more readily determine
where the bottlenecks are.  There tend to be three characteristic
ones:

  1.  The data provider may get overloaded.  "Why" is another question :-).

  2.  The subscriber may be less well appointed, and it might be getting
      overloaded.

  3.  The problem might be with the network connection; the slon may be too
      far away from, well, something...

With this data, it should be easier to distinguish between these
scenarios.

I now have a patch that seems to work reasonably:

http://lists.slony.info/pipermail/slony1-patches/2008-July/000039.html

Jan, you had warned of concerns about multiple threads; I *think* that
this "immunizes itself" in that sync_helper() and sync_event()
instantiate their own copies of the structure, pm, with "auto" extent,
so that if there are multiple threads operating concurrently, each one
should have its own independent copy of pm on the stack.

Perhaps I am woefully wrong, though :-).

I think I'd like to give the variables better names, and Jan suggested
adding a config varible to allow making this data collection optional.

Jan, please browse and see if there are any woeful misapprehensions...
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From rafael.domiciano at gmail.com  Fri Jul 18 05:52:50 2008
From: rafael.domiciano at gmail.com (Rafael Domiciano)
Date: Fri Jul 18 05:52:53 2008
Subject: [Slony1-general] Node not found in the runtime configuration
Message-ID: <3a0028490807180552r739065b5y27218bec33f6863c@mail.gmail.com>

Hi there,

People, I'm getting the following error in my Slony Log:
remoteWorkerThread_1: node -1 not found in runtime configuration

Does anybody knows what it means?
I am glooging but can't find anything similar to it.

Tnks,

Rafael Domiciano
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080718/=
f27696fe/attachment.htm
From stephane.schildknecht at postgresqlfr.org  Fri Jul 18 06:14:31 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Fri Jul 18 06:14:41 2008
Subject: [Slony1-general] Node not found in the runtime configuration
In-Reply-To: <3a0028490807180552r739065b5y27218bec33f6863c@mail.gmail.com>
References: <3a0028490807180552r739065b5y27218bec33f6863c@mail.gmail.com>
Message-ID: <48809737.7040808@postgresqlfr.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Rafael Domiciano a ?crit :

> Hi there,
> 
> People, I'm getting the following error in my Slony Log:
> remoteWorkerThread_1: node -1 not found in runtime configuration
> 
> Does anybody knows what it means?
> I am glooging but can't find anything similar to it.
> 
> Tnks,
> 
> Rafael Domiciano
> 

I would say you have an error in your config.
Did you use a perl script ?

- --
St?phane Schildknecht
PostgreSQLFr : http://www.postgresql.fr

Venez nous rencontrer le 4 octobre lors du plus important ?v?nement
PostgreSQL francophone : http://www.pgday.fr

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFIgJc3A+REPKWGI0ERAsoFAKCxtrWa8ZlOGE1QfK/S+mmvaERt0ACgh3NV
UCe7zsFO7EvmI+lYSv288sY=
=1wVc
-----END PGP SIGNATURE-----
From stephane.schildknecht at postgresqlfr.org  Fri Jul 18 06:19:38 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Fri Jul 18 06:19:47 2008
Subject: [Slony1-general] sl_nodelock messages
In-Reply-To: <CAB1644F1C94A24BAC68B05D0F97746302578E20@VIECLEX01.frequentis.frq>
References: <CAB1644F1C94A24BAC68B05D0F97746302578E0B@VIECLEX01.frequentis.frq><87k5flydoq.fsf@dba2.int.libertyrms.com>	<CAB1644F1C94A24BAC68B05D0F97746302578E0E@VIECLEX01.frequentis.frq>
	<CAB1644F1C94A24BAC68B05D0F97746302578E20@VIECLEX01.frequentis.frq>
Message-ID: <4880986A.2010302@postgresqlfr.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

LINHART Felix a ?crit :
> Hi all,
> 
> I have found out something new regarding my sl_nodelock messages. The
> following error occurred and also my slony pids for some nodes where
> different after this error message.
> 
> 2008-07-16 20:15:13 CEST ERROR  remoteListenThread_8: "select ev_origin,
> ev_seqno, ev_timestamp,        ev_minxid, ev_maxxid, ev_xip,        ev_type,
> ev_data1, ev_data2,        ev_data3, ev_data4,        ev_data5, ev_data6,
> ev_data7, ev_data8 from "_slony1".sl_event e where (e.ev_origin = '7' and
> e.ev_seqno > '7890') or (e.ev_origin = '8' and e.ev_seqno > '7885') or
> (e.ev_origin = '2' and e.ev_seqno > '7884') or (e.ev_origin = '3' and
> e.ev_seqno > '7880') or (e.ev_origin = '4' and e.ev_seqno > '7884') or
> (e.ev_origin = '5' and e.ev_seqno > '7881') or (e.ev_origin = '6' and
> e.ev_seqno > '7885') or (e.ev_origin = '9' and e.ev_seqno > '7895') order by
> e.ev_origin, e.ev_seqno" - server closed the connection unexpectedly
>         This probably means the server terminated abnormally
>         before or while processing the request.



In some way or another, your connection to PG was lost. Network problem, PG
restarting ?

Do you have any error message in you PG logs ?

- --
St?phane Schildknecht
PostgreSQLFr : http://www.postgresql.fr

Venez nous rencontrer le 4 octobre lors du plus important ?v?nement
PostgreSQL francophone : http://www.pgday.fr

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFIgJhqA+REPKWGI0ERAsDoAJ4n4B4HQ4bticAsC4wYdiPWAwom+ACfdubS
AlyimMnruMF4QyGnI67VSQE=
=gUOp
-----END PGP SIGNATURE-----
From rafael.domiciano at gmail.com  Fri Jul 18 06:22:10 2008
From: rafael.domiciano at gmail.com (Rafael Domiciano)
Date: Fri Jul 18 06:22:15 2008
Subject: [Slony1-general] Node not found in the runtime configuration
In-Reply-To: <48809737.7040808@postgresqlfr.org>
References: <3a0028490807180552r739065b5y27218bec33f6863c@mail.gmail.com>
	<48809737.7040808@postgresqlfr.org>
Message-ID: <3a0028490807180622y4a003f91m53f944f27aac70db@mail.gmail.com>

I'm using slonik scripts.
My config there is no problem, because i'm using the same config file that i
used in another replication, just changing the machine ip.
I'm doing db A to db B and C, and the A -> B is working fine

Everything all I read about, there is no solution and it's need to restart
the slony from the beggining, and don't want to do is

2008/7/18 "St=E9phane A. Schildknecht" <stephane.schildknecht@postgresqlfr.=
org
>:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Rafael Domiciano a =E9crit :
>
> > Hi there,
> >
> > People, I'm getting the following error in my Slony Log:
> > remoteWorkerThread_1: node -1 not found in runtime configuration
> >
> > Does anybody knows what it means?
> > I am glooging but can't find anything similar to it.
> >
> > Tnks,
> >
> > Rafael Domiciano
> >
>
> I would say you have an error in your config.
> Did you use a perl script ?
>
> - --
> St=E9phane Schildknecht
> PostgreSQLFr : http://www.postgresql.fr
>
> Venez nous rencontrer le 4 octobre lors du plus important =E9v=E9nement
> PostgreSQL francophone : http://www.pgday.fr
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.6 (GNU/Linux)
> Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org
>
> iD8DBQFIgJc3A+REPKWGI0ERAsoFAKCxtrWa8ZlOGE1QfK/S+mmvaERt0ACgh3NV
> UCe7zsFO7EvmI+lYSv288sY=3D
> =3D1wVc
> -----END PGP SIGNATURE-----
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080718/=
e507f288/attachment.htm
From stephane.schildknecht at postgresqlfr.org  Fri Jul 18 06:24:04 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Fri Jul 18 06:24:10 2008
Subject: [Slony1-general] Failover with unresponsive slaves
In-Reply-To: <b49a4abf0807160948r33e34fffqdc2649662644b8a0@mail.gmail.com>
References: <b49a4abf0807160948r33e34fffqdc2649662644b8a0@mail.gmail.com>
Message-ID: <48809974.5030806@postgresqlfr.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Quinn Jones a ?crit :
> Hello,
>  
> I've been lurking on the list for a while, but now have a problem that I
> haven't seen:  I tried to failover a node, but a slave was also
> unresponsive and slonik errored out after timing out (so the failover
> didn't happen).
>  
> Here's our set-up: We have a database replicated to three slave nodes
> and a total of three sites, like this
> site 1: db1 (master) and db2
> site 2: db3
> site 3: db4
>  
> Our problem started when site 1 went away completely and abruptly (so
> db1 and db2 were out of commission).  Our plan called for failing the
> database over to db3.  When I tried to failover, though, slonik timed
> out with the message 'could not connect to server: Connection timed
> out.  Is the server running on host "x.x.x.x" and accepting TCP/IP
> connections on port 5432?'.  The ip address was db2, so seeing that
> there is a logical problem to solve I tried dropping the downed slave
> node first.  This timed out as well, and the slave was not dropped.
>  
> While trying to figure out an intelligent next step, short of dropping
> replication entirely and just using db3 stand-alone (and rebuilding the
> cluster from scratch later) site1 mostly came back up.  We lucked out
> and in the end saved some time by not being able to fail over the way we
> wanted, though we did lose an unknown number of sales because we were
> effectively down.
>  
> How do we drop a non-responsive slave, or force the failover to ignore
> it?  This is a situation that shouldn't come up frequently for us, but
> it could and this was rather troublesome.  I understand why failover
> would want to communicate with every other server, but there must be a
> way to step over other dead servers to get a functional cluster (I just
> haven't found it yet).  Also, shouldn't dropping a slave node happen
> whether the node can be seen or not?
>  
> Quinn
>  

AFAIK, failover does not try to join a failed node. Could you tell us what you
tried ?
Did you get any error in PostgreSQL logs ?

- --
St?phane Schildknecht
PostgreSQLFr : http://www.postgresql.fr

Venez nous rencontrer le 4 octobre lors du plus important ?v?nement
PostgreSQL francophone : http://www.pgday.fr

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFIgJlzA+REPKWGI0ERAlHwAKCTnTD1sqvb4JIJYR3pefZB93N04ACfUSG7
BVDBGCt4CwDXFk9KHNYeZJI=
=c8Mw
-----END PGP SIGNATURE-----
From stephane.schildknecht at postgresqlfr.org  Fri Jul 18 06:29:00 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Fri Jul 18 06:29:07 2008
Subject: [Slony1-general] Node not found in the runtime configuration
In-Reply-To: <3a0028490807180622y4a003f91m53f944f27aac70db@mail.gmail.com>
References: <3a0028490807180552r739065b5y27218bec33f6863c@mail.gmail.com>	
	<48809737.7040808@postgresqlfr.org>
	<3a0028490807180622y4a003f91m53f944f27aac70db@mail.gmail.com>
Message-ID: <48809A9C.5080207@postgresqlfr.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Rafael Domiciano a ?crit :
> I'm using slonik scripts.
> My config there is no problem, because i'm using the same config file
> that i used in another replication, just changing the machine ip.
> I'm doing db A to db B and C, and the A -> B is working fine
> 
> Everything all I read about, there is no solution and it's need to
> restart the slony from the beggining, and don't want to

If only node 3 is in error, you shouldn't have to recreate the whole cluster.

SAS
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFIgJqcA+REPKWGI0ERAmczAKCKt3vP7Xo0lm8CJ+Meqh/UtD0gMQCgidXF
G05dWKWzB47BiPn9AdlCaiw=
=HpEv
-----END PGP SIGNATURE-----
From rafael.domiciano at gmail.com  Fri Jul 18 06:38:02 2008
From: rafael.domiciano at gmail.com (Rafael Domiciano)
Date: Fri Jul 18 06:38:08 2008
Subject: [Slony1-general] Node not found in the runtime configuration
In-Reply-To: <48809A9C.5080207@postgresqlfr.org>
References: <3a0028490807180552r739065b5y27218bec33f6863c@mail.gmail.com>
	<48809737.7040808@postgresqlfr.org>
	<3a0028490807180622y4a003f91m53f944f27aac70db@mail.gmail.com>
	<48809A9C.5080207@postgresqlfr.org>
Message-ID: <3a0028490807180638g1f8554afm62b8de15212fbcc4@mail.gmail.com>

i am doing two cluster differents...
my problem is in a set that break in error, because different structure of a
table. and then i get that error and everything i do i get the message in
the log:
"remoteWorkerThread_1: node -1 not found in runtime configuration"

2008/7/18 "St=E9phane A. Schildknecht" <stephane.schildknecht@postgresqlfr.=
org
>:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Rafael Domiciano a =E9crit :
> > I'm using slonik scripts.
> > My config there is no problem, because i'm using the same config file
> > that i used in another replication, just changing the machine ip.
> > I'm doing db A to db B and C, and the A -> B is working fine
> >
> > Everything all I read about, there is no solution and it's need to
> > restart the slony from the beggining, and don't want to
>
> If only node 3 is in error, you shouldn't have to recreate the whole
> cluster.
>
> SAS
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.6 (GNU/Linux)
> Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org
>
> iD8DBQFIgJqcA+REPKWGI0ERAmczAKCKt3vP7Xo0lm8CJ+Meqh/UtD0gMQCgidXF
> G05dWKWzB47BiPn9AdlCaiw=3D
> =3DHpEv
> -----END PGP SIGNATURE-----
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080718/=
3bbac375/attachment.htm
From stephane.schildknecht at postgresqlfr.org  Fri Jul 18 07:08:57 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Fri Jul 18 07:09:05 2008
Subject: [Slony1-general] Node not found in the runtime configuration
In-Reply-To: <3a0028490807180638g1f8554afm62b8de15212fbcc4@mail.gmail.com>
References: <3a0028490807180552r739065b5y27218bec33f6863c@mail.gmail.com>	
	<48809737.7040808@postgresqlfr.org>	
	<3a0028490807180622y4a003f91m53f944f27aac70db@mail.gmail.com>	
	<48809A9C.5080207@postgresqlfr.org>
	<3a0028490807180638g1f8554afm62b8de15212fbcc4@mail.gmail.com>
Message-ID: <4880A3F9.7010409@postgresqlfr.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Rafael Domiciano a ?crit :
> i am doing two cluster differents...
> my problem is in a set that break in error, because different structure
> of a table. and then i get that error and everything i do i get the
> message in the log:
> "remoteWorkerThread_1: node -1 not found in runtime configuration"
> 

There is something I don't understand...
When you talk about clusters, do you think Slony clusters or PG clusters ?

In case you are talking about two different slony clusters, I don't know why
they would interfere with each other.

Trouble is if you have difference in structure, I don't indeed know how you
could deal with it without recreating the whole slony cluster.


SAS
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFIgKP5A+REPKWGI0ERAgpEAJ9rMUOW4fa8LCWI/fORu3vxPWuNKACdEoPa
rXw1ALs503mDBCTGnfD5FcQ=
=Qflm
-----END PGP SIGNATURE-----
From Marcin.Kasperski at softax.com.pl  Fri Jul 18 11:14:29 2008
From: Marcin.Kasperski at softax.com.pl (Marcin Kasperski)
Date: Fri Jul 18 11:19:07 2008
Subject: [Slony1-general] Re-building the replication without copying
	everything?
Message-ID: <87bq0v9i0q.fsf@softax.com.pl>


Imagine I have two databases which are correctly replicated - maybe
I did dump&restore, maybe some other replication technique (*).
Imagine I can assure myself that those are equal.

Does there exist any method to fool slony so it skips initial DELETE &
COPY TABLE, but just starts replicating new data instead?


(*) Well, to say the true in the very scenario I survived recently, I
had Slony replication which went stuck after a few add node, drop
node, move set operations (I was just another victim of neverending
"ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep"
case). But if I read list archives correctly this is to be corrected
in 1.2.14 (I use 1.2.13 which ubuntu installs).


-- 
----------------------------------------------------------------------
| Marcin Kasperski   | Software is not released,
| http://mekk.waw.pl |  it is allowed to escape.
|                    |
----------------------------------------------------------------------

From quinn_jones at pobox.com  Fri Jul 18 12:16:34 2008
From: quinn_jones at pobox.com (Quinn Jones)
Date: Fri Jul 18 12:16:46 2008
Subject: [Slony1-general] Failover with unresponsive slaves
In-Reply-To: <48809974.5030806@postgresqlfr.org>
References: <b49a4abf0807160948r33e34fffqdc2649662644b8a0@mail.gmail.com>
	<48809974.5030806@postgresqlfr.org>
Message-ID: <b49a4abf0807181216p44156ecbkbcf0831e289369ba@mail.gmail.com>

The only errors I saw were the on-screen errors, which came after a lengthy
time-out.  There wasn't anything in the log besides a long string of
time-out errors.

Before I get any further and someone wants to know, we're running slony
1.2.11 and Postgres 8.2.7 on all nodes.

Here's the command I ran to failover:
cluster name =3D mycluster;
 node 1 admin conninfo=3D'host=3D10.0.0.1 dbname=3Dmydb user=3Dmyuser port=
=3D1234;
 node 2 admin conninfo=3D'host=3D10.0.0.2 dbname=3Dmydb user=3Dmyuser port=
=3D1234;
 node 3 admin conninfo=3D'host=3D10.1.0.1 dbname=3Dmydb user=3Dmyuser port=
=3D1234;
 node 4 admin conninfo=3D'host=3D10.2.0.1 dbname=3Dmydb user=3Dmyuser port=
=3D1234;
  try {
      failover (id =3D 1, backup node =3D 3);
  } on error {
      echo 'Failure to fail node 1 over to 3';
      exit 1;
  }
  echo 'Replication sets originating on 1 failed over to 3';

I'm not sure you understand the question, though, so I'll re-phrase.  I
wasn't trying to join anything.  My master node, and one slave, were
offline.  I wanted to failover to a different slave in the cluster, but the
failover command timed out with the message 'could not connect to server:
Connection timed out.  Is the server running on host "x.x.x.x" and accepting
TCP/IP connections on port 5432?', where x.x.x.x is the IP address of the
offline slave.

I also tried dropping the downed slave, hoping to side-step the error in
failover, so I ran this:
...same preamble...
  try {
      drop node (id =3D 2, event node =3D 1);
  } on error {
      echo 'Failed to drop node 2 from cluster';
      exit 1;
  }
  echo 'dropped node 2 cluster';

>From where I sit, it looks like slony didn't want to failover without
notifying all of the slaves, which couldn't happen so it quit instead.

Quinn


>
>
> AFAIK, failover does not try to join a failed node. Could you tell us what
> you
> tried ?
> Did you get any error in PostgreSQL logs ?
>
> - --
> St=E9phane Schildknecht
> PostgreSQLFr : http://www.postgresql.fr
>
> Venez nous rencontrer le 4 octobre lors du plus important =E9v=E9nement
> PostgreSQL francophone : http://www.pgday.fr
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.6 (GNU/Linux)
> Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org
>
> iD8DBQFIgJlzA+REPKWGI0ERAlHwAKCTnTD1sqvb4JIJYR3pefZB93N04ACfUSG7
> BVDBGCt4CwDXFk9KHNYeZJI=3D
> =3Dc8Mw
> -----END PGP SIGNATURE-----
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080718/=
9226a97f/attachment.htm
From yi.zhao at alibaba-inc.com  Mon Jul 21 04:45:29 2008
From: yi.zhao at alibaba-inc.com (Yi Zhao)
Date: Mon Jul 21 04:43:46 2008
Subject: [Slony1-general] where is the trigger named like
	_replication_denyaccess_2???
In-Reply-To: <3a0028490807180638g1f8554afm62b8de15212fbcc4@mail.gmail.com>
References: <3a0028490807180552r739065b5y27218bec33f6863c@mail.gmail.com>
	<48809737.7040808@postgresqlfr.org>
	<3a0028490807180622y4a003f91m53f944f27aac70db@mail.gmail.com>
	<48809A9C.5080207@postgresqlfr.org>
	<3a0028490807180638g1f8554afm62b8de15212fbcc4@mail.gmail.com>
Message-ID: <1216640729.3260.9.camel@localhost.localdomain>

hi, all:
after I adding some table which has primary key to slony set,
I use command '\d test_table' on the slave database, but......I can't
find the trigger as below:

"Triggers:
    _replication_denyaccess_2 BEFORE INSERT OR DELETE OR UPDATE ON users
FOR EACH ROW EXECUTE PROCEDURE _replication.denyaccess('_replication')"

I got above trigger description from another exists slony replication,
but, I don't know why I can't find it from my replication:(

I create the slony repliation set by slony-tools as below:
--------------------------------------------------
#!/bin/bash

#initialize slony
/usr/local/slony-i/slonik_init_cluster | /usr/local/pgsql/bin/slonik

#start slon daemon
/usr/local/slony-i/slon_start 1
/usr/local/slony-i/slon_start 2

#create replication set
/usr/local/slony-i/slonik_create_set 1 | /usr/local/pgsql/bin/slonik

#add master and it's slave
/usr/local/slony-i/slonik_subscribe_set 1 2
| /usr/local/pgsql/bin/slonik
--------------------------------------------------

ps:
I can find trigger added by slony from master database as "
_replication_logtrigger_5 AFTER INSERT OR DELETE OR UPDATE ON
content.thread_2 FOR EACH
 ROW EXECUTE PROCEDURE _replication.logtrigger('_replication', '5',
'kvvvvvvvvvvvvvvvvvvvvv
vv')"

From glynastill at yahoo.co.uk  Wed Jul 23 08:58:02 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Wed Jul 23 08:58:11 2008
Subject: [Slony1-general] Best way to check if a table is in replication?
Message-ID: <852185.80465.qm@web25803.mail.ukl.yahoo.com>

Hi Chaps,

What's the best way to check if a table is in slony-i replication without knowing anything specific about the replication cluster/set?  There is no need to tell if it's an origin or subscriber, just that slony is in operation on the table?

The best I could come up with was (where the table name is "credit"):

select count(*) from (pg_trigger join pg_class on tgrelid=pg_class.oid)
join pg_proc on (tgfoid=pg_proc.oid) where (proname = 'denyaccess' or proname = 'logtrigger') and relname = 'credit';

If the result > 0 then it's replicated.

Is there any case I'd be missing something here, or is there a better way?



      __________________________________________________________
Not happy with your email address?.
Get the one you really want - millions of new email addresses available now at Yahoo! http://uk.docs.yahoo.com/ymail/new.html
From cbbrowne at ca.afilias.info  Wed Jul 23 10:54:16 2008
From: cbbrowne at ca.afilias.info (cbbrowne)
Date: Wed Jul 23 10:54:27 2008
Subject: [Slony1-general] Best way to check if a table is in replication?
In-Reply-To: <852185.80465.qm@web25803.mail.ukl.yahoo.com>
References: <852185.80465.qm@web25803.mail.ukl.yahoo.com>
Message-ID: <48877048.90709@ca.afilias.info>

Glyn Astill wrote:
> Hi Chaps,
>
> What's the best way to check if a table is in slony-i replication without knowing anything specific about the replication cluster/set?  There is no need to tell if it's an origin or subscriber, just that slony is in operation on the table?
>
> The best I could come up with was (where the table name is "credit"):
>
> select count(*) from (pg_trigger join pg_class on tgrelid=pg_class.oid)
> join pg_proc on (tgfoid=pg_proc.oid) where (proname = 'denyaccess' or proname = 'logtrigger') and relname = 'credit';
>
> If the result > 0 then it's replicated.
>
> Is there any case I'd be missing something here, or is there a better way?
>   
I'd sorta like to point at something involving looking up the table in 
sl_table, but that requires determining the replication schema, which 
mandates either:
a) Doing a lookup to find any "sl_table" instances (there could be more 
than one, in principle!), and iterating over them, looking to see if the 
table is listed.  That makes the logic more complex in the application.
b) One could build a stored procedure that uses "EXECUTE" to do the 
logic in a); that eliminates extra round trips.

I'm not sure that's fundamentally any better than the approach you 
suggest; it's certainly more complex.

-- 
(format nil "~S@~S" "cbbrowne" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)

From Marcin.Kasperski at softax.com.pl  Thu Jul 24 01:31:01 2008
From: Marcin.Kasperski at softax.com.pl (Marcin Kasperski)
Date: Thu Jul 24 01:33:04 2008
Subject: [Slony1-general] Re: Best way to check if a table is in replication?
References: <852185.80465.qm@web25803.mail.ukl.yahoo.com>
Message-ID: <87r69j1y62.fsf@softax.com.pl>


> What's the best way to check if a table is in slony-i replication
> without knowing anything specific about the replication cluster/set?
> There is no need to tell if it's an origin or subscriber, just that
> slony is in operation on the table?

Run pgadmin3 and select 'Replication' under the database in charge ?


-- 
----------------------------------------------------------------------
| Marcin Kasperski   |    You have the right to say how long each
| http://mekk.waw.pl |  requirement will take you to implement, and
|                    | to revise estimates given experience. (Beck)
----------------------------------------------------------------------

From glynastill at yahoo.co.uk  Thu Jul 24 02:43:45 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Thu Jul 24 02:44:15 2008
Subject: [Slony1-general] Re: Best way to check if a table is in
	replication?
In-Reply-To: <87r69j1y62.fsf@softax.com.pl>
Message-ID: <150039.87871.qm@web25806.mail.ukl.yahoo.com>

Sorry Marcin, no banana.

I want'ed an easy query to fire off from a driver application. Looks like the easiest way is my initial one.


--- On Thu, 24/7/08, Marcin Kasperski <Marcin.Kasperski@softax.com.pl> wrote:

> From: Marcin Kasperski <Marcin.Kasperski@softax.com.pl>
> Subject: [Slony1-general] Re: Best way to check if a table is in replication?
> To: slony1-general@lists.slony.info
> Date: Thursday, 24 July, 2008, 9:31 AM
> > What's the best way to check if a table is in
> slony-i replication
> > without knowing anything specific about the
> replication cluster/set?
> > There is no need to tell if it's an origin or
> subscriber, just that
> > slony is in operation on the table?
> 
> Run pgadmin3 and select 'Replication' under the
> database in charge ?
> 
> 
> -- 
> ----------------------------------------------------------------------
> | Marcin Kasperski   |    You have the right to say how
> long each
> | http://mekk.waw.pl |  requirement will take you to
> implement, and
> |                    | to revise estimates given
> experience. (Beck)
> ----------------------------------------------------------------------
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


      __________________________________________________________
Not happy with your email address?.
Get the one you really want - millions of new email addresses available now at Yahoo! http://uk.docs.yahoo.com/ymail/new.html
From glynastill at yahoo.co.uk  Thu Jul 24 03:50:49 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Thu Jul 24 03:51:21 2008
Subject: [Slony1-general] Best way to check if a table is in replication?
In-Reply-To: <48877048.90709@ca.afilias.info>
Message-ID: <345880.34086.qm@web25804.mail.ukl.yahoo.com>

--- On Wed, 23/7/08, cbbrowne <cbbrowne@ca.afilias.info> wrote:
  
> I'd sorta like to point at something involving looking
> up the table in 
> sl_table, but that requires determining the replication
> schema, which 
> mandates either:
> a) Doing a lookup to find any "sl_table"
> instances (there could be more 
> than one, in principle!), and iterating over them, looking
> to see if the 
> table is listed.  That makes the logic more complex in the
> application.
> b) One could build a stored procedure that uses
> "EXECUTE" to do the 
> logic in a); that eliminates extra round trips.
> 
> I'm not sure that's fundamentally any better than
> the approach you 
> suggest; it's certainly more complex.
> 

Yeah, checking sl_table was actually the first way I thought I should do it, but the idea of having to hunt down sl_table, or as you say multiple instances of sl_table made me think otherwise.

I'm going to tyy the following and see how things go.

select count(*) from (pg_trigger join pg_class on tgrelid=pg_class.oid)
join pg_proc on (tgfoid=pg_proc.oid) join pg_namespace on (relnamespace=pg_namespace.oid)
where (proname = 'denyaccess' or proname = 'logtrigger') and relname = '<table name>' and nspname = '<schema name>';

Thanks
Glyn


      __________________________________________________________
Not happy with your email address?.
Get the one you really want - millions of new email addresses available now at Yahoo! http://uk.docs.yahoo.com/ymail/new.html
From singh.gurjeet at gmail.com  Fri Jul 25 01:48:06 2008
From: singh.gurjeet at gmail.com (Gurjeet Singh)
Date: Fri Jul 25 01:48:36 2008
Subject: [Slony1-general] CREATE SET hung waiting
Message-ID: <65937bea0807250148w64dcf051sb264b6c6d52b35cb@mail.gmail.com>

Hi All,

    The docs for CREATE SET say 'No application-visible locking should take
place'. But we saw that it was hung, and noticed that there was a VACUUM
running on master node. We waited for quite a while, and after we killed
that VACUUM, the CREATE SET moved forward.

    PG is 8.1.11, and Slony is 1.2.14, if that helps.

Best regards,
-- =

gurjeet[.singh]@EnterpriseDB.com
singh.gurjeet@{ gmail | hotmail | indiatimes | yahoo }.com

EnterpriseDB http://www.enterprisedb.com

Mail sent from my BlackLaptop device
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080725/=
31430ae1/attachment.htm
From lai at clustersolutions.net  Fri Jul 25 10:54:57 2008
From: lai at clustersolutions.net (lai@clustersolutions.net)
Date: Fri Jul 25 10:55:07 2008
Subject: [Slony1-general] Master/Slave at the same time
Message-ID: <1248.75.25.11.233.1217008497.squirrel@www.clustersolutions.net>

Hello, I have a production cluster of node a and node b where node a is
the master and node b is the slave. Node a at this time is replicating
table x in set 1 to node b. My question is is there a problem where I
would create a set 2 and name node b as the origin and have table y
replicating to node a? Have anyone done this before?

NODE A             NODE B
table x ------> table x (set1)
table y <------ table y (set2)

Thank you ahead for your help!

Regards,

Tim





From rafael.domiciano at gmail.com  Fri Jul 25 11:51:59 2008
From: rafael.domiciano at gmail.com (Rafael Domiciano)
Date: Fri Jul 25 11:52:09 2008
Subject: [Slony1-general] bandwidth used by Slony
Message-ID: <3a0028490807251151i385e154cofd45655be8ba63a3@mail.gmail.com>

Hi people,

There is how to limit the bandwidth used by Slony?
I have a limited bandwidth and I need to run 2 processes of Slony, but the
first one is using all the bandwidth.

Regards,

Rafael Domiciano
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080725/=
9f1307b6/attachment.htm
From rafael.domiciano at gmail.com  Fri Jul 25 12:18:54 2008
From: rafael.domiciano at gmail.com (Rafael Domiciano)
Date: Fri Jul 25 12:19:06 2008
Subject: [Slony1-general] CREATE SET hung waiting
In-Reply-To: <65937bea0807250148w64dcf051sb264b6c6d52b35cb@mail.gmail.com>
References: <65937bea0807250148w64dcf051sb264b6c6d52b35cb@mail.gmail.com>
Message-ID: <3a0028490807251218o4fe019fel507d8b31f4be33c@mail.gmail.com>

I have similar problem some time ago, and my whole base stayed innoperated.
Too bad.
Vacuum isn't good when Slony is initing for the first time (create set,
subscribe set and so on).
The only solution that I find is to do the sets slowly, one by one. Then the
Slony does the work great.

Regards,

Rafael Domiciano

2008/7/25 Gurjeet Singh <singh.gurjeet@gmail.com>

> Hi All,
>
>     The docs for CREATE SET say 'No application-visible locking should ta=
ke
> place'. But we saw that it was hung, and noticed that there was a VACUUM
> running on master node. We waited for quite a while, and after we killed
> that VACUUM, the CREATE SET moved forward.
>
>     PG is 8.1.11, and Slony is 1.2.14, if that helps.
>
> Best regards,
> --
> gurjeet[.singh]@EnterpriseDB.com
> singh.gurjeet@{ gmail | hotmail | indiatimes | yahoo }.com
>
> EnterpriseDB http://www.enterprisedb.com
>
> Mail sent from my BlackLaptop device
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080725/=
a51e7a18/attachment.htm
From by_pacitan at yahoo.com  Fri Jul 25 14:18:43 2008
From: by_pacitan at yahoo.com (Abdul kareem Al fudhail)
Date: Fri Jul 25 14:18:46 2008
Subject: [Slony1-general] Abdul kareem Al fudhail has invited you to join
	Friendster
Message-ID: <71a4kd$o4h25e@c350a.gbxsc.friendster.com>

WW91J3JlIGludml0ZWQgdG8gam9pbiBBYmR1bCBrYXJlZW0gQWwgZnVkaGFpbCdzIG5ldHdvcmsg
b2YgZnJpZW5kcy4gCgpCeSBqb2luaW5nIEZyaWVuZHN0ZXIsIHlvdSBjYW4gcmVjb25uZWN0IHdp
dGggb2xkCmZyaWVuZHMsIG1lZXQgbmV3IGZyaWVuZHMsIHN0YXJ0IGEgYmxvZywgYnVpbGQgYSBj
dXN0b20KcHJvZmlsZSwga2VlcCB0cmFjayBvZiBiaXJ0aGRheXMsIGFuZCBzbwptdWNoIG1vcmUh
CgpZb3UgY2FuIGV2ZW4gc3RheSBpbiB0b3VjaCBpZiB5b3UgbW92ZSBhd2F5LCBzd2l0Y2gKZW1h
aWwgYWRkcmVzc2VzLCBvciBsb3NlIHlvdXIgbW9iaWxlIHBob25lLgoKQ2xpY2sgYmVsb3cgdG8g
am9pbiBBYmR1bCBrYXJlZW0ncyBuZXR3b3JrLgoKaHR0cDovL3d3dy5mcmllbmRzdGVyLmNvbS9q
b2luLnBocD9pbnZpdGVyZWY9MTQzNTU0MDcmYW1wO2ludml0ZT1qeks0UFY3T2lGTGdDdTMtcmdu
ZlVCMnFRNGo2ZnVhTGZBcnhzTEJYVzVNKiZhbXA7bGFuZz1lbi1VUwoKKioqKioqKioqKioqKioq
KioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioKSWYgeW91
IGRvIG5vdCB3aXNoIHRvIHJlY2VpdmUgbm90aWZpY2F0aW9uIGVtYWlscyBmcm9tIEZyaWVuZHN0
ZXIsIHBsZWFzZSBjbGljayBiZWxvdzoKaHR0cDovL3d3dy5mcmllbmRzdGVyLmNvbS9ibG9ja2Vt
YWlscy5waHA/aW52aXRlPWMyeHZibmt4TFdkbGJtVnlZV3hBYkdsemRITXVjMnh2Ym5rdWFXNW1i
dyoqCi0tLS0tLS0tLS0tLS0tIG5leHQgcGFydCAtLS0tLS0tLS0tLS0tLQpBbiBIVE1MIGF0dGFj
aG1lbnQgd2FzIHNjcnViYmVkLi4uClVSTDogaHR0cDovL2xpc3RzLnNsb255LmluZm8vcGlwZXJt
YWlsL3Nsb255MS1nZW5lcmFsL2F0dGFjaG1lbnRzLzIwMDgwNzI1L2E0ODgxMWUyL2F0dGFjaG1l
bnQuaHRtCg==
From cbbrowne at ca.afilias.info  Fri Jul 25 14:54:49 2008
From: cbbrowne at ca.afilias.info (chris)
Date: Fri Jul 25 14:55:03 2008
Subject: [Slony1-general] Re: [Slony1-hackers] CREATE SET hung waiting
In-Reply-To: <65937bea0807250148w64dcf051sb264b6c6d52b35cb@mail.gmail.com>
	(Gurjeet Singh's message of "Fri, 25 Jul 2008 14:18:06 +0530")
References: <65937bea0807250148w64dcf051sb264b6c6d52b35cb@mail.gmail.com>
Message-ID: <87k5f9sk7q.fsf@dba2.int.libertyrms.com>

"Gurjeet Singh" <singh.gurjeet@gmail.com> writes:
> 								     Hi All,
>   ??? The docs for CREATE SET say 'No application-visible locking should take place'. But we saw that it was hung, and noticed that there was a
> 	   VACUUM running on master node. We waited for quite a while, and after we killed that VACUUM, the CREATE SET moved forward.
> 					      ??? PG is 8.1.11, and Slony is 1.2.14, if that helps.

The only thing that CREATE SET does, initially, that involves locking
of *anything* is that it takes out a lock out on sl_config_lock, a
table internal to Slony-I.

The only way for that to lock things that are application-visible is
if you have some application that's vacuuming *everything*, and which
therefore takes out a lock on sl_config_lock that prevents it from
being granted to the CREATE SET request.

The locking that is done should indeed not be visible to applications.
It only became visible because you had a VACUUM that was working on
the Slony-I schema.

I'm trying to think of what more to say, in the documentation; nothing
is really coming to me.  I don't think that the documentation is
misleading.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From cbbrowne at ca.afilias.info  Fri Jul 25 15:25:57 2008
From: cbbrowne at ca.afilias.info (chris)
Date: Fri Jul 25 15:26:13 2008
Subject: [Slony1-general] Node not found in the runtime configuration
In-Reply-To: <3a0028490807180638g1f8554afm62b8de15212fbcc4@mail.gmail.com>
	(Rafael Domiciano's message of "Fri, 18 Jul 2008 10:38:02 -0300")
References: <3a0028490807180552r739065b5y27218bec33f6863c@mail.gmail.com>
	<48809737.7040808@postgresqlfr.org>
	<3a0028490807180622y4a003f91m53f944f27aac70db@mail.gmail.com>
	<48809A9C.5080207@postgresqlfr.org>
	<3a0028490807180638g1f8554afm62b8de15212fbcc4@mail.gmail.com>
Message-ID: <87d4l1siru.fsf@dba2.int.libertyrms.com>

"Rafael Domiciano" <rafael.domiciano@gmail.com> writes:
> 						      i am doing two cluster differents...
>    my problem is in a set that break in error, because different structure of a table. and then i get that error and everything i do i get the
> 							       message in the log:
> 				       "remoteWorkerThread_1: node -1 not found in runtime configuration"

This error message is generated solely at the start of the function
copy_set(), at the start of a subscription.  It happens when the
search thru node configuration doesn't find the node data in memory,
which presumably takes place because something got confused in memory.

Restarting the slon should resolve the issue.

It should be reasonable to automate this; if the slon's configuration
is mussed up, it might as well die & restart.

Attached is a patch to implement this.

Jan, please review; I'm not sure I have usage of
slon_terminate_worker() correct...

 
Index: remote_worker.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/remote_worker.c,v
retrieving revision 1.173
diff -c -u -r1.173 remote_worker.c
--- remote_worker.c	27 Jun 2008 20:16:04 -0000	1.173
+++ remote_worker.c	25 Jul 2008 22:24:40 -0000
@@ -27,8 +27,10 @@
 extern int	STMTS[MAXSTATEMENTS];
 
 #define MAXGROUPSIZE 10000		/* What is the largest number of SYNCs we'd
-								 * want to group together??? */
+					 * want to group together??? */
 
+
+void slon_terminate_worker(void);
 /* ----------
  * Local definitions
  * ----------
@@ -2419,6 +2449,23 @@
 			break;
 		}
 	}
+	if (sub_provider < 0) {
+		rtcfg_unlock();
+		slon_log(SLON_ERROR, "remoteWorkerThread_%d: provider node for set %"
+				 "not found in runtime configuration\n",
+				 set_id);
+		slon_terminate_worker();
+		return -1;
+		
+	}
+	if (set_origin < 0) {
+		rtcfg_unlock();
+		slon_log(SLON_ERROR, "remoteWorkerThread_%d: origin node for set %"
+				 "not found in runtime configuration\n",
+				 set_id);
+		slon_terminate_worker();
+		return -1;
+	}
 	if (set == NULL)
 	{
 		rtcfg_unlock();
@@ -2430,7 +2477,7 @@
 	if ((sub_node = rtcfg_findNode(sub_provider)) == NULL)
 	{
 		rtcfg_unlock();
-		slon_log(SLON_ERROR, "remoteWorkerThread_%d: node %d "
+		slon_log(SLON_ERROR, "remoteWorkerThread_%d: provider node %d "
 				 "not found in runtime configuration\n",
 				 node->no_id, sub_provider);
 		return -1;

Index: slon.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/slon.c,v
retrieving revision 1.77
diff -c -u -r1.77 slon.c
--- slon.c	3 Jan 2008 15:47:21 -0000	1.77
+++ slon.c	25 Jul 2008 22:24:40 -0000
@@ -75,7 +75,7 @@
 #ifndef WIN32
 static void SlonWatchdog(void);
 static void sighandler(int signo);
-static void slon_terminate_worker(void);
+void slon_terminate_worker(void);
 #endif
 
 int			slon_log_level;
@@ -957,7 +957,7 @@
 void
 slon_terminate_worker()
 {
-	slon_log(SLON_DEBUG2, "slon: notify worker process to shutdown\n");
+	slon_log(SLON_INFO, "slon: notify worker process to shutdown\n");
 
 	if (pipewrite(sched_wakeuppipe[1], "p", 1) != 1)
 	{
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From singh.gurjeet at gmail.com  Sat Jul 26 07:55:15 2008
From: singh.gurjeet at gmail.com (Gurjeet Singh)
Date: Sat Jul 26 07:55:22 2008
Subject: [Slony1-general] Re: [Slony1-hackers] CREATE SET hung waiting
In-Reply-To: <87k5f9sk7q.fsf@dba2.int.libertyrms.com>
References: <65937bea0807250148w64dcf051sb264b6c6d52b35cb@mail.gmail.com>
	<87k5f9sk7q.fsf@dba2.int.libertyrms.com>
Message-ID: <65937bea0807260755o6ee05e48lab541dd28a694880@mail.gmail.com>

On Sat, Jul 26, 2008 at 3:24 AM, chris <cbbrowne@ca.afilias.info> wrote:

> "Gurjeet Singh" <singh.gurjeet@gmail.com> writes:
> >                                                                    Hi
> All,
> >       The docs for CREATE SET say 'No application-visible locking should
> take place'. But we saw that it was hung, and noticed that there was a
> >          VACUUM running on master node. We waited for quite a while, and
> after we killed that VACUUM, the CREATE SET moved forward.
> >                                                 PG is 8.1.11, and Slony
> is 1.2.14, if that helps.
>
> The only thing that CREATE SET does, initially, that involves locking
> of *anything* is that it takes out a lock out on sl_config_lock, a
> table internal to Slony-I.
>
> The only way for that to lock things that are application-visible is
> if you have some application that's vacuuming *everything*, and which
> therefore takes out a lock on sl_config_lock that prevents it from
> being granted to the CREATE SET request.
>
> The locking that is done should indeed not be visible to applications.
> It only became visible because you had a VACUUM that was working on
> the Slony-I schema.
>
> I'm trying to think of what more to say, in the documentation; nothing
> is really coming to me.  I don't think that the documentation is
> misleading.
>

I was not implying that docs are insufficient in any way; just trying to get
attention to a problem I faced in production environment. Maybe it's only an
issue with the old 8.1.11!

One more little thingy, I prefer to call the product simply Slony, rather
than Slony-I. I'd love to call it Slony-I, but only if Slony-II was any
nearer than 'beyond' the horizon; I don't see Slony-II progressing, and even
if it does get implemented, it'd be radically different technology than the
current implementation. So, lets spare all of us some (mild-) pain by making
'Slony' the official name; at least like PostgreSQL has accepted Postgres as
an alternate correct name.

On topic, the VACUUM i saw running was on one table, which was a user table,
not a Slony table. We do not have DB wide vacuuming policy (yet) (I'll
re-confirm if you wish), so I don't think it was a DB-wide vacuum; and even
if it was, IMHO vacuum takes locks on one table at a time.


Best regards and kudos to a great product,
-- =

gurjeet[.singh]@EnterpriseDB.com
singh.gurjeet@{ gmail | hotmail | indiatimes | yahoo }.com

EnterpriseDB http://www.enterprisedb.com

Mail sent from my BlackLaptop device
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080726/=
0cca2395/attachment-0001.htm
From stephane.schildknecht at postgresqlfr.org  Mon Jul 28 01:35:59 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Mon Jul 28 01:36:30 2008
Subject: [Slony1-general] Master/Slave at the same time
In-Reply-To: <1248.75.25.11.233.1217008497.squirrel@www.clustersolutions.net>
References: <1248.75.25.11.233.1217008497.squirrel@www.clustersolutions.net>
Message-ID: <488D84EF.2040800@postgresqlfr.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

lai@clustersolutions.net a ?crit :
> Hello, I have a production cluster of node a and node b where node a is
> the master and node b is the slave. Node a at this time is replicating
> table x in set 1 to node b. My question is is there a problem where I
> would create a set 2 and name node b as the origin and have table y
> replicating to node a? Have anyone done this before?
> 
> NODE A             NODE B
> table x ------> table x (set1)
> table y <------ table y (set2)
> 
> Thank you ahead for your help!
> 
> Regards,
> 
> Tim

One of the main Slony features is to allow cross replication.

So, be sure there is no problem in creating a second set and cross replicate
between node a (set1) and node b (set2).

Oh, and yes, I've done it, and it does work :-)

Regards,
- --
St?phane Schildknecht
PostgreSQLFr : http://www.postgresql.fr

Venez nous rencontrer le 4 octobre lors du plus important ?v?nement
PostgreSQL francophone : http://www.pgday.fr

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFIjYTvA+REPKWGI0ERAlcZAKDSBc4vG2wBgkl35yPkXzppEmMizQCgpkWP
GrWhw8Obe4wQF9+R8o2sT0g=
=gend
-----END PGP SIGNATURE-----
From jcasanov at systemguards.com.ec  Mon Jul 28 11:16:01 2008
From: jcasanov at systemguards.com.ec (Jaime Casanova)
Date: Mon Jul 28 11:16:10 2008
Subject: [Slony1-general] Bug in documentation
Message-ID: <3073cc9b0807281116t1084737bnef189c8d03544613@mail.gmail.com>

Hi,

Is the example in this page
(http://lists.slony.info/documentation/stmtmergeset.html) right? it
seems not to me...
or am i missing something?

-- 
Atentamente,
Jaime Casanova
Soporte y capacitaci?n de PostgreSQL
Guayaquil - Ecuador
Cel. (593) 87171157
From cbbrowne at ca.afilias.info  Mon Jul 28 11:21:56 2008
From: cbbrowne at ca.afilias.info (chris)
Date: Mon Jul 28 11:22:06 2008
Subject: [Slony1-general] Bug in documentation
In-Reply-To: <3073cc9b0807281116t1084737bnef189c8d03544613@mail.gmail.com>
	(Jaime Casanova's message of "Mon, 28 Jul 2008 13:16:01 -0500")
References: <3073cc9b0807281116t1084737bnef189c8d03544613@mail.gmail.com>
Message-ID: <871w1dswcb.fsf@dba2.int.libertyrms.com>

"Jaime Casanova" <jcasanov@systemguards.com.ec> writes:
> Is the example in this page
> (http://lists.slony.info/documentation/stmtmergeset.html) right? it
> seems not to me...
> or am i missing something?

Nothing is leaping out at me as being wrong.

What do you think is wrong about it?
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From rafael.domiciano at gmail.com  Mon Jul 28 12:04:47 2008
From: rafael.domiciano at gmail.com (Rafael Domiciano)
Date: Mon Jul 28 12:04:58 2008
Subject: [Slony1-general] Problem with Parent Node
Message-ID: <3a0028490807281204p420e78e1j11bed9b5f85fac64@mail.gmail.com>

Hi People,

I have been using Slony-I like this: A -> B -> C
So, today, I create a new set between A -> B.. everything's ok.
I did the subscribe_set command of the new set to replication C.
But my replication C (Parent from B) get down, and begin to give me the
following:

2008-07-28 15:53:40 BRT CONFIG storeNode: no_id=3D1 no_comment=3D'Node 1 -
Postgres@localhost'
2008-07-28 15:53:40 BRT DEBUG2 setNodeLastEvent: no_id=3D1 event_seq=3D4425=
35
2008-07-28 15:53:40 BRT CONFIG storeNode: no_id=3D2 no_comment=3D'Node 2 -
Postgres@172.16.5.19'
2008-07-28 15:53:40 BRT DEBUG2 setNodeLastEvent: no_id=3D2 event_seq=3D87272
2008-07-28 15:53:40 BRT CONFIG storePath: pa_server=3D1 pa_client=3D3
pa_conninfo=3D"host=3Dlocalhost dbname=3DPostgres user=3Dpostgres port=3D54=
32"
pa_connretry=3D10
2008-07-28 15:53:40 BRT CONFIG storePath: pa_server=3D2 pa_client=3D3
pa_conninfo=3D"host=3D172.16.5.19 dbname=3DPostgres user=3Dpostgres port=3D=
5432"
pa_connretry=3D10
2008-07-28 15:53:40 BRT CONFIG storeListen: li_origin=3D2 li_receiver=3D3
li_provider=3D2
2008-07-28 15:53:40 BRT CONFIG storeListen: li_origin=3D2 li_receiver=3D3
li_provider=3D1
2008-07-28 15:53:40 BRT CONFIG storeListen: li_origin=3D1 li_receiver=3D3
li_provider=3D2
2008-07-28 15:53:40 BRT CONFIG storeSet: set_id=3D11 set_origin=3D1
set_comment=3D'Set 11 for replication_Postgres'
2008-07-28 15:53:40 BRT WARN   remoteWorker_wakeup: node 1 - no worker
thread
2008-07-28 15:53:40 BRT DEBUG2 sched_wakeup_node(): no_id=3D1 (0 threads +
worker signaled)
2008-07-28 15:53:40 BRT CONFIG storeSet: set_id=3D12 set_origin=3D1
set_comment=3D'Set 12 for replication_Postgres'
2008-07-28 15:53:40 BRT WARN   remoteWorker_wakeup: node 1 - no worker
thread
2008-07-28 15:53:40 BRT DEBUG2 sched_wakeup_node(): no_id=3D1 (0 threads +
worker signaled)
2008-07-28 15:53:40 BRT CONFIG storeSet: set_id=3D1 set_origin=3D1
set_comment=3D'Set 1 for replication_Postgres'

2008-07-28 15:53:41 BRT DEBUG1 syncThread: thread done
2008-07-28 15:53:41 BRT DEBUG1 localListenThread: thread done
2008-07-28 15:53:41 BRT DEBUG1 main: scheduler mainloop returned
2008-07-28 15:53:41 BRT DEBUG2 main: wait for remote threads
2008-07-28 15:53:41 BRT DEBUG2 sched_wakeup_node(): no_id=3D1 (0 threads +
worker signaled)
2008-07-28 15:53:41 BRT DEBUG2 remoteListenThread_1: queue event 2,87311
SYNC
2008-07-28 15:53:41 BRT DEBUG2 remoteWorker_event: ignore new events due to
shutdown
2008-07-28 15:53:41 BRT DEBUG2 remoteListenThread_1: queue event 2,87312
SYNC
2008-07-28 15:53:41 BRT DEBUG2 remoteWorker_event: ignore new events due to
shutdown
2008-07-28 15:53:41 BRT DEBUG1 remoteWorkerThread_1: helper thread for
provider 2 terminated
2008-07-28 15:53:41 BRT DEBUG2 remoteListenThread_1: queue event 2,87313
SYNC
2008-07-28 15:53:41 BRT DEBUG2 remoteWorker_event: ignore new events due to
shutdown
2008-07-28 15:53:41 BRT DEBUG1 remoteWorkerThread_1: thread done
2008-07-28 15:53:41 BRT DEBUG2 remoteListenThread_1: queue event 2,87314
SYNC
2008-07-28 15:53:41 BRT DEBUG2 remoteWorker_event: ignore new events due to
shutdown
2008-07-28 15:53:41 BRT DEBUG2 remoteListenThread_1: queue event 2,87315
SYNC
2008-07-28 15:53:41 BRT DEBUG2 remoteWorker_event: ignore new events due to
shutdown

Does anybody already get something like this?

Tnks, People.

Regards,

Rafael Domiciano
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080728/=
a766ea82/attachment.htm
From jcasanov at systemguards.com.ec  Mon Jul 28 12:41:09 2008
From: jcasanov at systemguards.com.ec (Jaime Casanova)
Date: Mon Jul 28 12:41:19 2008
Subject: [Slony1-general] Bug in documentation
In-Reply-To: <871w1dswcb.fsf@dba2.int.libertyrms.com>
References: <3073cc9b0807281116t1084737bnef189c8d03544613@mail.gmail.com>
	<871w1dswcb.fsf@dba2.int.libertyrms.com>
Message-ID: <3073cc9b0807281241s7fdc809me9854d7cc29e61b2@mail.gmail.com>

On Mon, Jul 28, 2008 at 1:21 PM, chris <cbbrowne@ca.afilias.info> wrote:
> "Jaime Casanova" <jcasanov@systemguards.com.ec> writes:
>> Is the example in this page
>> (http://lists.slony.info/documentation/stmtmergeset.html) right? it
>> seems not to me...
>> or am i missing something?
>
> Nothing is leaping out at me as being wrong.
>
> What do you think is wrong about it?


-- CODE MODE ON --
     # Assuming that set 1 has direct subscribers 2 and 3
     SUBSCRIBE SET (ID = 999, PROVIDER = 1, RECEIVER = 2);
     SYNC (ID=1);
     WAIT FOR EVENT (ORIGIN = 1, CONFIRMED = 2, WAIT FOR=1);
     SUBSCRIBE SET (ID = 999, PROVIDER = 1, RECEIVER = 3);
     SYNC (ID=1);
     WAIT FOR EVENT (ORIGIN = 1, CONFIRMED = 3, WAIT FOR=1);
     MERGE SET ( ID = 1, ADD ID = 999, ORIGIN = 1 );
-- CODE MODE OFF --

first WAIT FOR, shouldn't be WAIT ON?
second, what's the idea of merging the same set (ID=999)? is the idea
of this example a merge between nodes?




-- 
Atentamente,
Jaime Casanova
Soporte y capacitaci?n de PostgreSQL
Guayaquil - Ecuador
Cel. (593) 87171157
From cbbrowne at ca.afilias.info  Mon Jul 28 12:56:31 2008
From: cbbrowne at ca.afilias.info (chris)
Date: Mon Jul 28 12:56:42 2008
Subject: [Slony1-general] Bug in documentation
In-Reply-To: <3073cc9b0807281241s7fdc809me9854d7cc29e61b2@mail.gmail.com>
	(Jaime Casanova's message of "Mon, 28 Jul 2008 14:41:09 -0500")
References: <3073cc9b0807281116t1084737bnef189c8d03544613@mail.gmail.com>
	<871w1dswcb.fsf@dba2.int.libertyrms.com>
	<3073cc9b0807281241s7fdc809me9854d7cc29e61b2@mail.gmail.com>
Message-ID: <87r69drde8.fsf@dba2.int.libertyrms.com>

"Jaime Casanova" <jcasanov@systemguards.com.ec> writes:
> On Mon, Jul 28, 2008 at 1:21 PM, chris <cbbrowne@ca.afilias.info> wrote:
>> "Jaime Casanova" <jcasanov@systemguards.com.ec> writes:
>>> Is the example in this page
>>> (http://lists.slony.info/documentation/stmtmergeset.html) right? it
>>> seems not to me...
>>> or am i missing something?
>>
>> Nothing is leaping out at me as being wrong.
>>
>> What do you think is wrong about it?
>
>
> -- CODE MODE ON --
>      # Assuming that set 1 has direct subscribers 2 and 3
>      SUBSCRIBE SET (ID = 999, PROVIDER = 1, RECEIVER = 2);
>      SYNC (ID=1);
>      WAIT FOR EVENT (ORIGIN = 1, CONFIRMED = 2, WAIT FOR=1);
>      SUBSCRIBE SET (ID = 999, PROVIDER = 1, RECEIVER = 3);
>      SYNC (ID=1);
>      WAIT FOR EVENT (ORIGIN = 1, CONFIRMED = 3, WAIT FOR=1);
>      MERGE SET ( ID = 1, ADD ID = 999, ORIGIN = 1 );
> -- CODE MODE OFF --
>
> first WAIT FOR, shouldn't be WAIT ON?

Ah, yes.  Because "FOR" and "ON" are both reasonable looking words in
that context, that wasn't visible.

That change *is* in place in HEAD; someone already pointed it out; we
just didn't have a copy put online with the fix.

> second, what's the idea of merging the same set (ID=999)? is the
> idea of this example a merge between nodes?

No, the idea is that you have some new tables you want subscribed; you
set up the new set, #999, get it subscribed everywhere, and then merge
it into set #1, getting rid of set #999.

There are two SUBSCRIBE SET requests on set #999 because, as is stated
in the assumptions, set #1 has direct subscribers 2 and 3.  Set #999
must have identical subscription paths as Set #1 in order to be able
to merge the sets.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From singh.gurjeet at gmail.com  Mon Jul 28 16:42:19 2008
From: singh.gurjeet at gmail.com (Gurjeet Singh)
Date: Mon Jul 28 16:42:35 2008
Subject: [Slony1-general] A minor bug-fix left-out in version 1.5
Message-ID: <65937bea0807281642i23032dd0nc9d7fd412e59a17c@mail.gmail.com>

I notice that version 1.5 of the test_slony_state-dbi.pl script fixed a
problem where the diagnosis messages would not show which node the
long-running transactions were running. But the 'add_problem()' function
call of that code still has the first parameter as $origin, which I think is
a copy-paste error from previous blocks of 'while' loops. This causes the
emails sent out to be confusing as to which node the problem is being
reported for!

Here's a patch that fixes it. It also fixes another annoying this I have to
edit out every time I download this script to make it excutable; it removes
the # -*- perl -*- comment from the first line.

<patch>
*** test_slony_state-dbi.pl     2008-07-28 23:26:51.000000000 +0000
--- test_slony_state-dbi.pl.fixed       2008-07-28 23:32:12.000000000 +0000
***************
*** 1,4 ****
! #!/usr/bin/perl   # -*- perl -*-
  # $Id: test_slony_state-dbi.pl,v 1.6 2008/07/15 22:41:59 cbbrowne Exp $
  # Christopher Browne
  # Copyright 2005
--- 1,4 ----
! #!/usr/bin/perl
  # $Id: test_slony_state-dbi.pl,v 1.6 2008/07/15 22:41:59 cbbrowne Exp $
  # Christopher Browne
  # Copyright 2005
***************
*** 282,288 ****
    while (my @row =3D $res->fetchrow_array) {
      my ($db, $pid, $user, $age, $query) =3D @row;
      printf "%15s %15d %15s %12s %20s\n", $db, $pid, $user, $age, $query;
!       add_problem($origin, "Old Transactions Kept Open",
                  qq{Old Transaction still running with age $age >
$ELDERLY_TXN

  Query: $query
--- 282,288 ----
    while (my @row =3D $res->fetchrow_array) {
      my ($db, $pid, $user, $age, $query) =3D @row;
      printf "%15s %15d %15s %12s %20s\n", $db, $pid, $user, $age, $query;
!       add_problem($node, "Old Transactions Kept Open",
                  qq{Old Transaction still running with age $age >
$ELDERLY_TXN

  Query: $query
</patch>


Best regards,
-- =

gurjeet[.singh]@EnterpriseDB.com
singh.gurjeet@{ gmail | hotmail | indiatimes | yahoo }.com

EnterpriseDB http://www.enterprisedb.com

Mail sent from my BlackLaptop device
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080729/=
956b6657/attachment.htm
From dba at richyen.com  Mon Jul 28 17:57:13 2008
From: dba at richyen.com (Richard Yen)
Date: Mon Jul 28 17:57:31 2008
Subject: [Slony1-general] questions regarding slony_logshipper
Message-ID: <78CCB51E-38A2-4232-ADAE-32AA1762C89D@richyen.com>

Hi,

Just had a couple of questions as I was tinkering with the code to  
make it fit my requirements...

1.  Seems like slony_logshipper attempts to parse each logfile as it  
performs archscan()...is this necessary?  My understanding is that the  
slon daemon outputs all the logfiles, and slony_logshipper serves its  
purpose by passing them into psql--shouldn't the slon daemon be  
responsible enough to output properly-formatted SQL?  As it is right  
now, it looks like there's a big performance hit as slony_logshipper  
attempts to read each logfile.  Is there a significant design purpose  
for this?

2.  Once slony_logshipper reaches the end of its queue, it waits  
forever.  Is there a design purpose at afilias for not re-reading the  
archive directory?  Looks like it waits for me to feed it another  
logfile.  Perhaps an option/function to re-read the archive would be  
good?

Thanks!
--Richard
From nelsonwc7 at hotmail.com  Tue Jul 29 10:16:39 2008
From: nelsonwc7 at hotmail.com (Nelson Correia)
Date: Tue Jul 29 10:16:48 2008
Subject: [Slony1-general] Replication not working
In-Reply-To: <BD03EB36DFDDC34EBB38F4946C7BA40B011ADF8B@spolsbex01.sapo.corppt.com>
References: <BD03EB36DFDDC34EBB38F4946C7BA40B011ADF8B@spolsbex01.sapo.corppt.com>
Message-ID: <BAY103-W8E281A5BEEC1586C22AFFF9820@phx.gbl>

Hi,
 =

Today I've noticed that my replication environment become out of sync yeste=
rday. The last item replicated was dated "2008-07-28 16:08:50.790741+01" UT=
C. =

So, I've checked slony log on that slave and it was saying:
 =

----------------------
.....
2008-07-29 17:16:50 WEST ERROR  slon_connectdb: PQconnectdb("dbname=3D<my_d=
atabase> host=3D<my_ip> port=3D<my_port> user=3D<my_user> password=3D<my_pa=
ssword>") failed - could not create socket: Too many open files2008-07-29 1=
7:16:50 WEST ERROR  remoteWorkerThread_1: cannot connect to data provider 1=
 on 'dbname=3D=3D<my_database> host=3D<my_ip> port=3D<my_port> user=3D<my_u=
ser> password=3D<my_password>
----------------------
 =

Then, I've restarted the slon daemon, and now it keeps saying:
 =

----------------------
.......
2008-07-29 18:12:07 WEST DEBUG2 remoteListenThread_1: queue event 1,766485 =
SYNC2008-07-29 18:12:07 WEST DEBUG2 remoteListenThread_1: queue event 1,766=
486 SYNC2008-07-29 18:12:07 WEST DEBUG2 remoteListenThread_1: queue event 1=
,766487 SYNC2008-07-29 18:12:07 WEST DEBUG2 remoteListenThread_1: queue eve=
nt 1,766488 SYNC2008-07-29 18:12:07 WEST DEBUG2 remoteListenThread_1: queue=
 event 1,766489 SYNC2008-07-29 18:12:07 WEST DEBUG2 remoteListenThread_1: q=
ueue event 1,766490 SYNC2008-07-29 18:12:07 WEST DEBUG2 remoteListenThread_=
1: queue event 1,766491 SYNC2008-07-29 18:12:07 WEST DEBUG2 remoteListenThr=
ead_1: queue event 1,766492 SYNC2008-07-29 18:12:07 WEST DEBUG2 remoteListe=
nThread_1: queue event 1,766493 SYNC2008-07-29 18:12:07 WEST DEBUG2 remoteL=
istenThread_1: queue event 1,766494 SYNC2008-07-29 18:12:07 WEST DEBUG2 rem=
oteListenThread_1: queue event 1,766495 SYNC2008-07-29 18:12:07 WEST DEBUG2=
 remoteListenThread_1: queue event 1,766496 SYNC2008-07-29 18:12:07 WEST DE=
BUG2 remoteListenThread_1: queue event 1,766497 SYNC2008-07-29 18:12:07 WES=
T DEBUG2 remoteListenThread_1: queue event 1,766498 SYNC2008-07-29 18:12:07=
 WEST DEBUG2 remoteListenThread_1: queue event 1,766499 SYNC2008-07-29 18:1=
2:07 WEST DEBUG2 remoteListenThread_1: queue event 1,766500 SYNC2008-07-29 =
18:12:07 WEST DEBUG2 remoteListenThread_1: queue event 1,766501 SYNC2008-07=
-29 18:12:07 WEST DEBUG2 remoteListenThread_1: queue event 1,766502 SYNC200=
8-07-29 18:12:07 WEST DEBUG2 remoteListenThread_1: queue event 1,766503 SYN=
C2008-07-29 18:12:07 WEST DEBUG2 remoteListenThread_1: queue event 1,766504=
 SYNC2008-07-29 18:12:08 WEST DEBUG2 remoteHelperThread_1_1: 1.431 seconds =
delay for first row2008-07-29 18:12:08 WEST DEBUG4 remoteHelperThread_1_1: =
fetched 100 log rows2008-07-29 18:12:08 WEST DEBUG4 remoteHelperThread_1_1:=
 deliver 10 lines to worker2008-07-29 18:12:08 WEST DEBUG4 remoteHelperThre=
ad_1_1: allocate line buffers2008-07-29 18:12:08 WEST DEBUG4 remoteHelperTh=
read_1_1: fetch from cursor2008-07-29 18:12:08 WEST DEBUG4 remoteHelperThre=
ad_1_1: fetched 100 log rows2008-07-29 18:12:08 WEST DEBUG4 remoteHelperThr=
ead_1_1: deliver 10 lines to worker2008-07-29 18:12:08 WEST DEBUG4 remoteHe=
lperThread_1_1: allocate line buffers2008-07-29 18:12:08 WEST DEBUG4 remote=
HelperThread_1_1: fetch from cursor2008-07-29 18:12:08 WEST DEBUG2 slon: ch=
ild terminated status: 11; pid: 18234, current worker pid: 182342008-07-29 =
18:12:08 WEST DEBUG1 slon: restart of worker in 10 seconds
----------------------
And does not do any sync.
 =

Any idea of what could have happened and how to solve it?
 =

Thanks,
 =

Nelson
_________________________________________________________________
News, entertainment and everything you care about at Live.com. Get it now!
http://www.live.com/getstarted.aspx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080729/=
4e7b5ad0/attachment.htm
From stephane.schildknecht at postgresqlfr.org  Wed Jul 30 04:30:26 2008
From: stephane.schildknecht at postgresqlfr.org (=?UTF-8?B?IlN0w6lwaGFuZSBBLiBTY2hpbGRrbmVjaHQi?=)
Date: Wed Jul 30 04:30:56 2008
Subject: [Slony1-general] subscribe_set falling in error
Message-ID: <489050D2.3050908@postgresqlfr.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi,

Slony 1.2.13, PG 8.2.{7|9}

I'm facing a really strange and annoying situation.

After having unsubscribed a node (72) from replication of set 1 (issuing the
command "select _slonrep.unsubscribeset(1,72);" on node 72), I am trying to
resubscribe it to the set by issuing the command "select
_slonrep.subscribeset(1,71,72,'t');" on node 71.

Trouble is averything seems to go well to a certain extent... Indeed,
subscription is done for tables, data are copied, but then I get the following
message :

2008-07-30 11:05:13 CEST ERROR  remoteWorkerThread_1: "select
"_slonturf".setAddSequence_int(1, 2, '"public"."some_seq"', 'sequence
public.some_seq')" PGRES_FATAL_ERROR ERREUR: Slony-I: setAddSequence_int():
sequence ID 2 has already been assigned
2008-07-30 11:05:13 CEST WARN   remoteWorkerThread_1: data copy for set 1
failed - sleep 15 seconds

Or, this sequence was not present in table _slonrep.sl_sequence before I issued
the subscribe_set() command.
Why does slon consider it to be already present now ?
What can I do now ?

My replication scheme is as follows :

In cascade, let's assume 1 -> 71 -> 72, to summary.
72 is master for set 2 and last subscriber for set 1 (provider is 71, master is
node 1).


Thanks in advance.

Regards,
- --
St?phane Schildknecht
PostgreSQLFr : http://www.postgresql.fr

Venez nous rencontrer le 4 octobre lors du plus important ?v?nement
PostgreSQL francophone : http://www.pgday.fr

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFIkFDSA+REPKWGI0ERAnIyAKDgNotxDDcYxVmfykwAS2Sy9y5HLACdFBp4
5E8zqiQBAFT3qOK/3e2K3ZY=
=vPAc
-----END PGP SIGNATURE-----
From stephane.schildknecht at postgresqlfr.org  Wed Jul 30 09:05:28 2008
From: stephane.schildknecht at postgresqlfr.org (=?UTF-8?B?IlN0w6lwaGFuZSBBLiBTY2hpbGRrbmVjaHQi?=)
Date: Wed Jul 30 09:05:40 2008
Subject: [Slony1-general] Help needed - can't drop node
Message-ID: <48909148.2090706@postgresqlfr.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi again,

After having switched a set from one node to another, I
did uninstallnode() on my failed node.

I didn't see that the only node that took it into account was the new provider.
But, now, I can't let the other nodes take into account that this node does not
exist anymore. And I can't even drop that node, as it is still considered as a
provider.

Is there something (sql-ish, maybe) I can do to actually drop that node and let
the other go on working nicely ?

Recreating that single node is an option, recreating the whole replication
cluster is not...

Thanks in advance.

Best regards,
- --
St?phane Schildknecht
PostgreSQLFr : http://www.postgresql.fr

Venez nous rencontrer le 4 octobre lors du plus important ?v?nement
PostgreSQL francophone : http://www.pgday.fr

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFIkJFIA+REPKWGI0ERAgNDAJ4/g6wWwXaNrYekq6sWONpzTEVWdgCfa0JG
zJTVXeHnSIYWCMUQ6eQc+Fw=
=+tI5
-----END PGP SIGNATURE-----
From stefan at vocalocity.com  Wed Jul 30 12:33:07 2008
From: stefan at vocalocity.com (Stefan Murphy)
Date: Wed Jul 30 12:33:19 2008
Subject: [Slony1-general] how to do maintenance on a slave server?
In-Reply-To: <48909148.2090706@postgresqlfr.org>
References: <48909148.2090706@postgresqlfr.org>
Message-ID: <925169557BAB6947A70CB145F894A75B2ACDC6@mail-41ps.atlarge.net>

Hi all,

Warning novice alert, we've only been using Slony for about a month.

Last night we were doing OS level work on a Slony slave server (Linux).  This involved bouncing the box multiple times, at least once when the box was unresponsive in a load spike from a Postgres issue.  We didn't shut down the slave daemons which were running on this server (in retrospect probably a bad idea).  When the box restarts we have the daemons auto restarting.  

After doing this work we found errors in the slave's log.  Inserts failing because of duplicate values in primary key (non-Slony tables).  Slony would hang on these errors.  Odd in that these were transient tables with sequences as primary keys.  No data stays in these tables on a permanent basis.  We tried removing all rows for these problem tables in Slony logs in master.  We also tried truncating the Slony logs in the master.  We weren't worried about data integrity in this problem period, just that replication worked going forward.  We ended up removing the primary keys from the problem tables in the slave DB just to keep replication working for other critical tables.

We plan an recreating the slave DB and reinitializing replication to fix the problem.

In doing this type of maintenance should we be shutting down the slave daemons? (Yes, I've probably answered my own question, but would like to confirm.)

I'm interesting in your feedback about the things we were trying.  Were we on the right track or being horribly stupid?  :)

Any idea what was happening?  Observationally it was like Slony was trying to do the same transaction multiple times in the salve DB.

Thanks for your help,

Stefan
From michaelsalomon78 at gmail.com  Thu Jul 31 09:22:26 2008
From: michaelsalomon78 at gmail.com (mikymike)
Date: Thu Jul 31 09:22:34 2008
Subject: [Slony1-general] Strange thing happens after switchover
Message-ID: <18758313.post@talk.nabble.com>


I'm new to this forum so allow me to say hello to each and everyone =)

Here is my problem...

I have three nodes and one replicated table.. Node 1 is master for two
slaves nodes (2 and 3)

I use Slony 1.2.11.

Here is the inital config after I complete the setup of my cluster :

node_id | hostname
---------+----------
       1 | Master
       2 | Slave 1
       3 | Slave 2

 repset_id | schema_name | table_name | origin_node
-----------+-------------+------------+-------------
         1 | my_schema     | my_table  |           1

 sub_set | sub_provider | sub_receiver | sub_forward | sub_active
---------+--------------+--------------+-------------+------------
       1 |            1 |            2 | t           | t
       1 |            1 |            3 | t           | t

Then I perform a failover from node 1 to node 2, basically doing this :

        failover (id = 1, backup node = 2);
        echo 'Failover done';

        echo 'Subscribing node 3 to node 2 and wait for synchronization';
        subscribe set ( id = 1, provider = 2, receiver = 3, forward = yes);
        sync (id = 2);
        wait for event ( origin = 2, confirmed = 3, wait on = 2);
        echo 'Subscription complete !';

        echo 'Dropping master node';
        drop node (id=1, event node=2);

Config is now :


 node_id | hostname
---------+----------
       2 | Slave 1
       3 | Slave 2



 repset_id | schema_name | table_name | origin_node
-----------+-------------+------------+-------------
         1 | my_schema     | my_table  |           2


 sub_set | sub_provider | sub_receiver | sub_forward | sub_active
---------+--------------+--------------+-------------+------------
       1 |            2 |            3 | t           | t


Then I perform a switchover from node 2 to old master 1 :

        echo 'Creation of Master Node';
        store node (id=1, comment= 'Master', event node=2);

        echo 'Storage of all pathes';
        store path (server=1, client=2, conninfo='service=master1');
        store path (server=1, client=3, conninfo='service=master1');
        store path (server=2, client=1, conninfo='service=slave2');
        store path (server=2, client=3, conninfo='service=slave2');
        store path (server=3, client=1, conninfo='service=slave3');
        store path (server=3, client=2, conninfo='service=slave3');

        echo 'Set subscribtion for both slave nodes';
        subscribe set (id=1, provider=2, receiver= 1, forward= yes);
        sync (id = 2);
        wait for event (origin  = 2, confirmed = 1, wait on = 2);

        subscribe set (id=1, provider=2, receiver= 3, forward= yes);
        sync (id = 2);
        wait for event (origin  = 2, confirmed = 3, wait on = 2);

        echo 'Moving master from node 2 to node 1...';
        lock set (id=1, origin=2);
        move set (id=1, old origin=2, new origin=1);
        echo 'waiting for completion of move...';
        wait for event (origin = ALL, confirmed = ALL);
        echo 'switch over complete';

And now here is the new config I end up to :

 node_id | hostname
---------+----------
       1 | Master
       2 | Slave 1
       3 | Slave 2

 repset_id | schema_name | table_name | origin_node
-----------+-------------+------------+-------------
         1 | my_schema     | my_table  |           1

 sub_set | sub_provider | sub_receiver | sub_forward | sub_active
---------+--------------+--------------+-------------+------------
       1 |            2 |            3 | t           | t
       1 |            1 |            2 | t           | t

You can see that sub_set 1 has now two different providers, instead of only
1 ... I really wounder why node 2 is still provider for this set. I was
thinking it should only display sub_provider = 1... Why is that ?

It does not really affect the replication because at this stage node 2 is
actually a slave for this set, but still I wanted to know if this was normal
and if somehow my scripts were incorrect...

thanks a lot for your help !!

michael 
-- 
View this message in context: http://www.nabble.com/Strange-thing-happens-after-switchover-tp18758313p18758313.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From ahodgson at simkin.ca  Thu Jul 31 11:05:04 2008
From: ahodgson at simkin.ca (Alan Hodgson)
Date: Thu Jul 31 11:05:14 2008
Subject: [Slony1-general] how to do maintenance on a slave server?
In-Reply-To: <925169557BAB6947A70CB145F894A75B2ACDC6@mail-41ps.atlarge.net>
References: <48909148.2090706@postgresqlfr.org>
	<925169557BAB6947A70CB145F894A75B2ACDC6@mail-41ps.atlarge.net>
Message-ID: <200807311105.04297@hal.medialogik.com>

On Wednesday 30 July 2008, "Stefan Murphy" <stefan@vocalocity.com> wrote:
> After doing this work we found errors in the slave's log.  Inserts
> failing because of duplicate values in primary key (non-Slony tables). 
> Slony would hang on these errors. 

This statement is confusing. If they're non-Slony tables then how could 
replication have any issues with them?

-- 
Alan
From cbbrowne at ca.afilias.info  Thu Jul 31 11:30:49 2008
From: cbbrowne at ca.afilias.info (chris)
Date: Thu Jul 31 11:31:02 2008
Subject: [Slony1-general] how to do maintenance on a slave server?
In-Reply-To: <200807311105.04297@hal.medialogik.com> (Alan Hodgson's message
	of "Thu, 31 Jul 2008 11:05:04 -0700")
References: <48909148.2090706@postgresqlfr.org>
	<925169557BAB6947A70CB145F894A75B2ACDC6@mail-41ps.atlarge.net>
	<200807311105.04297@hal.medialogik.com>
Message-ID: <874p65q52e.fsf@dba2.int.libertyrms.com>

Alan Hodgson <ahodgson@simkin.ca> writes:
> On Wednesday 30 July 2008, "Stefan Murphy" <stefan@vocalocity.com> wrote:
>> After doing this work we found errors in the slave's log.  Inserts
>> failing because of duplicate values in primary key (non-Slony tables). 
>> Slony would hang on these errors. 
>
> This statement is confusing. If they're non-Slony tables then how could 
> replication have any issues with them?

Confusing indeed.

Slony-I sets up a connection lock so that only 1 slon can be managing
a particular node at a time, and then processes everything within
tranactions against both provider and subscriber, so it should cope
gracefully with any of these failures:

   If you shut down the [subscriber DB] while the slon is applying
   changes, you'll have an uncommitted transaction that will be rolled
   back.  No damage done.

Indeed, that should characterize things pretty well for a number of
sorts of failure modes other than [subscriber DB].  For instance, the
statement should continue to be valid if we replace [subscriber DB]
with:

- [slon process]
- [provider DB]

And it shouldn't be invalidated by the failure being anywhere in the
following set:
- Killing the slon process;
- Stopping the subscriber backend process by killing it cleanly;
- Stopping the subscriber backend process by killing it uncleanly, thereby
  requiring crash recovery when the postmaster restarts;
- Stopping the provider backend process by killing it cleanly;
- Stopping the provider backend process by killing it uncleanly, thereby
  requiring crash recovery when the postmaster restarts.

If the provider or subscriber hosts are shut down, there is a
possibility of corruption of the filesystem which might invalidate one
or another of the databases; Slony-I can't really help with that.

We've seen such corruptions emerge from the following sorts of
phenomena:

  - IBM HACMP failover captured requests on the SCSI bus and tried to
    re-apply them, trashing the filesystem + database;

  - A gradual power outage might leave some fading signals on the SCSI
    or fibrechannel bus as the computer "died," leading to the disk
    array getting phantom writes, trashing the filesystem + database.

Do any of these failure modes seem familiar?  (e.g. - indicative of
what happened here?)
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From yi.zhao at alibaba-inc.com  Thu Jul 31 20:11:14 2008
From: yi.zhao at alibaba-inc.com (Yi Zhao)
Date: Thu Jul 31 20:09:06 2008
Subject: [Slony1-general] after I initialize a set,
	can I add other table to it again?
Message-ID: <1217560274.3229.3.camel@localhost.localdomain>

hi, all:
I have create a slony set for replication and it works well. but, I
found that I forgot to add some table, so i want to add this table to
it, I create this script named 1.slon:

cluster name = replication;
 node 1 admin conninfo='host=localhost dbname=bbs user=yahoo port=5432';
 node 2 admin conninfo='host=db40.bbs.cnb dbname=bbs user=yahoo
port=5433';
 node 3 admin conninfo='host=db40.bbs.cnb dbname=bbs user=yahoo
port=5434';

# ADD TABLE 
  try {
    SET ADD TABLE (set id = 1, origin = 1, id = 1, fully qualified name
= 'content.thread_preference', comment = 'nodesc');
  } on error {
    echo 'Could not add table content.thread_preference for
replication!';
    exit -1;
  }
------------
but, when I do this: cat 1.slon | /usr/local/pgsql/bin/slonik, I got:
<stdin>:7: PGRES_FATAL_ERROR select "_replication".setAddTable(1, 1,
'content.thread_preference', 'thread_treference_pkey', 'nodesc');  -
ERROR:  Slony-I: cannot add table to currently subscribed set 1
<stdin>:9: Could not add table content.thread_preference for
replication!

what's wrong with it?

any help is appreciated.
thanks.


From jeff at frostconsultingllc.com  Thu Jul 31 20:19:10 2008
From: jeff at frostconsultingllc.com (Jeff Frost)
Date: Thu Jul 31 20:19:38 2008
Subject: [Slony1-general] after I initialize a set, can I add other table
	to it again?
In-Reply-To: <1217560274.3229.3.camel@localhost.localdomain>
References: <1217560274.3229.3.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0807312017390.4433@discord.home.frostconsultingllc.com>

On Fri, 1 Aug 2008, Yi Zhao wrote:

> ------------
> but, when I do this: cat 1.slon | /usr/local/pgsql/bin/slonik, I got:
> <stdin>:7: PGRES_FATAL_ERROR select "_replication".setAddTable(1, 1,
> 'content.thread_preference', 'thread_treference_pkey', 'nodesc');  -
> ERROR:  Slony-I: cannot add table to currently subscribed set 1
> <stdin>:9: Could not add table content.thread_preference for
> replication!
>
> what's wrong with it?

That's because slony doesn't allow you to directly add tables to a subscribed 
set.  You have to create a new temporary set, add the table(s) and sequence(s) 
to this new set, subscribe the set to the same nodes as the original set, then 
merge the set.  See the "12.1. Adding a table to replication" section of 
the docs here:

http://www.slony.info/documentation/addthings.html

-- 
Jeff Frost, Owner 	<jeff@frostconsultingllc.com>
Frost Consulting, LLC 	http://www.frostconsultingllc.com/
Phone: 650-780-7908	FAX: 650-649-1954
From yi.zhao at alibaba-inc.com  Thu Jul 31 22:10:06 2008
From: yi.zhao at alibaba-inc.com (Yi Zhao)
Date: Thu Jul 31 22:08:00 2008
Subject: [Slony1-general] after I initialize a set, can I add other
	table to it again?
In-Reply-To: <Pine.LNX.4.64.0807312017390.4433@discord.home.frostconsultingllc.com>
References: <1217560274.3229.3.camel@localhost.localdomain>
	<Pine.LNX.4.64.0807312017390.4433@discord.home.frostconsultingllc.com>
Message-ID: <1217567406.3229.5.camel@localhost.localdomain>

when I merge: I got, 

<stdin>:13: Subscription set 2 created
<stdin>:14: Adding tables to subscription set
<stdin>:18: PGRES_FATAL_ERROR select "_replication".mergeSet(1, 2);  -
ERROR:  Slony-I: subscriber lists of set 1 and 2 are different

:D

On Thu, 2008-07-31 at 20:19 -0700, Jeff Frost wrote:
> On Fri, 1 Aug 2008, Yi Zhao wrote:
> 
> > ------------
> > but, when I do this: cat 1.slon | /usr/local/pgsql/bin/slonik, I got:
> > <stdin>:7: PGRES_FATAL_ERROR select "_replication".setAddTable(1, 1,
> > 'content.thread_preference', 'thread_treference_pkey', 'nodesc');  -
> > ERROR:  Slony-I: cannot add table to currently subscribed set 1
> > <stdin>:9: Could not add table content.thread_preference for
> > replication!
> >
> > what's wrong with it?
> 
> That's because slony doesn't allow you to directly add tables to a subscribed 
> set.  You have to create a new temporary set, add the table(s) and sequence(s) 
> to this new set, subscribe the set to the same nodes as the original set, then 
> merge the set.  See the "12.1. Adding a table to replication" section of 
> the docs here:
> 
> http://www.slony.info/documentation/addthings.html
> 

From jeff at frostconsultingllc.com  Thu Jul 31 22:30:45 2008
From: jeff at frostconsultingllc.com (Jeff Frost)
Date: Thu Jul 31 22:31:12 2008
Subject: [Slony1-general] after I initialize a set, can I add other table
	to it again?
In-Reply-To: <1217567406.3229.5.camel@localhost.localdomain>
References: <1217560274.3229.3.camel@localhost.localdomain> 
	<Pine.LNX.4.64.0807312017390.4433@discord.home.frostconsultingllc.com>
	<1217567406.3229.5.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0807312230080.4433@discord.home.frostconsultingllc.com>

On Fri, 1 Aug 2008, Yi Zhao wrote:

> when I merge: I got,
>
> <stdin>:13: Subscription set 2 created
> <stdin>:14: Adding tables to subscription set
> <stdin>:18: PGRES_FATAL_ERROR select "_replication".mergeSet(1, 2);  -
> ERROR:  Slony-I: subscriber lists of set 1 and 2 are different

Looks like you didn't subscribe any nodes to set 2.  Set 2 has to have 
identical subscriptions to set 1 before it can merge with set 1.

-- 
Jeff Frost, Owner 	<jeff@frostconsultingllc.com>
Frost Consulting, LLC 	http://www.frostconsultingllc.com/
Phone: 650-780-7908	FAX: 650-649-1954
From yi.zhao at alibaba-inc.com  Thu Jul 31 23:08:30 2008
From: yi.zhao at alibaba-inc.com (Yi Zhao)
Date: Thu Jul 31 23:06:25 2008
Subject: [Slony1-general] after I initialize a set, can I add other
	table to it again?
In-Reply-To: <Pine.LNX.4.64.0807312230080.4433@discord.home.frostconsultingllc.com>
References: <1217560274.3229.3.camel@localhost.localdomain>
	<Pine.LNX.4.64.0807312017390.4433@discord.home.frostconsultingllc.com>
	<1217567406.3229.5.camel@localhost.localdomain>
	<Pine.LNX.4.64.0807312230080.4433@discord.home.frostconsultingllc.com>
Message-ID: <1217570910.3229.8.camel@localhost.localdomain>

yes, as you said, It's my fault that I haven't add any node to set.

now, I have do that, but, when merge, I got:
<stdin>:13: Subscription set 2 created
<stdin>:15: Adding tables to subscription set
<stdin>:26: Subscribe created
<stdin>:28: PGRES_FATAL_ERROR select "_replication".mergeSet(1, 2);  -
ERROR:  Slony-I: set 2 has subscriptions in progress - cannot merge


should I stop all the slon process???

thanks:D

regards,
On Thu, 2008-07-31 at 22:30 -0700, Jeff Frost wrote:
> On Fri, 1 Aug 2008, Yi Zhao wrote:
> 
> > when I merge: I got,
> >
> > <stdin>:13: Subscription set 2 created
> > <stdin>:14: Adding tables to subscription set
> > <stdin>:18: PGRES_FATAL_ERROR select "_replication".mergeSet(1, 2);  -
> > ERROR:  Slony-I: subscriber lists of set 1 and 2 are different
> 
> Looks like you didn't subscribe any nodes to set 2.  Set 2 has to have 
> identical subscriptions to set 1 before it can merge with set 1.
> 

From jeff at frostconsultingllc.com  Thu Jul 31 23:33:15 2008
From: jeff at frostconsultingllc.com (Jeff Frost)
Date: Thu Jul 31 23:33:55 2008
Subject: [Slony1-general] after I initialize a set, can I add other table
	to it again?
In-Reply-To: <1217570910.3229.8.camel@localhost.localdomain>
References: <1217560274.3229.3.camel@localhost.localdomain> 
	<Pine.LNX.4.64.0807312017390.4433@discord.home.frostconsultingllc.com> 
	<1217567406.3229.5.camel@localhost.localdomain> 
	<Pine.LNX.4.64.0807312230080.4433@discord.home.frostconsultingllc.com>
	<1217570910.3229.8.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0807312332580.4433@discord.home.frostconsultingllc.com>

On Fri, 1 Aug 2008, Yi Zhao wrote:

> yes, as you said, It's my fault that I haven't add any node to set.
>
> now, I have do that, but, when merge, I got:
> <stdin>:13: Subscription set 2 created
> <stdin>:15: Adding tables to subscription set
> <stdin>:26: Subscribe created
> <stdin>:28: PGRES_FATAL_ERROR select "_replication".mergeSet(1, 2);  -
> ERROR:  Slony-I: set 2 has subscriptions in progress - cannot merge
>
>
> should I stop all the slon process???

No, there is no need to stop the slons.

-- 
Jeff Frost, Owner 	<jeff@frostconsultingllc.com>
Frost Consulting, LLC 	http://www.frostconsultingllc.com/
Phone: 650-780-7908	FAX: 650-649-1954
From yi.zhao at alibaba-inc.com  Thu Jul 31 23:45:22 2008
From: yi.zhao at alibaba-inc.com (Yi Zhao)
Date: Thu Jul 31 23:43:21 2008
Subject: [Slony1-general] after I initialize a set, can I add other
	table to it again?
In-Reply-To: <Pine.LNX.4.64.0807312332580.4433@discord.home.frostconsultingllc.com>
References: <1217560274.3229.3.camel@localhost.localdomain>
	<Pine.LNX.4.64.0807312017390.4433@discord.home.frostconsultingllc.com>
	<1217567406.3229.5.camel@localhost.localdomain>
	<Pine.LNX.4.64.0807312230080.4433@discord.home.frostconsultingllc.com>
	<1217570910.3229.8.camel@localhost.localdomain>
	<Pine.LNX.4.64.0807312332580.4433@discord.home.frostconsultingllc.com>
Message-ID: <1217573122.3229.11.camel@localhost.localdomain>

ok, what should do to merge it? my script is below:

  try {
    create set (id = 2, origin = 1, comment = 'Set 2 for replication');
  } on error {
    echo 'Could not create subscription set 2 for replication';
    exit -1;
  }
  set add table (set id = 2, origin = 1, id = 523, full qualified name =
'content.thread_preference', comment = 'nodesc');

  try {
    subscribe set (id = 2, provider = 1, receiver = 2, forward = yes);
    subscribe set (id = 2, provider = 1, receiver = 3, forward = yes);
  } on error {
    echo 'Could not create subscribe for replication';
    exit -1;
  }

  merge set (id = 1, add id = 2, origin = 1);

  try {
        drop set (id = 2, origin = 1);
  } on error {
        exit 1;
  }
  echo 'Set 2 droped';

thanks again for your fast reply.

regards.

On Thu, 2008-07-31 at 23:33 -0700, Jeff Frost wrote:
> On Fri, 1 Aug 2008, Yi Zhao wrote:
> 
> > yes, as you said, It's my fault that I haven't add any node to set.
> >
> > now, I have do that, but, when merge, I got:
> > <stdin>:13: Subscription set 2 created
> > <stdin>:15: Adding tables to subscription set
> > <stdin>:26: Subscribe created
> > <stdin>:28: PGRES_FATAL_ERROR select "_replication".mergeSet(1, 2);  -
> > ERROR:  Slony-I: set 2 has subscriptions in progress - cannot merge
> >
> >
> > should I stop all the slon process???
> 
> No, there is no need to stop the slons.
> 

From jeff at frostconsultingllc.com  Thu Jul 31 23:49:21 2008
From: jeff at frostconsultingllc.com (Jeff Frost)
Date: Thu Jul 31 23:49:54 2008
Subject: [Slony1-general] after I initialize a set, can I add other table
	to it again?
In-Reply-To: <1217573122.3229.11.camel@localhost.localdomain>
References: <1217560274.3229.3.camel@localhost.localdomain> 
	<Pine.LNX.4.64.0807312017390.4433@discord.home.frostconsultingllc.com> 
	<1217567406.3229.5.camel@localhost.localdomain> 
	<Pine.LNX.4.64.0807312230080.4433@discord.home.frostconsultingllc.com> 
	<1217570910.3229.8.camel@localhost.localdomain> 
	<Pine.LNX.4.64.0807312332580.4433@discord.home.frostconsultingllc.com>
	<1217573122.3229.11.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0807312345300.4433@discord.home.frostconsultingllc.com>

On Fri, 1 Aug 2008, Yi Zhao wrote:

> ok, what should do to merge it? my script is below:
>
>  try {
>    create set (id = 2, origin = 1, comment = 'Set 2 for replication');
>  } on error {
>    echo 'Could not create subscription set 2 for replication';
>    exit -1;
>  }
>  set add table (set id = 2, origin = 1, id = 523, full qualified name =
> 'content.thread_preference', comment = 'nodesc');
>
>  try {
>    subscribe set (id = 2, provider = 1, receiver = 2, forward = yes);
>    subscribe set (id = 2, provider = 1, receiver = 3, forward = yes);
>  } on error {
>    echo 'Could not create subscribe for replication';
>    exit -1;
>  }
>
>  merge set (id = 1, add id = 2, origin = 1);
>
>  try {
>        drop set (id = 2, origin = 1);
>  } on error {
>        exit 1;
>  }
>  echo 'Set 2 droped';
>
> thanks again for your fast reply.


You don't need to drop it after you've merged it.  The 2nd set will go away 
after the merge is complete.

If you want to do it all in a script, you may need to add some slonik logic 
like this to wait for the subscription to complete before trying to merge:

SUBSCRIBE SET (id=2, provider=1, receiver=2, forward=yes);
WAIT FOR EVENT (origin=2, confirmed=1, wait on=2);
SYNC(id = 1);
WAIT FOR EVENT (origin=1, confirmed=2, wait on=1);

SUBSCRIBE SET (id=2, provider=2, receiver=3, forward=no);
WAIT FOR EVENT (origin=3, confirmed=2, wait on=3);
SYNC(id = 2);
WAIT FOR EVENT (origin=2, confirmed=3, wait on=2);

MERGE SET ( id = 1, add id = 2, origin = 1 );

I think you can wrap them with trys and echos yourself.


>
> regards.
>
> On Thu, 2008-07-31 at 23:33 -0700, Jeff Frost wrote:
>> On Fri, 1 Aug 2008, Yi Zhao wrote:
>>
>>> yes, as you said, It's my fault that I haven't add any node to set.
>>>
>>> now, I have do that, but, when merge, I got:
>>> <stdin>:13: Subscription set 2 created
>>> <stdin>:15: Adding tables to subscription set
>>> <stdin>:26: Subscribe created
>>> <stdin>:28: PGRES_FATAL_ERROR select "_replication".mergeSet(1, 2);  -
>>> ERROR:  Slony-I: set 2 has subscriptions in progress - cannot merge
>>>
>>>
>>> should I stop all the slon process???
>>
>> No, there is no need to stop the slons.
>>
>
>

-- 
Jeff Frost, Owner 	<jeff@frostconsultingllc.com>
Frost Consulting, LLC 	http://www.frostconsultingllc.com/
Phone: 650-780-7908	FAX: 650-649-1954
From chris at dba2.int.libertyrms.com  Wed Jul  2 09:24:16 2008
From: chris at dba2.int.libertyrms.com (chris)
Date: Thu Sep 25 08:26:57 2008
Subject: [Slony1-general] RE: how to prevent EXECUTE SCRIPT from locking
In-Reply-To: <1215006690.2834.17.camel@PCD12478> (Csaba Nagy's message of
	"Wed, 02 Jul 2008 15:51:30 +0200")
References: <e0d7c3f50807011255k2da89ba0mc179238296ec863a@mail.gmail.com>
	<20080702134431.GB32287@crankycanuck.ca>
	<1215006690.2834.17.camel@PCD12478>
Message-ID: <87d4lw1hf1.fsf@dba2.int.libertyrms.com>

Csaba Nagy <nagy@ecircle-ag.com> writes:
> On Wed, 2008-07-02 at 09:44 -0400, Andrew Sullivan wrote:
>> One other thing: if you really understand the bare metal functions Slony is
>> using, there actually _is_ a way to do this for just one table.
>
> If there would be a slony script to add columns to just 1 table
> reliably, and do all the necessary things transparently to the user, I
> would argue that 90% of the use cases of EXECUTE SCRIPT would be
> covered. Is that a scenario which can be covered reliably with a script
> and only minimal locking involved ?

At one point, I proposed such a feature; essentially, to add a slonik
command that would look something like:

   alter table (table id=14, alteration='add column foo integer', event node=4);

The idea wasn't accepted, but I'm still not convinced it wouldn't be
worthwhile, for the reasons you observe.
-- 
output = ("cbbrowne" "@" "linuxfinances.info")
http://linuxfinances.info/info/
"The real  romance is   out   ahead and   yet to come.    The computer
revolution hasn't started yet. Don't be misled by the enormous flow of
money into bad defacto standards for unsophisticated buyers using poor
adaptations of incomplete ideas." -- Alan Kay
From chris at dba2.int.libertyrms.com  Tue Jul  8 10:34:28 2008
From: chris at dba2.int.libertyrms.com (chris)
Date: Thu Sep 25 08:26:59 2008
Subject: [Slony1-general] how to prevent EXECUTE SCRIPT	from	locking
	alltables
In-Reply-To: <20080707160151.3F28D12C7004@zeus.directinfos.com> (Jacques
	Caron's message of "Mon, 07 Jul 2008 18:01:31 +0200")
References: <486A0070.2060600@echo.fr>
	<082D8A131DF72A4D88C908A1AD3DEB2204213AF8@mail-1.rf.lan>
	<20080702083140.GA12941@depesz.com>
	<20080707101625.C523D12C700E@zeus.directinfos.com>
	<48720CAA.5000405@echo.fr>
	<20080707133229.97F7B12C7005@zeus.directinfos.com>
	<487232F8.9030407@echo.fr>
	<20080707160151.3F28D12C7004@zeus.directinfos.com>
Message-ID: <871w2446w4.fsf@dba2.int.libertyrms.com>

Jacques Caron <jc@oxado.com> writes:
> At 17:15 07/07/2008, Cyril SCETBON wrote:
>> it seems that it was not working without doing this action (adding
>> the v) even if our primary key is the first column.
>
> Yes, the current implementation of Slony needs a letter for each
> column telling it whether it's a key or a value. If you don't have
> enough letters in the argument, at best it will be missing columns in
> the update, at worst it will segfault. I sent a patch a while ago that
> allows you to add columns without having to update the trigger
> arguments, you should be able to find in the archives.

Jacques, can you take a look at the bug report in Bugzilla?  I have
attached a patch based on present CVS HEAD which also adds in a test
to exercise the code (which worked fine for me).

It appears that the changes to src/backend/slony1_funcs.sql have
already made it into HEAD, so all that needs to go in is the change to
the function that generates the logtrigger.  I changed it to use rtrim
rather than the full regular expression "change"; rtrim's a tad
simpler :-).

There's actually a little further change; the function rtrim needs,
for safety, to be referenced as pg_catalog.rtrim(), not just rtrim().
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lsf.html
Rules  of the  Evil Overlord  #145. "My  dungeon cell  decor  will not
feature exposed pipes.  While they add to the  gloomy atmosphere, they
are good  conductors of vibrations and  a lot of  prisoners know Morse
code." <http://www.eviloverlord.com/>
From jerome at jfg-networks.net  Fri Jul 18 05:48:12 2008
From: jerome at jfg-networks.net (=?ISO-8859-1?Q?J=E9r=F4me_Jouanin?=)
Date: Thu Sep 25 08:27:01 2008
Subject: [Slony1-general] How to manually add column on big tables ?
Message-ID: <210e1e060807180548g2192bb8dna9b473418a2c23cb@mail.gmail.com>

Hi,
I have a classical (but beautiful) school case to submit to the community.
We have a slony1 (1.2.13) replication between 10 PGnodes (8.3).
For performance reasons, we have to add columns in our biggest tables
(approximatively 10 millions of rows).
On the finest hardware of these nodes, it takes a few hours.
Because of production constraints, the challenge is to minimize the time of
this operation, not to use slony's execute_script command, which first alter
the master and then replicates the schema change to the others nodes.
So we planned to alter the table manually on each nodes in parallel by :

   1. down write access
   2. down slony
   3. then in a same transaction :
   1. save the sequence of a column (select nextval)
      2. altertablerestore(tab_id) to deactivate slony triggers, restore
      tables in initial state and permit the ddl change onto the replicated
      databases
      3. create the new table table_new by select into (+ alter table alter
      column set not null)
      4. drop original table
      5. alter table table_new rename to table
      6. alter table table inherit
      7. create column's sequence
      8. alter table add constraint & index
      9. create table's trigger
      10. reinject sequence (select setval)
      11. update manually sl_table and sl_sequence with the new oid of the
      recreated objects
      12. altertableforreplication(tab_id) to replicate the table
   4. VACUUM ANALYZE table

In a dev environnement, datas after schema change are replicated with no
error, so it appears to run successfully.
But I ask for your knowledge : if anyone experienced altertablerestore /
altertableforreplication manually in similar conditions, does this plan
appears to be correct ?

Many thanx for your interest

J=E9r=F4me Jouanin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080718/=
13d375d3/attachment-0001.htm
From michaelsalomon78 at gmail.com  Thu Jul 31 08:12:28 2008
From: michaelsalomon78 at gmail.com (mikymike)
Date: Thu Sep 25 08:27:01 2008
Subject: [Slony1-general] Strange thing happens after switchover
Message-ID: <18756790.post@talk.nabble.com>


I'm new to this forum so allow me to say hello to each and everyone ;)

Here is my problem...

I have three nodes and one replicated table.. Node 1 is master for two
slaves nodes (2 and 3)

I use Slony 1.2.11.

Here is the inital config after I complete the setup of my cluster :

node_id | hostname
---------+----------
       1 | Master
       2 | Slave 1
       3 | Slave 2

 repset_id | schema_name | table_name | origin_node
-----------+-------------+------------+-------------
         1 | my_schema     | my_table  |           1

 sub_set | sub_provider | sub_receiver | sub_forward | sub_active
---------+--------------+--------------+-------------+------------
       1 |            1 |            2 | t           | t
       1 |            1 |            3 | t           | t

Then I perform a failover from node 1 to node 2, basically doing this :

        failover (id = 1, backup node = 2);
        echo 'Failover done';

        echo 'Subscribing node 3 to node 2 and wait for synchronization';
        subscribe set ( id = 1, provider = 2, receiver = 3, forward = yes);
        sync (id = 2);
        wait for event ( origin = 2, confirmed = 3, wait on = 2);
        echo 'Subscription complete !';

        echo 'Dropping master node';
        drop node (id=1, event node=2);

Config is now :


 node_id | hostname
---------+----------
       2 | Slave 1
       3 | Slave 2



 repset_id | schema_name | table_name | origin_node
-----------+-------------+------------+-------------
         1 | my_schema     | my_table  |           2


 sub_set | sub_provider | sub_receiver | sub_forward | sub_active
---------+--------------+--------------+-------------+------------
       1 |            2 |            3 | t           | t


Then I perform a switchover from node 2 to old master 1 :

        echo 'Creation of Master Node';
        store node (id=1, comment= 'Master', event node=2);

        echo 'Storage of all pathes';
        store path (server=1, client=2, conninfo='service=master1');
        store path (server=1, client=3, conninfo='service=master1');
        store path (server=2, client=1, conninfo='service=slave2');
        store path (server=2, client=3, conninfo='service=slave2');
        store path (server=3, client=1, conninfo='service=slave3');
        store path (server=3, client=2, conninfo='service=slave3');

        echo 'Set subscribtion for both slave nodes';
        subscribe set (id=1, provider=2, receiver= 1, forward= yes);
        sync (id = 2);
        wait for event (origin  = 2, confirmed = 1, wait on = 2);

        subscribe set (id=1, provider=2, receiver= 3, forward= yes);
        sync (id = 2);
        wait for event (origin  = 2, confirmed = 3, wait on = 2);

        echo 'Moving master from node 2 to node 1...';
        lock set (id=1, origin=2);
        move set (id=1, old origin=2, new origin=1);
        echo 'waiting for completion of move...';
        wait for event (origin = ALL, confirmed = ALL);
        echo 'switch over complete';

And now here is the new config I end up to :

 node_id | hostname
---------+----------
       1 | Master
       2 | Slave 1
       3 | Slave 2

 repset_id | schema_name | table_name | origin_node
-----------+-------------+------------+-------------
         1 | my_schema     | my_table  |           1

 sub_set | sub_provider | sub_receiver | sub_forward | sub_active
---------+--------------+--------------+-------------+------------
       1 |            2 |            3 | t           | t
       1 |            1 |            2 | t           | t

You can see that sub_set 1 has now two different providers, instead of only
1 ... I really wounder why node 2 is still provider for this set. I was
thinking it should only display sub_provider = 1... Why is that ?

It does not really affect the replication because at this stage node 2 is
actually a slave for this set, but still I wanted to know if this was normal
and if somehow my scripts were incorrect...

thanks a lot for your help !!

michael



-- 
View this message in context: http://www.nabble.com/Strange-thing-happens-after-switchover-tp18756790p18756790.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From lukecottier at cottierestate.co.nz  Tue Jul  1 01:29:25 2008
From: lukecottier at cottierestate.co.nz (lukusc)
Date: Thu Sep 25 08:27:27 2008
Subject: [Slony1-general] 'make' Error!  Please help. Tearing my hair out!
Message-ID: <18211648.post@talk.nabble.com>


Hi There.  I'm totally new to this and I don't know where to go next.

I am running the following...
Xserve PPC G5
OS X 10.4.11

I have tried installing both slony1-1.1.5 and slony1-1.1.9

I already have slony1-1.1.5 running on another Xserve along with
postgres-8.1.9 so I would prefer to stick with slony1-1.1.5 to avoid needing
to upgrade other servers.

I have successfully installed postgres-8.1.9 and postgresql-8.3.3 to run the
install against but I get the same error.

gcc -g -O2 -D_THREAD_SAFE  -I../.. -I/usr/local/pgsql/include/ -o slon
slon.o runtime_config.o local_listen.o remote_listen.o remote_worker.o
sync_thread.o cleanup_thread.o scheduler.o dbutils.o conf-file.o
confoptions.o misc.o -D_THREAD_SAFE  -L/usr/local/pgsql/lib/
-L/usr/local/pgsql/lib/ -lpq   -L/usr/local/pgsql/lib/ -lpq
/usr/bin/ld: Undefined symbols:
_pg_qsort
collect2: ld returned 1 exit status
make[2]: *** [slon] Error 1
make[1]: *** [all] Error 2
make: *** [all] Error 2

I'm using gcc powerpc-apple-darwin8-gcc-4.0.1 

Any help would be much appreciated.

Thanks in advance
-- 
View this message in context: http://www.nabble.com/%27make%27-Error%21--Please-help.-Tearing-my-hair-out%21-tp18211648p18211648.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From victor.aluko at gmail.com  Wed Jul  2 03:15:50 2008
From: victor.aluko at gmail.com (ajcity)
Date: Thu Sep 25 08:27:28 2008
Subject: [Slony1-general] Upgrading Postgresql 8.3.1 to 8.3.3: Any issues
	with Slony1-1.2.13
Message-ID: <18234291.post@talk.nabble.com>


  Hi all,
  I have a master node running PGSQL 8.3.1 and Slony 1.2.13  RHEL 3 and 2
nodes with the same PGSQL and Slony versions on OpenSuSE 10.3
  I wanna upgrade the Postgres to 8.3.3 on all 3 nodes, how do I do this
without having to drop my replication clusters and are there any known
issues with between Slony 1.2.13 and Postgresql 8.3.3?
  Do I also have to upgrade the Slony cos I wanna wait till Slony 2.0 is
fully released before doing this?
  
  Thanks for you help in advance.

   Victor
-- 
View this message in context: http://www.nabble.com/Upgrading-Postgresql-8.3.1-to-8.3.3%3A-Any-issues-with-Slony1-1.2.13-tp18234291p18234291.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From andrey.astakhov at gmail.com  Fri Jul  4 04:58:03 2008
From: andrey.astakhov at gmail.com (AndreyA)
Date: Thu Sep 25 08:27:29 2008
Subject: [Slony1-general] Unable to run slony because of different master
	and slave versions
Message-ID: <18278087.post@talk.nabble.com>


My master host works on FreeBSD 6.2. Allowed slony version for this OS is
1.2.12.
Slave host works on Redhat Linux Fedora 9. Slony version from rpm is 1.2.14.

Running slony on master i get following error:

<stdin>:57: loading of file /.../postgresql//slony1_funcs.sql:
PGRES_FATAL_ERROR ERROR:  Slonik version: 1.2.12 != Slony-I version in PG
build 1.2.1                                                           4
ERROR:  Slonik version: 1.2.12 != Slony-I version in PG build 1.2.14

The problem is: i can't upgrade slony on master neither from package nor
manually because of compilation errors. Moreover i can't downgrade slony on
slave - the same promlems (but another errors).

Could you suggest me a way to solve this problem?

-- 
View this message in context: http://www.nabble.com/Unable-to-run-slony-because-of-different-master-and-slave-versions-tp18278087p18278087.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From Robert.Landrum at corp.aol.com  Thu Jul 17 17:21:01 2008
From: Robert.Landrum at corp.aol.com (Robert Landrum)
Date: Thu Sep 25 08:27:30 2008
Subject: [Slony1-general] Dropped Table Issue
Message-ID: <487FE1DE.6030800@corp.aol.com>

We have a replicated table that was dropped from the master.  References 
still exist in sl_table, and the like...  SET DROP TABLE bombs now, and 
it makes it so I'm unable to re-add the table to replication because the 
name already exists in sl_table.

Is it safe to manually delete the references to my table?  i.e. go 
through sl_table, sl_table-pkey, and sl_trigger and delete the rows of 
the missing table?

Thanks,

Rob
