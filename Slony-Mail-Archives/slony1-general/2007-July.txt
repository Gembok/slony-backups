From pgsql at j-davis.com  Sun Jul  1 16:05:56 2007
From: pgsql at j-davis.com (Jeff Davis)
Date: Sun Jul  1 16:06:15 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
Message-ID: <1183331156.4240.11.camel@dogma.ljc.laika.com>

On Fri, 2007-06-29 at 23:57 -0700, Andrew Hammond wrote:
> A really interesting win would be in detecting cases where you can go from
> 
> WHERE id IN ( a list )
> 
> to
> 
> WHERE a < id AND id < b
> 
> However I think this is only possible at the time the transaction
> happens (how else will you know if your sequence is contigious. And
> that suggests to me that it's not reasonable to do at this time.
> 

If we move the data from the provider to a temp table on the receiver,
we could also use an IN query rather than a range. I don't know when
this would be a win, but it seems like it would be useful in some cases.

A range is much nicer, but like you say, it's harder to detect in a
deterministic way.

> Also, ISTM that the big reason we don't like statement based
> replication is that SQL has many non-deterministic aspects. However,
> there is probably a pretty darn big subset of SQL which is provably
> non-deterministic. And for that subset, would it be any less rigorous
> to transmit those statements than to transmit the per-row change
> statments like we currently do?
> 

The pgpool guys have done a lot of research on statement replication
already. I think it's a very interesting line of research that is good
in a lot of cases. It's worth thinking about parts of pgpool that would
be useful in slony.

Regards,
	Jeff Davis

From david at fetter.org  Mon Jul  2 08:38:07 2007
From: david at fetter.org (David Fetter)
Date: Mon Jul  2 08:38:16 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
Message-ID: <20070702153807.GF10257@fetter.org>

On Thu, Jun 28, 2007 at 06:17:25PM -0400, Christopher Browne wrote:
> The v2.0 branch has already taken on the following fairly significant
> changes (listed below).  Jan and I had a chat this afternoon about
> some items still to do:

[snip]

> - Clone Node - to allow using pg_dump/PITR to populate a new
>   subscriber node.
> 
> Are there more items we should try to add?

It would be really great to be able to promote a PITR node to a
subscriber, or better still, to be able to use pg_dump + some kind of
ancillary information to do same.  This would help a lot with the
initial sync problem that makes replicating large databases so
painful.

Cheers,
D
-- 
David Fetter <david@fetter.org> http://fetter.org/
phone: +1 415 235 3778        AIM: dfetter666
                              Skype: davidfetter

Remember to vote!
Consider donating to PostgreSQL: http://www.postgresql.org/about/donate
From JanWieck at Yahoo.com  Mon Jul  2 10:32:46 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon Jul  2 10:33:00 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <20070702153807.GF10257@fetter.org>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<20070702153807.GF10257@fetter.org>
Message-ID: <468936BE.7020505@Yahoo.com>

On 7/2/2007 11:38 AM, David Fetter wrote:
> On Thu, Jun 28, 2007 at 06:17:25PM -0400, Christopher Browne wrote:
>> The v2.0 branch has already taken on the following fairly significant
>> changes (listed below).  Jan and I had a chat this afternoon about
>> some items still to do:
> 
> [snip]
> 
>> - Clone Node - to allow using pg_dump/PITR to populate a new
>>   subscriber node.
>> 
>> Are there more items we should try to add?
> 
> It would be really great to be able to promote a PITR node to a
> subscriber, or better still, to be able to use pg_dump + some kind of
> ancillary information to do same.  This would help a lot with the
> initial sync problem that makes replicating large databases so
> painful.

You are aware that only PITR will avoid a long running transaction?

The stuff I am currently (very slowly) working on is that very problem. 
Any long running transaction causes that the minxid in the SYNC's is 
stuck at that very xid during the entire runtime of the LRT. The problem 
with that is that the log selection in the slon worker uses an index 
scan who's only index scankey candidates are the minxid of one and the 
maxxid of another snapshot. That is the range of rows returned by the 
scan itself. Since the minxid is stuck, it will select larger and larger 
groups of log tuples only to filter out most of them on a higher level 
in the query via xxid_le_snapshot().

While the LRT is in progress, we don't have ANY chance of doing 
something at all because nobody knows if that transaction does any 
changes to the database that the next SYNC after it committed has to 
pick up. But in case it aborts, we know it did not change anything. So 
my idea right now is to trim that snapshot information by removing 
aborted transactions from it. That way slon will only suffer from an 
increasing index scankey range while the LRT is in progress, but not any 
more after it aborted. This will at least take care of pg_dump and slony 
copy_set(), because both never commit. LRT's that really do updates will 
continue to cause that problem.

pg_dump can now be done against a subscriber, and all long running 
reporting and stuff people tend to move off to slaves anyway. So all 
that is really destined to create trouble are long running housekeeping 
transactions.


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From markokr at gmail.com  Mon Jul  2 10:45:52 2007
From: markokr at gmail.com (Marko Kreen)
Date: Mon Jul  2 10:45:58 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <468936BE.7020505@Yahoo.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<20070702153807.GF10257@fetter.org> <468936BE.7020505@Yahoo.com>
Message-ID: <e51f66da0707021045t761a5188h32750153c55c49a2@mail.gmail.com>

On 7/2/07, Jan Wieck <JanWieck@yahoo.com> wrote:
> The stuff I am currently (very slowly) working on is that very problem.
> Any long running transaction causes that the minxid in the SYNC's is
> stuck at that very xid during the entire runtime of the LRT. The problem
> with that is that the log selection in the slon worker uses an index
> scan who's only index scankey candidates are the minxid of one and the
> maxxid of another snapshot. That is the range of rows returned by the
> scan itself. Since the minxid is stuck, it will select larger and larger
> groups of log tuples only to filter out most of them on a higher level
> in the query via xxid_le_snapshot().

How the LRT problem is avoided in PGQ:

http://cvs.pgfoundry.org/cgi-bin/cvsweb.cgi/skytools/skytools/sql/pgq/functions/pgq.batch_event_sql.sql?rev=1.2&content-type=text/x-cvsweb-markup

Basic idea is that there are only few LRT's, so its reasonable
to pick up bottom half of range by event txid, one-by-one.

-- 
marko
From JanWieck at Yahoo.com  Mon Jul  2 11:03:20 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon Jul  2 11:03:30 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <e51f66da0707021045t761a5188h32750153c55c49a2@mail.gmail.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>	
	<20070702153807.GF10257@fetter.org> <468936BE.7020505@Yahoo.com>
	<e51f66da0707021045t761a5188h32750153c55c49a2@mail.gmail.com>
Message-ID: <46893DE8.6060202@Yahoo.com>

On 7/2/2007 1:45 PM, Marko Kreen wrote:
> On 7/2/07, Jan Wieck <JanWieck@yahoo.com> wrote:
>> The stuff I am currently (very slowly) working on is that very problem.
>> Any long running transaction causes that the minxid in the SYNC's is
>> stuck at that very xid during the entire runtime of the LRT. The problem
>> with that is that the log selection in the slon worker uses an index
>> scan who's only index scankey candidates are the minxid of one and the
>> maxxid of another snapshot. That is the range of rows returned by the
>> scan itself. Since the minxid is stuck, it will select larger and larger
>> groups of log tuples only to filter out most of them on a higher level
>> in the query via xxid_le_snapshot().
> 
> How the LRT problem is avoided in PGQ:
> 
> http://cvs.pgfoundry.org/cgi-bin/cvsweb.cgi/skytools/skytools/sql/pgq/functions/pgq.batch_event_sql.sql?rev=1.2&content-type=text/x-cvsweb-markup
> 
> Basic idea is that there are only few LRT's, so its reasonable
> to pick up bottom half of range by event txid, one-by-one.

Hmmm, that is an interesting idea. And it is (in contrast to what I've 
been playing with) node insensitive, since it doesn't need info only 
available on the event origin, like CLOG. Thanks.


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From JanWieck at Yahoo.com  Mon Jul  2 12:49:07 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon Jul  2 12:49:29 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <46893DE8.6060202@Yahoo.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>	
	<20070702153807.GF10257@fetter.org> <468936BE.7020505@Yahoo.com>
	<e51f66da0707021045t761a5188h32750153c55c49a2@mail.gmail.com>
	<46893DE8.6060202@Yahoo.com>
Message-ID: <468956B3.1000904@Yahoo.com>

On 7/2/2007 2:03 PM, Jan Wieck wrote:
> On 7/2/2007 1:45 PM, Marko Kreen wrote:
>> On 7/2/07, Jan Wieck <JanWieck@yahoo.com> wrote:
>>> The stuff I am currently (very slowly) working on is that very problem.
>>> Any long running transaction causes that the minxid in the SYNC's is
>>> stuck at that very xid during the entire runtime of the LRT. The problem
>>> with that is that the log selection in the slon worker uses an index
>>> scan who's only index scankey candidates are the minxid of one and the
>>> maxxid of another snapshot. That is the range of rows returned by the
>>> scan itself. Since the minxid is stuck, it will select larger and larger
>>> groups of log tuples only to filter out most of them on a higher level
>>> in the query via xxid_le_snapshot().
>> 
>> How the LRT problem is avoided in PGQ:
>> 
>> http://cvs.pgfoundry.org/cgi-bin/cvsweb.cgi/skytools/skytools/sql/pgq/functions/pgq.batch_event_sql.sql?rev=1.2&content-type=text/x-cvsweb-markup
>> 
>> Basic idea is that there are only few LRT's, so its reasonable
>> to pick up bottom half of range by event txid, one-by-one.
> 
> Hmmm, that is an interesting idea. And it is (in contrast to what I've 
> been playing with) node insensitive, since it doesn't need info only 
> available on the event origin, like CLOG. Thanks.

Not only is it interesting, but it is astonishing simple to adopt into 
our code. I want to do some more testing before I commit this change, 
but the really interesting thing here is that it is only a 3 line change 
in the remote_worker.c file, which could easily be backported into 1.2.

I had created a really pathetic test case here by SIGSTOP'ing the slon 
while doing the copy_set() for a day, so it had some 90000 events 
backlog. About a third into that backlog, it was down to 60+ seconds 
delay for first log row and due to the dynamic in the group size, doing 
that on a single event base. That same database is now moving through 
the backlog in batches of 5-8 minutes each, has a <1 second delay for 
first log row and does those groups in 50-70 seconds.

This looks very promising.


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From cbbrowne at mail.libertyrms.com  Mon Jul  2 12:53:18 2007
From: cbbrowne at mail.libertyrms.com (Christopher Browne)
Date: Mon Jul  2 12:53:34 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <468956B3.1000904@Yahoo.com> (Jan Wieck's message of "Mon,
	02 Jul 2007 15:49:07 -0400")
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<20070702153807.GF10257@fetter.org> <468936BE.7020505@Yahoo.com>
	<e51f66da0707021045t761a5188h32750153c55c49a2@mail.gmail.com>
	<46893DE8.6060202@Yahoo.com> <468956B3.1000904@Yahoo.com>
Message-ID: <60k5ti60r5.fsf@dba2.int.libertyrms.com>

Jan Wieck <JanWieck@Yahoo.com> writes:
> On 7/2/2007 2:03 PM, Jan Wieck wrote:
>> On 7/2/2007 1:45 PM, Marko Kreen wrote:
>>> On 7/2/07, Jan Wieck <JanWieck@yahoo.com> wrote:
>>>> The stuff I am currently (very slowly) working on is that very problem.
>>>> Any long running transaction causes that the minxid in the SYNC's is
>>>> stuck at that very xid during the entire runtime of the LRT. The problem
>>>> with that is that the log selection in the slon worker uses an index
>>>> scan who's only index scankey candidates are the minxid of one and the
>>>> maxxid of another snapshot. That is the range of rows returned by the
>>>> scan itself. Since the minxid is stuck, it will select larger and larger
>>>> groups of log tuples only to filter out most of them on a higher level
>>>> in the query via xxid_le_snapshot().
>>> How the LRT problem is avoided in PGQ:
>>> http://cvs.pgfoundry.org/cgi-bin/cvsweb.cgi/skytools/skytools/sql/pgq/functions/pgq.batch_event_sql.sql?rev=1.2&content-type=text/x-cvsweb-markup
>>> Basic idea is that there are only few LRT's, so its reasonable
>>> to pick up bottom half of range by event txid, one-by-one.
>> Hmmm, that is an interesting idea. And it is (in contrast to what
>> I've been playing with) node insensitive, since it doesn't need info
>> only available on the event origin, like CLOG. Thanks.
>
> Not only is it interesting, but it is astonishing simple to adopt into
> our code. I want to do some more testing before I commit this change,
> but the really interesting thing here is that it is only a 3 line
> change in the remote_worker.c file, which could easily be backported
> into 1.2.
>
> I had created a really pathetic test case here by SIGSTOP'ing the slon
> while doing the copy_set() for a day, so it had some 90000 events
> backlog. About a third into that backlog, it was down to 60+ seconds
> delay for first log row and due to the dynamic in the group size,
> doing that on a single event base. That same database is now moving
> through the backlog in batches of 5-8 minutes each, has a <1 second
> delay for first log row and does those groups in 50-70 seconds.
>
> This looks very promising.

Drew Hammond's keen on having some BSD-oriented scripts put into the
1.2 branch that I had only put into HEAD; this might be an excuse for
a 1.2.11.
-- 
let name="cbbrowne" and tld="linuxdatabases.info" in name ^ "@" ^ tld;;
http://linuxdatabases.info/info/linux.html
There are two kinds of people in the world: People who think there are
two kinds of people and people who don't.
From andrew.george.hammond at gmail.com  Mon Jul  2 16:50:47 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Mon Jul  2 16:51:11 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <60k5ti60r5.fsf@dba2.int.libertyrms.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<20070702153807.GF10257@fetter.org> <468936BE.7020505@Yahoo.com>
	<e51f66da0707021045t761a5188h32750153c55c49a2@mail.gmail.com>
	<46893DE8.6060202@Yahoo.com> <468956B3.1000904@Yahoo.com>
	<60k5ti60r5.fsf@dba2.int.libertyrms.com>
Message-ID: <5a0a9d6f0707021650j144f0d49r8b97f6e7ddd06463@mail.gmail.com>

> Drew Hammond's keen on having some BSD-oriented scripts put into the
> 1.2 branch that I had only put into HEAD; this might be an excuse for
> a 1.2.11.

s/BSD/djb daemontools/

Andrew
From wmoran at collaborativefusion.com  Mon Jul  2 19:54:35 2007
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Mon Jul  2 19:55:10 2007
Subject: [Slony1-general] timestamp with time zone insanity
Message-ID: <20070702225435.e4ea9a2e.wmoran@collaborativefusion.com>


Postgresl 8.1.9 replicating to 8.1.8 using Slony 1.2.6.

I have a number of "timestamp with time zone" columns that simply will
not replicate correctly and I'm stumped to the point of insanity as
to why.

On both systems, the database is set to est5edt timezone, yet some
rows (not all, mind you) change the timezone from edt to est when
the data is replicated.

It's always the same rows, but not all rows in the table.

This has obviously got something to do with the data, but I'm befuddled
as to what to do about it, or even how to determine the exact nature
of the problem.  New rows are being added to this table all the time
and they are replicated correctly, it seems as if it's always the same
rows that magically switch timezones during replication.

I've gone so far as to completely deinstall Slony from this database and
rebuild the Slony config from scratch, and it still does the exact same
thing.

Any help will be met with great appreciation.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From JanWieck at Yahoo.com  Mon Jul  2 20:15:38 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon Jul  2 20:16:22 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <60k5ti60r5.fsf@dba2.int.libertyrms.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>	<20070702153807.GF10257@fetter.org>
	<468936BE.7020505@Yahoo.com>	<e51f66da0707021045t761a5188h32750153c55c49a2@mail.gmail.com>	<46893DE8.6060202@Yahoo.com>
	<468956B3.1000904@Yahoo.com>
	<60k5ti60r5.fsf@dba2.int.libertyrms.com>
Message-ID: <4689BF5A.7060305@Yahoo.com>

On 7/2/2007 3:53 PM, Christopher Browne wrote:
> Jan Wieck <JanWieck@Yahoo.com> writes:
>> On 7/2/2007 2:03 PM, Jan Wieck wrote:
>>> On 7/2/2007 1:45 PM, Marko Kreen wrote:
>>>> On 7/2/07, Jan Wieck <JanWieck@yahoo.com> wrote:
>>>>> The stuff I am currently (very slowly) working on is that very problem.
>>>>> Any long running transaction causes that the minxid in the SYNC's is
>>>>> stuck at that very xid during the entire runtime of the LRT. The problem
>>>>> with that is that the log selection in the slon worker uses an index
>>>>> scan who's only index scankey candidates are the minxid of one and the
>>>>> maxxid of another snapshot. That is the range of rows returned by the
>>>>> scan itself. Since the minxid is stuck, it will select larger and larger
>>>>> groups of log tuples only to filter out most of them on a higher level
>>>>> in the query via xxid_le_snapshot().
>>>> How the LRT problem is avoided in PGQ:
>>>> http://cvs.pgfoundry.org/cgi-bin/cvsweb.cgi/skytools/skytools/sql/pgq/functions/pgq.batch_event_sql.sql?rev=1.2&content-type=text/x-cvsweb-markup
>>>> Basic idea is that there are only few LRT's, so its reasonable
>>>> to pick up bottom half of range by event txid, one-by-one.
>>> Hmmm, that is an interesting idea. And it is (in contrast to what
>>> I've been playing with) node insensitive, since it doesn't need info
>>> only available on the event origin, like CLOG. Thanks.
>>
>> Not only is it interesting, but it is astonishing simple to adopt into
>> our code. I want to do some more testing before I commit this change,
>> but the really interesting thing here is that it is only a 3 line
>> change in the remote_worker.c file, which could easily be backported
>> into 1.2.
>>
>> I had created a really pathetic test case here by SIGSTOP'ing the slon
>> while doing the copy_set() for a day, so it had some 90000 events
>> backlog. About a third into that backlog, it was down to 60+ seconds
>> delay for first log row and due to the dynamic in the group size,
>> doing that on a single event base. That same database is now moving
>> through the backlog in batches of 5-8 minutes each, has a <1 second
>> delay for first log row and does those groups in 50-70 seconds.
>>
>> This looks very promising.
> 
> Drew Hammond's keen on having some BSD-oriented scripts put into the
> 1.2 branch that I had only put into HEAD; this might be an excuse for
> a 1.2.11.

IF ... if ... (really if) ... this all checks out to do what it is 
supposed to do. We have to test if this sort of change does have any 
adverse side effects on slony installations running with hundreds of 
concurrent DB connections, for example. So far it only looks pretty 
against a simple N1->N2 setup bombarded with a -c5 pgbench. That isn't 
quite the testing you want to have done before committing such a 
substantial change in the inner core log selection logic of STABLE code, 
is it?


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From craig_james at emolecules.com  Mon Jul  2 20:52:16 2007
From: craig_james at emolecules.com (Craig James)
Date: Mon Jul  2 20:47:42 2007
Subject: [Slony1-general] timestamp with time zone insanity
In-Reply-To: <20070702225435.e4ea9a2e.wmoran@collaborativefusion.com>
References: <20070702225435.e4ea9a2e.wmoran@collaborativefusion.com>
Message-ID: <4689C7F0.5010202@emolecules.com>

Bill Moran wrote:
> Postgresl 8.1.9 replicating to 8.1.8 using Slony 1.2.6.
> 
> I have a number of "timestamp with time zone" columns that simply will
> not replicate correctly and I'm stumped to the point of insanity as
> to why.
> 
> On both systems, the database is set to est5edt timezone, yet some
> rows (not all, mind you) change the timezone from edt to est when
> the data is replicated.

This is just a wild guess, nothing to do with Slony ... make sure the kernel of both systems are operating on the same time zone.  You can have the OS running of one on GMT (common for database systems), and the OS of the other on est5edt.  You won't see this normally if you have your own TZ set in your $HOME environment, because on the GMT kernel, it will convert the system clock to your timezone, but on the est5edt kernel, it recognizes that your personal timezone is the same as the kernel's.  Everyone sees the correct time, even though the kernels are operating in different time zones.  Then when you start Postgres, which timezone does it inherit?  Usually the kernel's, but it depends how you start it.

Craig

From tarthurs at jobflash.com  Mon Jul  2 22:36:52 2007
From: tarthurs at jobflash.com (Tom Arthurs)
Date: Mon Jul  2 22:37:33 2007
Subject: [Slony1-general] timestamp with time zone insanity
In-Reply-To: <20070702225435.e4ea9a2e.wmoran@collaborativefusion.com>
References: <20070702225435.e4ea9a2e.wmoran@collaborativefusion.com>
Message-ID: <4689E074.60708@jobflash.com>

Here's another wild guess:

http://www.postgresql.org/docs/8.1/interactive/release.html#RELEASE-8-1-9


    E.1.2. Changes


    ....


    Fix POSIX-style timezone specs to follow new USA DST rules (Tom)


TIme zone rules changed in 8.1.9, and if the timestamps in question are 
between 3/11/07 and 4/1/07, then you got bit by this bug.  You can 
probably backpatch the 3.1.8 installation to have the correct timezone 
-- or just upgrade to 3.1.9, it's probably easier than patching.


Bill Moran wrote:
> Postgresl 8.1.9 replicating to 8.1.8 using Slony 1.2.6.
>
> I have a number of "timestamp with time zone" columns that simply will
> not replicate correctly and I'm stumped to the point of insanity as
> to why.
>
> On both systems, the database is set to est5edt timezone, yet some
> rows (not all, mind you) change the timezone from edt to est when
> the data is replicated.
>
> It's always the same rows, but not all rows in the table.
>
> This has obviously got something to do with the data, but I'm befuddled
> as to what to do about it, or even how to determine the exact nature
> of the problem.  New rows are being added to this table all the time
> and they are replicated correctly, it seems as if it's always the same
> rows that magically switch timezones during replication.
>
> I've gone so far as to completely deinstall Slony from this database and
> rebuild the Slony config from scratch, and it still does the exact same
> thing.
>
> Any help will be met with great appreciation.
>
>   

From wmoran at collaborativefusion.com  Tue Jul  3 04:51:30 2007
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Tue Jul  3 04:52:28 2007
Subject: [Slony1-general] timestamp with time zone insanity
In-Reply-To: <4689E074.60708@jobflash.com>
References: <20070702225435.e4ea9a2e.wmoran@collaborativefusion.com>
	<4689E074.60708@jobflash.com>
Message-ID: <20070703075130.ea219e1f.wmoran@collaborativefusion.com>

In response to Tom Arthurs <tarthurs@jobflash.com>:

> Here's another wild guess:
> 
> http://www.postgresql.org/docs/8.1/interactive/release.html#RELEASE-8-1-9
> 
> 
>     E.1.2. Changes
> 
> 
>     ....
> 
> 
>     Fix POSIX-style timezone specs to follow new USA DST rules (Tom)
> 
> 
> TIme zone rules changed in 8.1.9, and if the timestamps in question are 
> between 3/11/07 and 4/1/07, then you got bit by this bug.  You can 
> probably backpatch the 3.1.8 installation to have the correct timezone 
> -- or just upgrade to 3.1.9, it's probably easier than patching.

Looks like this is it, Tom.  Thanks for the reply.

> Bill Moran wrote:
> > Postgresl 8.1.9 replicating to 8.1.8 using Slony 1.2.6.
> >
> > I have a number of "timestamp with time zone" columns that simply will
> > not replicate correctly and I'm stumped to the point of insanity as
> > to why.
> >
> > On both systems, the database is set to est5edt timezone, yet some
> > rows (not all, mind you) change the timezone from edt to est when
> > the data is replicated.
> >
> > It's always the same rows, but not all rows in the table.
> >
> > This has obviously got something to do with the data, but I'm befuddled
> > as to what to do about it, or even how to determine the exact nature
> > of the problem.  New rows are being added to this table all the time
> > and they are replicated correctly, it seems as if it's always the same
> > rows that magically switch timezones during replication.
> >
> > I've gone so far as to completely deinstall Slony from this database and
> > rebuild the Slony config from scratch, and it still does the exact same
> > thing.
> >
> > Any help will be met with great appreciation.
> >
> >   
> 
> 
> 
> 
> 
> 
> 


-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023

****************************************************************
IMPORTANT: This message contains confidential information and is
intended only for the individual named. If the reader of this
message is not an intended recipient (or the individual
responsible for the delivery of this message to an intended
recipient), please be advised that any re-use, dissemination,
distribution or copying of this message is prohibited. Please
notify the sender immediately by e-mail if you have received
this e-mail by mistake and delete this e-mail from your system.
E-mail transmission cannot be guaranteed to be secure or
error-free as information could be intercepted, corrupted, lost,
destroyed, arrive late or incomplete, or contain viruses. The
sender therefore does not accept liability for any errors or
omissions in the contents of this message, which arise as a
result of e-mail transmission.
****************************************************************
From cbbrowne at ca.afilias.info  Tue Jul  3 09:33:12 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul  3 09:33:26 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	(Andrew Hammond's message of "Fri, 29 Jun 2007 23:57:47 -0700")
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
Message-ID: <60ved14fcn.fsf@dba2.int.libertyrms.com>

"Andrew Hammond" <andrew.george.hammond@gmail.com> writes:
> On 6/29/07, Christopher Browne <cbbrowne@mail.libertyrms.com> wrote:
> A really interesting win would be in detecting cases where you can go from
>
> WHERE id IN ( a list )
>
> to
>
> WHERE a < id AND id < b
>
> However I think this is only possible at the time the transaction
> happens (how else will you know if your sequence is contigious. And
> that suggests to me that it's not reasonable to do at this time.

That also seems near-nondeterministic in that we're capturing data
based on the state of things when the transactions (multiple!) happen,
on the data source, when the effects will be based on the state of
things on destination nodes, at a different point in time.

I'll see about doing an experiment on this to see if, for the DELETE
case, it seems to actually help.  It may be that the performance
effects are small to none, so that the added code complication isn't
worthwhile.

> Also, ISTM that the big reason we don't like statement based
> replication is that SQL has many non-deterministic aspects. However,
> there is probably a pretty darn big subset of SQL which is provably
> non-deterministic. And for that subset, would it be any less
> rigorous to transmit those statements than to transmit the per-row
> change statments like we currently do?

Well, by capturing the values, we have captured a deterministic form
of the update.

Jan and I had a chat last week on ideas of how to do "wilder
transformations" (e.g. - like adding/dropping columns, or of
replicating "WHERE FOO IN ('BAR')"); what we arrived at was that, in
such cases, what we'd need to do is to have custom 'logtrigger'
functions that would have full access to OLD.* and NEW.* (e.g. - the
two sets of columns, old and new), which would then use them, perhaps
with arbitrary complexity, construct sl_log_n entries.

The "fully general" logtrigger function would be *way* less efficient
than the present ones; you don't get complex transformations for free.

>> It would take some parsing of the log_cmddata to do this, nonetheless,
>> I think it ought to be possible to compress this into some smaller
>> number of queries.  Again, if we limited each query to process 100
>> tuples, at most, that would still seem like enough to call it a "win."
>
> I can see two places to find these wins. When the statement is parsed
> (probably very affordable) and, as you mentioned above, by inspecting
> the log tables. I think that we'd have to be pretty clever with the
> log tables to avoid having it get too expensive. I wonder if full text
> indexing with an "sql stemmer" might be clever way to index that data
> usefully.

I have a *small* regret in this; it would be very nice if data in
sl_log_[n].log_cmddata were split into two portions:

1.  For an INSERT, split between the column name list and the VALUES
    portion;

    You could, in principle, join together a set of VALUES entries for
    the same table as long as the list of column names match.

2.  For an UPDATE, split between the SET portion and the WHERE
    portion;

    You could, in principle, join together a set of entries which
    have identical SET portions by folding together the WHERE
    clauses.

3.  For DELETE, there's nothing to be split :-).

    It's trivial to fold DELETE requests together as I previously
    showed.

> Two downsides of the parser approach that I can see are
> 1) the postgresql parser / planner is already plenty complex
> 2) it doesn't group stuff across multiple statements

I don't see any possibility of using a parser-based approach; that
jumps us back into statement-based replication, which is susceptible
to nondeterminism problems.

Remember, the thought we started with was:
   "What if we could do something that would make mass operations less
    expensive?"

I don't want to introduce anything that can materially increase
processing costs.

The more intelligent we try to get, the more expensive the
logtrigger() function gets, and if the price is high enough, then we
gain nothing.

The only "win" I see is if we can opportunistically join some
statements together.  If we have to make the log trigger function
universally *WAY* more expensive, well, that's a performance loss :-(.
-- 
let name="cbbrowne" and tld="cbbrowne.com" in String.concat "@" [name;tld];;
http://cbbrowne.com/info/unix.html
Rules of the  Evil Overlord #207. "Employees will  have conjugal visit
trailers which  they may use provided  they call in  a replacement and
sign out on  the timesheet. Given this, anyone caught  making out in a
closet  while  leaving  their   station  unmonitored  will  be  shot."
<http://www.eviloverlord.com/>
From cbbrowne at ca.afilias.info  Tue Jul  3 09:39:39 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul  3 09:39:52 2007
Subject: [Slony1-general] PGXS and Slony-I 2.0
Message-ID: <60r6np4f1w.fsf@dba2.int.libertyrms.com>

One of the items on the "to do" list was to consider using PGXS.  

That was introduced in PostgreSQL 8.0, so could only be seriously
considered when 7.4 support was to be dropped.

Well, with v2.0, we are planning to drop 7.4, 8.0, 8.1, and 8.2, so
that certainly drops out 7.4 :-).

Seems to me that we ought to consider doing this now.  It looks likely
to simplify the configure script...
-- 
let name="cbbrowne" and tld="cbbrowne.com" in name ^ "@" ^ tld;;
http://cbbrowne.com/info/
Rules of  the Evil  Overlord #102.  "I will not  waste time  making my
enemy's death look  like an accident -- I'm  not accountable to anyone
and my other enemies wouldn't believe it.
From cbbrowne at ca.afilias.info  Tue Jul  3 11:12:24 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul  3 11:12:43 2007
Subject: [Slony1-general] timestamp with time zone insanity
In-Reply-To: <20070702225435.e4ea9a2e.wmoran@collaborativefusion.com> (Bill
	Moran's message of "Mon, 2 Jul 2007 22:54:35 -0400")
References: <20070702225435.e4ea9a2e.wmoran@collaborativefusion.com>
Message-ID: <60myyd4arb.fsf@dba2.int.libertyrms.com>

Bill Moran <wmoran@collaborativefusion.com> writes:
> Postgresl 8.1.9 replicating to 8.1.8 using Slony 1.2.6.
>
> I have a number of "timestamp with time zone" columns that simply will
> not replicate correctly and I'm stumped to the point of insanity as
> to why.
>
> On both systems, the database is set to est5edt timezone, yet some
> rows (not all, mind you) change the timezone from edt to est when
> the data is replicated.
>
> It's always the same rows, but not all rows in the table.
>
> This has obviously got something to do with the data, but I'm befuddled
> as to what to do about it, or even how to determine the exact nature
> of the problem.  New rows are being added to this table all the time
> and they are replicated correctly, it seems as if it's always the same
> rows that magically switch timezones during replication.
>
> I've gone so far as to completely deinstall Slony from this database and
> rebuild the Slony config from scratch, and it still does the exact same
> thing.
>
> Any help will be met with great appreciation.

It sounds like you've got a resolution (namely where there is
evidently differing interpretation of timezones between PG 8.1.8 and
8.1.9)...

Could I get some samples of timestamptz values that caused you grief,
to stow them in as sample data somewhere?  

I'm not sure that I'll necessarily set up a test case to be regularly
run to test this scenario, but adding a few tuples to one of the tests
is cheap, so we might as well have the data...
-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@" [name;tld];;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From pgsql at j-davis.com  Tue Jul  3 11:16:08 2007
From: pgsql at j-davis.com (Jeff Davis)
Date: Tue Jul  3 11:16:32 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <60ved14fcn.fsf@dba2.int.libertyrms.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com>
Message-ID: <1183486568.10735.14.camel@dogma.ljc.laika.com>

On Tue, 2007-07-03 at 12:33 -0400, Christopher Browne wrote:
> I'll see about doing an experiment on this to see if, for the DELETE
> case, it seems to actually help.  It may be that the performance
> effects are small to none, so that the added code complication isn't
> worthwhile.
> 

In a simple test I ran, DELETE of the entire 5M record table using
sequential scan was MUCH faster (9.41s) than 5M individual DELETE
statements in a single transaction (552.49s).

5M records is small enough to fit into memory. I expect the difference
would be even greater when the index and table can't both fit into
memory and the deletes are distributed randomly over the table.

I think it is worth exploring ways of solving this problem. Right now
slony is great for small inserts, updates, and deletes. But any large
update/delete on the origin can cause the subscribers to fall way
behind.

> Remember, the thought we started with was:
>    "What if we could do something that would make mass operations less
>     expensive?"
> 
> I don't want to introduce anything that can materially increase
> processing costs.
> 
> The more intelligent we try to get, the more expensive the
> logtrigger() function gets, and if the price is high enough, then we
> gain nothing.
> 
> The only "win" I see is if we can opportunistically join some
> statements together.  If we have to make the log trigger function
> universally *WAY* more expensive, well, that's a performance loss :-(.

Is there any way that we could detect (even without 100% confidence)
that a transaction is "big" and we should spend more effort trying to
optimize it?

I know that's a big project (in the general case), but there might be
some simple things that would work.

Regards,
	Jeff Davis


From cbbrowne at ca.afilias.info  Tue Jul  3 14:52:33 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul  3 14:52:56 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <1183486568.10735.14.camel@dogma.ljc.laika.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>	
	<1183151091.28247.44.camel@dogma.v10.wvs>	
	<608xa25sak.fsf@dba2.int.libertyrms.com>	
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>	
	<60ved14fcn.fsf@dba2.int.libertyrms.com>
	<1183486568.10735.14.camel@dogma.ljc.laika.com>
Message-ID: <468AC521.3000804@ca.afilias.info>

Jeff Davis wrote:
> On Tue, 2007-07-03 at 12:33 -0400, Christopher Browne wrote:
>   
>> I'll see about doing an experiment on this to see if, for the DELETE
>> case, it seems to actually help.  It may be that the performance
>> effects are small to none, so that the added code complication isn't
>> worthwhile.
>>
>>     
>
> In a simple test I ran, DELETE of the entire 5M record table using
> sequential scan was MUCH faster (9.41s) than 5M individual DELETE
> statements in a single transaction (552.49s).
>
> 5M records is small enough to fit into memory. I expect the difference
> would be even greater when the index and table can't both fit into
> memory and the deletes are distributed randomly over the table.
>
> I think it is worth exploring ways of solving this problem. Right now
> slony is great for small inserts, updates, and deletes. But any large
> update/delete on the origin can cause the subscribers to fall way
> behind.
>   
The "handy" alternative test (which would be a good "smoke test" for 
whether it's worth bothering to put *any* effort into this) would be to 
try to do some partial groupings to see if they'd help.

Thus, if the whole delete goes across the range id = 0 thru id = 
5000000, then things to try would be (each case involving 1 transaction):

1.  Delete with the 5M individual DELETE statements.  (Which you found 
took 552.49s)
2.  Delete with 50K DELETE statements, each having a WHERE clause with 
100 items in it.
3.  Delete with 5K DELETE statements, each having a WHERE clause with 1K 
items in it.

If 2. or 3. come *way* closer to 9.41s, then it may be worth exploring 
the complexity of folding together adjacent deletes on the same table.  
There could also be a case made for trying sequential versus random 
orderings (e.g. - in the former case, each DELETE statement takes on a 
specific range of items whereas in the latter, each selects items more 
or less at random).

I'll see about constructing a series of tests like this; won't be 
running before I send this :-).  If you have time to generate 2. and/or 
3., on your system and get timings there, I'd be much obliged.

;;;; Here's some relevant code :-)
(format t "begin;")
(loop for i from 0 to 49999
  do (format t "delete from foo where ")
  (loop for j from 0 to 99
       do (format t "id=~D or " (+ j (* i 100))))
  do (format t "id=~D;~%" (* i 100)))
(format t "commit;")

I don't think we can save the full 543 seconds, but if we could save a 
good portion of it, it's worth trying to pursue...

>> Remember, the thought we started with was:
>>    "What if we could do something that would make mass operations less
>>     expensive?"
>>
>> I don't want to introduce anything that can materially increase
>> processing costs.
>>
>> The more intelligent we try to get, the more expensive the
>> logtrigger() function gets, and if the price is high enough, then we
>> gain nothing.
>>
>> The only "win" I see is if we can opportunistically join some
>> statements together.  If we have to make the log trigger function
>> universally *WAY* more expensive, well, that's a performance loss :-(.
>>     
>
> Is there any way that we could detect (even without 100% confidence)
> that a transaction is "big" and we should spend more effort trying to
> optimize it?
>
>   
Regrettably, no.  For us to switch over to a sort of log trigger that 
supports "doing something smarter" requires that we add in logic that 
will have some (definitely non-zero) cost any time it *isn't* 
worthwhile.  And "usual sorts of OLTP activity" will fall into the 
category where performance would be injured.
> I know that's a big project (in the general case), but there might be
> some simple things that would work.
>   
Well, the cases I suggested (2. and 3.) would fall into "simple cases".
From cbbrowne at ca.afilias.info  Tue Jul  3 15:07:17 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul  3 15:07:39 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <1183486568.10735.14.camel@dogma.ljc.laika.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>	
	<1183151091.28247.44.camel@dogma.v10.wvs>	
	<608xa25sak.fsf@dba2.int.libertyrms.com>	
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>	
	<60ved14fcn.fsf@dba2.int.libertyrms.com>
	<1183486568.10735.14.camel@dogma.ljc.laika.com>
Message-ID: <468AC895.6060808@ca.afilias.info>

Jeff Davis wrote:
> On Tue, 2007-07-03 at 12:33 -0400, Christopher Browne wrote:
>   
>> I'll see about doing an experiment on this to see if, for the DELETE
>> case, it seems to actually help.  It may be that the performance
>> effects are small to none, so that the added code complication isn't
>> worthwhile.
>>
>>     
>
> In a simple test I ran, DELETE of the entire 5M record table using
> sequential scan was MUCH faster (9.41s) than 5M individual DELETE
> statements in a single transaction (552.49s).
>
>   
>   
I have a test running here...

cbbrowne@dba2:~/records/2007/2007-07-03> for i in *log; do
for> echo $i
for> echo "-------------------------------------------------------"
for> cat $i
for> echo "======================================================="
for> done
hundreds.log
-------------------------------------------------------
Started
Tue Jul  3 22:03:16 UTC 2007
=======================================================
onedelete.log
-------------------------------------------------------
Started
Tue Jul  3 21:57:04 UTC 2007
Started purging by onedelete
Tue Jul  3 21:58:41 UTC 2007
Completed deletions
Tue Jul  3 21:59:24 UTC 2007
=======================================================
thousands.log
-------------------------------------------------------
Started
Tue Jul  3 21:47:07 UTC 2007
Started purging by thousands
Tue Jul  3 21:47:22 UTC 2007
Completed deletions
Tue Jul  3 21:53:12 UTC 2007
=======================================================


My PC is evidently slower than yours; it took ~43s for the "one big delete"

Doing it in groups of 1K took 4:50 (e.g. - 4 minutes 50 seconds)

I'll be running against groups of 100 and against groups of 1 overnight; 
presumably both will be worse than my other numbers.  It'll be 
interesting to see how much worse than 4:50 it gets...
From cbbrowne at ca.afilias.info  Tue Jul  3 15:53:22 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul  3 15:53:46 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <468AC895.6060808@ca.afilias.info>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>		<1183151091.28247.44.camel@dogma.v10.wvs>		<608xa25sak.fsf@dba2.int.libertyrms.com>		<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>		<60ved14fcn.fsf@dba2.int.libertyrms.com>	<1183486568.10735.14.camel@dogma.ljc.laika.com>
	<468AC895.6060808@ca.afilias.info>
Message-ID: <468AD362.7090008@ca.afilias.info>

Christopher Browne wrote:
> I'll be running against groups of 100 and against groups of 1 
> overnight; presumably both will be worse than my other numbers.  It'll 
> be interesting to see how much worse than 4:50 it gets...

hundreds.log
-------------------------------------------------------
Started
Tue Jul  3 22:03:16 UTC 2007
Started purging by hundreds
Tue Jul  3 22:04:59 UTC 2007
Completed deletions
Tue Jul  3 22:08:14 UTC 2007
=======================================================
oneatatime.log
-------------------------------------------------------
Started
Tue Jul  3 22:08:15 UTC 2007
Started purging by oneatatime
Tue Jul  3 22:09:50 UTC 2007
Completed deletions
Tue Jul  3 22:25:01 UTC 2007
=======================================================
onedelete.log
-------------------------------------------------------
Started
Tue Jul  3 21:57:04 UTC 2007
Started purging by onedelete
Tue Jul  3 21:58:41 UTC 2007
Completed deletions
Tue Jul  3 21:59:24 UTC 2007
=======================================================
thousands.log
-------------------------------------------------------
Started
Tue Jul  3 21:47:07 UTC 2007
Started purging by thousands
Tue Jul  3 21:47:22 UTC 2007
Completed deletions
Tue Jul  3 21:53:12 UTC 2007
=======================================================


One at a time took about 15 minutes
100 at a time took 3:15
1000 at a time took longer than 100 at a time (curious, that!)
all in one shot took 43 seconds.

Something's fishy about the 1K at a time case; I'm rerunning that.  
Retrying, it took 4:22, again, longer than at 100/DELETE.

I expect that we run into some parsing overhead where doing 1000 at a 
shot imposes some burden (memory, query plan, whatever) that is greater 
than the savings gotten from cutting down on the number of queries.

There is definitely a material savings to be had; deleting a bunch of 
tuples in one query *does* provide a considerable savings over doing one 
at a time.  Whether it's worth implementing may be another question :-).
From ssinger_pg at sympatico.ca  Tue Jul  3 16:43:36 2007
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Tue Jul  3 16:44:07 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <60k5ti60r5.fsf@dba2.int.libertyrms.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<20070702153807.GF10257@fetter.org> <468936BE.7020505@Yahoo.com>
	<e51f66da0707021045t761a5188h32750153c55c49a2@mail.gmail.com>
	<46893DE8.6060202@Yahoo.com> <468956B3.1000904@Yahoo.com>
	<60k5ti60r5.fsf@dba2.int.libertyrms.com>
Message-ID: <Pine.LNX.4.62.0707031916320.5356@mini.atlantida.localdomain>



Is there anything we can do for 2.0 to improve DDL use cases?

I had sent out a patch a while back that lets EXECUTE SCRIPT take a list of 
tables to lock (thus not locking everything).  I never did get any feedback 
on the patch.  If there is interest I can try to bring it up to the current 
2.0 head and resend it.

Do the new was of disabling triggers have any effect on the dangers of doing 
alter tables outside of an execute script? Is there anything we can do to 
improve that (at least for DDL where the order it is applied on doesn't need 
to match on the slaves)


Steve

From dmitry at koterov.ru  Wed Jul  4 01:03:11 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Wed Jul  4 01:04:00 2007
Subject: [Slony1-general] Updates without any data are added to sl_log_1,
	why?
Message-ID: <d7df81620707040103s6d532e6cu5a6a6001509ffa78@mail.gmail.com>

Hello.

Suppose I perform the query on a Master:

UPDATE tbl SET tbl_id=3Dtbl_id WHERE tbl_id=3D123;

Its purpose is just to all all associated triggers for the row 123 in tbl.
Then I watch sl_log_1 and see the following event in it:

log_cmddata=3D"tbl_id=3D123 WHERE tbl_id=3D123"

The question is: why? Slaves do not need to perform this query at all, but
it is propagated to them.

I suppose that Slony generates log_cmddata comparing OLD and NEW
field-by-field and then - insert only differences to log_cmddata. (If it is
not true, and Slony performs a different algorythm, please detalize.) But,
if there is no differences between OLD and NEW (as in above sample), no need
to generate an event at all...
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070704/=
a5625036/attachment.htm
From nagy at ecircle-ag.com  Wed Jul  4 01:20:22 2007
From: nagy at ecircle-ag.com (Csaba Nagy)
Date: Wed Jul  4 01:21:13 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <468AD362.7090008@ca.afilias.info>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com>
	<1183486568.10735.14.camel@dogma.ljc.laika.com>
	<468AC895.6060808@ca.afilias.info> <468AD362.7090008@ca.afilias.info>
Message-ID: <1183537222.28714.27.camel@coppola.muc.ecircle.de>

On Wed, 2007-07-04 at 00:53, Christopher Browne wrote:
> One at a time took about 15 minutes
> 100 at a time took 3:15
> 1000 at a time took longer than 100 at a time (curious, that!)
> all in one shot took 43 seconds.

Check the plans for the 100 vs. 1000 cases: I'm pretty sure 100 goes for
bitmap index scan and 1000 goes for sequential scan... and 10 * 100
bitmap index scans are probably somewhat faster than 1 sequential scan
on your table/box. I guess 1000 is close to the limit between the
performance turnover between the index scan and sequential scan on your
table/box/setup, but the sequential scan is slightly underestimated by
the planner.

BTW, the bitmap index scan case should theoretically be the fastest, so
aiming for the highest chunk size where the planner still chooses bitmap
index scan (or downright forcing it to do so if possible) would give the
best performance.

Cheers,
Csaba.


From JanWieck at Yahoo.com  Wed Jul  4 07:10:08 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed Jul  4 07:10:29 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <60ved14fcn.fsf@dba2.int.libertyrms.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>	<1183151091.28247.44.camel@dogma.v10.wvs>	<608xa25sak.fsf@dba2.int.libertyrms.com>	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com>
Message-ID: <468BAA40.1030505@Yahoo.com>

On 7/3/2007 12:33 PM, Christopher Browne wrote:
> "Andrew Hammond" <andrew.george.hammond@gmail.com> writes:

>> Also, ISTM that the big reason we don't like statement based
>> replication is that SQL has many non-deterministic aspects. However,
>> there is probably a pretty darn big subset of SQL which is provably
>> non-deterministic. And for that subset, would it be any less
>> rigorous to transmit those statements than to transmit the per-row
>> change statments like we currently do?
> 
> Well, by capturing the values, we have captured a deterministic form
> of the update.

How to figure out what is deterministic and what isn't? A simple

     insert into summary select id, sum(value) from detail group by id;

seems pretty deterministic, doesn't it? But the result of it depends on 
the exact commit order and the transaction isolation level. We don't 
capture the commit order of single transactions, nor do we care for it 
anywhere in the Slony-I logic.


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From ajs at crankycanuck.ca  Wed Jul  4 07:44:51 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Jul  4 07:45:20 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <468BAA40.1030505@Yahoo.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com>
	<468BAA40.1030505@Yahoo.com>
Message-ID: <20070704144451.GA14262@phlogiston.dyndns.org>

On Wed, Jul 04, 2007 at 10:10:08AM -0400, Jan Wieck wrote:
> seems pretty deterministic, doesn't it? But the result of it depends on 
> the exact commit order and the transaction isolation level. We don't 
> capture the commit order of single transactions, nor do we care for it 
> anywhere in the Slony-I logic.

I think this is key.  The current arrangement solves the problem
where the visibility rules as they were in force on the origin are
followed while applying on the replica.  You're going to need to do
quite a bit of theoretical work here to show that the agreeable order
rules are followed in any grouping approach you take.  Please see the
original concept paper on this exact point.  MVCC is hard.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
However important originality may be in some fields, restraint and 
adherence to procedure emerge as the more significant virtues in a 
great many others.   --Alain de Botton
From cbbrowne at ca.afilias.info  Wed Jul  4 07:52:59 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul  4 07:53:14 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <468BAA40.1030505@Yahoo.com> (Jan Wieck's message of "Wed,
	04 Jul 2007 10:10:08 -0400")
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com> <468BAA40.1030505@Yahoo.com>
Message-ID: <60ir9043w4.fsf@dba2.int.libertyrms.com>

Jan Wieck <JanWieck@Yahoo.com> writes:
> On 7/3/2007 12:33 PM, Christopher Browne wrote:
>> "Andrew Hammond" <andrew.george.hammond@gmail.com> writes:
>
>>> Also, ISTM that the big reason we don't like statement based
>>> replication is that SQL has many non-deterministic aspects. However,
>>> there is probably a pretty darn big subset of SQL which is provably
>>> non-deterministic. And for that subset, would it be any less
>>> rigorous to transmit those statements than to transmit the per-row
>>> change statments like we currently do?
>> Well, by capturing the values, we have captured a deterministic form
>> of the update.
>
> How to figure out what is deterministic and what isn't? A simple
>
>     insert into summary select id, sum(value) from detail group by id;
>
> seems pretty deterministic, doesn't it? But the result of it depends
> on the exact commit order and the transaction isolation level. We
> don't capture the commit order of single transactions, nor do we care
> for it anywhere in the Slony-I logic.

But at the time that we apply these changes in log_actionseq order, we
have imposed a deterministic order.  (Which happens to be repeatable,
on each node.)
-- 
"cbbrowne","@","linuxfinances.info"
http://linuxdatabases.info/info/lisp.html
Do not worry  about the bullet that  has got your name on  it. It will
hit you and it will kill  you, no questions asked. The rounds to worry
about are the ones marked: TO WHOM IT MAY CONCERN.
From cbbrowne at ca.afilias.info  Wed Jul  4 07:56:57 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul  4 07:57:05 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <20070704144451.GA14262@phlogiston.dyndns.org> (Andrew Sullivan's
	message of "Wed, 4 Jul 2007 10:44:51 -0400")
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com> <468BAA40.1030505@Yahoo.com>
	<20070704144451.GA14262@phlogiston.dyndns.org>
Message-ID: <60ejjo43pi.fsf@dba2.int.libertyrms.com>

Andrew Sullivan <ajs@crankycanuck.ca> writes:
> On Wed, Jul 04, 2007 at 10:10:08AM -0400, Jan Wieck wrote:
>> seems pretty deterministic, doesn't it? But the result of it depends on 
>> the exact commit order and the transaction isolation level. We don't 
>> capture the commit order of single transactions, nor do we care for it 
>> anywhere in the Slony-I logic.
>
> I think this is key.  The current arrangement solves the problem
> where the visibility rules as they were in force on the origin are
> followed while applying on the replica.  You're going to need to do
> quite a bit of theoretical work here to show that the agreeable order
> rules are followed in any grouping approach you take.  Please see the
> original concept paper on this exact point.  MVCC is hard.

The only change I'd propose in handling grouping is to
opportunistically see if there are consecutive operations that may be
trivially joined together.

In effect, if the *old* logic generated the sequence of queries:

  delete from my_table where id = 25;
  delete from my_table where id = 82;
  delete from another_table where id = 19;
  delete from my_table where id = 45;

then there is only one "joining" possible, which is to combine the
first two delete queries into one, so that the overall sequence of
queries becomes thus:

  delete from my_table where id = 25 or id = 82;
  delete from another_table where id = 19;
  delete from my_table where id = 45;

That doesn't change anything about "agreeable ordering" as far as I
can see.

Of course, that example isn't much of a "win."  What would be way more
interesting (from a performance perspective) is the case where there
are 25 deletes in a row from my_table that could be folded together.
-- 
(reverse (concatenate 'string "ofni.sesabatadxunil" "@" "enworbbc"))
http://linuxdatabases.info/info/sgml.html
Health is merely the slowest possible rate at which one can die.
From cbbrowne at ca.afilias.info  Wed Jul  4 08:13:53 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul  4 08:14:03 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <1183537222.28714.27.camel@coppola.muc.ecircle.de>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>	
	<1183151091.28247.44.camel@dogma.v10.wvs>	
	<608xa25sak.fsf@dba2.int.libertyrms.com>	
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>	
	<60ved14fcn.fsf@dba2.int.libertyrms.com>	
	<1183486568.10735.14.camel@dogma.ljc.laika.com>	
	<468AC895.6060808@ca.afilias.info>
	<468AD362.7090008@ca.afilias.info>
	<1183537222.28714.27.camel@coppola.muc.ecircle.de>
Message-ID: <468BB931.3070101@ca.afilias.info>

Csaba Nagy wrote:
> On Wed, 2007-07-04 at 00:53, Christopher Browne wrote:
>   
>> One at a time took about 15 minutes
>> 100 at a time took 3:15
>> 1000 at a time took longer than 100 at a time (curious, that!)
>> all in one shot took 43 seconds.
>>     
>
> Check the plans for the 100 vs. 1000 cases: I'm pretty sure 100 goes for
> bitmap index scan and 1000 goes for sequential scan... and 10 * 100
> bitmap index scans are probably somewhat faster than 1 sequential scan
> on your table/box. I guess 1000 is close to the limit between the
> performance turnover between the index scan and sequential scan on your
> table/box/setup, but the sequential scan is slightly underestimated by
> the planner.
>
> BTW, the bitmap index scan case should theoretically be the fastest, so
> aiming for the highest chunk size where the planner still chooses bitmap
> index scan (or downright forcing it to do so if possible) would give the
> best performance.
>   
Interestingly, the query plans for 100 and 1000 are much the same:

-  Bitmap Heap Scan on foo atop
    100 (or 1000) ORed Bitmap Index Scans.

I was fully expecting to discover some differing query plan; it was 
quite a surprise to discover both:

a) That the plans were essentially identical, and
b) That it's more expensive to delete 1K tuples at a shot than 100.
From nagy at ecircle-ag.com  Wed Jul  4 08:29:11 2007
From: nagy at ecircle-ag.com (Csaba Nagy)
Date: Wed Jul  4 08:29:22 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <468BB931.3070101@ca.afilias.info>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com>
	<1183486568.10735.14.camel@dogma.ljc.laika.com>
	<468AC895.6060808@ca.afilias.info> <468AD362.7090008@ca.afilias.info>
	<1183537222.28714.27.camel@coppola.muc.ecircle.de>
	<468BB931.3070101@ca.afilias.info>
Message-ID: <1183562951.28714.49.camel@coppola.muc.ecircle.de>

On Wed, 2007-07-04 at 17:13, Christopher Browne wrote:
> Interestingly, the query plans for 100 and 1000 are much the same:
> 
> -  Bitmap Heap Scan on foo atop
>     100 (or 1000) ORed Bitmap Index Scans.
> 
> I was fully expecting to discover some differing query plan; it was 
> quite a surprise to discover both:
> 
> a) That the plans were essentially identical, and
> b) That it's more expensive to delete 1K tuples at a shot than 100.

OK, that surprises me too... and it would be interesting to know why is
that so ? It is completely counterintuitive...

Cheers,
Csaba.


From ajs at crankycanuck.ca  Wed Jul  4 08:35:15 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Jul  4 08:35:30 2007
Subject: [Slony1-general] Updates without any data are added to sl_log_1,
	why?
In-Reply-To: <d7df81620707040103s6d532e6cu5a6a6001509ffa78@mail.gmail.com>
References: <d7df81620707040103s6d532e6cu5a6a6001509ffa78@mail.gmail.com>
Message-ID: <20070704153515.GD14262@phlogiston.dyndns.org>

On Wed, Jul 04, 2007 at 12:03:11PM +0400, Dmitry Koterov wrote:
> log_cmddata="tbl_id=123 WHERE tbl_id=123"
> 
> The question is: why? Slaves do not need to perform this query at all, but
> it is propagated to them.

Slony can't know that, can it?

> I suppose that Slony generates log_cmddata comparing OLD and NEW
> field-by-field and then - insert only differences to log_cmddata. (If it is
> not true, and Slony performs a different algorythm, please detalize.) But,
> if there is no differences between OLD and NEW (as in above sample), no need
> to generate an event at all...

You might have to.  For instance, there could be a trigger STOREd on
the replica that needs to have this query show up there too.  One of
the problems with a general tool like Slony-I is that it has to do
work even when your specific case doesn't need it, because it can't
know everything (and getting it to check everything would impose
overhead that probably we don't want).

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
In the future this spectacle of the middle classes shocking the avant-
garde will probably become the textbook definition of Postmodernism. 
                --Brad Holland
From ajs at crankycanuck.ca  Wed Jul  4 09:30:19 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Jul  4 09:30:38 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <60ejjo43pi.fsf@dba2.int.libertyrms.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com>
	<468BAA40.1030505@Yahoo.com>
	<20070704144451.GA14262@phlogiston.dyndns.org>
	<60ejjo43pi.fsf@dba2.int.libertyrms.com>
Message-ID: <20070704163019.GC14501@phlogiston.dyndns.org>

On Wed, Jul 04, 2007 at 10:56:57AM -0400, Christopher Browne wrote:
> That doesn't change anything about "agreeable ordering" as far as I
> can see.
> 
> Of course, that example isn't much of a "win."  What would be way more
> interesting (from a performance perspective) is the case where there
> are 25 deletes in a row from my_table that could be folded together.

Right.  But the actual key here is to make sure that (1) no agreeable
ordering actually changes _and_ (2) that doing this work for every
sequence is in fact going to be a winner for most cases.  Jan and I
talked, IIRC, about this sort of optimisation in the early days of
1.0 design work, and neither of us were able to come up with a set of
tests that could both demonstrate this was a win _and_ that it
wouldn't kill ordinary-case performance (or blow out memory).  I'm
not saying it's impossible, but I'm trying to suggest that some
general algorithm work in this area is going to be needed, and it's
not going to be ameable merely to empircal tests.  You also have to
have at least _prima facie_ evidence that, for ordinary cases, this
won't suck.  I'd like to see that argument.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
When my information changes, I alter my conclusions.  What do you do sir?
		--attr. John Maynard Keynes
From ajs at crankycanuck.ca  Wed Jul  4 09:37:22 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Jul  4 09:37:40 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <4689BF5A.7060305@Yahoo.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<20070702153807.GF10257@fetter.org> <468936BE.7020505@Yahoo.com>
	<e51f66da0707021045t761a5188h32750153c55c49a2@mail.gmail.com>
	<46893DE8.6060202@Yahoo.com> <468956B3.1000904@Yahoo.com>
	<60k5ti60r5.fsf@dba2.int.libertyrms.com>
	<4689BF5A.7060305@Yahoo.com>
Message-ID: <20070704163722.GD14501@phlogiston.dyndns.org>

On Mon, Jul 02, 2007 at 11:15:38PM -0400, Jan Wieck wrote:

> against a simple N1->N2 setup bombarded with a -c5 pgbench. That isn't 
> quite the testing you want to have done before committing such a 
> substantial change in the inner core log selection logic of STABLE code, 
> is it?

What, we're not gonna pants-seat fly?  Sigh.  No guts, no glory ;-)

Seriously, I agree with Jan here: let's be _really_ conservative with
this one.  Indeed, given that it's a small patch, I'd be inclined to
issue a .11 with a contrib/pgq-apprach.patch file and suggest people
try it before back patching for real.  The HEAD is a good place for
architectural changes, but the supposedly STABLE releases aren't. 
I'm not a fan of the Linux-style, "rewrite the PCI subsystem in
x.x.8" STABLE-style releases.  And I think this project has been
often enough bitten by such exuberance that we should be cautious.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
A certain description of men are for getting out of debt, yet are
against all taxes for raising money to pay it off.
		--Alexander Hamilton
From JanWieck at Yahoo.com  Wed Jul  4 11:31:50 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed Jul  4 11:32:20 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <60ejjo43pi.fsf@dba2.int.libertyrms.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>	<1183151091.28247.44.camel@dogma.v10.wvs>	<608xa25sak.fsf@dba2.int.libertyrms.com>	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>	<60ved14fcn.fsf@dba2.int.libertyrms.com>
	<468BAA40.1030505@Yahoo.com>	<20070704144451.GA14262@phlogiston.dyndns.org>
	<60ejjo43pi.fsf@dba2.int.libertyrms.com>
Message-ID: <468BE796.2030807@Yahoo.com>

On 7/4/2007 10:56 AM, Christopher Browne wrote:
> Andrew Sullivan <ajs@crankycanuck.ca> writes:
>> On Wed, Jul 04, 2007 at 10:10:08AM -0400, Jan Wieck wrote:
>>> seems pretty deterministic, doesn't it? But the result of it depends on 
>>> the exact commit order and the transaction isolation level. We don't 
>>> capture the commit order of single transactions, nor do we care for it 
>>> anywhere in the Slony-I logic.
>>
>> I think this is key.  The current arrangement solves the problem
>> where the visibility rules as they were in force on the origin are
>> followed while applying on the replica.  You're going to need to do
>> quite a bit of theoretical work here to show that the agreeable order
>> rules are followed in any grouping approach you take.  Please see the
>> original concept paper on this exact point.  MVCC is hard.
> 
> The only change I'd propose in handling grouping is to
> opportunistically see if there are consecutive operations that may be
> trivially joined together.
> 
> In effect, if the *old* logic generated the sequence of queries:
> 
>   delete from my_table where id = 25;
>   delete from my_table where id = 82;
>   delete from another_table where id = 19;
>   delete from my_table where id = 45;
> 
> then there is only one "joining" possible, which is to combine the
> first two delete queries into one, so that the overall sequence of
> queries becomes thus:
> 
>   delete from my_table where id = 25 or id = 82;
>   delete from another_table where id = 19;
>   delete from my_table where id = 45;

Actually, only an insert or update to the same table would interrupt the 
possible grouping. But that is academic.

What I see here is that we are trying to come up with a special case 
optimization for mass-deletes. No mass insert or update operations will 
benefit from any of this. Do people do mass deletes that much that we 
really have to worry about them?


Jan

> 
> That doesn't change anything about "agreeable ordering" as far as I
> can see.
> 
> Of course, that example isn't much of a "win."  What would be way more
> interesting (from a performance perspective) is the case where there
> are 25 deletes in a row from my_table that could be folded together.


-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From JanWieck at Yahoo.com  Wed Jul  4 11:52:14 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed Jul  4 11:52:43 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <60ir9043w4.fsf@dba2.int.libertyrms.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>	<1183151091.28247.44.camel@dogma.v10.wvs>	<608xa25sak.fsf@dba2.int.libertyrms.com>	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>	<60ved14fcn.fsf@dba2.int.libertyrms.com>
	<468BAA40.1030505@Yahoo.com>
	<60ir9043w4.fsf@dba2.int.libertyrms.com>
Message-ID: <468BEC5E.1040303@Yahoo.com>

On 7/4/2007 10:52 AM, Christopher Browne wrote:
> Jan Wieck <JanWieck@Yahoo.com> writes:
>> On 7/3/2007 12:33 PM, Christopher Browne wrote:
>>> "Andrew Hammond" <andrew.george.hammond@gmail.com> writes:
>>
>>>> Also, ISTM that the big reason we don't like statement based
>>>> replication is that SQL has many non-deterministic aspects. However,
>>>> there is probably a pretty darn big subset of SQL which is provably
>>>> non-deterministic. And for that subset, would it be any less
>>>> rigorous to transmit those statements than to transmit the per-row
>>>> change statments like we currently do?
>>> Well, by capturing the values, we have captured a deterministic form
>>> of the update.
>>
>> How to figure out what is deterministic and what isn't? A simple
>>
>>     insert into summary select id, sum(value) from detail group by id;
>>
>> seems pretty deterministic, doesn't it? But the result of it depends
>> on the exact commit order and the transaction isolation level. We
>> don't capture the commit order of single transactions, nor do we care
>> for it anywhere in the Slony-I logic.
> 
> But at the time that we apply these changes in log_actionseq order, we
> have imposed a deterministic order.  (Which happens to be repeatable,
> on each node.)

The question was, how do we figure out which SQL statement would be 
deterministic and thus a candidate for SQL query string propagation - 
aside from the fact that there is no standard way in Postgres to capture 
query strings or parsetrees anyway. So far we have only established that 
the logging of the changes has to be done the way we are doing it now, 
on a row base where the actionseq determines the repeatable order. I 
don't see how going through a lot of effort to group those individual 
log rows together again will gain us a lot.

Unless the effort also attempts to group together consecutive insert and 
update statements affecting the same columns and using prepared 
statements, and unless we have some evidence that doing so will gain 
more than the effort of all that grouping costs, I don't think it is a 
good idea to make that part of remote_worker.c any more complicated than 
it is today. How many developers do we have who actually understand how 
that part of slon really works and who could go in and fix some bug in 
there?


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From darcyb at commandprompt.com  Wed Jul  4 12:36:35 2007
From: darcyb at commandprompt.com (Darcy Buskermolen)
Date: Wed Jul  4 12:37:08 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <468BEC5E.1040303@Yahoo.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<60ir9043w4.fsf@dba2.int.libertyrms.com>
	<468BEC5E.1040303@Yahoo.com>
Message-ID: <200707041236.35771.darcyb@commandprompt.com>

On July 4, 2007 11:52 am, Jan Wieck wrote:
> On 7/4/2007 10:52 AM, Christopher Browne wrote:
> > Jan Wieck <JanWieck@Yahoo.com> writes:
> >> On 7/3/2007 12:33 PM, Christopher Browne wrote:
> >>> "Andrew Hammond" <andrew.george.hammond@gmail.com> writes:
> >>>> Also, ISTM that the big reason we don't like statement based
> >>>> replication is that SQL has many non-deterministic aspects. However,
> >>>> there is probably a pretty darn big subset of SQL which is provably
> >>>> non-deterministic. And for that subset, would it be any less
> >>>> rigorous to transmit those statements than to transmit the per-row
> >>>> change statments like we currently do?
> >>>
> >>> Well, by capturing the values, we have captured a deterministic form
> >>> of the update.
> >>
> >> How to figure out what is deterministic and what isn't? A simple
> >>
> >>     insert into summary select id, sum(value) from detail group by id;
> >>
> >> seems pretty deterministic, doesn't it? But the result of it depends
> >> on the exact commit order and the transaction isolation level. We
> >> don't capture the commit order of single transactions, nor do we care
> >> for it anywhere in the Slony-I logic.
> >
> > But at the time that we apply these changes in log_actionseq order, we
> > have imposed a deterministic order.  (Which happens to be repeatable,
> > on each node.)
>
> The question was, how do we figure out which SQL statement would be
> deterministic and thus a candidate for SQL query string propagation -
> aside from the fact that there is no standard way in Postgres to capture
> query strings or parsetrees anyway. So far we have only established that
> the logging of the changes has to be done the way we are doing it now,
> on a row base where the actionseq determines the repeatable order. I
> don't see how going through a lot of effort to group those individual
> log rows together again will gain us a lot.
>
> Unless the effort also attempts to group together consecutive insert and
> update statements affecting the same columns and using prepared
> statements, and unless we have some evidence that doing so will gain
> more than the effort of all that grouping costs, I don't think it is a
> good idea to make that part of remote_worker.c any more complicated than
> it is today. How many developers do we have who actually understand how
> that part of slon really works and who could go in and fix some bug in
> there?

I have a better understanding of how this works today than i did a month ago, 
but it still feels a lot like black magic in there so I'm with Jan on this 
one, unless we can show that there is a significant advantage to doing so 
it's not worth the complication. 

Effort spent keeping slony from suffering from ill effects of Long Running 
Transactions feels like a much better basket to place development eggs into 
to me.



>
>
> Jan

-- 
Darcy Buskermolen
Command Prompt, Inc.
+1.503.667.4564 X 102
http://www.commandprompt.com/
PostgreSQL solutions since 1997
From nagy at ecircle-ag.com  Thu Jul  5 01:13:00 2007
From: nagy at ecircle-ag.com (Csaba Nagy)
Date: Thu Jul  5 01:13:52 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <468BE796.2030807@Yahoo.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com> <468BAA40.1030505@Yahoo.com>
	<20070704144451.GA14262@phlogiston.dyndns.org>
	<60ejjo43pi.fsf@dba2.int.libertyrms.com> <468BE796.2030807@Yahoo.com>
Message-ID: <1183623179.28714.62.camel@coppola.muc.ecircle.de>

> What I see here is that we are trying to come up with a special case 
> optimization for mass-deletes. No mass insert or update operations will 
> benefit from any of this. Do people do mass deletes that much that we 
> really have to worry about them?

I occasionally did such mass delete operations, which did cause slony to
lag behind but not so much that I would cry for such a feature...

In fact our postgres DB scaling is done by splitting the DB in 2 when
the customers on it grow too much. We do this by replicating a copy via
slony, and then cut the replication and configure half of the customers
to point to the first one, the rest point to the other one. Then delete
the unneeded data on both DBs. This procedure is transparent to the
customers, the required downtime is a few minutes while the application
is restarted with the new configuration, which can be done in a low
traffic period.

So back to the point, deleting the unneeded half of the data is such a
bulk delete operation, and sometimes I had to do it while slony was
active on the DB.

Cheers,
Csaba.


From markokr at gmail.com  Thu Jul  5 02:27:58 2007
From: markokr at gmail.com (Marko Kreen)
Date: Thu Jul  5 02:28:54 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <20070704163722.GD14501@phlogiston.dyndns.org>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<20070702153807.GF10257@fetter.org> <468936BE.7020505@Yahoo.com>
	<e51f66da0707021045t761a5188h32750153c55c49a2@mail.gmail.com>
	<46893DE8.6060202@Yahoo.com> <468956B3.1000904@Yahoo.com>
	<60k5ti60r5.fsf@dba2.int.libertyrms.com> <4689BF5A.7060305@Yahoo.com>
	<20070704163722.GD14501@phlogiston.dyndns.org>
Message-ID: <e51f66da0707050227h2e20644cs7bb2d0d76ae391f9@mail.gmail.com>

On 7/4/07, Andrew Sullivan <ajs@crankycanuck.ca> wrote:
> On Mon, Jul 02, 2007 at 11:15:38PM -0400, Jan Wieck wrote:
>
> > against a simple N1->N2 setup bombarded with a -c5 pgbench. That isn't
> > quite the testing you want to have done before committing such a
> > substantial change in the inner core log selection logic of STABLE code,
> > is it?
>
> What, we're not gonna pants-seat fly?  Sigh.  No guts, no glory ;-)
>
> Seriously, I agree with Jan here: let's be _really_ conservative with
> this one.  Indeed, given that it's a small patch, I'd be inclined to
> issue a .11 with a contrib/pgq-apprach.patch file and suggest people
> try it before back patching for real.  The HEAD is a good place for
> architectural changes, but the supposedly STABLE releases aren't.
> I'm not a fan of the Linux-style, "rewrite the PCI subsystem in
> x.x.8" STABLE-style releases.  And I think this project has been
> often enough bitten by such exuberance that we should be cautious.

I think the patch is fine correctness-wise.  Main problem
you can have with new approach is that Postgres gets confused
and turns the whole query into seqscan.

It should not happen in 8.3 but could be a problem with
7.4 or 8.0.

OTOH, my experience was with int8 txid, maybe they are
more intelligent when handling int4.

-- 
marko
From bnichols at ca.afilias.info  Thu Jul  5 06:25:56 2007
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Thu Jul  5 06:25:35 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <468BE796.2030807@Yahoo.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com> <468BAA40.1030505@Yahoo.com>
	<20070704144451.GA14262@phlogiston.dyndns.org>
	<60ejjo43pi.fsf@dba2.int.libertyrms.com> <468BE796.2030807@Yahoo.com>
Message-ID: <1183641956.5615.7.camel@bnicholson-desktop>

On Wed, 2007-07-04 at 14:31 -0400, Jan Wieck wrote:
> What I see here is that we are trying to come up with a special case 
> optimization for mass-deletes. No mass insert or update operations will 
> benefit from any of this. Do people do mass deletes that much that we 
> really have to worry about them?
> 


Yes, there are a few places that I can think of where we would directly
benefit from this.  Trimming data from very active log tables is the
main case.  We also recently had a case where we needed to delete a fair
amount of data from a table quickly to prevent degraded performance in a
front line system.  We ended up dropping the table, deleting and
re-subbing. We were not is a situation where we could have done smaller
batches that slony would have liked.

-- 
Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.

From nagy at ecircle-ag.com  Thu Jul  5 06:33:50 2007
From: nagy at ecircle-ag.com (Csaba Nagy)
Date: Thu Jul  5 06:33:57 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <1183623179.28714.62.camel@coppola.muc.ecircle.de>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com> <468BAA40.1030505@Yahoo.com>
	<20070704144451.GA14262@phlogiston.dyndns.org>
	<60ejjo43pi.fsf@dba2.int.libertyrms.com> <468BE796.2030807@Yahoo.com>
	<1183623179.28714.62.camel@coppola.muc.ecircle.de>
Message-ID: <1183642430.28714.103.camel@coppola.muc.ecircle.de>

On Thu, 2007-07-05 at 10:13, Csaba Nagy wrote:
> [snip] Then delete
> the unneeded data on both DBs. [snip]

OK, I should have said that the deletion is done in reasonably sized
chunks so that each deletion transaction doesn't take ages. Maybe this
is why slony was up to the task, even if the delete statement ratio from
master to slave would be between 1...10K.

So then no, I don't have a scenario where I would delete millions of
rows in one statement and slony would have to replicate that to millions
of single statements... that would kill the master too...

Cheers,
Csaba.


From JanWieck at Yahoo.com  Thu Jul  5 06:50:29 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu Jul  5 06:51:00 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <1183642430.28714.103.camel@coppola.muc.ecircle.de>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>	<1183151091.28247.44.camel@dogma.v10.wvs>	<608xa25sak.fsf@dba2.int.libertyrms.com>	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>	<60ved14fcn.fsf@dba2.int.libertyrms.com>
	<468BAA40.1030505@Yahoo.com>	<20070704144451.GA14262@phlogiston.dyndns.org>	<60ejjo43pi.fsf@dba2.int.libertyrms.com>
	<468BE796.2030807@Yahoo.com>	<1183623179.28714.62.camel@coppola.muc.ecircle.de>
	<1183642430.28714.103.camel@coppola.muc.ecircle.de>
Message-ID: <468CF725.2040005@Yahoo.com>

On 7/5/2007 9:33 AM, Csaba Nagy wrote:
> On Thu, 2007-07-05 at 10:13, Csaba Nagy wrote:
>> [snip] Then delete
>> the unneeded data on both DBs. [snip]
> 
> OK, I should have said that the deletion is done in reasonably sized
> chunks so that each deletion transaction doesn't take ages. Maybe this
> is why slony was up to the task, even if the delete statement ratio from
> master to slave would be between 1...10K.
> 
> So then no, I don't have a scenario where I would delete millions of
> rows in one statement and slony would have to replicate that to millions
> of single statements... that would kill the master too...

You could do THOSE mass deletes via EXECUTE SCRIPT ;-)


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From dmitry at koterov.ru  Thu Jul  5 07:51:29 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Thu Jul  5 07:51:39 2007
Subject: [Slony1-general] Re: Backslashes inside an update statement for
	slonik are stripped
In-Reply-To: <d7df81620706281202h10ac8986h45d31cffbcdf6a78@mail.gmail.com>
References: <d7df81620706281202h10ac8986h45d31cffbcdf6a78@mail.gmail.com>
Message-ID: <d7df81620707050751n38b700d1g4c5a788e579c8ca0@mail.gmail.com>

I think this bug is easily reproducible, we just need to feed some text with
backslashes to slonik and then - watch all machines log to be sure that
double slashes are incorrectly stripped.

On 6/28/07, Dmitry Koterov <dmitry@koterov.ru> wrote:
>
> Hello.
>
> I feed the slonik with the following SQL:
>
> DDL Statement 2: (299,471) [
>
> CREATE UNIQUE INDEX "i_dictionary_uni_abbr" ON "static"."dictionary"
>   USING btree ((substring(dic_russian, E'^([^(]*[^( ]) *\\('::text)))
>   WHERE (dic_category_id =3D 26);
>
> ] DDL Statement failed - PGRES_FATAL_ERROR
>
>
> You see, it generates an error. Here is a portion of postgres logs:
>
> 2007-06-28 18:56:55 GMT 87.250.244.99(55965)ERROR:  invalid regular
> expression: parentheses () not balanced
> 2007-06-28 18:56:55 GMT 87.250.244.99(55965)STATEMENT:
>         CREATE UNIQUE INDEX "i_dictionary_uni_abbr" ON
> "static"."dictionary"
>           USING btree ((substring(dic_russian, E'^([^(]*[^( ])
> *\('::text)))
>           WHERE (dic_category_id =3D 26);
>
>
> Note the \\( part above: it is sent to the server as \(.
> Seems slonik replaces \\ by \ before sending it to postgres?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070705/=
6e897f94/attachment.htm
From JanWieck at Yahoo.com  Thu Jul  5 08:09:43 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu Jul  5 08:10:11 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <Pine.LNX.4.62.0707031916320.5356@mini.atlantida.localdomain>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>	<20070702153807.GF10257@fetter.org>
	<468936BE.7020505@Yahoo.com>	<e51f66da0707021045t761a5188h32750153c55c49a2@mail.gmail.com>	<46893DE8.6060202@Yahoo.com>
	<468956B3.1000904@Yahoo.com>	<60k5ti60r5.fsf@dba2.int.libertyrms.com>
	<Pine.LNX.4.62.0707031916320.5356@mini.atlantida.localdomain>
Message-ID: <468D09B7.9090900@Yahoo.com>

On 7/3/2007 7:43 PM, Steve Singer wrote:
> 
> Is there anything we can do for 2.0 to improve DDL use cases?
> 
> I had sent out a patch a while back that lets EXECUTE SCRIPT take a list of 
> tables to lock (thus not locking everything).  I never did get any feedback 
> on the patch.  If there is interest I can try to bring it up to the current 
> 2.0 head and resend it.
> 
> Do the new was of disabling triggers have any effect on the dangers of doing 
> alter tables outside of an execute script? Is there anything we can do to 
> improve that (at least for DDL where the order it is applied on doesn't need 
> to match on the slaves)

They do indeed have an effect on precisely that, because now the system 
catalog is clean and you actually can do arbitrary DDL without going 
through EXECUTE SCRIPT at all. Whether doing so makes sense or not and 
eventually screws up your data because the commands affect different 
sets of rows is another question.

Since this also means that EXECUTE SCRIPT does not lock any tables by 
itself, one can issue the required LOCK TABLE statements at the 
beginning of the script, so I don't think such a patch is required any 
more. Unless I am missing something, that is.


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From jerry at jerrysievers.com  Thu Jul  5 12:03:00 2007
From: jerry at jerrysievers.com (Jerry Sievers)
Date: Thu Jul  5 12:03:27 2007
Subject: [Slony1-general] 
	New master failing; still trying to see old master?
Message-ID: <m3fy42oeqj.fsf@mama.jerrysievers.com>

Crisis today.  Complete power failure leaves a corrupt table on old
master. 

I did moveset() and dropnode() to reconfigure the cluster.   The old
master was node 2.    New master is node 1.   There are now just 2
slaves 3 and 4.

For some reason however, when I try to fire up the slon on the master,
it complains of node #2 does not exist right after reporting having
init'd node 4. 

I have no clue what's going wrong here and hope not to have to undo
and reconfig the cluster from scratch.  These DBs are too large now
for easy subscription during live processing. 

Any help much appreciated. 


-----------------------------------------
2007-07-05 18:19:18 GMT CONFIG main: edb-replication version 1.1.5 starting up
2007-07-05 18:19:19 GMT CONFIG main: local node id = 1
2007-07-05 18:19:19 GMT CONFIG main: launching sched_start_mainloop
2007-07-05 18:19:19 GMT CONFIG main: loading current cluster configuration
2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=3 no_comment='slave node 3'
2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=4 no_comment='slave node 4'
2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=3 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=4 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=1 set_origin=1 set_comment='RT3/VCASE replication set'
2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=2 set_origin=1 set_comment='new set for adding tables'
2007-07-05 18:19:19 GMT CONFIG main: configuration complete - starting threads
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=12520
2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=3
2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=4
2007-07-05 18:19:19 GMT FATAL  enableNode: unknown node ID 2
2007-07-05 18:19:19 GMT INFO   remoteListenThread_4: disconnecting from 'dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
2007-07-05 18:19:20 GMT INFO   remoteListenThread_3: disconnecting from 'dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'

-- 
-------------------------------------------------------------------------------
Jerry Sievers   732 365-2844 (work)     Production Database Administrator
                305 321-1144 (mobil	WWW E-Commerce Consultant
From JanWieck at Yahoo.com  Thu Jul  5 12:23:14 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu Jul  5 12:23:44 2007
Subject: [Slony1-general] 	New master failing; still trying to see old
	master?
In-Reply-To: <m3fy42oeqj.fsf@mama.jerrysievers.com>
References: <m3fy42oeqj.fsf@mama.jerrysievers.com>
Message-ID: <468D4522.8040804@Yahoo.com>

On 7/5/2007 3:03 PM, Jerry Sievers wrote:
> Crisis today.  Complete power failure leaves a corrupt table on old
> master. 
> 
> I did moveset() and dropnode() to reconfigure the cluster.   The old
> master was node 2.    New master is node 1.   There are now just 2
> slaves 3 and 4.
> 
> For some reason however, when I try to fire up the slon on the master,
> it complains of node #2 does not exist right after reporting having
> init'd node 4. 
> 
> I have no clue what's going wrong here and hope not to have to undo
> and reconfig the cluster from scratch.  These DBs are too large now
> for easy subscription during live processing. 
> 
> Any help much appreciated. 
> 
> 
> -----------------------------------------
> 2007-07-05 18:19:18 GMT CONFIG main: edb-replication version 1.1.5 starting up
> 2007-07-05 18:19:19 GMT CONFIG main: local node id = 1
> 2007-07-05 18:19:19 GMT CONFIG main: launching sched_start_mainloop
> 2007-07-05 18:19:19 GMT CONFIG main: loading current cluster configuration
> 2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=3 no_comment='slave node 3'
> 2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=4 no_comment='slave node 4'
> 2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=3 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
> 2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=4 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
> 2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
> 2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
> 2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=1 set_origin=1 set_comment='RT3/VCASE replication set'
> 2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=2 set_origin=1 set_comment='new set for adding tables'
> 2007-07-05 18:19:19 GMT CONFIG main: configuration complete - starting threads
> NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=12520
> 2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=3
> 2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=4
> 2007-07-05 18:19:19 GMT FATAL  enableNode: unknown node ID 2
> 2007-07-05 18:19:19 GMT INFO   remoteListenThread_4: disconnecting from 'dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
> 2007-07-05 18:19:20 GMT INFO   remoteListenThread_3: disconnecting from 'dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
> 

It appears that there is an ENABLE_NODE event on either node 3 or 4 
which node 1 tries to replicate. How that could have been lurking around 
there forever is another question though.

What is the content of sl_status for all three nodes?

Also, you now might want to change the password for user slony on those 
servers ;-)


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From andrew.george.hammond at gmail.com  Thu Jul  5 12:40:35 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Thu Jul  5 12:41:00 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <1183641956.5615.7.camel@bnicholson-desktop>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com> <468BAA40.1030505@Yahoo.com>
	<20070704144451.GA14262@phlogiston.dyndns.org>
	<60ejjo43pi.fsf@dba2.int.libertyrms.com> <468BE796.2030807@Yahoo.com>
	<1183641956.5615.7.camel@bnicholson-desktop>
Message-ID: <5a0a9d6f0707051240t44717edekfb3b7b8feef731e5@mail.gmail.com>

On 7/5/07, Brad Nicholson <bnichols@ca.afilias.info> wrote:
> On Wed, 2007-07-04 at 14:31 -0400, Jan Wieck wrote:
> > What I see here is that we are trying to come up with a special case
> > optimization for mass-deletes. No mass insert or update operations will
> > benefit from any of this. Do people do mass deletes that much that we
> > really have to worry about them?
> >
>
>
> Yes, there are a few places that I can think of where we would directly
> benefit from this.  Trimming data from very active log tables is the
> main case.  We also recently had a case where we needed to delete a fair
> amount of data from a table quickly to prevent degraded performance in a
> front line system.  We ended up dropping the table, deleting and
> re-subbing. We were not is a situation where we could have done smaller
> batches that slony would have liked.

May I rephrase Jan's question?  Are there any cases where people do
mass deletion that couldn't be solved using existing table
partitioning approaches? In the case of the log files Brad mentions
above, a solution is to use tables partitioned by temporal range, say
on a monthly basis. Once all the data in a given partition has reached
it's retention schedule, simply drop the partition.

This costs some additional EXECUTE DDL scripting, but I think would
solve the problem quite effectively. It also requires no additions or
changes to slony. Finally, it requires the user to be running a more
modern version of PostgreSQL, however if you care about performance,
that seems a reasonable assumption.

Andrew
From jerry at jerrysievers.com  Thu Jul  5 13:34:04 2007
From: jerry at jerrysievers.com (Jerry Sievers)
Date: Thu Jul  5 13:34:40 2007
Subject: [Slony1-general] 	New master failing;
	still trying to see old master?
In-Reply-To: <468D4522.8040804@Yahoo.com>
References: <m3fy42oeqj.fsf@mama.jerrysievers.com> <468D4522.8040804@Yahoo.com>
Message-ID: <m3bqeq1tfn.fsf@mama.jerrysievers.com>

select * from sl_status on the three nodes still configured. 

Please advise. 

Pager usage is off.
Expanded display is on.
-[ RECORD 1 ]-------------+-----------------------------
st_origin                 | 1
st_received               | 3
st_last_event             | 2225235
st_last_event_ts          | 05-JUL-07 15:54:54.810343
st_last_received          | 2225131
st_last_received_ts       | 05-JUL-07 15:16:23.496708
st_last_received_event_ts | 05-JUL-07 14:58:07.240334
st_lag_num_events         | 104
st_lag_time               | @ 5 hours 21 mins 45.53 secs
-[ RECORD 2 ]-------------+-----------------------------
st_origin                 | 1
st_received               | 4
st_last_event             | 2225235
st_last_event_ts          | 05-JUL-07 15:54:54.810343
st_last_received          | 2225131
st_last_received_ts       | 05-JUL-07 15:14:07.409965
st_last_received_event_ts | 05-JUL-07 14:58:07.240334
st_lag_num_events         | 104
st_lag_time               | @ 5 hours 21 mins 45.53 secs

Pager usage is off.
Expanded display is on.
-[ RECORD 1 ]-------------+-----------------------------
st_origin                 | 3
st_received               | 4
st_last_event             | 1863901
st_last_event_ts          | 05-JUL-07 18:21:49.29024
st_last_received          | 1863896
st_last_received_ts       | 05-JUL-07 18:21:04.101713
st_last_received_event_ts | 05-JUL-07 18:20:59.06034
st_lag_num_events         | 5
st_lag_time               | @ 2 hours 2 mins 21.48 secs
-[ RECORD 2 ]-------------+-----------------------------
st_origin                 | 3
st_received               | 1
st_last_event             | 1863901
st_last_event_ts          | 05-JUL-07 18:21:49.29024
st_last_received          | 1862809
st_last_received_ts       | 05-JUL-07 14:57:21.461858
st_last_received_event_ts | 05-JUL-07 15:00:46.848899
st_lag_num_events         | 1092
st_lag_time               | @ 5 hours 22 mins 33.69 secs

Pager usage is off.
Expanded display is on.
-[ RECORD 1 ]-------------+-----------------------------
st_origin                 | 4
st_received               | 1
st_last_event             | 1864550
st_last_event_ts          | 05-JUL-07 18:21:01.700228
st_last_received          | 1863465
st_last_received_ts       | 05-JUL-07 14:57:21.23512
st_last_received_event_ts | 05-JUL-07 15:00:49.830356
st_lag_num_events         | 1085
st_lag_time               | @ 5 hours 22 mins 33.96 secs
-[ RECORD 2 ]-------------+-----------------------------
st_origin                 | 4
st_received               | 3
st_last_event             | 1864550
st_last_event_ts          | 05-JUL-07 18:21:01.700228
st_last_received          | 1864550
st_last_received_ts       | 05-JUL-07 18:20:56.67848
st_last_received_event_ts | 05-JUL-07 18:21:01.700228
st_lag_num_events         | 0
st_lag_time               | @ 2 hours 2 mins 22.09 secs




Jan Wieck <JanWieck@Yahoo.com> writes:

> On 7/5/2007 3:03 PM, Jerry Sievers wrote:
> 
> > Crisis today.  Complete power failure leaves a corrupt table on old
> > master. I did moveset() and dropnode() to reconfigure the cluster.
> > The old
> > master was node 2.    New master is node 1.   There are now just 2
> > slaves 3 and 4.
> > For some reason however, when I try to fire up the slon on the
> > master,
> > it complains of node #2 does not exist right after reporting having
> > init'd node 4. I have no clue what's going wrong here and hope not
> > to have to undo
> > and reconfig the cluster from scratch.  These DBs are too large now
> > for easy subscription during live processing. Any help much
> > appreciated. -----------------------------------------
> > 2007-07-05 18:19:18 GMT CONFIG main: edb-replication version 1.1.5 starting up
> > 2007-07-05 18:19:19 GMT CONFIG main: local node id = 1
> > 2007-07-05 18:19:19 GMT CONFIG main: launching sched_start_mainloop
> > 2007-07-05 18:19:19 GMT CONFIG main: loading current cluster configuration
> > 2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=3 no_comment='slave node 3'
> > 2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=4 no_comment='slave node 4'
> > 2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=3 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
> > 2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=4 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
> > 2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
> > 2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
> > 2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=1 set_origin=1 set_comment='RT3/VCASE replication set'
> > 2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=2 set_origin=1 set_comment='new set for adding tables'
> > 2007-07-05 18:19:19 GMT CONFIG main: configuration complete - starting threads
> > NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=12520
> > 2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=3
> > 2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=4
> > 2007-07-05 18:19:19 GMT FATAL  enableNode: unknown node ID 2
> > 2007-07-05 18:19:19 GMT INFO   remoteListenThread_4: disconnecting from 'dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
> > 2007-07-05 18:19:20 GMT INFO   remoteListenThread_3: disconnecting from 'dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
> >
> 
> It appears that there is an ENABLE_NODE event on either node 3 or 4
> which node 1 tries to replicate. How that could have been lurking
> around there forever is another question though.
> 
> What is the content of sl_status for all three nodes?
> 
> Also, you now might want to change the password for user slony on
> those servers ;-)
> 
> 
> Jan
> 
> -- 
> #======================================================================#
> # It's easier to get forgiveness for being wrong than for being right. #
> # Let's break this rule - forgive me.                                  #
> #================================================== JanWieck@Yahoo.com #
> 

-- 
-------------------------------------------------------------------------------
Jerry Sievers   732 365-2844 (work)     Production Database Administrator
                305 321-1144 (mobil	WWW E-Commerce Consultant
From JanWieck at Yahoo.com  Thu Jul  5 13:47:33 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu Jul  5 13:48:12 2007
Subject: [Slony1-general] 	New master failing;	still trying to see old
	master?
In-Reply-To: <m3bqeq1tfn.fsf@mama.jerrysievers.com>
References: <m3fy42oeqj.fsf@mama.jerrysievers.com> <468D4522.8040804@Yahoo.com>
	<m3bqeq1tfn.fsf@mama.jerrysievers.com>
Message-ID: <468D58E5.9070000@Yahoo.com>

On 7/5/2007 4:34 PM, Jerry Sievers wrote:
> select * from sl_status on the three nodes still configured. 

Apparently node 1 didn't receive any events from 3 or 4 for over 5 
hours. Well, what does

     select * from sl_event where ev_type = 'ENABLE_NODE';

give you one all 3 nodes?


Jan

> 
> Please advise. 
> 
> Pager usage is off.
> Expanded display is on.
> -[ RECORD 1 ]-------------+-----------------------------
> st_origin                 | 1
> st_received               | 3
> st_last_event             | 2225235
> st_last_event_ts          | 05-JUL-07 15:54:54.810343
> st_last_received          | 2225131
> st_last_received_ts       | 05-JUL-07 15:16:23.496708
> st_last_received_event_ts | 05-JUL-07 14:58:07.240334
> st_lag_num_events         | 104
> st_lag_time               | @ 5 hours 21 mins 45.53 secs
> -[ RECORD 2 ]-------------+-----------------------------
> st_origin                 | 1
> st_received               | 4
> st_last_event             | 2225235
> st_last_event_ts          | 05-JUL-07 15:54:54.810343
> st_last_received          | 2225131
> st_last_received_ts       | 05-JUL-07 15:14:07.409965
> st_last_received_event_ts | 05-JUL-07 14:58:07.240334
> st_lag_num_events         | 104
> st_lag_time               | @ 5 hours 21 mins 45.53 secs
> 
> Pager usage is off.
> Expanded display is on.
> -[ RECORD 1 ]-------------+-----------------------------
> st_origin                 | 3
> st_received               | 4
> st_last_event             | 1863901
> st_last_event_ts          | 05-JUL-07 18:21:49.29024
> st_last_received          | 1863896
> st_last_received_ts       | 05-JUL-07 18:21:04.101713
> st_last_received_event_ts | 05-JUL-07 18:20:59.06034
> st_lag_num_events         | 5
> st_lag_time               | @ 2 hours 2 mins 21.48 secs
> -[ RECORD 2 ]-------------+-----------------------------
> st_origin                 | 3
> st_received               | 1
> st_last_event             | 1863901
> st_last_event_ts          | 05-JUL-07 18:21:49.29024
> st_last_received          | 1862809
> st_last_received_ts       | 05-JUL-07 14:57:21.461858
> st_last_received_event_ts | 05-JUL-07 15:00:46.848899
> st_lag_num_events         | 1092
> st_lag_time               | @ 5 hours 22 mins 33.69 secs
> 
> Pager usage is off.
> Expanded display is on.
> -[ RECORD 1 ]-------------+-----------------------------
> st_origin                 | 4
> st_received               | 1
> st_last_event             | 1864550
> st_last_event_ts          | 05-JUL-07 18:21:01.700228
> st_last_received          | 1863465
> st_last_received_ts       | 05-JUL-07 14:57:21.23512
> st_last_received_event_ts | 05-JUL-07 15:00:49.830356
> st_lag_num_events         | 1085
> st_lag_time               | @ 5 hours 22 mins 33.96 secs
> -[ RECORD 2 ]-------------+-----------------------------
> st_origin                 | 4
> st_received               | 3
> st_last_event             | 1864550
> st_last_event_ts          | 05-JUL-07 18:21:01.700228
> st_last_received          | 1864550
> st_last_received_ts       | 05-JUL-07 18:20:56.67848
> st_last_received_event_ts | 05-JUL-07 18:21:01.700228
> st_lag_num_events         | 0
> st_lag_time               | @ 2 hours 2 mins 22.09 secs
> 
> 
> 
> 
> Jan Wieck <JanWieck@Yahoo.com> writes:
> 
>> On 7/5/2007 3:03 PM, Jerry Sievers wrote:
>> 
>> > Crisis today.  Complete power failure leaves a corrupt table on old
>> > master. I did moveset() and dropnode() to reconfigure the cluster.
>> > The old
>> > master was node 2.    New master is node 1.   There are now just 2
>> > slaves 3 and 4.
>> > For some reason however, when I try to fire up the slon on the
>> > master,
>> > it complains of node #2 does not exist right after reporting having
>> > init'd node 4. I have no clue what's going wrong here and hope not
>> > to have to undo
>> > and reconfig the cluster from scratch.  These DBs are too large now
>> > for easy subscription during live processing. Any help much
>> > appreciated. -----------------------------------------
>> > 2007-07-05 18:19:18 GMT CONFIG main: edb-replication version 1.1.5 starting up
>> > 2007-07-05 18:19:19 GMT CONFIG main: local node id = 1
>> > 2007-07-05 18:19:19 GMT CONFIG main: launching sched_start_mainloop
>> > 2007-07-05 18:19:19 GMT CONFIG main: loading current cluster configuration
>> > 2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=3 no_comment='slave node 3'
>> > 2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=4 no_comment='slave node 4'
>> > 2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=3 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
>> > 2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=4 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
>> > 2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
>> > 2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
>> > 2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=1 set_origin=1 set_comment='RT3/VCASE replication set'
>> > 2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=2 set_origin=1 set_comment='new set for adding tables'
>> > 2007-07-05 18:19:19 GMT CONFIG main: configuration complete - starting threads
>> > NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=12520
>> > 2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=3
>> > 2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=4
>> > 2007-07-05 18:19:19 GMT FATAL  enableNode: unknown node ID 2
>> > 2007-07-05 18:19:19 GMT INFO   remoteListenThread_4: disconnecting from 'dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
>> > 2007-07-05 18:19:20 GMT INFO   remoteListenThread_3: disconnecting from 'dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
>> >
>> 
>> It appears that there is an ENABLE_NODE event on either node 3 or 4
>> which node 1 tries to replicate. How that could have been lurking
>> around there forever is another question though.
>> 
>> What is the content of sl_status for all three nodes?
>> 
>> Also, you now might want to change the password for user slony on
>> those servers ;-)
>> 
>> 
>> Jan
>> 
>> -- 
>> #======================================================================#
>> # It's easier to get forgiveness for being wrong than for being right. #
>> # Let's break this rule - forgive me.                                  #
>> #================================================== JanWieck@Yahoo.com #
>> 
> 


-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From JanWieck at Yahoo.com  Thu Jul  5 13:49:06 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu Jul  5 13:49:40 2007
Subject: [Slony1-general] 	New master failing; still trying to see old
	master?
In-Reply-To: <m3fy42oeqj.fsf@mama.jerrysievers.com>
References: <m3fy42oeqj.fsf@mama.jerrysievers.com>
Message-ID: <468D5942.7030902@Yahoo.com>

On 7/5/2007 3:03 PM, Jerry Sievers wrote:
> Crisis today.  Complete power failure leaves a corrupt table on old
> master. 
> 
> I did moveset() and dropnode() to reconfigure the cluster.   The old
> master was node 2.    New master is node 1.   There are now just 2
> slaves 3 and 4.

Another question: Did you wait for the moveset() to propagate before you 
dropped node 2?


Jan

> 
> For some reason however, when I try to fire up the slon on the master,
> it complains of node #2 does not exist right after reporting having
> init'd node 4. 
> 
> I have no clue what's going wrong here and hope not to have to undo
> and reconfig the cluster from scratch.  These DBs are too large now
> for easy subscription during live processing. 
> 
> Any help much appreciated. 
> 
> 
> -----------------------------------------
> 2007-07-05 18:19:18 GMT CONFIG main: edb-replication version 1.1.5 starting up
> 2007-07-05 18:19:19 GMT CONFIG main: local node id = 1
> 2007-07-05 18:19:19 GMT CONFIG main: launching sched_start_mainloop
> 2007-07-05 18:19:19 GMT CONFIG main: loading current cluster configuration
> 2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=3 no_comment='slave node 3'
> 2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=4 no_comment='slave node 4'
> 2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=3 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
> 2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=4 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
> 2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
> 2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
> 2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=1 set_origin=1 set_comment='RT3/VCASE replication set'
> 2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=2 set_origin=1 set_comment='new set for adding tables'
> 2007-07-05 18:19:19 GMT CONFIG main: configuration complete - starting threads
> NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=12520
> 2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=3
> 2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=4
> 2007-07-05 18:19:19 GMT FATAL  enableNode: unknown node ID 2
> 2007-07-05 18:19:19 GMT INFO   remoteListenThread_4: disconnecting from 'dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
> 2007-07-05 18:19:20 GMT INFO   remoteListenThread_3: disconnecting from 'dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
> 


-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From ajs at crankycanuck.ca  Thu Jul  5 13:55:21 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Jul  5 13:56:04 2007
Subject: [Slony1-general] timestamp with time zone insanity
In-Reply-To: <20070702225435.e4ea9a2e.wmoran@collaborativefusion.com>
References: <20070702225435.e4ea9a2e.wmoran@collaborativefusion.com>
Message-ID: <20070705205521.GS17424@phlogiston.dyndns.org>

On Mon, Jul 02, 2007 at 10:54:35PM -0400, Bill Moran wrote:
> On both systems, the database is set to est5edt timezone, yet some
> rows (not all, mind you) change the timezone from edt to est when
> the data is replicated.

Just as a point of information, since I see you have the problem
fixed, this can _never_ be true in the sense of changing the data. 
PostgreSQL stores its dates internally as UTC, so if your client
timezone setting changes, you'll see a difference.  So whenever you
see something like this, look for a problem around how the data is
being presented (like in your case, different timezone rules).

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
The very definition of "news" is "something that hardly ever happens."	
		--Bruce Schneier
From jerry at jerrysievers.com  Thu Jul  5 14:08:12 2007
From: jerry at jerrysievers.com (Jerry Sievers)
Date: Thu Jul  5 14:08:43 2007
Subject: [Slony1-general] 	New master failing;
	still trying to see old master?
In-Reply-To: <468D5942.7030902@Yahoo.com>
References: <m3fy42oeqj.fsf@mama.jerrysievers.com> <468D5942.7030902@Yahoo.com>
Message-ID: <m3y7huzhhf.fsf@mama.jerrysievers.com>

Jan Wieck <JanWieck@Yahoo.com> writes:

> On 7/5/2007 3:03 PM, Jerry Sievers wrote:
> 
> 
> Another question: Did you wait for the moveset() to propagate before
> you dropped node 2?

I'm not sure.  The guy who set this up didn't use Slonik so I had to
run the functions by hand. 

The system  was quiescent but no solid guarantee that it was fully
propogated. 

Fooey!

> Jan
> 
> 
> 
> -- 
> #======================================================================#
> # It's easier to get forgiveness for being wrong than for being right. #
> # Let's break this rule - forgive me.                                  #
> #================================================== JanWieck@Yahoo.com #
> 

-- 
-------------------------------------------------------------------------------
Jerry Sievers   732 365-2844 (work)     Production Database Administrator
                305 321-1144 (mobil	WWW E-Commerce Consultant
From jerry at jerrysievers.com  Thu Jul  5 14:15:01 2007
From: jerry at jerrysievers.com (Jerry Sievers)
Date: Thu Jul  5 14:15:33 2007
Subject: [Slony1-general] 	New master failing;
	still trying to see old master?
In-Reply-To: <468D58E5.9070000@Yahoo.com>
References: <m3fy42oeqj.fsf@mama.jerrysievers.com>
	<468D4522.8040804@Yahoo.com> <m3bqeq1tfn.fsf@mama.jerrysievers.com>
	<468D58E5.9070000@Yahoo.com>
Message-ID: <m3ps36zh62.fsf@mama.jerrysievers.com>

Hi Jan;  here's a quick look at what sorts of events are in sl_event. 


Pager usage is off.
 ev_origin | ev_seqno | ev_timestamp | ev_minxid | ev_maxxid | ev_xip | ev_type | ev_data1 | ev_data2 | ev_data3 | ev_data4 | ev_data5 | ev_data6 | ev_data7 | ev_data8 
-----------+----------+--------------+-----------+-----------+--------+---------+----------+----------+----------+----------+----------+----------+----------+----------
(0 rows)

Pager usage is off.
 ev_origin | ev_seqno | ev_timestamp | ev_minxid | ev_maxxid | ev_xip | ev_type | ev_data1 | ev_data2 | ev_data3 | ev_data4 | ev_data5 | ev_data6 | ev_data7 | ev_data8 
-----------+----------+--------------+-----------+-----------+--------+---------+----------+----------+----------+----------+----------+----------+----------+----------
(0 rows)

Pager usage is off.
 ev_origin | ev_seqno | ev_timestamp | ev_minxid | ev_maxxid | ev_xip | ev_type | ev_data1 | ev_data2 | ev_data3 | ev_data4 | ev_data5 | ev_data6 | ev_data7 | ev_data8 
-----------+----------+--------------+-----------+-----------+--------+---------+----------+----------+----------+----------+----------+----------+----------+----------
(0 rows)



Jan Wieck <JanWieck@Yahoo.com> writes:

> On 7/5/2007 4:34 PM, Jerry Sievers wrote:
> 
> > select * from sl_status on the three nodes still configured.
> 
> Apparently node 1 didn't receive any events from 3 or 4 for over 5
> hours. Well, what does
> 
>      select * from sl_event where ev_type = 'ENABLE_NODE';
> 
> give you one all 3 nodes?
> 
> 
> Jan
> 
> > Please advise. Pager usage is off.
> > Expanded display is on.
> > -[ RECORD 1 ]-------------+-----------------------------
> > st_origin                 | 1
> > st_received               | 3
> > st_last_event             | 2225235
> > st_last_event_ts          | 05-JUL-07 15:54:54.810343
> > st_last_received          | 2225131
> > st_last_received_ts       | 05-JUL-07 15:16:23.496708
> > st_last_received_event_ts | 05-JUL-07 14:58:07.240334
> > st_lag_num_events         | 104
> > st_lag_time               | @ 5 hours 21 mins 45.53 secs
> > -[ RECORD 2 ]-------------+-----------------------------
> > st_origin                 | 1
> > st_received               | 4
> > st_last_event             | 2225235
> > st_last_event_ts          | 05-JUL-07 15:54:54.810343
> > st_last_received          | 2225131
> > st_last_received_ts       | 05-JUL-07 15:14:07.409965
> > st_last_received_event_ts | 05-JUL-07 14:58:07.240334
> > st_lag_num_events         | 104
> > st_lag_time               | @ 5 hours 21 mins 45.53 secs
> > Pager usage is off.
> > Expanded display is on.
> > -[ RECORD 1 ]-------------+-----------------------------
> > st_origin                 | 3
> > st_received               | 4
> > st_last_event             | 1863901
> > st_last_event_ts          | 05-JUL-07 18:21:49.29024
> > st_last_received          | 1863896
> > st_last_received_ts       | 05-JUL-07 18:21:04.101713
> > st_last_received_event_ts | 05-JUL-07 18:20:59.06034
> > st_lag_num_events         | 5
> > st_lag_time               | @ 2 hours 2 mins 21.48 secs
> > -[ RECORD 2 ]-------------+-----------------------------
> > st_origin                 | 3
> > st_received               | 1
> > st_last_event             | 1863901
> > st_last_event_ts          | 05-JUL-07 18:21:49.29024
> > st_last_received          | 1862809
> > st_last_received_ts       | 05-JUL-07 14:57:21.461858
> > st_last_received_event_ts | 05-JUL-07 15:00:46.848899
> > st_lag_num_events         | 1092
> > st_lag_time               | @ 5 hours 22 mins 33.69 secs
> > Pager usage is off.
> > Expanded display is on.
> > -[ RECORD 1 ]-------------+-----------------------------
> > st_origin                 | 4
> > st_received               | 1
> > st_last_event             | 1864550
> > st_last_event_ts          | 05-JUL-07 18:21:01.700228
> > st_last_received          | 1863465
> > st_last_received_ts       | 05-JUL-07 14:57:21.23512
> > st_last_received_event_ts | 05-JUL-07 15:00:49.830356
> > st_lag_num_events         | 1085
> > st_lag_time               | @ 5 hours 22 mins 33.96 secs
> > -[ RECORD 2 ]-------------+-----------------------------
> > st_origin                 | 4
> > st_received               | 3
> > st_last_event             | 1864550
> > st_last_event_ts          | 05-JUL-07 18:21:01.700228
> > st_last_received          | 1864550
> > st_last_received_ts       | 05-JUL-07 18:20:56.67848
> > st_last_received_event_ts | 05-JUL-07 18:21:01.700228
> > st_lag_num_events         | 0
> > st_lag_time               | @ 2 hours 2 mins 22.09 secs
> > Jan Wieck <JanWieck@Yahoo.com> writes:
> >
> >> On 7/5/2007 3:03 PM, Jerry Sievers wrote:
> >> > Crisis today.  Complete power failure leaves a corrupt table on
> >> old
> >> > master. I did moveset() and dropnode() to reconfigure the cluster.
> >> > The old
> >> > master was node 2.    New master is node 1.   There are now just 2
> >> > slaves 3 and 4.
> >> > For some reason however, when I try to fire up the slon on the
> >> > master,
> >> > it complains of node #2 does not exist right after reporting having
> >> > init'd node 4. I have no clue what's going wrong here and hope not
> >> > to have to undo
> >> > and reconfig the cluster from scratch.  These DBs are too large now
> >> > for easy subscription during live processing. Any help much
> >> > appreciated. -----------------------------------------
> >> > 2007-07-05 18:19:18 GMT CONFIG main: edb-replication version 1.1.5 starting up
> >> > 2007-07-05 18:19:19 GMT CONFIG main: local node id = 1
> >> > 2007-07-05 18:19:19 GMT CONFIG main: launching sched_start_mainloop
> >> > 2007-07-05 18:19:19 GMT CONFIG main: loading current cluster configuration
> >> > 2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=3 no_comment='slave node 3'
> >> > 2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=4 no_comment='slave node 4'
> >> > 2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=3 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
> >> > 2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=4 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
> >> > 2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
> >> > 2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
> >> > 2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=1 set_origin=1 set_comment='RT3/VCASE replication set'
> >> > 2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=2 set_origin=1 set_comment='new set for adding tables'
> >> > 2007-07-05 18:19:19 GMT CONFIG main: configuration complete - starting threads
> >> > NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=12520
> >> > 2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=3
> >> > 2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=4
> >> > 2007-07-05 18:19:19 GMT FATAL  enableNode: unknown node ID 2
> >> > 2007-07-05 18:19:19 GMT INFO   remoteListenThread_4: disconnecting from 'dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
> >> > 2007-07-05 18:19:20 GMT INFO   remoteListenThread_3: disconnecting from 'dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
> >> >
> >> It appears that there is an ENABLE_NODE event on either node 3 or 4
> >> which node 1 tries to replicate. How that could have been lurking
> >> around there forever is another question though.
> >> What is the content of sl_status for all three nodes?
> >> Also, you now might want to change the password for user slony on
> >> those servers ;-)
> >> Jan
> >> -- 
> >> #======================================================================#
> >> # It's easier to get forgiveness for being wrong than for being right. #
> >> # Let's break this rule - forgive me.                                  #
> >> #================================================== JanWieck@Yahoo.com #
> >>
> >
> 
> 
> -- 
> #======================================================================#
> # It's easier to get forgiveness for being wrong than for being right. #
> # Let's break this rule - forgive me.                                  #
> #================================================== JanWieck@Yahoo.com #
> 

-- 
-------------------------------------------------------------------------------
Jerry Sievers   732 365-2844 (work)     Production Database Administrator
                305 321-1144 (mobil	WWW E-Commerce Consultant
From jerry at jerrysievers.com  Thu Jul  5 14:22:57 2007
From: jerry at jerrysievers.com (Jerry Sievers)
Date: Thu Jul  5 14:23:28 2007
Subject: [Slony1-general] 	New master failing;
	still trying to see old master?
In-Reply-To: <468D5942.7030902@Yahoo.com>
References: <m3fy42oeqj.fsf@mama.jerrysievers.com> <468D5942.7030902@Yahoo.com>
Message-ID: <m3lkduzgsu.fsf@mama.jerrysievers.com>

Selecting all non-sync events from each of the 3 nodes ordered by
ev_seqno.

Thanks!


Pager usage is off.
Expanded display is on.
-[ RECORD 1 ]+------------------------------------------------------------------------
ev_origin    | 1
ev_seqno     | 2225126
ev_timestamp | 05-JUL-07 14:57:16.056801
ev_minxid    | 884391402
ev_maxxid    | 884391412
ev_xip       | '884391409','884391411'
ev_type      | ACCEPT_SET
ev_data1     | 1
ev_data2     | 2
ev_data3     | 1
ev_data4     | 
ev_data5     | 
ev_data6     | 
ev_data7     | 
ev_data8     | 
-[ RECORD 2 ]+------------------------------------------------------------------------
ev_origin    | 1
ev_seqno     | 2225133
ev_timestamp | 05-JUL-07 14:58:26.439281
ev_minxid    | 884391608
ev_maxxid    | 884391609
ev_xip       | 
ev_type      | ACCEPT_SET
ev_data1     | 2
ev_data2     | 2
ev_data3     | 1
ev_data4     | 
ev_data5     | 
ev_data6     | 
ev_data7     | 
ev_data8     | 
-[ RECORD 3 ]+------------------------------------------------------------------------
ev_origin    | 1
ev_seqno     | 2225224
ev_timestamp | 05-JUL-07 15:49:54.253471
ev_minxid    | 884528335
ev_maxxid    | 884697167
ev_xip       | '884528335','884697160','884697162','884697161','884587782','884697166'
ev_type      | DROP_NODE
ev_data1     | 2
ev_data2     | 
ev_data3     | 
ev_data4     | 
ev_data5     | 
ev_data6     | 
ev_data7     | 
ev_data8     | 

Pager usage is off.
Expanded display is on.
-[ RECORD 1 ]+--------------------------
ev_origin    | 4
ev_seqno     | 1863698
ev_timestamp | 05-JUL-07 15:52:40.518681
ev_minxid    | 385609088
ev_maxxid    | 385609089
ev_xip       | 
ev_type      | DROP_NODE
ev_data1     | 2
ev_data2     | 
ev_data3     | 
ev_data4     | 
ev_data5     | 
ev_data6     | 
ev_data7     | 
ev_data8     | 

Pager usage is off.
Expanded display is on.
-[ RECORD 1 ]+--------------------------
ev_origin    | 4
ev_seqno     | 1863698
ev_timestamp | 05-JUL-07 15:52:40.518681
ev_minxid    | 385609088
ev_maxxid    | 385609089
ev_xip       | 
ev_type      | DROP_NODE
ev_data1     | 2
ev_data2     | 
ev_data3     | 
ev_data4     | 
ev_data5     | 
ev_data6     | 
ev_data7     | 
ev_data8     | 



Jan Wieck <JanWieck@Yahoo.com> writes:

> On 7/5/2007 3:03 PM, Jerry Sievers wrote:
> 
> > Crisis today.  Complete power failure leaves a corrupt table on old
> > master. I did moveset() and dropnode() to reconfigure the cluster.
> > The old
> > master was node 2.    New master is node 1.   There are now just 2
> > slaves 3 and 4.
> 
> Another question: Did you wait for the moveset() to propagate before
> you dropped node 2?
> 
> 
> Jan
> 
> > For some reason however, when I try to fire up the slon on the
> > master,
> > it complains of node #2 does not exist right after reporting having
> > init'd node 4. I have no clue what's going wrong here and hope not
> > to have to undo
> > and reconfig the cluster from scratch.  These DBs are too large now
> > for easy subscription during live processing. Any help much
> > appreciated. -----------------------------------------
> > 2007-07-05 18:19:18 GMT CONFIG main: edb-replication version 1.1.5 starting up
> > 2007-07-05 18:19:19 GMT CONFIG main: local node id = 1
> > 2007-07-05 18:19:19 GMT CONFIG main: launching sched_start_mainloop
> > 2007-07-05 18:19:19 GMT CONFIG main: loading current cluster configuration
> > 2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=3 no_comment='slave node 3'
> > 2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=4 no_comment='slave node 4'
> > 2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=3 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
> > 2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=4 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
> > 2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
> > 2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
> > 2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=1 set_origin=1 set_comment='RT3/VCASE replication set'
> > 2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=2 set_origin=1 set_comment='new set for adding tables'
> > 2007-07-05 18:19:19 GMT CONFIG main: configuration complete - starting threads
> > NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=12520
> > 2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=3
> > 2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=4
> > 2007-07-05 18:19:19 GMT FATAL  enableNode: unknown node ID 2
> > 2007-07-05 18:19:19 GMT INFO   remoteListenThread_4: disconnecting from 'dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
> > 2007-07-05 18:19:20 GMT INFO   remoteListenThread_3: disconnecting from 'dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
> >
> 
> 
> -- 
> #======================================================================#
> # It's easier to get forgiveness for being wrong than for being right. #
> # Let's break this rule - forgive me.                                  #
> #================================================== JanWieck@Yahoo.com #
> 

-- 
-------------------------------------------------------------------------------
Jerry Sievers   732 365-2844 (work)     Production Database Administrator
                305 321-1144 (mobil	WWW E-Commerce Consultant
From andrew.george.hammond at gmail.com  Thu Jul  5 14:48:59 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Thu Jul  5 14:49:31 2007
Subject: [Slony1-general] New master failing;
	still trying to see old master?
In-Reply-To: <m3y7huzhhf.fsf@mama.jerrysievers.com>
References: <m3fy42oeqj.fsf@mama.jerrysievers.com> <468D5942.7030902@Yahoo.com>
	<m3y7huzhhf.fsf@mama.jerrysievers.com>
Message-ID: <5a0a9d6f0707051448q31544d12l365d9f8cb3311732@mail.gmail.com>

On 05 Jul 2007 17:08:12 -0400, Jerry Sievers <jerry@jerrysievers.com> wrote:
> Jan Wieck <JanWieck@Yahoo.com> writes:
>
> > On 7/5/2007 3:03 PM, Jerry Sievers wrote:
> >
> >
> > Another question: Did you wait for the moveset() to propagate before
> > you dropped node 2?
>
> I'm not sure.  The guy who set this up didn't use Slonik so I had to
> run the functions by hand.

Run the functions, in the database, by hand via psql? Which functions
did you run? Do you have a typescript or psql history file of this?

> The system  was quiescent but no solid guarantee that it was fully
> propogated.

If you waited a few minutes between the move set and the drop node
then probably the move propagated first. If you ran the drop
immediately after the move, then almost certainly the move did not
propagate.

Andrew
From JanWieck at Yahoo.com  Thu Jul  5 14:52:34 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu Jul  5 14:53:35 2007
Subject: [Slony1-general] 	New master failing;	still trying to see old
	master?
In-Reply-To: <m3lkduzgsu.fsf@mama.jerrysievers.com>
References: <m3fy42oeqj.fsf@mama.jerrysievers.com> <468D5942.7030902@Yahoo.com>
	<m3lkduzgsu.fsf@mama.jerrysievers.com>
Message-ID: <468D6822.9080309@Yahoo.com>

On 7/5/2007 5:22 PM, Jerry Sievers wrote:
> Selecting all non-sync events from each of the 3 nodes ordered by
> ev_seqno.

I think I see what's going on here ... maybe.

This is probably a pilot error in connection with a copy/paste mistake 
sitting in slon for ages.

The copy/paste mistake is:
     the error message in disableNode() says "enableNode(): ...".
     I claim ownership of that one.

The pilot error is:
     the dropnode() was issued multiple times against different nodes
     without giving them time to propagate (in this case nodes 1 and 4).
     They are events (1,2225224) and (4,1863698).

Nice screwup. However since all 3 nodes don't have node 2 in the sl_node 
table any more (at least from what I see they should not), it is safe to

     DELETE FROM sl_event WHERE ev_origin = 4 and ev_seqno = 1863698;
     DELETE FROM sl_event WHERE ev_origin = 1 and ev_seqno = 2225224;


Jan

> 
> Thanks!
> 
> 
> Pager usage is off.
> Expanded display is on.
> -[ RECORD 1 ]+------------------------------------------------------------------------
> ev_origin    | 1
> ev_seqno     | 2225126
> ev_timestamp | 05-JUL-07 14:57:16.056801
> ev_minxid    | 884391402
> ev_maxxid    | 884391412
> ev_xip       | '884391409','884391411'
> ev_type      | ACCEPT_SET
> ev_data1     | 1
> ev_data2     | 2
> ev_data3     | 1
> ev_data4     | 
> ev_data5     | 
> ev_data6     | 
> ev_data7     | 
> ev_data8     | 
> -[ RECORD 2 ]+------------------------------------------------------------------------
> ev_origin    | 1
> ev_seqno     | 2225133
> ev_timestamp | 05-JUL-07 14:58:26.439281
> ev_minxid    | 884391608
> ev_maxxid    | 884391609
> ev_xip       | 
> ev_type      | ACCEPT_SET
> ev_data1     | 2
> ev_data2     | 2
> ev_data3     | 1
> ev_data4     | 
> ev_data5     | 
> ev_data6     | 
> ev_data7     | 
> ev_data8     | 
> -[ RECORD 3 ]+------------------------------------------------------------------------
> ev_origin    | 1
> ev_seqno     | 2225224
> ev_timestamp | 05-JUL-07 15:49:54.253471
> ev_minxid    | 884528335
> ev_maxxid    | 884697167
> ev_xip       | '884528335','884697160','884697162','884697161','884587782','884697166'
> ev_type      | DROP_NODE
> ev_data1     | 2
> ev_data2     | 
> ev_data3     | 
> ev_data4     | 
> ev_data5     | 
> ev_data6     | 
> ev_data7     | 
> ev_data8     | 
> 
> Pager usage is off.
> Expanded display is on.
> -[ RECORD 1 ]+--------------------------
> ev_origin    | 4
> ev_seqno     | 1863698
> ev_timestamp | 05-JUL-07 15:52:40.518681
> ev_minxid    | 385609088
> ev_maxxid    | 385609089
> ev_xip       | 
> ev_type      | DROP_NODE
> ev_data1     | 2
> ev_data2     | 
> ev_data3     | 
> ev_data4     | 
> ev_data5     | 
> ev_data6     | 
> ev_data7     | 
> ev_data8     | 
> 
> Pager usage is off.
> Expanded display is on.
> -[ RECORD 1 ]+--------------------------
> ev_origin    | 4
> ev_seqno     | 1863698
> ev_timestamp | 05-JUL-07 15:52:40.518681
> ev_minxid    | 385609088
> ev_maxxid    | 385609089
> ev_xip       | 
> ev_type      | DROP_NODE
> ev_data1     | 2
> ev_data2     | 
> ev_data3     | 
> ev_data4     | 
> ev_data5     | 
> ev_data6     | 
> ev_data7     | 
> ev_data8     | 
> 
> 
> 
> Jan Wieck <JanWieck@Yahoo.com> writes:
> 
>> On 7/5/2007 3:03 PM, Jerry Sievers wrote:
>> 
>> > Crisis today.  Complete power failure leaves a corrupt table on old
>> > master. I did moveset() and dropnode() to reconfigure the cluster.
>> > The old
>> > master was node 2.    New master is node 1.   There are now just 2
>> > slaves 3 and 4.
>> 
>> Another question: Did you wait for the moveset() to propagate before
>> you dropped node 2?
>> 
>> 
>> Jan
>> 
>> > For some reason however, when I try to fire up the slon on the
>> > master,
>> > it complains of node #2 does not exist right after reporting having
>> > init'd node 4. I have no clue what's going wrong here and hope not
>> > to have to undo
>> > and reconfig the cluster from scratch.  These DBs are too large now
>> > for easy subscription during live processing. Any help much
>> > appreciated. -----------------------------------------
>> > 2007-07-05 18:19:18 GMT CONFIG main: edb-replication version 1.1.5 starting up
>> > 2007-07-05 18:19:19 GMT CONFIG main: local node id = 1
>> > 2007-07-05 18:19:19 GMT CONFIG main: launching sched_start_mainloop
>> > 2007-07-05 18:19:19 GMT CONFIG main: loading current cluster configuration
>> > 2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=3 no_comment='slave node 3'
>> > 2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=4 no_comment='slave node 4'
>> > 2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=3 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
>> > 2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=4 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
>> > 2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
>> > 2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
>> > 2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=1 set_origin=1 set_comment='RT3/VCASE replication set'
>> > 2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=2 set_origin=1 set_comment='new set for adding tables'
>> > 2007-07-05 18:19:19 GMT CONFIG main: configuration complete - starting threads
>> > NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=12520
>> > 2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=3
>> > 2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=4
>> > 2007-07-05 18:19:19 GMT FATAL  enableNode: unknown node ID 2
>> > 2007-07-05 18:19:19 GMT INFO   remoteListenThread_4: disconnecting from 'dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
>> > 2007-07-05 18:19:20 GMT INFO   remoteListenThread_3: disconnecting from 'dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
>> >
>> 
>> 
>> -- 
>> #======================================================================#
>> # It's easier to get forgiveness for being wrong than for being right. #
>> # Let's break this rule - forgive me.                                  #
>> #================================================== JanWieck@Yahoo.com #
>> 
> 


-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From jerry at jerrysievers.com  Thu Jul  5 15:23:57 2007
From: jerry at jerrysievers.com (Jerry Sievers)
Date: Thu Jul  5 15:24:37 2007
Subject: [Slony1-general] New master failing;
	still trying to see old master?
In-Reply-To: <5a0a9d6f0707051448q31544d12l365d9f8cb3311732@mail.gmail.com>
References: <m3fy42oeqj.fsf@mama.jerrysievers.com>
	<468D5942.7030902@Yahoo.com> <m3y7huzhhf.fsf@mama.jerrysievers.com>
	<5a0a9d6f0707051448q31544d12l365d9f8cb3311732@mail.gmail.com>
Message-ID: <m3fy42zdz6.fsf@mama.jerrysievers.com>

"Andrew Hammond" <andrew.george.hammond@gmail.com> writes:

> On 05 Jul 2007 17:08:12 -0400, Jerry Sievers <jerry@jerrysievers.com> wrote:
> 
> > Jan Wieck <JanWieck@Yahoo.com> writes:
> >
> > > On 7/5/2007 3:03 PM, Jerry Sievers wrote:
> > >
> > >
> > > Another question: Did you wait for the moveset() to propagate before
> > > you dropped node 2?
> >
> > I'm not sure.  The guy who set this up didn't use Slonik so I had to
> > run the functions by hand.
> 
> Run the functions, in the database, by hand via psql? Which functions
> did you run? Do you have a typescript or psql history file of this?

moveset and dropnode.

There was a lot happening while I tried to get this going.  I regret
not having complete details about  exactly what happened, where and
when. 

Thanks 

> > The system  was quiescent but no solid guarantee that it was fully
> > propogated.
> 
> If you waited a few minutes between the move set and the drop node
> then probably the move propagated first. If you ran the drop
> immediately after the move, then almost certainly the move did not
> propagate.
> 
> Andrew
> 

-- 
-------------------------------------------------------------------------------
Jerry Sievers   732 365-2844 (work)     Production Database Administrator
                305 321-1144 (mobil	WWW E-Commerce Consultant
From by_pacitan at yahoo.com  Fri Jul  6 01:25:40 2007
From: by_pacitan at yahoo.com (angga erwina)
Date: Fri Jul  6 01:26:38 2007
Subject: [Slony1-general] tutorial for the beginner
Message-ID: <292814.54031.qm@web56608.mail.re3.yahoo.com>

iam the new user and trying to use slony to replicate
my database. i read in some articles, this is probably
become the new high end technology to replication with
asynchronous transmition system. so i interested to
trying.i've try to follow the sample.txt but there is
still some error which i dont know what is that and
how to solve it.actually,iam the new one in linux and
postgre..could you help me please for the easiest
tutorial or where i can get the info about it..
thanks,u
regards,
bayu


       
____________________________________________________________________________________
Boardwalk for $500? In 2007? Ha! Play Monopoly Here and Now (it's updated for today's economy) at Yahoo! Games.
http://get.games.yahoo.com/proddesc?gamekey=monopolyherenow  
From by_pacitan at yahoo.com  Fri Jul  6 03:26:59 2007
From: by_pacitan at yahoo.com (angga erwina)
Date: Fri Jul  6 03:28:02 2007
Subject: [Slony1-general] help me please
Message-ID: <987173.68155.qm@web56601.mail.re3.yahoo.com>


Help me please.....
my slony-I 1.2.9
postgre8.2.4
i've installed pgbench and plpgsql
I've make setup.sh like this, but there error messages
like under this??
#!/bin/sh
CLUSTER=test1
MASTERDBNAME=pgbench_node1
SLAVEDBNAME=pgbench_node2
MASTERHOST=localhost
SLAVEHOST=localhost
SLONY_USER=postgres
PGBENCH_USER=postgres
/usr/local/pgsql/bin/slonik <<_EOF_
    cluster name = $CLUSTER;
    node 1 admin conninfo = 'dbname=$MASTERDBNAME
host=$MASTERHOST user=$SLONY_USER';
    node 2 admin conninfo = 'dbname=$SLAVEDBNAME
host=$SLAVEHOST user=$SLONY_USER';
    init cluster ( id = 1, comment = 'Node 1' );
    table add key ( node id = 1, fully qualified name
= 'public.history' );
    create set ( id = 1, origin = 1, comment = 'All
pgbench tables' );
    set add table ( set id = 1, origin = 1,
        id = 1, fully qualified name =
'public.accounts',
        comment = 'Table accounts' );
    set add table ( set id = 1, origin = 1,
        id = 2, fully qualified name =
'public.branches',
        comment = 'Table branches' );
    set add table ( set id = 1, origin = 1,
        id = 3, fully qualified name =
'public.tellers',
        comment = 'Table tellers' );
    set add table ( set id = 1, origin = 1,
        id = 4, fully qualified name =
'public.history',
        key = serial,
        comment = 'Table history' );
        store node ( id = 2, comment = 'Node 2' );
    store path ( server = 1, client = 2,
        conninfo = 'dbname=$MASTERDBNAME
host=$MASTERHOST user=$SLONY_USER');
    store path ( server = 2, client = 1,
        conninfo = 'dbname=$SLAVEDBNAME
host=$SLAVEHOST user=$SLONY_USER');
    store listen ( origin = 1, provider = 1, receiver
= 2 );
    store listen ( origin = 2, provider = 2, receiver
= 1 );
_EOF_

there  were  error messages like this

<stdin>:4: Error: namespace "_test1" already exists in
database of node 1
<stdin>:4: ERROR: no admin conninfo for node 149930256



when i make the subscribe.sh like this

#!/bin/sh
CLUSTER=test1
MASTERDBNAME=pgbench_node1
SLAVEDBNAME=pgbench_node2
MASTERHOST=localhost
SLAVEHOST=localhost
SLONY_USER=postgres
PGBENCH_USER=postgres
/usr/local/pgsql/bin/slonik <<_EOF_
    cluster name = $CLUSTER;
   node 1 admin conninfo = 'dbname=$MASTERDBNAME
host=$MASTERHOST user=$SLONY_USER';
    node 2 admin conninfo = 'dbname=$SLAVEDBNAME
host=$SLAVEHOST user=$SLONY_USER';
    subscribe set ( id = 1, provider = 1, receiver =
2, forward = no);
_EOF_

there were  error messages like this : 

<stdin>:18: PGRES_FATAL_ERROR select
"_test1".subscribeSet(1, 1, 2, 'f');  - ERROR:  insert
or update on table "sl_path" violates foreign key
constraint "pa_client-no_id-ref"
DETAIL:  Key (pa_client)=(2) is not present in table
"sl_node".
CONTEXT:  SQL statement "INSERT INTO "_test1".sl_path
(pa_server, pa_client, pa_conninfo, pa_connretry)
values ( $1 ,  $2 , '<event pending>', 10)"
PL/pgSQL function "subscribeset_int" line 53 at SQL
statement
SQL statement "SELECT  "_test1".subscribeSet_int( $1 ,
 $2 ,  $3 ,  $4 )"
PL/pgSQL function "subscribeset" line 63 at perform

so,what happend in my replicator?? and what should i
do to solve this error

thank kyu
regards,
bayu



       
____________________________________________________________________________________
Be a better Heartthrob. Get better relationship answers from someone who knows. Yahoo! Answers - Check it out. 
http://answers.yahoo.com/dir/?link=list&sid=396545433
From dmitry at koterov.ru  Fri Jul  6 05:46:43 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Fri Jul  6 05:46:49 2007
Subject: [Slony1-general] Bug: slonik sometimes incorrectly splits its
	command by "; "
Message-ID: <d7df81620707060546g6631aaffw504df6fdc3923a7b@mail.gmail.com>

Hello.

I feed the following syntax-correct query to slonik:

CREATE RULE "position_get_last_id_on_insert" AS ON INSERT TO
"public"."position"
DO (SELECT currval('position_position_id_seq'::regclass) AS id;);

And in pg logs I see:

2007-07-06 12:39:08 GMT 87.250.244.99(34424)ERROR:  syntax error at end of
input at character 149
2007-07-06 12:39:08 GMT 87.250.244.99(34424)STATEMENT:
        CREATE RULE "position_get_last_id_on_insert" AS ON INSERT TO
"public"."position"
        DO (SELECT currval('position_position_id_seq'::regclass) AS id;

Seems slonik treats intermediate ";" as a queries delimiter and splits
queries by it.
Possibly slonik should care about correct brackets nesting?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070706/=
68bfba97/attachment.htm
From ajs at crankycanuck.ca  Fri Jul  6 07:54:03 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri Jul  6 07:54:24 2007
Subject: [Slony1-general] tutorial for the beginner
In-Reply-To: <292814.54031.qm@web56608.mail.re3.yahoo.com>
References: <292814.54031.qm@web56608.mail.re3.yahoo.com>
Message-ID: <20070706145403.GI20115@phlogiston.dyndns.org>

On Fri, Jul 06, 2007 at 01:25:40AM -0700, angga erwina wrote:
> postgre..could you help me please for the easiest
> tutorial or where i can get the info about it..

The tutuorial you used is the easiest I know of.  You might want to
check out articles on varlena.com as well.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
In the future this spectacle of the middle classes shocking the avant-
garde will probably become the textbook definition of Postmodernism. 
                --Brad Holland
From darcyb at commandprompt.com  Fri Jul  6 08:16:35 2007
From: darcyb at commandprompt.com (Darcy Buskermolen)
Date: Fri Jul  6 08:17:13 2007
Subject: [Slony1-general] Bug: slonik sometimes incorrectly splits its
	command by "; "
In-Reply-To: <d7df81620707060546g6631aaffw504df6fdc3923a7b@mail.gmail.com>
References: <d7df81620707060546g6631aaffw504df6fdc3923a7b@mail.gmail.com>
Message-ID: <200707060816.36496.darcyb@commandprompt.com>

On July 6, 2007 05:46 am, Dmitry Koterov wrote:
> Hello.
>
> I feed the following syntax-correct query to slonik:
>
> CREATE RULE "position_get_last_id_on_insert" AS ON INSERT TO
> "public"."position"
> DO (SELECT currval('position_position_id_seq'::regclass) AS id;);
>
> And in pg logs I see:
>
> 2007-07-06 12:39:08 GMT 87.250.244.99(34424)ERROR:  syntax error at end of
> input at character 149
> 2007-07-06 12:39:08 GMT 87.250.244.99(34424)STATEMENT:
>         CREATE RULE "position_get_last_id_on_insert" AS ON INSERT TO
> "public"."position"
>         DO (SELECT currval('position_position_id_seq'::regclass) AS id;
>
> Seems slonik treats intermediate ";" as a queries delimiter and splits
> queries by it.
> Possibly slonik should care about correct brackets nesting?

How did you feed the to slonik?  slonik does not speek SQL, but is a interface 
to the internal slon functions.



-- 
Darcy Buskermolen
Command Prompt, Inc.
+1.503.667.4564 X 102
http://www.commandprompt.com/
PostgreSQL solutions since 1997
From darcy at dbitech.ca  Fri Jul  6 08:20:35 2007
From: darcy at dbitech.ca (Darcy Buskermolen)
Date: Fri Jul  6 08:21:04 2007
Subject: [Slony1-general] tutorial for the beginner
In-Reply-To: <20070706145403.GI20115@phlogiston.dyndns.org>
References: <292814.54031.qm@web56608.mail.re3.yahoo.com>
	<20070706145403.GI20115@phlogiston.dyndns.org>
Message-ID: <200707060820.36967.darcy@dbitech.ca>

On Friday 06 July 2007 07:54, Andrew Sullivan wrote:
> On Fri, Jul 06, 2007 at 01:25:40AM -0700, angga erwina wrote:
> > postgre..could you help me please for the easiest
> > tutorial or where i can get the info about it..
>
> The tutuorial you used is the easiest I know of.  You might want to
> check out articles on varlena.com as well.

The more I think about these tutorials, the more I think we are doing ourself 
disfavor by using shell expression substution etc, I have a feeling that 
there are a fair number of people exploring slony who are not as familiar 
with the world of shell as they ought to be to inorder to understand what is 
going on in here.    I think we would be better off using the define and 
include features to make this all a bit more end user 
friendly/understandable.   Thoughts?

>
> A
From cbbrowne at ca.afilias.info  Fri Jul  6 08:32:00 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Jul  6 08:32:51 2007
Subject: [Slony1-general] Bug: slonik sometimes incorrectly splits its
	command by "; "
In-Reply-To: <200707060816.36496.darcyb@commandprompt.com> (Darcy
	Buskermolen's message of "Fri, 6 Jul 2007 08:16:35 -0700")
References: <d7df81620707060546g6631aaffw504df6fdc3923a7b@mail.gmail.com>
	<200707060816.36496.darcyb@commandprompt.com>
Message-ID: <60r6nl1rbj.fsf@dba2.int.libertyrms.com>

Darcy Buskermolen <darcyb@commandprompt.com> writes:

> On July 6, 2007 05:46 am, Dmitry Koterov wrote:
>> Hello.
>>
>> I feed the following syntax-correct query to slonik:
>>
>> CREATE RULE "position_get_last_id_on_insert" AS ON INSERT TO
>> "public"."position"
>> DO (SELECT currval('position_position_id_seq'::regclass) AS id;);
>>
>> And in pg logs I see:
>>
>> 2007-07-06 12:39:08 GMT 87.250.244.99(34424)ERROR:  syntax error at end of
>> input at character 149
>> 2007-07-06 12:39:08 GMT 87.250.244.99(34424)STATEMENT:
>>         CREATE RULE "position_get_last_id_on_insert" AS ON INSERT TO
>> "public"."position"
>>         DO (SELECT currval('position_position_id_seq'::regclass) AS id;
>>
>> Seems slonik treats intermediate ";" as a queries delimiter and splits
>> queries by it.
>> Possibly slonik should care about correct brackets nesting?
>
> How did you feed the to slonik?  slonik does not speek SQL, but is a interface 
> to the internal slon functions.

Regrettably, this looks like it is indeed a bug in the statement
parser in slonik.

If I add the following queries to the test set:
=========================================================
-- Here is a rule creation with an embedded semicolon

create table "public"."position";

CREATE RULE "position_get_last_id_on_insert"
AS ON INSERT TO "public"."position" DO (SELECT
currval('position_position_id_seq'::regclass) AS id;);
=========================================================

Here is the (relevant parts of) output from the scanner tester:

=========================================================

statement 14
-------------------------------------------



-- Here is a rule creation with an embedded semicolon

create table "public"."position";
statement 15
-------------------------------------------


CREATE RULE "position_get_last_id_on_insert2"
AS ON INSERT TO "public"."position" DO (SELECT
currval('position_position_id_seq'::regclass) AS id;
statement 16
-------------------------------------------
);%  
=========================================================

The statement is being split at the too-early semicolon.

This is evidently a bug.

I think it shouldn't be too difficult to add parenthesis counting to
the parser, but that won't help dmitry too soon or too easily.

There is, happily, a workaround to be offered; if the semicolon is
removed, the query should get processed as expected:

=========================================================
CREATE RULE "position_get_last_id_on_insert2"
AS ON INSERT TO "public"."position" DO (SELECT
currval('position_position_id_seq'::regclass) AS id);
=========================================================
-- 
let name="cbbrowne" and tld="ca.afilias.info" in name ^ "@" ^ tld;;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From ajs at crankycanuck.ca  Fri Jul  6 09:12:58 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri Jul  6 09:13:19 2007
Subject: [Slony1-general] tutorial for the beginner
In-Reply-To: <200707060820.36967.darcy@dbitech.ca>
References: <292814.54031.qm@web56608.mail.re3.yahoo.com>
	<20070706145403.GI20115@phlogiston.dyndns.org>
	<200707060820.36967.darcy@dbitech.ca>
Message-ID: <20070706161258.GT20115@phlogiston.dyndns.org>

On Fri, Jul 06, 2007 at 08:20:35AM -0700, Darcy Buskermolen wrote:
> disfavor by using shell expression substution etc, I have a feeling that 
> there are a fair number of people exploring slony who are not as familiar 
> with the world of shell as they ought to be to inorder to understand what is 
> going on in here.    

I think that's possibly true.

> I think we would be better off using the define and 
> include features to make this all a bit more end user 
> friendly/understandable.   Thoughts?

I also think that _might_ make things better, although it's by no
means plain to me that someone who doesn't understand shell
substitution will understand define and include features, either.

I think the _real_ problem lies elsewhere: the administration
functions for Slony remain extremely hacky.  I suspect we need
something much shinier to make this easy to use.  The problem I have
in suggesting how to do that is that _I_ find command line tools
easier (I type this from a Mac with at least 6 terminal windows open,
and three or four Aquamacs windows to boot).  Probably someone with a
stronger UI leaning would be more helpful.  Anyone?  

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
This work was visionary and imaginative, and goes to show that visionary
and imaginative work need not end up well. 
		--Dennis Ritchie
From jerry at jerrysievers.com  Fri Jul  6 09:52:07 2007
From: jerry at jerrysievers.com (Jerry Sievers)
Date: Fri Jul  6 09:52:23 2007
Subject: [Slony1-general] 	New master failing;
	still trying to see old master?
In-Reply-To: <468D6822.9080309@Yahoo.com>
References: <m3fy42oeqj.fsf@mama.jerrysievers.com>
	<468D5942.7030902@Yahoo.com> <m3lkduzgsu.fsf@mama.jerrysievers.com>
	<468D6822.9080309@Yahoo.com>
Message-ID: <m3k5tdcw5k.fsf@mama.jerrysievers.com>

Jan;  A great big thanks for your assistance yesterday. 

I believe the tips on what to manually remove from the events tables
did the trick.  

The Slon  quit failing on the master and  eventually, both of the
slaves came up to date again.  This is a big relief due to these DBs
being about 120GB in size and 650 max TPS with approx 300TPS sustained
workload. 

We've found it impossible now during near past attempts to init a new
slave due to the huge event backlog that's in place once the sets are
subscribed.  As such, we can't afford to lose slaves, one of which we
did lose yesterday due to a corrupt application table.

I suppose reconfiguring the system to have several sets and possibly
even sets having only one large table and trying to bring up a new
slave this way may be an option.  This DB is poorly designed and has
no FKs whatsoever (a bad thing turned good in this case.)

Anyway, that's a whole other issue.

Have a great weekend. 


Jan Wieck <JanWieck@Yahoo.com> writes:

> On 7/5/2007 5:22 PM, Jerry Sievers wrote:
> 
> > Selecting all non-sync events from each of the 3 nodes ordered by
> > ev_seqno.
> 
> I think I see what's going on here ... maybe.
> 
> This is probably a pilot error in connection with a copy/paste mistake
> sitting in slon for ages.
> 
> The copy/paste mistake is:
>      the error message in disableNode() says "enableNode(): ...".
>      I claim ownership of that one.
> 
> The pilot error is:
>      the dropnode() was issued multiple times against different nodes
>      without giving them time to propagate (in this case nodes 1 and 4).
>      They are events (1,2225224) and (4,1863698).
> 
> Nice screwup. However since all 3 nodes don't have node 2 in the
> sl_node table any more (at least from what I see they should not), it
> is safe to
> 
>      DELETE FROM sl_event WHERE ev_origin = 4 and ev_seqno = 1863698;
>      DELETE FROM sl_event WHERE ev_origin = 1 and ev_seqno = 2225224;
> 
> 
> Jan
> 
> > Thanks!
> > Pager usage is off.
> > Expanded display is on.
> > -[ RECORD 1 ]+------------------------------------------------------------------------
> > ev_origin    | 1
> > ev_seqno     | 2225126
> > ev_timestamp | 05-JUL-07 14:57:16.056801
> > ev_minxid    | 884391402
> > ev_maxxid    | 884391412
> > ev_xip       | '884391409','884391411'
> > ev_type      | ACCEPT_SET
> > ev_data1     | 1
> > ev_data2     | 2
> > ev_data3     | 1
> > ev_data4     | ev_data5     | ev_data6     | ev_data7     | ev_data8
> > | -[ RECORD 2
> > ]+------------------------------------------------------------------------
> > ev_origin    | 1
> > ev_seqno     | 2225133
> > ev_timestamp | 05-JUL-07 14:58:26.439281
> > ev_minxid    | 884391608
> > ev_maxxid    | 884391609
> > ev_xip       | ev_type      | ACCEPT_SET
> > ev_data1     | 2
> > ev_data2     | 2
> > ev_data3     | 1
> > ev_data4     | ev_data5     | ev_data6     | ev_data7     | ev_data8
> > | -[ RECORD 3
> > ]+------------------------------------------------------------------------
> > ev_origin    | 1
> > ev_seqno     | 2225224
> > ev_timestamp | 05-JUL-07 15:49:54.253471
> > ev_minxid    | 884528335
> > ev_maxxid    | 884697167
> > ev_xip       | '884528335','884697160','884697162','884697161','884587782','884697166'
> > ev_type      | DROP_NODE
> > ev_data1     | 2
> > ev_data2     | ev_data3     | ev_data4     | ev_data5     | ev_data6
> > | ev_data7     | ev_data8     | Pager usage is off.
> > Expanded display is on.
> > -[ RECORD 1 ]+--------------------------
> > ev_origin    | 4
> > ev_seqno     | 1863698
> > ev_timestamp | 05-JUL-07 15:52:40.518681
> > ev_minxid    | 385609088
> > ev_maxxid    | 385609089
> > ev_xip       | ev_type      | DROP_NODE
> > ev_data1     | 2
> > ev_data2     | ev_data3     | ev_data4     | ev_data5     | ev_data6
> > | ev_data7     | ev_data8     | Pager usage is off.
> > Expanded display is on.
> > -[ RECORD 1 ]+--------------------------
> > ev_origin    | 4
> > ev_seqno     | 1863698
> > ev_timestamp | 05-JUL-07 15:52:40.518681
> > ev_minxid    | 385609088
> > ev_maxxid    | 385609089
> > ev_xip       | ev_type      | DROP_NODE
> > ev_data1     | 2
> > ev_data2     | ev_data3     | ev_data4     | ev_data5     | ev_data6
> > | ev_data7     | ev_data8     | Jan Wieck <JanWieck@Yahoo.com>
> > writes:
> >
> >> On 7/5/2007 3:03 PM, Jerry Sievers wrote:
> >> > Crisis today.  Complete power failure leaves a corrupt table on
> >> old
> >> > master. I did moveset() and dropnode() to reconfigure the cluster.
> >> > The old
> >> > master was node 2.    New master is node 1.   There are now just 2
> >> > slaves 3 and 4.
> >> Another question: Did you wait for the moveset() to propagate before
> >> you dropped node 2?
> >> Jan
> >> > For some reason however, when I try to fire up the slon on the
> >> > master,
> >> > it complains of node #2 does not exist right after reporting having
> >> > init'd node 4. I have no clue what's going wrong here and hope not
> >> > to have to undo
> >> > and reconfig the cluster from scratch.  These DBs are too large now
> >> > for easy subscription during live processing. Any help much
> >> > appreciated. -----------------------------------------
> >> > 2007-07-05 18:19:18 GMT CONFIG main: edb-replication version 1.1.5 starting up
> >> > 2007-07-05 18:19:19 GMT CONFIG main: local node id = 1
> >> > 2007-07-05 18:19:19 GMT CONFIG main: launching sched_start_mainloop
> >> > 2007-07-05 18:19:19 GMT CONFIG main: loading current cluster configuration
> >> > 2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=3 no_comment='slave node 3'
> >> > 2007-07-05 18:19:19 GMT CONFIG storeNode: no_id=4 no_comment='slave node 4'
> >> > 2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=3 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
> >> > 2007-07-05 18:19:19 GMT CONFIG storePath: pa_server=4 pa_client=1 pa_conninfo="dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432" pa_connretry=5
> >> > 2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=3 li_receiver=1 li_provider=3
> >> > 2007-07-05 18:19:19 GMT CONFIG storeListen: li_origin=4 li_receiver=1 li_provider=4
> >> > 2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=1 set_origin=1 set_comment='RT3/VCASE replication set'
> >> > 2007-07-05 18:19:19 GMT CONFIG storeSet: set_id=2 set_origin=1 set_comment='new set for adding tables'
> >> > 2007-07-05 18:19:19 GMT CONFIG main: configuration complete - starting threads
> >> > NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=12520
> >> > 2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=3
> >> > 2007-07-05 18:19:19 GMT CONFIG enableNode: no_id=4
> >> > 2007-07-05 18:19:19 GMT FATAL  enableNode: unknown node ID 2
> >> > 2007-07-05 18:19:19 GMT INFO   remoteListenThread_4: disconnecting from 'dbname=rt3_01 host=192.168.30.173 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
> >> > 2007-07-05 18:19:20 GMT INFO   remoteListenThread_3: disconnecting from 'dbname=rt3_01 host=192.168.30.172 user=slonik password=foo.j1MiTikGop0rytQuedPid8 port=5432'
> >> >
> >> -- 
> >> #======================================================================#
> >> # It's easier to get forgiveness for being wrong than for being right. #
> >> # Let's break this rule - forgive me.                                  #
> >> #================================================== JanWieck@Yahoo.com #
> >>
> >
> 
> 
> -- 
> #======================================================================#
> # It's easier to get forgiveness for being wrong than for being right. #
> # Let's break this rule - forgive me.                                  #
> #================================================== JanWieck@Yahoo.com #
> 

-- 
-------------------------------------------------------------------------------
Jerry Sievers   732 365-2844 (work)     Production Database Administrator
                305 321-1144 (mobil	WWW E-Commerce Consultant
From darcyb at commandprompt.com  Fri Jul  6 10:06:57 2007
From: darcyb at commandprompt.com (Darcy Buskermolen)
Date: Fri Jul  6 10:07:24 2007
Subject: [Slony1-general] tutorial for the beginner
In-Reply-To: <20070706161258.GT20115@phlogiston.dyndns.org>
References: <292814.54031.qm@web56608.mail.re3.yahoo.com>
	<200707060820.36967.darcy@dbitech.ca>
	<20070706161258.GT20115@phlogiston.dyndns.org>
Message-ID: <200707061006.57931.darcyb@commandprompt.com>

On July 6, 2007 09:12 am, Andrew Sullivan wrote:
> On Fri, Jul 06, 2007 at 08:20:35AM -0700, Darcy Buskermolen wrote:
> > disfavor by using shell expression substution etc, I have a feeling that
> > there are a fair number of people exploring slony who are not as familiar
> > with the world of shell as they ought to be to inorder to understand what
> > is going on in here.
>
> I think that's possibly true.
>
> > I think we would be better off using the define and
> > include features to make this all a bit more end user
> > friendly/understandable.   Thoughts?
>
> I also think that _might_ make things better, although it's by no
> means plain to me that someone who doesn't understand shell
> substitution will understand define and include features, either.

Ther's less need for escaping/quoting and the like using the defines than 
there is in shell.

Also by moving to the includes/defines I think it will create a better best 
practices example.  I've run into many an occasion where because the example 
uses shell and heredoc style notation, that people try and munge their 
production replication scripts into that format and get really baffled when 
it parse errors, or otherwise dosn't work.



>
> I think the _real_ problem lies elsewhere: the administration
> functions for Slony remain extremely hacky.  I suspect we need
> something much shinier to make this easy to use.  The problem I have
> in suggesting how to do that is that _I_ find command line tools
> easier (I type this from a Mac with at least 6 terminal windows open,
> and three or four Aquamacs windows to boot).  Probably someone with a
> stronger UI leaning would be more helpful.  Anyone?

Agreed we probabbly need something shiny,  there is always pgadminIII.  But 
along this line PostgreSQL does not provide anything more shiny to the end 
user than psql, granted it's very polished.

>
> A

-- 
Darcy Buskermolen
Command Prompt, Inc.
+1.503.667.4564 X 102
http://www.commandprompt.com/
PostgreSQL solutions since 1997
From vivek at khera.org  Fri Jul  6 11:16:42 2007
From: vivek at khera.org (Vivek Khera)
Date: Fri Jul  6 11:16:57 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
Message-ID: <57E4C58C-10A3-43D1-8726-AA14914DF49C@khera.org>


On Jun 28, 2007, at 6:17 PM, Christopher Browne wrote:

>
> - getting the scripted tests to generate some SQL INSERT statements
> summarizing the way the tests went; that would very much ease
> collecting data about conformance

I hope you're not gonna accept these raw from random sources on the  
net posting test results...  You'd be better off posting a tabular  
format from which you can parse the data into SQL.

From vivek at khera.org  Fri Jul  6 11:18:52 2007
From: vivek at khera.org (Vivek Khera)
Date: Fri Jul  6 11:19:05 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <92869e660706290358l2d37b429k30bab22d55e868ae@mail.gmail.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com> <4684DBEB.70602@iol.ie>
	<92869e660706290358l2d37b429k30bab22d55e868ae@mail.gmail.com>
Message-ID: <FD5D9E0E-E72A-41FB-968F-4428EC2C0924@khera.org>


On Jun 29, 2007, at 6:58 AM, Filip Rembia?kowski wrote:

> To produce a "clean" dump ( that is, without Slony-I schema and
> triggers ) we have to install temporary backup node, wait for it to
> catch up, DROP NODE it , and dump it.  Whole process takes almost
> twice longer as plain pg_dump (and is more failure-prone)

I do a "plain" pg_dump on the origin node.

To restore, I run pg_restore -l to generate a list of objects,  
comment out anything inside the _REPLICATION schema and any non-data  
resources, then restore into a clean schema generated via the script  
Jan created to make the clean schema :-)

From vivek at khera.org  Fri Jul  6 11:25:19 2007
From: vivek at khera.org (Vivek Khera)
Date: Fri Jul  6 11:25:34 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <468BE796.2030807@Yahoo.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>	<1183151091.28247.44.camel@dogma.v10.wvs>	<608xa25sak.fsf@dba2.int.libertyrms.com>	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>	<60ved14fcn.fsf@dba2.int.libertyrms.com>
	<468BAA40.1030505@Yahoo.com>	<20070704144451.GA14262@phlogiston.dyndns.org>
	<60ejjo43pi.fsf@dba2.int.libertyrms.com>
	<468BE796.2030807@Yahoo.com>
Message-ID: <64F18FEF-F70C-4CA6-BC99-5130973F6FF8@khera.org>


On Jul 4, 2007, at 2:31 PM, Jan Wieck wrote:

> What I see here is that we are trying to come up with a special  
> case optimization for mass-deletes. No mass insert or update  
> operations will benefit from any of this. Do people do mass deletes  
> that much that we really have to worry about them?

I do mass deletes twice monthly to prune out old data.  Usually  
several tens of millions of rows.  On the origin it looks like  
'delete from XXX where object_id=N' which cascade deletes via FK  
relationships to gazillions of rows.

From cbbrowne at ca.afilias.info  Fri Jul  6 11:40:28 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Jul  6 11:40:42 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <57E4C58C-10A3-43D1-8726-AA14914DF49C@khera.org> (Vivek Khera's
	message of "Fri, 6 Jul 2007 14:16:42 -0400")
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<57E4C58C-10A3-43D1-8726-AA14914DF49C@khera.org>
Message-ID: <60d4z51ilf.fsf@dba2.int.libertyrms.com>

Vivek Khera <vivek@khera.org> writes:
> On Jun 28, 2007, at 6:17 PM, Christopher Browne wrote:
>
>>
>> - getting the scripted tests to generate some SQL INSERT statements
>> summarizing the way the tests went; that would very much ease
>> collecting data about conformance
>
> I hope you're not gonna accept these raw from random sources on the
> net posting test results...  You'd be better off posting a tabular
> format from which you can parse the data into SQL.

Good point.  Some might consider it an "SQL injection" attack ;-).

I think I'll still work provisionally with SQL for the time being;
that lets us make stronger inferences about data types and such.  And
before letting random sources to post test results, this will need to
get turned into something rather less "security challenging."

I have noted this in the TODO.
-- 
"Note that if I can get you  to `su and say' something just by asking,
you have a very serious security problem on your system and you should
look into it."  -- Paul Vixie, vixie-cron 3.0.1 installation notes
From cbbrowne at ca.afilias.info  Fri Jul  6 11:42:33 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Jul  6 11:42:51 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <64F18FEF-F70C-4CA6-BC99-5130973F6FF8@khera.org> (Vivek Khera's
	message of "Fri, 6 Jul 2007 14:25:19 -0400")
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com> <468BAA40.1030505@Yahoo.com>
	<20070704144451.GA14262@phlogiston.dyndns.org>
	<60ejjo43pi.fsf@dba2.int.libertyrms.com> <468BE796.2030807@Yahoo.com>
	<64F18FEF-F70C-4CA6-BC99-5130973F6FF8@khera.org>
Message-ID: <608x9t1ihy.fsf@dba2.int.libertyrms.com>

Vivek Khera <vivek@khera.org> writes:
> On Jul 4, 2007, at 2:31 PM, Jan Wieck wrote:
>> What I see here is that we are trying to come up with a special
>> case optimization for mass-deletes. No mass insert or update
>> operations will benefit from any of this. Do people do mass deletes
>> that much that we really have to worry about them?
>
> I do mass deletes twice monthly to prune out old data.  Usually
> several tens of millions of rows.  On the origin it looks like
> 'delete from XXX where object_id=N' which cascade deletes via FK
> relationships to gazillions of rows.

That would be a case where there could be some "win" from joining
deletes together at least somewhat...  So I think this feature is one
that can't be so quickly ruled out...
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From ajs at crankycanuck.ca  Fri Jul  6 12:36:50 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri Jul  6 12:37:21 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <608x9t1ihy.fsf@dba2.int.libertyrms.com>
References: <1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com>
	<468BAA40.1030505@Yahoo.com>
	<20070704144451.GA14262@phlogiston.dyndns.org>
	<60ejjo43pi.fsf@dba2.int.libertyrms.com>
	<468BE796.2030807@Yahoo.com>
	<64F18FEF-F70C-4CA6-BC99-5130973F6FF8@khera.org>
	<608x9t1ihy.fsf@dba2.int.libertyrms.com>
Message-ID: <20070706193650.GA21078@phlogiston.dyndns.org>

On Fri, Jul 06, 2007 at 02:42:33PM -0400, Christopher Browne wrote:
> That would be a case where there could be some "win" from joining
> deletes together at least somewhat...  So I think this feature is one
> that can't be so quickly ruled out...

I also wonder, though, whether it mightn't be exposed by EXECUTE or
something instead, presumably with a lowish lock level.  I'm just
worried about the potential for this to turn into a big expensive
operation for ordinary cases because we've optimised the admittedly
awful mass-delete behaviour (made even worse because PostgreSQL
itself is sort of brutal with mass deletes, due to all the dead
rows).  Something that was special that way would also reduce the
exposure from hairy, complicated bits in the main line code, which as
others have already noted is somewhat mysterious to many developers.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
The fact that technology doesn't work is no bar to success in the marketplace.
		--Philip Greenspun
From pgsql at j-davis.com  Fri Jul  6 15:51:02 2007
From: pgsql at j-davis.com (Jeff Davis)
Date: Fri Jul  6 15:51:34 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <468AC521.3000804@ca.afilias.info>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com>
	<1183486568.10735.14.camel@dogma.ljc.laika.com>
	<468AC521.3000804@ca.afilias.info>
Message-ID: <1183762262.10735.33.camel@dogma.ljc.laika.com>

On Tue, 2007-07-03 at 21:52 +0000, Christopher Browne wrote:
> 1.  Delete with the 5M individual DELETE statements.  (Which you found 
> took 552.49s)
> 2.  Delete with 50K DELETE statements, each having a WHERE clause with 
> 100 items in it.
> 3.  Delete with 5K DELETE statements, each having a WHERE clause with 1K 
> items in it.
> 
> If 2. or 3. come *way* closer to 9.41s, then it may be worth exploring 
> the complexity of folding together adjacent deletes on the same table.  
> There could also be a case made for trying sequential versus random 
> orderings (e.g. - in the former case, each DELETE statement takes on a 
> specific range of items whereas in the latter, each selects items more 
> or less at random).
> 
> I'll see about constructing a series of tests like this; won't be 
> running before I send this :-).  If you have time to generate 2. and/or 
> 3., on your system and get timings there, I'd be much obliged.
> 

I apologize for the slow response. Here are my results:

DELETE 5000000 at a time: 19.571929
DELETE    1000 at a time: 15.999146
DELETE     100 at a time: 18.946113
DELETE       1 at a time: 770.507714
UPDATE 5000000 at a time: 94.686762
UPDATE    1000 at a time: 86.327752
UPDATE     100 at a time: 103.473719
UPDATE       1 at a time: 963.358307

My test scripts are attached. I am seeing the same kind of slowness when
it's deleting all versus in chunks of 1000, which is confusing me also.

Notice in my scripts that the way the table is created is perfectly
clustered.

Regards,
	Jeff Davis
-------------- next part --------------
require 'postgres'

db = PGconn.connect(nil,nil,nil,nil,'db02','jdavis',nil)

db.exec %q{
        drop table if exists delete_test;
        create table delete_test( a1 int8, a2 int8, a3 int8, a4 int8 );
        insert into delete_test select
                generate_series,
                generate_series+1,
                generate_series+2,
                generate_series+3
                from generate_series(1,5000000);
        create unique index delete_test_idx on delete_test(a1);
        analyze delete_test;
}

sleep 100

t1 = Time.now
db.exec "BEGIN"
i = 0
x = ARGV[0].to_i
while i < 5000000 do
        db.exec "DELETE FROM delete_test WHERE a1 > #{i} and a1 <= #{i+x}"
        i += x
end
db.exec "COMMIT"
t2 = Time.now

puts t2-t1

-------------- next part --------------
require 'postgres'

db = PGconn.connect(nil,nil,nil,nil,'db02','jdavis',nil)

db.exec %q{
        drop table if exists update_test;
        create table update_test( a1 int8, a2 int8, a3 int8, a4 int8 );
        insert into update_test select
                generate_series,
                generate_series+1,
                generate_series+2,
                generate_series+3
                from generate_series(1,5000000);
        create unique index update_test_idx on update_test(a1);
        analyze update_test;
}

sleep 100

t1 = Time.now
db.exec "BEGIN"
i = 0
x = ARGV[0].to_i
while i < 5000000 do
        db.exec "UPDATE update_test SET a2=-a2 WHERE a1 > #{i} and a1 <= #{i+x}"
        i += x
end
db.exec "COMMIT"
t2 = Time.now

puts t2-t1

From pgsql at j-davis.com  Fri Jul  6 15:54:17 2007
From: pgsql at j-davis.com (Jeff Davis)
Date: Fri Jul  6 15:54:45 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <1183537222.28714.27.camel@coppola.muc.ecircle.de>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com>
	<1183486568.10735.14.camel@dogma.ljc.laika.com>
	<468AC895.6060808@ca.afilias.info> <468AD362.7090008@ca.afilias.info>
	<1183537222.28714.27.camel@coppola.muc.ecircle.de>
Message-ID: <1183762457.10735.35.camel@dogma.ljc.laika.com>

On Wed, 2007-07-04 at 10:20 +0200, Csaba Nagy wrote:
> On Wed, 2007-07-04 at 00:53, Christopher Browne wrote:
> > One at a time took about 15 minutes
> > 100 at a time took 3:15
> > 1000 at a time took longer than 100 at a time (curious, that!)
> > all in one shot took 43 seconds.
> 
> Check the plans for the 100 vs. 1000 cases: I'm pretty sure 100 goes for
> bitmap index scan and 1000 goes for sequential scan... and 10 * 100
> bitmap index scans are probably somewhat faster than 1 sequential scan
> on your table/box. I guess 1000 is close to the limit between the
> performance turnover between the index scan and sequential scan on your
> table/box/setup, but the sequential scan is slightly underestimated by
> the planner.
> 
> BTW, the bitmap index scan case should theoretically be the fastest, so
> aiming for the highest chunk size where the planner still chooses bitmap
> index scan (or downright forcing it to do so if possible) would give the
> best performance.
> 

Why would a bitmap index scan be faster than a sequential scan when
deleting the entire table?

Regards,
	Jeff Davis

From pgsql at j-davis.com  Fri Jul  6 16:01:12 2007
From: pgsql at j-davis.com (Jeff Davis)
Date: Fri Jul  6 16:05:35 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <468BE796.2030807@Yahoo.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com> <468BAA40.1030505@Yahoo.com>
	<20070704144451.GA14262@phlogiston.dyndns.org>
	<60ejjo43pi.fsf@dba2.int.libertyrms.com> <468BE796.2030807@Yahoo.com>
Message-ID: <1183762872.10735.37.camel@dogma.ljc.laika.com>

On Wed, 2007-07-04 at 14:31 -0400, Jan Wieck wrote:
> What I see here is that we are trying to come up with a special case 
> optimization for mass-deletes. No mass insert or update operations will 
> benefit from any of this. Do people do mass deletes that much that we 
> really have to worry about them?

Why does this not apply to UPDATEs as well? 

Regards,
	Jeff Davis

From cbbrowne at ca.afilias.info  Sat Jul  7 11:00:41 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Sat Jul  7 11:00:59 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <1183762457.10735.35.camel@dogma.ljc.laika.com> (Jeff Davis's
	message of "Fri, 06 Jul 2007 15:54:17 -0700")
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com>
	<1183486568.10735.14.camel@dogma.ljc.laika.com>
	<468AC895.6060808@ca.afilias.info> <468AD362.7090008@ca.afilias.info>
	<1183537222.28714.27.camel@coppola.muc.ecircle.de>
	<1183762457.10735.35.camel@dogma.ljc.laika.com>
Message-ID: <60zm283xh2.fsf@dba2.int.libertyrms.com>

Jeff Davis <pgsql@j-davis.com> writes:
> On Wed, 2007-07-04 at 10:20 +0200, Csaba Nagy wrote:
>> On Wed, 2007-07-04 at 00:53, Christopher Browne wrote:
>> > One at a time took about 15 minutes
>> > 100 at a time took 3:15
>> > 1000 at a time took longer than 100 at a time (curious, that!)
>> > all in one shot took 43 seconds.
>> 
>> Check the plans for the 100 vs. 1000 cases: I'm pretty sure 100 goes for
>> bitmap index scan and 1000 goes for sequential scan... and 10 * 100
>> bitmap index scans are probably somewhat faster than 1 sequential scan
>> on your table/box. I guess 1000 is close to the limit between the
>> performance turnover between the index scan and sequential scan on your
>> table/box/setup, but the sequential scan is slightly underestimated by
>> the planner.
>> 
>> BTW, the bitmap index scan case should theoretically be the fastest, so
>> aiming for the highest chunk size where the planner still chooses bitmap
>> index scan (or downright forcing it to do so if possible) would give the
>> best performance.
>> 
>
> Why would a bitmap index scan be faster than a sequential scan when
> deleting the entire table?

It wouldn't be...

The thing is, on the subscriber side, the deletes take place one tuple
at a time.  In THAT case, putting several into one DELETE request does
validly favour a bitmap index scan.

This may remain controversial for a while; it seems evident that there
is a gain to be had by doing several DELETEs together.  Unfortunately,
it is also evident that this will complicate the code.

>From your numbers, it looks like "maxxing out" at 100 in a statement
is likely optimal, *way* faster than doing a delete at a time, and not
materially worse than the "where-less" DELETE FROM FOO;

This is not the next thing to change, but I think it should be changed
someday...
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From adam.cassar at netregistry.com.au  Sun Jul  8 22:04:18 2007
From: adam.cassar at netregistry.com.au (Adam Cassar)
Date: Sun Jul  8 05:04:53 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <60d4z51ilf.fsf@dba2.int.libertyrms.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>	<57E4C58C-10A3-43D1-8726-AA14914DF49C@khera.org>
	<60d4z51ilf.fsf@dba2.int.libertyrms.com>
Message-ID: <4691C1D2.4080907@netregistry.com.au>

What about an easy way to resync the slaves from the master?


From by_pacitan at yahoo.com  Sun Jul  8 09:07:21 2007
From: by_pacitan at yahoo.com (angga erwina)
Date: Sun Jul  8 09:07:27 2007
Subject: [Slony1-general] help me please
Message-ID: <448258.66036.qm@web56608.mail.re3.yahoo.com>

Help me please.....
  my slony-I 1.2.9
  postgre8.2.4
  i've installed pgbench and plpgsql
  I've make setup.sh like this, but there error messages like under this??
  #!/bin/sh
  CLUSTER=test1
  MASTERDBNAME=pgbench_node1
  SLAVEDBNAME=pgbench_node2
  MASTERHOST=localhost
  SLAVEHOST=localhost
  SLONY_USER=postgres
  PGBENCH_USER=postgres
  /usr/local/pgsql/bin/slonik <<_EOF_
      cluster name = $CLUSTER;
      node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST user=$SLONY_USER';
      node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST user=$SLONY_USER';
      init cluster ( id = 1, comment = 'Node 1' );
      table add key ( node id = 1, fully qualified name = 'public.history' );
      create set ( id = 1, origin = 1, comment = 'All pgbench tables' );
      set add table ( set id = 1, origin = 1,
          id = 1, fully qualified name = 'public.accounts',
          comment = 'Table accounts' );
      set add table ( set id = 1, origin = 1,
          id = 2, fully qualified name = 'public.branches',
          comment = 'Table branches' );
      set add table ( set id = 1, origin = 1,
          id = 3, fully qualified name = 'public.tellers',
          comment = 'Table tellers' );
      set add table ( set id = 1, origin = 1,
          id = 4, fully qualified name = 'public.history',
          key = serial,
          comment = 'Table history' );
          store node ( id = 2, comment = 'Node 2' );
      store path ( server = 1, client = 2,
          conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST user=$SLONY_USER');
      store path ( server = 2, client = 1,
          conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST user=$SLONY_USER');
      store listen ( origin = 1, provider = 1, receiver = 2 );
      store listen ( origin = 2, provider = 2, receiver = 1 );
  _EOF_
   
  there  were  error messages like this
  <stdin>:4: Error: namespace "_test1" already exists in database of node 1
  <stdin>:4: ERROR: no admin conninfo for node 149930256
   
   
   
  when i make the subscribe.sh like this
  #!/bin/sh
  CLUSTER=test1
  MASTERDBNAME=pgbench_node1
  SLAVEDBNAME=pgbench_node2
  MASTERHOST=localhost
  SLAVEHOST=localhost
  SLONY_USER=postgres
  PGBENCH_USER=postgres
  /usr/local/pgsql/bin/slonik <<_EOF_
      cluster name = $CLUSTER;
     node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST user=$SLONY_USER';
      node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST user=$SLONY_USER';
      subscribe set ( id = 1, provider = 1, receiver = 2, forward = no);
  _EOF_
   
  there were  error messages like this : 
  <stdin>:18: PGRES_FATAL_ERROR select "_test1".subscribeSet(1, 1, 2, 'f');  - ERROR:  insert or update on table "sl_path" violates foreign key constraint "pa_client-no_id-ref"
  DETAIL:  Key (pa_client)=(2) is not present in table "sl_node".
  CONTEXT:  SQL statement "INSERT INTO "_test1".sl_path (pa_server, pa_client, pa_conninfo, pa_connretry) values ( $1 ,  $2 , '<event pending>', 10)"
  PL/pgSQL function "subscribeset_int" line 53 at SQL statement
  SQL statement "SELECT  "_test1".subscribeSet_int( $1 ,  $2 ,  $3 ,  $4 )"
  PL/pgSQL function "subscribeset" line 63 at perform
   
  so,what happend in my replicator?? and what should i do to solve this error
   
  thank kyu
  regards,
  bayu

 
---------------------------------
TV dinner still cooling?
Check out "Tonight's Picks" on Yahoo! TV.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070708/5ce6b5ca/attachment.htm
From ajs at crankycanuck.ca  Sun Jul  8 09:42:29 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Sun Jul  8 09:42:50 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <4691C1D2.4080907@netregistry.com.au>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<57E4C58C-10A3-43D1-8726-AA14914DF49C@khera.org>
	<60d4z51ilf.fsf@dba2.int.libertyrms.com>
	<4691C1D2.4080907@netregistry.com.au>
Message-ID: <20070708164229.GH25608@phlogiston.dyndns.org>

On Sun, Jul 08, 2007 at 10:04:18PM -0700, Adam Cassar wrote:
> What about an easy way to resync the slaves from the master?

Please expand on what you mean by that.

A


-- 
Andrew Sullivan  | ajs@crankycanuck.ca
When my information changes, I alter my conclusions.  What do you do sir?
		--attr. John Maynard Keynes
From JanWieck at Yahoo.com  Mon Jul  9 05:46:20 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon Jul  9 05:46:38 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <1183762872.10735.37.camel@dogma.ljc.laika.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>	
	<1183151091.28247.44.camel@dogma.v10.wvs>	
	<608xa25sak.fsf@dba2.int.libertyrms.com>	
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>	
	<60ved14fcn.fsf@dba2.int.libertyrms.com>
	<468BAA40.1030505@Yahoo.com>	
	<20070704144451.GA14262@phlogiston.dyndns.org>	
	<60ejjo43pi.fsf@dba2.int.libertyrms.com>
	<468BE796.2030807@Yahoo.com>
	<1183762872.10735.37.camel@dogma.ljc.laika.com>
Message-ID: <46922E1C.4000602@Yahoo.com>

On 7/6/2007 7:01 PM, Jeff Davis wrote:
> On Wed, 2007-07-04 at 14:31 -0400, Jan Wieck wrote:
>> What I see here is that we are trying to come up with a special case 
>> optimization for mass-deletes. No mass insert or update operations will 
>> benefit from any of this. Do people do mass deletes that much that we 
>> really have to worry about them?
> 
> Why does this not apply to UPDATEs as well? 

I can see how multiple log rows for DELETE might be combined into one 
subscriber side DELETE statement with a WHERE clause using IN (BETWEEN 
would be difficult in the case of multi-column primary keys).

How you intend to do the same for INSERT or UPDATE is unclear to me. As 
said before, there is no SQL statement logging available and even if it 
where, due to MVCC it won't do us any good because Slony does not 
replicate single transactions in their exact commit order. This 
visibility problem is one of the tricky details that killed pgcluster 1. 
I won't let it sneak into Slony.


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From JanWieck at Yahoo.com  Mon Jul  9 05:49:31 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon Jul  9 05:49:40 2007
Subject: [Slony1-general] help me please
In-Reply-To: <448258.66036.qm@web56608.mail.re3.yahoo.com>
References: <448258.66036.qm@web56608.mail.re3.yahoo.com>
Message-ID: <46922EDB.50806@Yahoo.com>

On 7/8/2007 12:07 PM, angga erwina wrote:
> Help me please.....
> my slony-I 1.2.9
> postgre8.2.4
> i've installed pgbench and plpgsql
> I've make setup.sh like this, but there error messages like under this??

I don't see a "store node" command for node 2 in there.


Jan

> #!/bin/sh
> CLUSTER=test1
> MASTERDBNAME=pgbench_node1
> SLAVEDBNAME=pgbench_node2
> MASTERHOST=localhost
> SLAVEHOST=localhost
> SLONY_USER=postgres
> PGBENCH_USER=postgres
> /usr/local/pgsql/bin/slonik <<_EOF_
>     cluster name = $CLUSTER;
>     node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST 
> user=$SLONY_USER';
>     node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST 
> user=$SLONY_USER';
>     init cluster ( id = 1, comment = 'Node 1' );
>     table add key ( node id = 1, fully qualified name = 'public.history' );
>     create set ( id = 1, origin = 1, comment = 'All pgbench tables' );
>     set add table ( set id = 1, origin = 1,
>         id = 1, fully qualified name = 'public.accounts',
>         comment = 'Table accounts' );
>     set add table ( set id = 1, origin = 1,
>         id = 2, fully qualified name = 'public.branches',
>         comment = 'Table branches' );
>     set add table ( set id = 1, origin = 1,
>         id = 3, fully qualified name = 'public.tellers',
>         comment = 'Table tellers' );
>     set add table ( set id = 1, origin = 1,
>         id = 4, fully qualified name = 'public.history',
>         key = serial,
>         comment = 'Table history' );
>         store node ( id = 2, comment = 'Node 2' );
>     store path ( server = 1, client = 2,
>         conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST 
> user=$SLONY_USER');
>     store path ( server = 2, client = 1,
>         conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST user=$SLONY_USER');
>     store listen ( origin = 1, provider = 1, receiver = 2 );
>     store listen ( origin = 2, provider = 2, receiver = 1 );
> _EOF_
>  
> there  were  error messages like this
> <stdin>:4: Error: namespace "_test1" already exists in database of node 1
> <stdin>:4: ERROR: no admin conninfo for node 149930256
>  
>  
>  
> when i make the subscribe.sh like this
> #!/bin/sh
> CLUSTER=test1
> MASTERDBNAME=pgbench_node1
> SLAVEDBNAME=pgbench_node2
> MASTERHOST=localhost
> SLAVEHOST=localhost
> SLONY_USER=postgres
> PGBENCH_USER=postgres
> /usr/local/pgsql/bin/slonik <<_EOF_
>     cluster name = $CLUSTER;
>    node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST 
> user=$SLONY_USER';
>     node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST 
> user=$SLONY_USER';
>     subscribe set ( id = 1, provider = 1, receiver = 2, forward = no);
> _EOF_
>  
> there were  error messages like this :
> <stdin>:18: PGRES_FATAL_ERROR select "_test1".subscribeSet(1, 1, 2, 
> 'f');  - ERROR:  insert or update on table "sl_path" violates foreign 
> key constraint "pa_client-no_id-ref"
> DETAIL:  Key (pa_client)=(2) is not present in table "sl_node".
> CONTEXT:  SQL statement "INSERT INTO "_test1".sl_path (pa_server, 
> pa_client, pa_conninfo, pa_connretry) values ( $1 ,  $2 , '<event 
> pending>', 10)"
> PL/pgSQL function "subscribeset_int" line 53 at SQL statement
> SQL statement "SELECT  "_test1".subscribeSet_int( $1 ,  $2 ,  $3 ,  $4 )"
> PL/pgSQL function "subscribeset" line 63 at perform
>  
> so,what happend in my replicator?? and what should i do to solve this error
>  
> thank kyu
> regards,
> bayu
> 
> ------------------------------------------------------------------------
> TV dinner still cooling?
> Check out "Tonight's Picks" 
> <http://us.rd.yahoo.com/evt=49979/*http://tv.yahoo.com/> on Yahoo! TV.
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From pgsql at j-davis.com  Mon Jul  9 12:18:17 2007
From: pgsql at j-davis.com (Jeff Davis)
Date: Mon Jul  9 12:18:29 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <46922E1C.4000602@Yahoo.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com> <468BAA40.1030505@Yahoo.com>
	<20070704144451.GA14262@phlogiston.dyndns.org>
	<60ejjo43pi.fsf@dba2.int.libertyrms.com> <468BE796.2030807@Yahoo.com>
	<1183762872.10735.37.camel@dogma.ljc.laika.com>
	<46922E1C.4000602@Yahoo.com>
Message-ID: <1184008697.5309.38.camel@dogma.ljc.laika.com>

On Mon, 2007-07-09 at 08:46 -0400, Jan Wieck wrote:
> > Why does this not apply to UPDATEs as well? 
> 
> I can see how multiple log rows for DELETE might be combined into one 
> subscriber side DELETE statement with a WHERE clause using IN (BETWEEN 
> would be difficult in the case of multi-column primary keys).
> 
> How you intend to do the same for INSERT or UPDATE is unclear to me. As 
> said before, there is no SQL statement logging available and even if it 
> where, due to MVCC it won't do us any good because Slony does not 
> replicate single transactions in their exact commit order. This 
> visibility problem is one of the tricky details that killed pgcluster 1. 
> I won't let it sneak into Slony.

I am _not_ suggesting statement replication in Slony, and that was not
the direction I intended to take this discussion (statement replication
is really more the domain of pgpool, which is interesting but a
completely different design).

As I understand it, the subscriber reads the log from the provider and
turns the log into individual statements on the subscriber.

Instead, the subscriber could read the log on the provider into a temp
table on the subscriber, then the subscriber could do a "DELETE FROM foo
WHERE ... IN (select ... from temp_table)" or an "UPDATE ... SET ...
FROM temp_table WHERE ...". This should not be any different than
running individual statements within a single transaction, but is more
efficient in some cases (bulk deletes/updates).

In the quick test I just ran (attached ruby script), it took 160s to
COPY the 5M updates into a temp table, and then run one UPDATE that
would update every record in the table by joining with the temp table
(the COPY was a few seconds, so most of the 160s was the single UPDATE
statement). That's way faster than the 15 minutes it took to update one
record at a time.

Regards,
	Jeff Davis

From JanWieck at Yahoo.com  Mon Jul  9 22:12:06 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon Jul  9 22:12:51 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <1184008697.5309.38.camel@dogma.ljc.laika.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>	
	<1183151091.28247.44.camel@dogma.v10.wvs>	
	<608xa25sak.fsf@dba2.int.libertyrms.com>	
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>	
	<60ved14fcn.fsf@dba2.int.libertyrms.com>
	<468BAA40.1030505@Yahoo.com>	
	<20070704144451.GA14262@phlogiston.dyndns.org>	
	<60ejjo43pi.fsf@dba2.int.libertyrms.com>
	<468BE796.2030807@Yahoo.com>	
	<1183762872.10735.37.camel@dogma.ljc.laika.com>	
	<46922E1C.4000602@Yahoo.com>
	<1184008697.5309.38.camel@dogma.ljc.laika.com>
Message-ID: <46931526.8050205@Yahoo.com>

On 7/9/2007 3:18 PM, Jeff Davis wrote:
> On Mon, 2007-07-09 at 08:46 -0400, Jan Wieck wrote:
>> > Why does this not apply to UPDATEs as well? 
>> 
>> I can see how multiple log rows for DELETE might be combined into one 
>> subscriber side DELETE statement with a WHERE clause using IN (BETWEEN 
>> would be difficult in the case of multi-column primary keys).
>> 
>> How you intend to do the same for INSERT or UPDATE is unclear to me. As 
>> said before, there is no SQL statement logging available and even if it 
>> where, due to MVCC it won't do us any good because Slony does not 
>> replicate single transactions in their exact commit order. This 
>> visibility problem is one of the tricky details that killed pgcluster 1. 
>> I won't let it sneak into Slony.
> 
> I am _not_ suggesting statement replication in Slony, and that was not
> the direction I intended to take this discussion (statement replication
> is really more the domain of pgpool, which is interesting but a
> completely different design).
> 
> As I understand it, the subscriber reads the log from the provider and
> turns the log into individual statements on the subscriber.
> 
> Instead, the subscriber could read the log on the provider into a temp
> table on the subscriber, then the subscriber could do a "DELETE FROM foo
> WHERE ... IN (select ... from temp_table)" or an "UPDATE ... SET ...
> FROM temp_table WHERE ...". This should not be any different than
> running individual statements within a single transaction, but is more
> efficient in some cases (bulk deletes/updates).
> 
> In the quick test I just ran (attached ruby script), it took 160s to
> COPY the 5M updates into a temp table, and then run one UPDATE that
> would update every record in the table by joining with the temp table
> (the COPY was a few seconds, so most of the 160s was the single UPDATE
> statement). That's way faster than the 15 minutes it took to update one
> record at a time.

Although the ruby script is missing, I think I get the picture. This 
would work if the log trigger would record every column of every updated 
row. But it doesn't do that. I only logs the columns that have a new 
value that is distinct from the old one.

The speed gain in your test has a couple of sources. The parsing 
overhead of 5 million update statements vs one copy is one of them, the 
network latency for 5 million round trips another and the execution plan 
(which only applies if all or at least a substantial part of the entire 
table is updated) of course. Sure is one such mega update of the entire 
table better done in a merge over the two sorted tuple sets. But the 
planner might chose another strategy if you updated only 500,000 out of 
10 million rows.

It would be interesting to see how 5 million updates done via prepared 
statement using parameters fare in the comparison. Those mass updates 
would at least tend to use a fairly small number of different column 
combinations (this might even be generally true for an entire 
application). Using prepared statements to apply the changes will deal 
with some part of the parsing and planning overhead. It will still force 
your case to do 5 million index scans instead of two sorts and a merge. 
But that comparison really only applies to cases where you update a very 
substantial part of the whole table.

There might be a completely different possibility of dealing with the 
problem. Unfortunately I will not have the time to implement it. But the 
idea goes as follows.

The log is selected by actually doing a COPY over a SELECT (the usual 
log select). That COPY result is fed into the current log table on the 
subscriber. A trigger on that log table will suppress the actual insert 
operation if the subscriber is not in forwarding mode and the operation 
is for a subscribed table (the node might be origin to something else). 
What it also does is doing the actual leg work of applying the changes 
via prepared SPI statements or maybe even direct heap and index updates. 
This method cuts down on the parsing and planning, as well as on the 
network round trips.


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From pshem.k at gmail.com  Tue Jul 10 18:37:43 2007
From: pshem.k at gmail.com (Pshem Kowalczyk)
Date: Tue Jul 10 18:38:12 2007
Subject: [Slony1-general] Multiply databases on single node
Message-ID: <20fe625b0707101837ge57b856j322f17ad2b82005a@mail.gmail.com>

Hi,

First I would like to thank you for great product - slony1 is a great
master-slave replication solution :-)

Due to historical reasons we have multiply databases running on a
single machine (using the same db engine). One of those databases is
replicated using slony1 to other machines. Now we have to replicate
the other database (across the same physical hardware)
Should I create new node for every database and the create a new
independent set?
and have something like this:

master db:
node 1 - database 1
node 11 - database 2

slave 1
node 2 - database 1
node 12 - database 2

slave 2
node 3 - database 1
node 13 - database 2

or is there another way of doing this?

kind regards
pshem
From cbbrowne at mail.libertyrms.com  Tue Jul 10 19:08:55 2007
From: cbbrowne at mail.libertyrms.com (Christopher Browne)
Date: Tue Jul 10 19:09:30 2007
Subject: [Slony1-general] Multiply databases on single node
In-Reply-To: <20fe625b0707101837ge57b856j322f17ad2b82005a@mail.gmail.com>
	(Pshem Kowalczyk's message of "Wed, 11 Jul 2007 13:37:43 +1200")
References: <20fe625b0707101837ge57b856j322f17ad2b82005a@mail.gmail.com>
Message-ID: <60vecr4rpk.fsf@dba2.int.libertyrms.com>

"Pshem Kowalczyk" <pshem.k@gmail.com> writes:
> First I would like to thank you for great product - slony1 is a great
> master-slave replication solution :-)
>
> Due to historical reasons we have multiply databases running on a
> single machine (using the same db engine). One of those databases is
> replicated using slony1 to other machines. Now we have to replicate
> the other database (across the same physical hardware)
> Should I create new node for every database and the create a new
> independent set?
> and have something like this:
>
> master db:
> node 1 - database 1
> node 11 - database 2
>
> slave 1
> node 2 - database 1
> node 12 - database 2
>
> slave 2
> node 3 - database 1
> node 13 - database 2
>
> or is there another way of doing this?

Could you be a little more precise about this?  I suspect that the
answer is "you need a new node each time," but it is a bit ambiguous
based on what you have said.

You need a node for each distinct database that you are using, for sure.

Thus, if the conninfo for nodes 1 and 11 are different, then they
definitely need to be separate nodes.

Suppose, for node 1, the DB is identified as:
  dbname=db1 port=5432 host=masterhost

and for node 11, the DB is identified as:
  dbname=db2 port=5432 host=masterhost

then it's quite clear that you need 2 nodes corresponding to your
"node 1" and "node 11."

It is less obvious that nodes 2 and 12 need to be distinct, ditto for
3 and 13.  They may be able to coexist happily as one node on each of
those servers.

But if these databases are quite independent, I'd be inclined to have
them be *completely* separate, as two Slony-I clusters that don't
necessarily share much of anything.
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From cbbrowne at ca.afilias.info  Wed Jul 11 11:58:02 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 11 11:58:17 2007
Subject: [Slony1-general] Re: Split Set
In-Reply-To: <6A894B6C-122A-481B-A01F-6D996CAE176D@sitesell.com>
References: <6A894B6C-122A-481B-A01F-6D996CAE176D@sitesell.com>
Message-ID: <4695283A.9000904@ca.afilias.info>

[Aside... You may want to head over to lists.slony.info, and subscribe 
there; you sent this to gBorg, which isn't terribly active these days...

Rod Taylor wrote:
> Hi Chrisopher,
>
> Recently I've been thinking about the reason why initial data 
> transfers cannot be programmed to occur one structure at a time. As 
> you know, syncing up 500 structures individually and merging into a 
> master set as you go instead of in a single SET transfer (single large 
> transaction) is far better for the master node.
>
> The reason it cannot be done currently is that a single SET cannot be 
> transferred into components without causing issues.
>
>
> What about adding a command that is the opposite of a MERGE?
>
> When subscribing to a set, slony could do the following:
>
> The big catch is the set number has to change.
>
> Externally we issue something like:
>
> SUBSCRIBE SET (ID = 10, PROVIDER = 1, RECEIVER = 2, TRANSFERRED SET = 
> 11);
>
>
> Slony creates an empty set 11 which is immediately subscribed to 
> receiver 2.
>
> Then it does the following:
>
> foreach $table (@tables_in_set_10) {
>
> # Split set copies the state of ID 10 to a newly built set 999.
> # It moves the table from being a part of 10 to 999
> SPLIT SET (ID = 10, MOVE TABLE = $table, NEW SET = 999);
> WAIT FOR EVENT (ORIGIN = 1, CONFIRMED = 2);
> SYNC (ID = 1);
>
> SUBSCRIBE SET (ID = 999, PROVIDER = 1, RECEIVER = 2);
> WAIT FOR EVENT (ORIGIN = 2, CONFIRMED = 1);
> SYNC (ID = 1);
>
> MERGE SET (ID = 11, ADD ID = 999, ORIGIN = 1);
> WAIT FOR EVENT (ORIGIN = 2, CONFIRMED = 1);
> SYNC (ID = 1);
> }
>
> # All tables are now in set 11. Heck, could even rename 11 to 10
> REMOVE SET (ID = 10);
>
>
>
> Sure, it's a little fragile but would be a very welcome addition. A 
> built in SPLIT SET function would allow us to write the wrapper on our 
> own.
There's something interesting there, though simultaneously something 
very clumsy about having to ask to do the split this way...

There's a more "literal" way to handle this, by the way, as there is a 
"SET MOVE TABLE" command. Let me redo the loop:

### Move all tables from set 1 to #998
create set (id=998, provider=1, comment='Temporary set');
foreach $table (@tables_in_set_10) {
set move table (origin=1, id=$table, new set=998);
}
### Get set #10 subscribed
subscribe set (id=10, provider=1, receiver=2, forward=yes);
### Subscribe tables, one by one, and merge into set #10
foreach $table (@tables_in_set_11) {
create set (id=999, origin=1, comment='Temporary pseudoset');
set move table (origin=1, id=$table, new set=999);
subscribe set (id=999, provider=1, receiver=2);
wait for event (origin=2, confirmed=1)
sync(id=1);
merge set (id=10, add id=999, origin=1);
}

All the tables are, at the end, back in set #10, where they belong.

This could be extended to coping with doing a *second* subscription, 
which is worth outlining because it gives you pretty much "full 
generality" in that it should allow coping with having as many cascaded 
subscribers as needed.

### Move all tables from set 1 to #998
create set (id=998, provider=1, comment='Temporary set');
[subscribe #998 identically to set #10]
foreach $table (@tables_in_set_10) {
set move table (origin=1, id=$table, new set=998);
}
### Get set #10 (now emptied of tables) subscribed to the new node
subscribe set (id=10, provider=1, receiver=3, forward=yes);
### Subscribe tables, one by one, and merge into set #10
foreach $table (@tables_in_set_11) {
create set (id=999, origin=1, comment='Temporary pseudoset');
[subscribe #999 identically to set #998]
set move table (origin=1, id=$table, new set=999);
subscribe set (id=999, provider=1, receiver=2);
wait for event (origin=2, confirmed=1)
sync(id=1);
merge set (id=10, add id=999, origin=1);
}

This is do-able, but it feels mighty clumsy, and even more fragile.

It seems really tempting to think about having a "virtual replication 
set" for this, and putting that into Slony-I proper.

The notion of the "virtual replication set" still seems pretty clumsy.

Here's an outline of a very different approach that Jan and I have been 
talking about that we call "COPY pipelining." It ought to help by 
parallelizing the data load. It's in the TODO...

======================================================
COPY pipelining

- the notion here is to try to parallelize the data load at
SUBSCRIBE time. Suppose we decide we can process 4 tables at a
time, we set up 4 threads. We then iterate thus:

For each table
- acquire a thread (waiting as needed)
- submit COPY TO stdout to the provider, and feed to
COPY FROM stdin on the subscriber
- Submit the REINDEX request on the subscriber

Even with a fairly small number of threads, we should be able to
process the whole subscription in as long as it takes to process
the single largest table.

This introduces a risk of locking problems not true at present
(alas) in that, at present, the subscription process is able to
demand exclusive locks on all tables up front; that is no longer
possible if the subscriptions are split across multiple tables.
In addition, the updates will COMMIT across some period of time on
the subscriber rather than appearing at one instant in time.

The timing improvement is probably still worthwhile.

http://lists.slony.info/pipermail/slony1-hackers/2007-April/000000.html
======================================================

From cbbrowne at ca.afilias.info  Wed Jul 11 12:44:08 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 11 12:44:28 2007
Subject: [Slony1-general] Re: Split Set
In-Reply-To: <36D1DF10-E01C-4BA1-86F0-6EAE2C2D934F@sitesell.com> (Rod Taylor's
	message of "Wed, 11 Jul 2007 15:16:34 -0400")
References: <6A894B6C-122A-481B-A01F-6D996CAE176D@sitesell.com>
	<4695283A.9000904@ca.afilias.info>
	<36D1DF10-E01C-4BA1-86F0-6EAE2C2D934F@sitesell.com>
Message-ID: <60vecqwws7.fsf@dba2.int.libertyrms.com>

Rod Taylor <rbt@sitesell.com> writes:
>> This is do-able, but it feels mighty clumsy, and even more fragile.
>>
>> It seems really tempting to think about having a "virtual
>> replication set" for this, and putting that into Slony-I proper.
>>
>> The notion of the "virtual replication set" still seems pretty clumsy.
>
> Not too sure about hiding it. It is very fragile if it breaks mid-way
> through. If it is hidden it will be difficult to fix, unless you
> think you can make it continue when it finds a virtual set partially
> subscribed. Good transaction boundaries leave the number of possible
> broken states at a small enough count.
>
>> Here's an outline of a very different approach that Jan and I have
>> been talking about that we call "COPY pipelining." It ought to help
>> by parallelizing the data load. It's in the TODO...
>
> Fine for small to mid sized databases. If you're using table
> partitioning because of the size of the structures, and have lots of
> partitions, this isn't going to do much to eliminate the large
> transaction problem.
>
> Granted, it would be very nice if subscribes went a little quicker
> :)

If you set up 4 pipelines, then that should improve the speed by a
factor of around 4, which should be a not unappreciable improvement
:-).

FYI, Jan has been trying out an approach that the Skype guys pointed
out that seems to *massively* improve the ability to catch up after
large transactions, so I think that may get helped soon enough (e.g. -
v2.0).
<http://lists.slony.info/pipermail/slony1-commit/2007-July/001854.html>
That may be the "better change."  Note that it's a very small change;
it might well work in 1.2, too.

In any case, for a database with a whole lot of partitions, I don't
think it's too outrageous to directly head to the "subscribe in steps"
approach.

foreach $table (@tables_in_set_10) {
   create set (id=999+$table, provider=1, comment='Temporary set';
   set move table (origin=1, id=$table, new set=999+$table);
   subscribe set (id=999+$table, provider=1, receiver=2);
}
subscribe set (id=10, provider=1, receiver=2);

and then clean up with a bunch of MERGE SET requests afterwards.
-- 
"cbbrowne","@","cbbrowne.com"
http://cbbrowne.com/info/rdbms.html
Rules of the Evil Overlord #78.  "I will not tell my Legions of Terror
"And he must  be taken alive!" The command will be:  ``And try to take
him alive if it is reasonably practical.''"
<http://www.eviloverlord.com/>
From dmitry at koterov.ru  Thu Jul 12 08:47:30 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Thu Jul 12 08:47:47 2007
Subject: [Slony1-general] 3 bugs in Slonik: wrong SQL parsing into DDL
	statements
Message-ID: <d7df81620707120847k72bdf78sc70c47a25b03bac9@mail.gmail.com>

Hello.

Here is the third bug I found in slonik with SQL parsing. Try to feed the
following to slonik:

CREATE INDEX aaa ON public.bbb USING btree ((-ccc), ddd);
hjkhjkhjkjhk;

You will see:

DDL script consisting of 0 SQL statements
Submit DDL Event to subscribers...
DDL on origin - PGRES_TUPLES_OK

If you replace "-ccc" with "ccc", all works fine: "2 SQL statements".



The second bug:

CREATE RULE "position_get_last_id_on_insert"
AS ON INSERT TO "public"."position" DO (SELECT
currval('position_position_id_seq'::regclass) AS id;);

Incorrectly splitted by "id;".



And the first bug:

DDL Statement 2: (299,471) [
CREATE UNIQUE INDEX "i_dictionary_uni_abbr" ON "static"."dictionary"
  USING btree ((substring(dic_russian, E'^([^(]*[^( ]) *\\('::text)))
  WHERE (dic_category_id =3D 26);
]
DDL Statement failed - PGRES_FATAL_ERROR

You see, it generates an error. Here is a portion of postgres logs:

2007-06-28 18:56:55 GMT 87.250.244.99(55965)ERROR:  invalid regular
expression: parentheses () not balanced
2007-06-28 18:56:55 GMT 87.250.244.99(55965)STATEMENT:
        CREATE UNIQUE INDEX "i_dictionary_uni_abbr" ON "static"."dictionary"
          USING btree ((substring(dic_russian, E'^([^(]*[^( ]) *\('::text)))

          WHERE (dic_category_id =3D 26);

Note the \\( part above: it is sent to the server as \(.
Seems slonik replaces \\ by \ before sending it to postgres?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070712/=
e8771b5a/attachment.htm
From dmitry at koterov.ru  Thu Jul 12 09:14:27 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Thu Jul 12 09:14:42 2007
Subject: [Slony1-general] Re: 3 bugs in Slonik: wrong SQL parsing into DDL
	statements
In-Reply-To: <d7df81620707120847k72bdf78sc70c47a25b03bac9@mail.gmail.com>
References: <d7df81620707120847k72bdf78sc70c47a25b03bac9@mail.gmail.com>
Message-ID: <d7df81620707120914xdc17c88r5cb1630b4917d886@mail.gmail.com>

The quick & dirty fix for the first one is:

slony1-1.2.9\src\parsestatements\scanner.c

if (state =3D=3D Q_NORMAL_STATE) {
    state =3D Q_HOPE_TO_DASH;
    break;
}

replace to

if (state =3D=3D Q_NORMAL_STATE && extended_statement[cpos+1] =3D=3D '-') {
    state =3D Q_HOPE_TO_DASH;
    break;
}


On 7/12/07, Dmitry Koterov <dmitry@koterov.ru> wrote:
>
> Hello.
>
> Here is the third bug I found in slonik with SQL parsing. Try to feed the
> following to slonik:
>
> CREATE INDEX aaa ON public.bbb USING btree ((-ccc), ddd);
> hjkhjkhjkjhk;
>
> You will see:
>
> DDL script consisting of 0 SQL statements
> Submit DDL Event to subscribers...
> DDL on origin - PGRES_TUPLES_OK
>
> If you replace "-ccc" with "ccc", all works fine: "2 SQL statements".
>
>
>
> The second bug:
>
> CREATE RULE "position_get_last_id_on_insert"
> AS ON INSERT TO "public"."position" DO (SELECT
> currval('position_position_id_seq'::regclass) AS id;);
>
> Incorrectly splitted by "id;".
>
>
>
> And the first bug:
>
> DDL Statement 2: (299,471) [
> CREATE UNIQUE INDEX "i_dictionary_uni_abbr" ON "static"."dictionary"
>   USING btree ((substring(dic_russian, E'^([^(]*[^( ]) *\\('::text)))
>   WHERE (dic_category_id =3D 26);
> ]
> DDL Statement failed - PGRES_FATAL_ERROR
>
> You see, it generates an error. Here is a portion of postgres logs:
>
> 2007-06-28 18:56:55 GMT 87.250.244.99(55965)ERROR:  invalid regular
> expression: parentheses () not balanced
> 2007-06-28 18:56:55 GMT 87.250.244.99(55965)STATEMENT:
>         CREATE UNIQUE INDEX "i_dictionary_uni_abbr" ON
> "static"."dictionary"
>           USING btree ((substring(dic_russian, E'^([^(]*[^( ])
> *\('::text)))
>           WHERE (dic_category_id =3D 26);
>
> Note the \\( part above: it is sent to the server as \(.
> Seems slonik replaces \\ by \ before sending it to postgres?
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070712/=
02686858/attachment.htm
From cbbrowne at ca.afilias.info  Thu Jul 12 09:40:18 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Jul 12 09:40:36 2007
Subject: [Slony1-general] 3 bugs in Slonik: wrong SQL parsing into DDL
	statements
In-Reply-To: <d7df81620707120847k72bdf78sc70c47a25b03bac9@mail.gmail.com>
	(Dmitry Koterov's message of "Thu, 12 Jul 2007 19:47:30 +0400")
References: <d7df81620707120847k72bdf78sc70c47a25b03bac9@mail.gmail.com>
Message-ID: <60myy1wp71.fsf@dba2.int.libertyrms.com>

"Dmitry Koterov" <dmitry@koterov.ru> writes:
> CREATE INDEX aaa ON public.bbb USING btree ((-ccc), ddd);

It would be useful to simplify this by taking as many Slony-I
components *out* of the picture as possible.

There is a program in the build, src/parsestatements/test-scanner,
which takes the scanner used in Slony-I for splitting EXECUTE SCRIPT
requests into individual statements, and exposes it as a simple
program which you can use to process a script consisting of multiple
SQL requests.

The problem you had with CREATE RULE is nicely shown to be a bug in
the scanner if you pass that SQL through "test-scanner".

I have posted a fix to the scanner which addresses that problem; it is
not yet in any Slony-I release.  I suppose that what we need to do is
to check these various cases to see what is breaking.

For the three cases that you have outlined in the most recent email, I
can suggest workarounds:

1.  CREATE INDEX aaa ...

 - You can quite safely run this against each node without resorting
   to the use of EXECUTE SCRIPT.  Creating indices does not require
   using EXECUTE SCRIPT.

2.  CREATE RULE "position_get_last_id_on_insert" ...

 - If you remove the final semicolon inside the brackets, then the
   scanner used by EXECUTE SCRIPT will not cut it short.

3.  CREATE UNIQUE INDEX "i_dictionary_uni_abbr" ...

 - As with 1., you can quite safely run this against each node without
   needing to use EXECUTE SCRIPT.

I'll take a look at these queries a little later to see how they may
affect the scanner.
-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@" [name;tld];;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From dmitry at koterov.ru  Thu Jul 12 10:19:58 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Thu Jul 12 10:20:17 2007
Subject: [Slony1-general] Re: 3 bugs in Slonik: wrong SQL parsing into DDL
	statements
In-Reply-To: <d7df81620707120914xdc17c88r5cb1630b4917d886@mail.gmail.com>
References: <d7df81620707120847k72bdf78sc70c47a25b03bac9@mail.gmail.com>
	<d7df81620707120914xdc17c88r5cb1630b4917d886@mail.gmail.com>
Message-ID: <d7df81620707121019t26f96e8y7508b1d47344846f@mail.gmail.com>

Attention! Note that this fix must be applied NOT for slonik only, BUT for
slon too on EACH machine!
This is because slon uses the same parser as slonik and starts an endless
cycle trying to run the command which patched slonik successfully processed.

On 7/12/07, Dmitry Koterov <dmitry@koterov.ru> wrote:
>
> The quick & dirty fix for the first one is:
>
> slony1-1.2.9\src\parsestatements\scanner.c
>
> if (state =3D=3D Q_NORMAL_STATE) {
>     state =3D Q_HOPE_TO_DASH;
>     break;
> }
>
> replace to
>
> if (state =3D=3D Q_NORMAL_STATE && extended_statement[cpos+1] =3D=3D '-')=
 {
>     state =3D Q_HOPE_TO_DASH;
>     break;
> }
>
>
> On 7/12/07, Dmitry Koterov <dmitry@koterov.ru> wrote:
> >
> > Hello.
> >
> > Here is the third bug I found in slonik with SQL parsing. Try to feed
> > the following to slonik:
> >
> > CREATE INDEX aaa ON public.bbb USING btree ((-ccc), ddd);
> > hjkhjkhjkjhk;
> >
> > You will see:
> >
> > DDL script consisting of 0 SQL statements
> > Submit DDL Event to subscribers...
> > DDL on origin - PGRES_TUPLES_OK
> >
> > If you replace "-ccc" with "ccc", all works fine: "2 SQL statements".
> >
> >
> >
> > The second bug:
> >
> > CREATE RULE "position_get_last_id_on_insert"
> > AS ON INSERT TO "public"."position" DO (SELECT
> > currval('position_position_id_seq'::regclass) AS id;);
> >
> > Incorrectly splitted by "id;".
> >
> >
> >
> > And the first bug:
> >
> > DDL Statement 2: (299,471) [
> > CREATE UNIQUE INDEX "i_dictionary_uni_abbr" ON "static"."dictionary"
> >   USING btree ((substring(dic_russian, E'^([^(]*[^( ]) *\\('::text)))
> >   WHERE (dic_category_id =3D 26);
> > ]
> > DDL Statement failed - PGRES_FATAL_ERROR
> >
> > You see, it generates an error. Here is a portion of postgres logs:
> >
> > 2007-06-28 18:56:55 GMT 87.250.244.99(55965)ERROR:  invalid regular
> > expression: parentheses () not balanced
> > 2007-06-28 18:56:55 GMT 87.250.244.99(55965)STATEMENT:
> >         CREATE UNIQUE INDEX "i_dictionary_uni_abbr" ON
> > "static"."dictionary"
> >           USING btree ((substring(dic_russian, E'^([^(]*[^( ])
> > *\('::text)))
> >           WHERE (dic_category_id =3D 26);
> >
> > Note the \\( part above: it is sent to the server as \(.
> > Seems slonik replaces \\ by \ before sending it to postgres?
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070712/=
b8f47423/attachment-0001.htm
From pgsql at j-davis.com  Thu Jul 12 11:02:29 2007
From: pgsql at j-davis.com (Jeff Davis)
Date: Thu Jul 12 11:02:57 2007
Subject: [Slony1-general] Soliciting ideas for v2.0
In-Reply-To: <46931526.8050205@Yahoo.com>
References: <60tzsr67wq.fsf@dba2.int.libertyrms.com>
	<1183151091.28247.44.camel@dogma.v10.wvs>
	<608xa25sak.fsf@dba2.int.libertyrms.com>
	<5a0a9d6f0706292357idf07fffpa130c1deb427d08b@mail.gmail.com>
	<60ved14fcn.fsf@dba2.int.libertyrms.com> <468BAA40.1030505@Yahoo.com>
	<20070704144451.GA14262@phlogiston.dyndns.org>
	<60ejjo43pi.fsf@dba2.int.libertyrms.com> <468BE796.2030807@Yahoo.com>
	<1183762872.10735.37.camel@dogma.ljc.laika.com>
	<46922E1C.4000602@Yahoo.com>
	<1184008697.5309.38.camel@dogma.ljc.laika.com>
	<46931526.8050205@Yahoo.com>
Message-ID: <1184263349.5309.60.camel@dogma.ljc.laika.com>

On Tue, 2007-07-10 at 01:12 -0400, Jan Wieck wrote:
> The speed gain in your test has a couple of sources. The parsing 
> overhead of 5 million update statements vs one copy is one of them, the 
> network latency for 5 million round trips another and the execution plan 
> (which only applies if all or at least a substantial part of the entire 
> table is updated) of course. Sure is one such mega update of the entire 
> table better done in a merge over the two sorted tuple sets. But the 
> planner might chose another strategy if you updated only 500,000 out of 
> 10 million rows.

The time it takes to do 5M "SELECT #" calls, where # is a constant.
These are run over a local socket, not a real network, so the latency is
less: 305s

The time it takes to do 5M "UPDATE" queries on a 5M record table, and
none of the WHERE clauses match any tuples (so only an index lookup is
done), in one transaction: 623s

The time it takes to do 5M of the same update queries, but prepared
first: 398s

I know tests involving this overhead have been done before, but this is
on my machine, and it could be a reference point for the other numbers I
stated earlier. 

> application). Using prepared statements to apply the changes will deal 
> with some part of the parsing and planning overhead. It will still force 
> your case to do 5 million index scans instead of two sorts and a merge. 
> But that comparison really only applies to cases where you update a very 
> substantial part of the whole table.

Yes, my test was designed to show that there are areas of potential
improvement without moving to statement-based replication.

> The log is selected by actually doing a COPY over a SELECT (the usual 
> log select). That COPY result is fed into the current log table on the 
> subscriber. A trigger on that log table will suppress the actual insert 
> operation if the subscriber is not in forwarding mode and the operation 
> is for a subscribed table (the node might be origin to something else). 
> What it also does is doing the actual leg work of applying the changes 
> via prepared SPI statements or maybe even direct heap and index updates. 
> This method cuts down on the parsing and planning, as well as on the 
> network round trips.
> 

Interesting idea. I don't think I understand the implications of doing
direct heap/index updates. Would you have to log the visibility
information as well?

Regards,
	Jeff Davis

From cbbrowne at ca.afilias.info  Thu Jul 12 13:31:05 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Jul 12 13:31:31 2007
Subject: [Slony1-general] Re: 3 bugs in Slonik: wrong SQL parsing into DDL
	statements
In-Reply-To: <d7df81620707120914xdc17c88r5cb1630b4917d886@mail.gmail.com>
	(Dmitry Koterov's message of "Thu, 12 Jul 2007 20:14:27 +0400")
References: <d7df81620707120847k72bdf78sc70c47a25b03bac9@mail.gmail.com>
	<d7df81620707120914xdc17c88r5cb1630b4917d886@mail.gmail.com>
Message-ID: <60bqehweie.fsf@dba2.int.libertyrms.com>

"Dmitry Koterov" <dmitry@koterov.ru> writes:
> The quick & dirty fix for the first one is:
> slony1-1.2.9\src\parsestatements\scanner.c
> if (state == Q_NORMAL_STATE) {
> ??? state = Q_HOPE_TO_DASH;
> ??? break;
> }
> replace to
> if (state == Q_NORMAL_STATE && extended_statement[cpos+1] == '-') {
> ??? state = Q_HOPE_TO_DASH;
> ??? break;
> }

I was going to object on the basis of "what if we're at the end of the string?"

The string is expected to end with a NULL, so if we've got a dash in
the present position, we can be pretty confident that there will be at
least one more byte in the string.  So I'm OK with that...

I'm not sure what's up with the "folding" of backslashes; the output
from test-scanner seems fine.

Below is the patch, as it stands now...

===================================================================
Index: emptytestresult.expected
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/parsestatements/emptytestresult.expected,v
retrieving revision 1.1
diff -c -u -r1.1 emptytestresult.expected
Binary files /tmp/cvsVpl54F and emptytestresult.expected differ
Index: scanner.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/parsestatements/scanner.c,v
retrieving revision 1.3
diff -c -u -r1.3 scanner.c
--- scanner.c	4 Aug 2006 20:40:19 -0000	1.3
+++ scanner.c	12 Jul 2007 20:29:27 -0000
@@ -11,6 +11,9 @@
   char cchar;
   int d1start, d1end, d2start, d2end, d1stemp;
   int statements;
+  int nparens;
+  int nbrokets;
+  int nsquigb;
   
   /* Initialize */
   cpos = 0;
@@ -21,13 +24,49 @@
   d2start = 0;
   d1end = 0;
   state = Q_NORMAL_STATE;
+  nparens = 0;
+  nbrokets = 0;
+  nsquigb = 0;
   
   while (state != Q_DONE) {
     cchar = extended_statement[cpos];
     switch (cchar) {
     case '\0':
+      STMTS[statements++] = ++cpos;
       state = Q_DONE;
       break;
+
+    case '(':
+      if (state == Q_NORMAL_STATE) {
+	nparens ++;
+	break;
+      }
+    case ')':
+      if (state == Q_NORMAL_STATE) {
+	nparens --;
+	break;
+      }
+    case '[':
+      if (state == Q_NORMAL_STATE) {
+	nbrokets ++;
+	break;
+      }
+    case ']':
+      if (state == Q_NORMAL_STATE) {
+	nbrokets --;
+	break;
+      }
+    case '{':
+      if (state == Q_NORMAL_STATE) {
+	nsquigb ++;
+	break;
+      }
+    case '}':
+      if (state == Q_NORMAL_STATE) {
+	nsquigb --;
+	break;
+      }
+
     case '/':
       if (state == Q_NORMAL_STATE) {
 	state = Q_HOPE_TO_CCOMMENT;
@@ -51,8 +90,7 @@
 	  bpos = cpos;
 	  break;
 	}
-      } 
-
+      }
       break;
     case '$':
       if (state == Q_NORMAL_STATE) {
@@ -123,7 +161,7 @@
       }
       break;
     case '-':
-      if (state == Q_NORMAL_STATE) {
+      if (state == Q_NORMAL_STATE && extended_statement[cpos+1] == '-') {
 	state = Q_HOPE_TO_DASH;
 	break;
       }
@@ -151,7 +189,7 @@
       if (state == Q_DOLLAR_UNBUILDING) state = Q_DOLLAR_QUOTING;
       break;
     case ';':
-      if (state == Q_NORMAL_STATE) {
+      if ((state == Q_NORMAL_STATE) && (nparens == 0) && (nbrokets == 0) && (nsquigb == 0)) {
 	STMTS[statements++] = ++cpos;
 	if (statements >= MAXSTATEMENTS) {
 	  return statements;
Index: test_sql.expected
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/parsestatements/test_sql.expected,v
retrieving revision 1.2
diff -c -u -r1.2 test_sql.expected
--- test_sql.expected	24 Feb 2006 18:48:12 -0000	1.2
+++ test_sql.expected	12 Jul 2007 20:29:27 -0000
@@ -37,6 +37,31 @@
 $$ language plpgsql;
 
 
+-- Here is a rule creation with an embedded semicolon
+-- "Dmitry Koterov" <dmitry@koterov.ru>
+
+create table "public"."position";
+
+CREATE RULE "position_get_last_id_on_insert2"
+AS ON INSERT TO "public"."position" DO (SELECT
+currval('position_position_id_seq'::regclass) AS id;);
+
+-- Added to verify handling of queries tried by
+-- "Dmitry Koterov" <dmitry@koterov.ru>
+
+CREATE INDEX aaa ON public.bbb USING btree ((-ccc), ddd);
+
+--  Apparently a pair of backslashes fold down into one?
+-- "Dmitry Koterov" <dmitry@koterov.ru>
+
+CREATE UNIQUE INDEX "i_dictionary_uni_abbr" ON "static"."dictionary"
+USING btree ((substring(dic_russian, E'^([^(]*[^( ]) *\\('::text)))
+WHERE (dic_category_id = 26);
+
+-- Force a query to be at the end...
+
+create table foo;
+
 statement 0
 -------------------------------------------
 select * from foo;
@@ -114,4 +139,48 @@
     select $24$ -- another " " thing ' ' \\\'\$ $24$;
     return NULL;
   end;
-$$ language plpgsql;
\ No newline at end of file
+$$ language plpgsql;
+statement 14
+-------------------------------------------
+
+
+
+-- Here is a rule creation with an embedded semicolon
+-- "Dmitry Koterov" <dmitry@koterov.ru>
+
+create table "public"."position";
+statement 15
+-------------------------------------------
+
+
+CREATE RULE "position_get_last_id_on_insert2"
+AS ON INSERT TO "public"."position" DO (SELECT
+currval('position_position_id_seq'::regclass) AS id;);
+statement 16
+-------------------------------------------
+
+
+-- Added to verify handling of queries tried by
+-- "Dmitry Koterov" <dmitry@koterov.ru>
+
+CREATE INDEX aaa ON public.bbb USING btree ((-ccc), ddd);
+statement 17
+-------------------------------------------
+
+
+--  Apparently a pair of backslashes fold down into one?
+-- "Dmitry Koterov" <dmitry@koterov.ru>
+
+CREATE UNIQUE INDEX "i_dictionary_uni_abbr" ON "static"."dictionary"
+USING btree ((substring(dic_russian, E'^([^(]*[^( ]) *\\('::text)))
+WHERE (dic_category_id = 26);
+statement 18
+-------------------------------------------
+
+
+-- Force a query to be at the end...
+
+create table foo;
+statement 19
+-------------------------------------------
+. 
\ No newline at end of file
Index: test_sql.sql
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/parsestatements/test_sql.sql,v
retrieving revision 1.1
diff -c -u -r1.1 test_sql.sql
--- test_sql.sql	24 Feb 2006 18:33:02 -0000	1.1
+++ test_sql.sql	12 Jul 2007 20:29:27 -0000
@@ -35,3 +35,29 @@
     return NULL;
   end;
 $$ language plpgsql;
+
+
+-- Here is a rule creation with an embedded semicolon
+-- "Dmitry Koterov" <dmitry@koterov.ru>
+
+create table "public"."position";
+
+CREATE RULE "position_get_last_id_on_insert2"
+AS ON INSERT TO "public"."position" DO (SELECT
+currval('position_position_id_seq'::regclass) AS id;);
+
+-- Added to verify handling of queries tried by
+-- "Dmitry Koterov" <dmitry@koterov.ru>
+
+CREATE INDEX aaa ON public.bbb USING btree ((-ccc), ddd);
+
+--  Apparently a pair of backslashes fold down into one?
+-- "Dmitry Koterov" <dmitry@koterov.ru>
+
+CREATE UNIQUE INDEX "i_dictionary_uni_abbr" ON "static"."dictionary"
+USING btree ((substring(dic_russian, E'^([^(]*[^( ]) *\\('::text)))
+WHERE (dic_category_id = 26);
+
+-- Force a query to be at the end...
+
+create table foo;
\ No newline at end of file

-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From weslee.bilodeau at hypermediasystems.com  Thu Jul 12 14:02:37 2007
From: weslee.bilodeau at hypermediasystems.com (Weslee Bilodeau)
Date: Thu Jul 12 14:03:09 2007
Subject: [Slony1-general] Re: 3 bugs in Slonik: wrong SQL parsing into
	DDL	statements
In-Reply-To: <60bqehweie.fsf@dba2.int.libertyrms.com>
References: <d7df81620707120847k72bdf78sc70c47a25b03bac9@mail.gmail.com>	<d7df81620707120914xdc17c88r5cb1630b4917d886@mail.gmail.com>
	<60bqehweie.fsf@dba2.int.libertyrms.com>
Message-ID: <469696ED.9040902@hypermediasystems.com>

Christopher Browne wrote:
> "Dmitry Koterov" <dmitry@koterov.ru> writes:
>> The quick & dirty fix for the first one is:
>> slony1-1.2.9\src\parsestatements\scanner.c
>> if (state == Q_NORMAL_STATE) {
>>     state = Q_HOPE_TO_DASH;
>>     break;
>> }
>> replace to
>> if (state == Q_NORMAL_STATE && extended_statement[cpos+1] == '-') {
>>     state = Q_HOPE_TO_DASH;
>>     break;
>> }
> 
> I was going to object on the basis of "what if we're at the end of the string?"
> 
> The string is expected to end with a NULL, so if we've got a dash in
> the present position, we can be pretty confident that there will be at
> least one more byte in the string.  So I'm OK with that...
> 
> I'm not sure what's up with the "folding" of backslashes; the output
> from test-scanner seems fine.
> 
> Below is the patch, as it stands now...
> 

Small question on this.

I would imagine one could easily find a lot of edge cases that can break
the current parser.

I'm guessing they found them in psql, which is why psql stole the lexer
from the backend itself.

Is there any reason why slony didn't go the same route?

http://developer.postgresql.org/cvsweb.cgi/pgsql/src/bin/psql/psqlscan.l?rev=1.21;content-type=text%2Fx-cvsweb-markup

It was written for the exact same task -

------------------------------------------------------------
This code is mainly needed to determine where the end of a SQL statement
is: we are looking for semicolons that are not within quotes, comments,
or parentheses.  The most reliable way to handle this is to borrow the
backend's flex lexer rules, lock, stock, and barrel.  The rules below
are (except for a few) the same as the backend's, but their actions are
just ECHO whereas the backend's actions generally do other things.
------------------------------------------------------------


Or has this already been discussed and dismissed?


Weslee
From arosenthal at AtlantaHand.com  Fri Jul 13 13:12:52 2007
From: arosenthal at AtlantaHand.com (Al Rosenthal)
Date: Fri Jul 13 13:13:18 2007
Subject: [Slony1-general] Trouble with large DB and windows
Message-ID: <000a01c7c58a$319f4a30$b900a8c0@Tablet>

Several weeks ago I posted about trouble replicating on a windows system us=
ing a wireless network.  I followed the advice by the list members and used=
 a wired portion of the network.  Now I can replicate my 400 MB db without =
any problems.

What I still can not do is replicate a 22 GB database.  My initial trials a=
ll failed after about 12 to 15 GB with an error 10055 in the logs.  Accordi=
ng to windows knowlegebase, this is because of a lack of TCP buffers.  I fo=
llowed microsoft's advice and edited the registry and changed such things a=
s the MaxUserPort and TCPTimedWaitDelay and  TcpNumConnections to their max=
imums.  Still no luck.  I have tried about 30 times and always the replicat=
ion fails.  If I leave everything alone, replication restarts on its own, b=
ut fails sooner.

Any suggestions would be appreciated.  Doing a daily backup of 22GB and the=
n copying to the other machine is very time consuming.

Al
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070713/=
3bd1ef1e/attachment.htm
From ajs at crankycanuck.ca  Fri Jul 13 13:26:48 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri Jul 13 13:27:25 2007
Subject: [Slony1-general] Trouble with large DB and windows
In-Reply-To: <000a01c7c58a$319f4a30$b900a8c0@Tablet>
References: <000a01c7c58a$319f4a30$b900a8c0@Tablet>
Message-ID: <20070713202648.GX8844@phlogiston.dyndns.org>

On Fri, Jul 13, 2007 at 04:12:52PM -0400, Al Rosenthal wrote:

> What I still can not do is replicate a 22 GB database.  My initial
> trials all failed after about 12 to 15 GB with an error 10055 in
> the logs.  According to windows knowlegebase, this is because of a
> lack of TCP buffers.  I followed microsoft's advice and edited the
> registry and changed such things as the MaxUserPort and
> TCPTimedWaitDelay and TcpNumConnections to their maximums.  

I wonder if you're timing out.  What I know about Windows any more is
very close to zero, but my suspicion is that you might be building an
index or something that is taking a long time.  You will have issued
a command, and then you wait for it to complete.  If the time to
complete is longer than the time your settings are willing to wait,
then you can get in trouble.  (This could also be due to an
intermediate firewall deciding that your session is "idle".)

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
Everything that happens in the world happens at some place.
		--Jane Jacobs 
From craig_james at emolecules.com  Fri Jul 13 13:54:50 2007
From: craig_james at emolecules.com (Craig James)
Date: Fri Jul 13 13:49:02 2007
Subject: [Slony1-general] Can I create a "pre-populated" node?
In-Reply-To: <46814B76.50208@Yahoo.com>
References: <467C1469.2070201@emolecules.com>		<20070622142913.47bd37f0.wmoran@collaborativefusion.com>		<5a0a9d6f0706221310o1975f2bci5d8b063c144ff2be@mail.gmail.com>		<467C3B21.9090006@emolecules.com>
	<467FF7E5.20304@emolecules.com>	<5a0a9d6f0706251038s65289e7fm9eeb99c284815f41@mail.gmail.com>
	<46802D44.3070509@emolecules.com> <46814B76.50208@Yahoo.com>
Message-ID: <4697E69A.2070605@emolecules.com>

Is it possible to create a slave Slony node on an existing clone, rather than an empty database?  I have a 64 GB database that is an exact clone of its "master", and it would take a huge amount of time and network bandwidth to re-clone the entire thing using Slony.  I'd rather start with the two copies I have, and just put Slony on top to keep them in sync.

Thanks,
Craig
From ajs at crankycanuck.ca  Fri Jul 13 14:00:53 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri Jul 13 14:01:24 2007
Subject: [Slony1-general] Can I create a "pre-populated" node?
In-Reply-To: <4697E69A.2070605@emolecules.com>
References: <467C1469.2070201@emolecules.com>
	<20070622142913.47bd37f0.wmoran@collaborativefusion.com>
	<5a0a9d6f0706221310o1975f2bci5d8b063c144ff2be@mail.gmail.com>
	<467C3B21.9090006@emolecules.com> <467FF7E5.20304@emolecules.com>
	<5a0a9d6f0706251038s65289e7fm9eeb99c284815f41@mail.gmail.com>
	<46802D44.3070509@emolecules.com> <46814B76.50208@Yahoo.com>
	<4697E69A.2070605@emolecules.com>
Message-ID: <20070713210053.GY8844@phlogiston.dyndns.org>

On Fri, Jul 13, 2007 at 01:54:50PM -0700, Craig James wrote:
> Is it possible to create a slave Slony node on an existing clone, rather 
> than an empty database? 

Yes, but the building of it will promptly drop your existing data and
rebuild it all.  Yes, it's annoying.  There have been proposals to
solve this.  2.0 may even get one of them.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
However important originality may be in some fields, restraint and 
adherence to procedure emerge as the more significant virtues in a 
great many others.   --Alain de Botton
From cbbrowne at ca.afilias.info  Fri Jul 13 15:10:42 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Jul 13 15:11:14 2007
Subject: [Slony1-general] Re: 3 bugs in Slonik: wrong SQL parsing into
	DDL	statements
In-Reply-To: <469696ED.9040902@hypermediasystems.com> (Weslee Bilodeau's
	message of "Thu, 12 Jul 2007 14:02:37 -0700")
References: <d7df81620707120847k72bdf78sc70c47a25b03bac9@mail.gmail.com>
	<d7df81620707120914xdc17c88r5cb1630b4917d886@mail.gmail.com>
	<60bqehweie.fsf@dba2.int.libertyrms.com>
	<469696ED.9040902@hypermediasystems.com>
Message-ID: <60d4ywx8d9.fsf@dba2.int.libertyrms.com>

Weslee Bilodeau <weslee.bilodeau@hypermediasystems.com> writes:
> Christopher Browne wrote:
>> "Dmitry Koterov" <dmitry@koterov.ru> writes:
>>> The quick & dirty fix for the first one is:
>>> slony1-1.2.9\src\parsestatements\scanner.c
>>> if (state == Q_NORMAL_STATE) {
>>>     state = Q_HOPE_TO_DASH;
>>>     break;
>>> }
>>> replace to
>>> if (state == Q_NORMAL_STATE && extended_statement[cpos+1] == '-') {
>>>     state = Q_HOPE_TO_DASH;
>>>     break;
>>> }
>> 
>> I was going to object on the basis of "what if we're at the end of the string?"
>> 
>> The string is expected to end with a NULL, so if we've got a dash in
>> the present position, we can be pretty confident that there will be at
>> least one more byte in the string.  So I'm OK with that...
>> 
>> I'm not sure what's up with the "folding" of backslashes; the output
>> from test-scanner seems fine.
>> 
>> Below is the patch, as it stands now...
>> 
>
> Small question on this.
>
> I would imagine one could easily find a lot of edge cases that can break
> the current parser.
>
> I'm guessing they found them in psql, which is why psql stole the lexer
> from the backend itself.
>
> Is there any reason why slony didn't go the same route?
>
> http://developer.postgresql.org/cvsweb.cgi/pgsql/src/bin/psql/psqlscan.l?rev=1.21;content-type=text%2Fx-cvsweb-markup
>
> It was written for the exact same task -
>
> ------------------------------------------------------------
> This code is mainly needed to determine where the end of a SQL statement
> is: we are looking for semicolons that are not within quotes, comments,
> or parentheses.  The most reliable way to handle this is to borrow the
> backend's flex lexer rules, lock, stock, and barrel.  The rules below
> are (except for a few) the same as the backend's, but their actions are
> just ECHO whereas the backend's actions generally do other things.
> ------------------------------------------------------------
>
>
> Or has this already been discussed and dismissed?

If memory serves, I initially tried to introduce a second parser, but
ran into trouble with the notion of linking in 2 parsers.

I then reviewed syntax, and figured I had enough of it covered, which
was apparently not correct.

I'll take another crack at it...
-- 
"cbbrowne","@","ca.afilias.info"
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From weslee.bilodeau at hypermediasystems.com  Fri Jul 13 15:51:01 2007
From: weslee.bilodeau at hypermediasystems.com (Weslee Bilodeau)
Date: Fri Jul 13 15:51:32 2007
Subject: [Slony1-general] Re: 3 bugs in Slonik: wrong SQL parsing into
	DDL	statements
In-Reply-To: <60d4ywx8d9.fsf@dba2.int.libertyrms.com>
References: <d7df81620707120847k72bdf78sc70c47a25b03bac9@mail.gmail.com>	<d7df81620707120914xdc17c88r5cb1630b4917d886@mail.gmail.com>	<60bqehweie.fsf@dba2.int.libertyrms.com>	<469696ED.9040902@hypermediasystems.com>
	<60d4ywx8d9.fsf@dba2.int.libertyrms.com>
Message-ID: <469801D5.1060309@hypermediasystems.com>

Christopher Browne wrote:
> Weslee Bilodeau <weslee.bilodeau@hypermediasystems.com> writes:
>> Small question on this.
>>
>> I would imagine one could easily find a lot of edge cases that can break
>> the current parser.
>>
>> I'm guessing they found them in psql, which is why psql stole the lexer
>> from the backend itself.
>>
>> Is there any reason why slony didn't go the same route?
>>
>> http://developer.postgresql.org/cvsweb.cgi/pgsql/src/bin/psql/psqlscan.l?rev=1.21;content-type=text%2Fx-cvsweb-markup
>>
>> It was written for the exact same task -
>>
>> ------------------------------------------------------------
>> This code is mainly needed to determine where the end of a SQL statement
>> is: we are looking for semicolons that are not within quotes, comments,
>> or parentheses.  The most reliable way to handle this is to borrow the
>> backend's flex lexer rules, lock, stock, and barrel.  The rules below
>> are (except for a few) the same as the backend's, but their actions are
>> just ECHO whereas the backend's actions generally do other things.
>> ------------------------------------------------------------
>>
>>
>> Or has this already been discussed and dismissed?
> 
> If memory serves, I initially tried to introduce a second parser, but
> ran into trouble with the notion of linking in 2 parsers.
> 
> I then reviewed syntax, and figured I had enough of it covered, which
> was apparently not correct.
> 
> I'll take another crack at it...

I figure the $_$, $$, etc edge-casees would be another fun one to roll
into a custom parser.

CREATE FUNCTION test( ) RETURNS text AS $_$ SELECT ';', E'\';\'',
'"";""', E'"\';' ; SELECT 'OK'::text ; $_$ LANGUAGE SQL ;

SELECT $_$ hello; this ; - is '\" a '''' test $_$ ;

SELECT $$ $ test ; $ ;  $$ ;

All really funky, but perfectly valid.


And yeah, rolling two lexers into one, that does have its own challenges.

Maybe expand the first lexer and add an SQL state, so it can parse SQL
within the first one directly?


Weslee

From by_pacitan at yahoo.com  Sun Jul 15 09:39:42 2007
From: by_pacitan at yahoo.com (angga erwina)
Date: Sun Jul 15 09:39:57 2007
Subject: [Slony1-general] slony over LAN and VPN
Message-ID: <777986.81647.qm@web56603.mail.re3.yahoo.com>

hi, all...
i would like to replicate my dbase over LAN and VPN,could you help me please,give the tutorial or url that i can follow it step-by-step..especially about the configuration in admin conninfo...
thank you very much,
regards,
Bayu
       
---------------------------------
Yahoo! oneSearch: Finally,  mobile search that gives answers, not web links. 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070715/4cec02f4/attachment.htm
From cbbrowne at mail.libertyrms.com  Sun Jul 15 15:38:50 2007
From: cbbrowne at mail.libertyrms.com (Christopher Browne)
Date: Sun Jul 15 15:39:19 2007
Subject: [Slony1-general] slony over LAN and VPN
In-Reply-To: <777986.81647.qm@web56603.mail.re3.yahoo.com> (angga erwina's
	message of "Sun, 15 Jul 2007 09:39:42 -0700 (PDT)")
References: <777986.81647.qm@web56603.mail.re3.yahoo.com>
Message-ID: <60ejj92sxx.fsf@dba2.int.libertyrms.com>

angga erwina <by_pacitan@yahoo.com> writes:
> hi, all...  i would like to replicate my dbase over LAN and
> VPN,could you help me please,give the tutorial or url that i can
> follow it step-by-step..especially about the configuration in admin
> conninfo...

There is nothing terribly special about LAN/VPN.  It merely involves
having network addresses that span the WAN.  The tutorial wouldn't
vary from any of the existing ones except in the names of the hosts.
-- 
(reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From arosenthal at AtlantaHand.com  Mon Jul 16 04:50:46 2007
From: arosenthal at AtlantaHand.com (Al Rosenthal)
Date: Mon Jul 16 04:51:51 2007
Subject: [Slony1-general] Trouble with large DB and windows
Message-ID: <000b01c7c79f$8c3c5480$b900a8c0@Tablet>

I have thought about firewalls and timeouts.  I just have two machines, 
sitting next to each other connected by a switch.  One has Vista, the other 
XP.  I tried running the slon engines on either one.  I also tried changing 
all of the IP settings available.  I also tried adding a linux machine and 
running the slon process on that machine.  In every case, the machine that 
is the slave failed after about 12 - 15 GB with a error 10055 (not enough 
buffer space or enough tcp buffers).  Also, the database I am trying to 
replicate is very simple.  One table, no indexes.  Even the table is simple. 
On serial column as primary key, the other is bytea data.

Any suggestions?

Al


>> What I still can not do is replicate a 22 GB database.  My initial
>> trials all failed after about 12 to 15 GB with an error 10055 in
>> the logs.  According to windows knowlegebase, this is because of a
>> lack of TCP buffers.  I followed microsoft's advice and edited the
>> registry and changed such things as the MaxUserPort and
>> TCPTimedWaitDelay and TcpNumConnections to their maximums.

>I wonder if you're timing out.  What I know about Windows any more is
>very close to zero, but my suspicion is that you might be building an
>index or something that is taking a long time.  You will have issued
>a command, and then you wait for it to complete.  If the time to
>complete is longer than the time your settings are willing to wait,
>then you can get in trouble.  (This could also be due to an
>intermediate firewall deciding that your session is "idle".) 

From wmoran at collaborativefusion.com  Mon Jul 16 05:56:31 2007
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Mon Jul 16 05:56:43 2007
Subject: [Slony1-general] Trouble with large DB and windows
In-Reply-To: <000b01c7c79f$8c3c5480$b900a8c0@Tablet>
References: <000b01c7c79f$8c3c5480$b900a8c0@Tablet>
Message-ID: <20070716085631.2358c673.wmoran@collaborativefusion.com>

In response to "Al Rosenthal" <arosenthal@AtlantaHand.com>:

> I have thought about firewalls and timeouts.  I just have two machines, 
> sitting next to each other connected by a switch.  One has Vista, the other 
> XP.  I tried running the slon engines on either one.  I also tried changing 
> all of the IP settings available.  I also tried adding a linux machine and 
> running the slon process on that machine.  In every case, the machine that 
> is the slave failed after about 12 - 15 GB with a error 10055 (not enough 
> buffer space or enough tcp buffers).

Perhaps try bumping up slon's log level and try to correlate what it's doing
when the problem occurs.

> Also, the database I am trying to 
> replicate is very simple.  One table, no indexes.  Even the table is simple. 
> On serial column as primary key, the other is bytea data.

Note that Slony can not replicate bytea fields.

> Any suggestions?

Don't top-post?

> >> What I still can not do is replicate a 22 GB database.  My initial
> >> trials all failed after about 12 to 15 GB with an error 10055 in
> >> the logs.  According to windows knowlegebase, this is because of a
> >> lack of TCP buffers.  I followed microsoft's advice and edited the
> >> registry and changed such things as the MaxUserPort and
> >> TCPTimedWaitDelay and TcpNumConnections to their maximums.
> 
> >I wonder if you're timing out.  What I know about Windows any more is
> >very close to zero, but my suspicion is that you might be building an
> >index or something that is taking a long time.  You will have issued
> >a command, and then you wait for it to complete.  If the time to
> >complete is longer than the time your settings are willing to wait,
> >then you can get in trouble.  (This could also be due to an
> >intermediate firewall deciding that your session is "idle".) 
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 
> 
> 
> 
> 


-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023

****************************************************************
IMPORTANT: This message contains confidential information and is
intended only for the individual named. If the reader of this
message is not an intended recipient (or the individual
responsible for the delivery of this message to an intended
recipient), please be advised that any re-use, dissemination,
distribution or copying of this message is prohibited. Please
notify the sender immediately by e-mail if you have received
this e-mail by mistake and delete this e-mail from your system.
E-mail transmission cannot be guaranteed to be secure or
error-free as information could be intercepted, corrupted, lost,
destroyed, arrive late or incomplete, or contain viruses. The
sender therefore does not accept liability for any errors or
omissions in the contents of this message, which arise as a
result of e-mail transmission.
****************************************************************
From nagy at ecircle-ag.com  Mon Jul 16 06:00:29 2007
From: nagy at ecircle-ag.com (Csaba Nagy)
Date: Mon Jul 16 06:00:51 2007
Subject: [Slony1-general] Trouble with large DB and windows
In-Reply-To: <20070716085631.2358c673.wmoran@collaborativefusion.com>
References: <000b01c7c79f$8c3c5480$b900a8c0@Tablet>
	<20070716085631.2358c673.wmoran@collaborativefusion.com>
Message-ID: <1184590828.24101.477.camel@coppola.muc.ecircle.de>

> Note that Slony can not replicate bytea fields.

Huh ? I thought it has absolutely no problems replicating bytea
fields... only the BLOB support via the OID type had problems AFAIK...
am I wrong ? Cause I do replicate some bytea fields here...

Cheers,
Csaba.

From JanWieck at Yahoo.com  Mon Jul 16 06:19:09 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon Jul 16 06:19:34 2007
Subject: [Slony1-general] Trouble with large DB and windows
In-Reply-To: <20070716085631.2358c673.wmoran@collaborativefusion.com>
References: <000b01c7c79f$8c3c5480$b900a8c0@Tablet>
	<20070716085631.2358c673.wmoran@collaborativefusion.com>
Message-ID: <469B704D.5050009@Yahoo.com>

On 7/16/2007 8:56 AM, Bill Moran wrote:
> In response to "Al Rosenthal" <arosenthal@AtlantaHand.com>:
> 
>> I have thought about firewalls and timeouts.  I just have two machines, 
>> sitting next to each other connected by a switch.  One has Vista, the other 
>> XP.  I tried running the slon engines on either one.  I also tried changing 
>> all of the IP settings available.  I also tried adding a linux machine and 
>> running the slon process on that machine.  In every case, the machine that 
>> is the slave failed after about 12 - 15 GB with a error 10055 (not enough 
>> buffer space or enough tcp buffers).
> 
> Perhaps try bumping up slon's log level and try to correlate what it's doing
> when the problem occurs.

That's what I'd like to know. What exect operation slon is doing is 
actually failing (if it isn't actually Postgres that is getting that 
specific error).

Also interesting would be the output of "netstat -na" at the time of the 
failure. Is there anything else going on on the systems while the slons 
are creating the replica? Any anti virus software that might mess with 
the network? Have you tried to disable useless services like indexing 
and such?

> 
>> Also, the database I am trying to 
>> replicate is very simple.  One table, no indexes.  Even the table is simple. 
>> On serial column as primary key, the other is bytea data.
> 
> Note that Slony can not replicate bytea fields.

When and by whom was that broken? As per design, Slony cannot replicate 
Postgres' large objects, but bytea should be just fine.


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From ismail.yenigul at endersys.com.tr  Mon Jul 16 06:22:00 2007
From: ismail.yenigul at endersys.com.tr (Ismail YENIGUL)
Date: Mon Jul 16 06:22:09 2007
Subject: [Slony1-general] Trouble with large DB and windows
In-Reply-To: <1184590828.24101.477.camel@coppola.muc.ecircle.de>
References: <000b01c7c79f$8c3c5480$b900a8c0@Tablet>
	<20070716085631.2358c673.wmoran@collaborativefusion.com>
	<1184590828.24101.477.camel@coppola.muc.ecircle.de>
Message-ID: <1974053368.20070716152200@endersys.com.tr>

Hi Csaba,

Hi,


Yes, slony can replicate bytea fields. We are already doing that for
our some customers.

Thanks.

Monday, July 16, 2007, 3:00:29 PM, you wrote:

>> Note that Slony can not replicate bytea fields.

> Huh ? I thought it has absolutely no problems replicating bytea
> fields... only the BLOB support via the OID type had problems AFAIK...
> am I wrong ? Cause I do replicate some bytea fields here...

> Cheers,
> Csaba.

> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general



-- 

Ismail YENIGUL
Tel: +90 533 747 3665
ismail.yenigul@endersys.com.tr
http://www.endersys.com
http://www.endersys.com.tr

From darcyb at commandprompt.com  Mon Jul 16 07:18:05 2007
From: darcyb at commandprompt.com (Darcy Buskermolen)
Date: Mon Jul 16 07:20:26 2007
Subject: [Slony1-general] Trouble with large DB and windows
In-Reply-To: <000b01c7c79f$8c3c5480$b900a8c0@Tablet>
References: <000b01c7c79f$8c3c5480$b900a8c0@Tablet>
Message-ID: <200707160718.05959.darcyb@commandprompt.com>

On Monday 16 July 2007 04:50:46 Al Rosenthal wrote:
> I have thought about firewalls and timeouts.  I just have two machines,
> sitting next to each other connected by a switch.  One has Vista, the other
> XP.  I tried running the slon engines on either one.  I also tried changing
> all of the IP settings available.  I also tried adding a linux machine and
> running the slon process on that machine.  In every case, the machine that
> is the slave failed after about 12 - 15 GB with a error 10055 (not enough
> buffer space or enough tcp buffers).  Also, the database I am trying to
> replicate is very simple.  One table, no indexes.  Even the table is
> simple. On serial column as primary key, the other is bytea data.
>
> Any suggestions?

Based on the descriptions you have given, I'm fairly convinced that this is 
not a slony problem, but rather a PostgreSQL more specificaly a Windows 
problem,   During the subscribe phase of the replication slony basicly mimics 
pg_dump | psql.   The fact that you've moved the slon processes to a linux 
machine yet still get this error,  pretty much confirms this for me.  Some 
googeling around leads me to see the following: 
http://archives.postgresql.org/pgsql-ports/2005-11/msg00000.php  Which sounds 
like it could be exactly what you are experiencing.

What version of PostgreSQL are we dealing with here?


>
> Al
>
> >> What I still can not do is replicate a 22 GB database.  My initial
> >> trials all failed after about 12 to 15 GB with an error 10055 in
> >> the logs.  According to windows knowlegebase, this is because of a
> >> lack of TCP buffers.  I followed microsoft's advice and edited the
> >> registry and changed such things as the MaxUserPort and
> >> TCPTimedWaitDelay and TcpNumConnections to their maximums.
> >
> >I wonder if you're timing out.  What I know about Windows any more is
> >very close to zero, but my suspicion is that you might be building an
> >index or something that is taking a long time.  You will have issued
> >a command, and then you wait for it to complete.  If the time to
> >complete is longer than the time your settings are willing to wait,
> >then you can get in trouble.  (This could also be due to an
> >intermediate firewall deciding that your session is "idle".)
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
-- 


Darcy Buskermolen
The PostgreSQL company, Command Prompt Inc.
http://www.commandprompt.com/
From wmoran at collaborativefusion.com  Mon Jul 16 08:02:25 2007
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Mon Jul 16 08:02:38 2007
Subject: [Slony1-general] Trouble with large DB and windows
In-Reply-To: <469B704D.5050009@Yahoo.com>
References: <000b01c7c79f$8c3c5480$b900a8c0@Tablet>
	<20070716085631.2358c673.wmoran@collaborativefusion.com>
	<469B704D.5050009@Yahoo.com>
Message-ID: <20070716110225.998eb380.wmoran@collaborativefusion.com>

In response to Jan Wieck <JanWieck@Yahoo.com>:

> >> Also, the database I am trying to 
> >> replicate is very simple.  One table, no indexes.  Even the table is simple. 
> >> On serial column as primary key, the other is bytea data.
> > 
> > Note that Slony can not replicate bytea fields.
> 
> When and by whom was that broken? As per design, Slony cannot replicate 
> Postgres' large objects, but bytea should be just fine.

My mistake.  Got my wires cross on that one.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From darcyb at commandprompt.com  Tue Jul 17 14:49:09 2007
From: darcyb at commandprompt.com (Darcy Buskermolen)
Date: Tue Jul 17 14:50:03 2007
Subject: [Slony1-general] Trouble with large DB and windows
In-Reply-To: <004101c7c8ba$2b89ec80$b900a8c0@Tablet>
References: <000b01c7c79f$8c3c5480$b900a8c0@Tablet>
	<200707160718.05959.darcyb@commandprompt.com>
	<004101c7c8ba$2b89ec80$b900a8c0@Tablet>
Message-ID: <200707171449.10312.darcyb@commandprompt.com>

On July 17, 2007 02:33 pm, Al Rosenthal wrote:
> Hello,
>
> Thank you for your reply.  I think that you may be correct.  I looked at
> the link that you suggested and that sounds very much like the problem. 
> Some of the bytea data is greater than 64 mb.  I am running 8.2.3 on
> windows.  I have tried changing all of the registry parameters for TCP in
> windows but to no avail. I would try changing the winsock parameters but it
> doesn't sound like that has made a difference either.  When I do an insert
> on this table it can often be a large amount of binary data (250+ pages of
> scanned documents).  So far, the PQexecParams statement works fine and
> never complains.  Maybe some of the internal buffers in winsock are not
> getting released.
>
> Think maybe I'm stuck with a bunch of pg_dumps and storage on a removable
> hard disk.  Pity.  A restore on a db of that size would take almost a day.
> Not much for availability.

I'd try bumping this thread over to the postgresql mailing list and see if the 
windows hackers there have any other ideas.

>
>
> Again thanks.
>
> Al Rosenthal
>
> > Based on the descriptions you have given, I'm fairly convinced that this
> > is
> > not a slony problem, but rather a PostgreSQL more specificaly a Windows
> > problem,   During the subscribe phase of the replication slony basicly
> > mimics
> > pg_dump | psql.   The fact that you've moved the slon processes to a
> > linux machine yet still get this error,  pretty much confirms this for
> > me.  Some googeling around leads me to see the following:
> > http://archives.postgresql.org/pgsql-ports/2005-11/msg00000.php  Which
> > sounds
> > like it could be exactly what you are experiencing.
> >
> >
> >
> > Darcy Buskermolen
> > The PostgreSQL company, Command Prompt Inc.
> > http://www.commandprompt.com/

-- 
Darcy Buskermolen
Command Prompt, Inc.
+1.503.667.4564 X 102
http://www.commandprompt.com/
PostgreSQL solutions since 1997
From pshem.k at gmail.com  Tue Jul 17 21:48:52 2007
From: pshem.k at gmail.com (Pshem Kowalczyk)
Date: Tue Jul 17 21:49:37 2007
Subject: [Slony1-general] Problems with triggers
Message-ID: <20fe625b0707172148n46e50e39mf9ab971888f0c603@mail.gmail.com>

Hi,

I have simple setup with one master db and 2 slaves. I have a stored
procedure that gets executed before insert on one of the replicated
tables. Everything propagates automatically fine, except for one small
problem - the slave nodes have 2 rows entered for every 1 on the
master - probably because the trigger gets fired there too.
I've added the trigger and procedure manually (that particular table
is not used atm).
The procedure checks if a row is already in the table and if so -
updates it (instead of doing and insert), in addition to that it
inserts one row to another table (which is replicated).

The easiest way would be to disable the trigger on the slaves, the
problem is that if master dies my application tries to write to the
slaves (there is a system in place that promotes one of the slaves  to
a master role).

How do you usually solve this sort of problem?

kind regards
Pshem
From dmitry at koterov.ru  Wed Jul 18 00:55:38 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Wed Jul 18 00:56:33 2007
Subject: [Slony1-general] Replication and deferrable constraints
Message-ID: <d7df81620707180055y74b7df09n99d2f377f267ead5@mail.gmail.com>

Hello.

The question is: does Slony correctly support DEFERRABLE foreign keys during
the replication?

Suppose I have a table with deferrable foreign key constraint referenced to
self:

tbl(id INTEGER, parent_id INTEGER);

The field parent_id references to the table tbl with deferrable constraint:

ALTER TABLE tbl ADD CONSTRAINT parent FOREIGN KEY (parent_id)
REFERENCES tbl(id) DEFERRABLE INITIALLY DEFERRED;

Then I insert a child element in tbl BEFORE inserting a parent one:

BEGIN;
INSERT INTO tbl(id, parent_id) VALUES(1, 2);    -- child first!
INSERT INTO tbl(id, parent_id) VALUES(2, null);  -- parent
COMMIT;

It is correct, because the constraint is deferrable and checked at the end
of transaction.

But I suppose that in the event log this two INSERTs will possibly (?) be
presented one after one and in DIFFERENT trtansactions, so, the first one
will fail with constraint check error immediately. Please say, could it
happen or not? Or, if some statements were initially in a single
transaction, slon daemon will run them in a single transaction too on a
replica?

P.S.
Please do not advice to change the order of INSERTs, because in a real
application I run these inserts as:
INSERT INTO tbl(id, parent_id) SELECT * FROM othertbl;
and the order is totally unpredictable.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070718/=
493ede2a/attachment.htm
From cbbrowne at ca.afilias.info  Wed Jul 18 07:45:12 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 18 07:45:26 2007
Subject: [Slony1-general] Replication and deferrable constraints
In-Reply-To: <d7df81620707180055y74b7df09n99d2f377f267ead5@mail.gmail.com>
	(Dmitry Koterov's message of "Wed, 18 Jul 2007 11:55:38 +0400")
References: <d7df81620707180055y74b7df09n99d2f377f267ead5@mail.gmail.com>
Message-ID: <60odi9n53b.fsf@dba2.int.libertyrms.com>

"Dmitry Koterov" <dmitry@koterov.ru> writes:
> Hello.
> The question is: does Slony correctly support DEFERRABLE foreign keys during the replication?
> Suppose I have a table with deferrable foreign key constraint referenced to self:
> tbl(id INTEGER, parent_id INTEGER);
> The field parent_id references to the table tbl with deferrable constraint:
> ALTER TABLE tbl ADD CONSTRAINT parent FOREIGN KEY (parent_id)
> REFERENCES tbl(id) DEFERRABLE INITIALLY DEFERRED;
> Then I insert a child element in tbl BEFORE inserting a parent one:
> BEGIN;
> INSERT INTO tbl(id, parent_id) VALUES(1, 2);??? -- child first!
> INSERT INTO tbl(id, parent_id) VALUES(2, null);? -- parent
> COMMIT;
> It is correct, because the constraint is deferrable and checked at the end of transaction.
> But I suppose that in the event log this two INSERTs will possibly (?) be presented one after one and in DIFFERENT trtansactions, so, the first one will fail with
> constraint check error immediately. Please say, could it happen or not? Or, if some statements were initially in a single transaction, slon daemon will run them in a
> single transaction too on a replica?
> P.S.
> Please do not advice to change the order of INSERTs, because in a real application I run these inserts as:
> INSERT INTO tbl(id, parent_id) SELECT * FROM othertbl;
> and the order is totally unpredictable.

Slony-I does not expressly, as such "support deferrable foreign keys;"
most of Slony-I's features are not directed towards supporting one or
another specific PostgreSQL capability.  Instead, it seeks to provide
functionality that works alongside the DBMS so that the usual answer
to questions of the form "Does Slony-I support <X>?" ought to be "No
reason why it shouldn't!"

That is indeed the case in this case.

Yes, in the event log, your two INSERTs will be submitted; one of the
things Slony-I does is to attach a sequence value to the log (called
log_actionid).  The first INSERT will have a smaller value than the
second one, and when updates are applied to subscribers, log_actionid
is considered as part of the ordering of updates.  Thus, whether those
INSERTs are in the same transaction or not, the first one will indeed
be applied before the second one.

The "log_actionid" supports the notion, in Slony-I, of applying
changes in what it terms an "agreeable order."  See section 2.2 of the
Slony-I Concept paper.

http://developer.postgresql.org/~wieck/slony1/Slony-I-concept.pdf
-- 
output = ("cbbrowne" "@" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From dmitry at koterov.ru  Wed Jul 18 09:11:40 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Wed Jul 18 09:11:54 2007
Subject: [Slony1-general] Replication and deferrable constraints
In-Reply-To: <60odi9n53b.fsf@dba2.int.libertyrms.com>
References: <d7df81620707180055y74b7df09n99d2f377f267ead5@mail.gmail.com>
	<60odi9n53b.fsf@dba2.int.libertyrms.com>
Message-ID: <d7df81620707180911r34f28f4cge18cd04c3049a57e@mail.gmail.com>

Thanks.
But could you please be more practical?

Will those two INSERT go ALWAYS in the same transaction on a subscriber or
not?
In other words, does slon daemon try to execute all subscriber updates in
the same transaction as on master or not?

If yes, deferrable constraints are okay.
If not, nobody should use deferrable constraints together with Slony,
because sometimes query sequence correct for master will be incorrect for
slave.

On 7/18/07, Christopher Browne <cbbrowne@ca.afilias.info> wrote:
>
> "Dmitry Koterov" <dmitry@koterov.ru> writes:
> > Hello.
> > The question is: does Slony correctly support DEFERRABLE foreign keys
> during the replication?
> > Suppose I have a table with deferrable foreign key constraint referenced
> to self:
> > tbl(id INTEGER, parent_id INTEGER);
> > The field parent_id references to the table tbl with deferrable
> constraint:
> > ALTER TABLE tbl ADD CONSTRAINT parent FOREIGN KEY (parent_id)
> > REFERENCES tbl(id) DEFERRABLE INITIALLY DEFERRED;
> > Then I insert a child element in tbl BEFORE inserting a parent one:
> > BEGIN;
> > INSERT INTO tbl(id, parent_id) VALUES(1, 2); -- child first!
> > INSERT INTO tbl(id, parent_id) VALUES(2, null); -- parent
> > COMMIT;
> > It is correct, because the constraint is deferrable and checked at the
> end of transaction.
> > But I suppose that in the event log this two INSERTs will possibly (?)
> be presented one after one and in DIFFERENT trtansactions, so, the first =
one
> will fail with
> > constraint check error immediately. Please say, could it happen or not?
> Or, if some statements were initially in a single transaction, slon daemon
> will run them in a
> > single transaction too on a replica?
> > P.S.
> > Please do not advice to change the order of INSERTs, because in a real
> application I run these inserts as:
> > INSERT INTO tbl(id, parent_id) SELECT * FROM othertbl;
> > and the order is totally unpredictable.
>
> Slony-I does not expressly, as such "support deferrable foreign keys;"
> most of Slony-I's features are not directed towards supporting one or
> another specific PostgreSQL capability.  Instead, it seeks to provide
> functionality that works alongside the DBMS so that the usual answer
> to questions of the form "Does Slony-I support <X>?" ought to be "No
> reason why it shouldn't!"
>
> That is indeed the case in this case.
>
> Yes, in the event log, your two INSERTs will be submitted; one of the
> things Slony-I does is to attach a sequence value to the log (called
> log_actionid).  The first INSERT will have a smaller value than the
> second one, and when updates are applied to subscribers, log_actionid
> is considered as part of the ordering of updates.  Thus, whether those
> INSERTs are in the same transaction or not, the first one will indeed
> be applied before the second one.
>
> The "log_actionid" supports the notion, in Slony-I, of applying
> changes in what it terms an "agreeable order."  See section 2.2 of the
> Slony-I Concept paper.
>
> http://developer.postgresql.org/~wieck/slony1/Slony-I-concept.pdf
> --
> output =3D ("cbbrowne" "@" "ca.afilias.info")
> <http://dba2.int.libertyrms.com/>
> Christopher Browne
> (416) 673-4124 (land)
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070718/=
951670fb/attachment.htm
From ajs at crankycanuck.ca  Wed Jul 18 09:17:16 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Jul 18 09:17:39 2007
Subject: [Slony1-general] Problems with triggers
In-Reply-To: <20fe625b0707172148n46e50e39mf9ab971888f0c603@mail.gmail.com>
References: <20fe625b0707172148n46e50e39mf9ab971888f0c603@mail.gmail.com>
Message-ID: <20070718161716.GQ24908@phlogiston.dyndns.org>

On Wed, Jul 18, 2007 at 04:48:52PM +1200, Pshem Kowalczyk wrote:
> I've added the trigger and procedure manually (that particular table
> is not used atm).

This is your problem, and yes, this is because it fires in two
places.  This is why triggers on the replica are to be disabled.  If
you don't want that, use STORE TRIGGER.  You can write your trigger
procedure so that it figures out whether it's on the origin and
performs nothing in that case, which allows you to have a trigger
only on a replica, for instance.

> The easiest way would be to disable the trigger on the slaves, the
> problem is that if master dies my application tries to write to the
> slaves (there is a system in place that promotes one of the slaves  to
> a master role).

That would also automatically turn the trigger back on.  This is in
the docs, although evidently in a way that isn't perfectly clear.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
Everything that happens in the world happens at some place.
		--Jane Jacobs 
From ajs at crankycanuck.ca  Wed Jul 18 09:22:51 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Jul 18 09:23:10 2007
Subject: [Slony1-general] Replication and deferrable constraints
In-Reply-To: <d7df81620707180911r34f28f4cge18cd04c3049a57e@mail.gmail.com>
References: <d7df81620707180055y74b7df09n99d2f377f267ead5@mail.gmail.com>
	<60odi9n53b.fsf@dba2.int.libertyrms.com>
	<d7df81620707180911r34f28f4cge18cd04c3049a57e@mail.gmail.com>
Message-ID: <20070718162251.GR24908@phlogiston.dyndns.org>

On Wed, Jul 18, 2007 at 08:11:40PM +0400, Dmitry Koterov wrote:
> Will those two INSERT go ALWAYS in the same transaction on a subscriber or
> not?

No.

> If not, nobody should use deferrable constraints together with Slony,
> because sometimes query sequence correct for master will be incorrect for
> slave.

Also no.  Slony does things in "agreeable order", which is where much
of the actual invention in Slony is (this was entirely Jan's idea,
and it's a very clever one).  That is, it respects the visibility
effects of MVCC _without_ having to replay the transactions in
exactly the same order as they happened on the origin.  The
explication of how this is to work is in the "concepts" doc that Jan
originally wrote, which Chris already pointed you to.

A
-- 
Andrew Sullivan  | ajs@crankycanuck.ca
However important originality may be in some fields, restraint and 
adherence to procedure emerge as the more significant virtues in a 
great many others.   --Alain de Botton
From andrew.george.hammond at gmail.com  Wed Jul 18 10:00:18 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Wed Jul 18 10:00:33 2007
Subject: [Slony1-general] adding --version option to binaries
Message-ID: <5a0a9d6f0707181000g787934esd67863106995c23e@mail.gmail.com>

Our binaries (slon and slonik) don't currently support the --version
convention. Would it make sense to add -v or --version or -V which
returns the version?

Andrew
From cbbrowne at ca.afilias.info  Wed Jul 18 10:48:18 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 18 10:48:35 2007
Subject: [Slony1-general] adding --version option to binaries
In-Reply-To: <5a0a9d6f0707181000g787934esd67863106995c23e@mail.gmail.com>
References: <5a0a9d6f0707181000g787934esd67863106995c23e@mail.gmail.com>
Message-ID: <469E5262.8010706@ca.afilias.info>

Andrew Hammond wrote:
> Our binaries (slon and slonik) don't currently support the --version
> convention. Would it make sense to add -v or --version or -V which
> returns the version?
Jan added the "-v" switch almost three years ago; looking at CVS:

----------------------------
revision 1.36
date: 2004-10-19 00:53:11 +0000;  author: wieck;  state: Exp;  lines: +7 -2
Added switch -v to slon and slonik to report version number and exit.

Jan
----------------------------

From dmitry at koterov.ru  Wed Jul 18 12:06:44 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Wed Jul 18 12:07:05 2007
Subject: [Slony1-general] Replication and deferrable constraints
In-Reply-To: <20070718162251.GR24908@phlogiston.dyndns.org>
References: <d7df81620707180055y74b7df09n99d2f377f267ead5@mail.gmail.com>
	<60odi9n53b.fsf@dba2.int.libertyrms.com>
	<d7df81620707180911r34f28f4cge18cd04c3049a57e@mail.gmail.com>
	<20070718162251.GR24908@phlogiston.dyndns.org>
Message-ID: <d7df81620707181206v82e3803r4fedbd554c80d80c@mail.gmail.com>

Sorry, but I have not undersand this clearly. :-(
Could you please answer the single practical question (just "yes" or "no"):
if I perform

BEGIN;
INSERT INTO tbl(id, parent_id) VALUES(1, 2);    -- child first!
INSERT INTO tbl(id, parent_id) VALUES(2, null);  -- parent
COMMIT;

on master (assuming that parent_id foreign key is deferrable), DOES Slony
GUARANTEE that ANY subscriber will receive and SUCESSFULLY process this
data, or there is a probability that a subscriber will fail to update?


P.S.
Contrary to that, the query

BEGIN;
INSERT INTO tbl(id, parent_id) VALUES(2, null);  -- PARENT first
INSERT INTO tbl(id, parent_id) VALUES(1, 2);    -- child
COMMIT;

GUARANTEES that a subscriber will receive and process the data correctly,
it's obvious.


On 7/18/07, Andrew Sullivan <ajs@crankycanuck.ca> wrote:
>
> On Wed, Jul 18, 2007 at 08:11:40PM +0400, Dmitry Koterov wrote:
> > Will those two INSERT go ALWAYS in the same transaction on a subscriber
> or
> > not?
>
> No.
>
> > If not, nobody should use deferrable constraints together with Slony,
> > because sometimes query sequence correct for master will be incorrect
> for
> > slave.
>
> Also no.  Slony does things in "agreeable order", which is where much
> of the actual invention in Slony is (this was entirely Jan's idea,
> and it's a very clever one).  That is, it respects the visibility
> effects of MVCC _without_ having to replay the transactions in
> exactly the same order as they happened on the origin.  The
> explication of how this is to work is in the "concepts" doc that Jan
> originally wrote, which Chris already pointed you to.
>
> A
> --
> Andrew Sullivan  | ajs@crankycanuck.ca
> However important originality may be in some fields, restraint and
> adherence to procedure emerge as the more significant virtues in a
> great many others.   --Alain de Botton
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070718/=
c42380d3/attachment.htm
From ajs at crankycanuck.ca  Wed Jul 18 12:50:51 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Jul 18 12:51:23 2007
Subject: [Slony1-general] Replication and deferrable constraints
In-Reply-To: <d7df81620707181206v82e3803r4fedbd554c80d80c@mail.gmail.com>
References: <d7df81620707180055y74b7df09n99d2f377f267ead5@mail.gmail.com>
	<60odi9n53b.fsf@dba2.int.libertyrms.com>
	<d7df81620707180911r34f28f4cge18cd04c3049a57e@mail.gmail.com>
	<20070718162251.GR24908@phlogiston.dyndns.org>
	<d7df81620707181206v82e3803r4fedbd554c80d80c@mail.gmail.com>
Message-ID: <20070718195051.GW24908@phlogiston.dyndns.org>

On Wed, Jul 18, 2007 at 11:06:44PM +0400, Dmitry Koterov wrote:
> Sorry, but I have not undersand this clearly. :-(
> Could you please answer the single practical question (just "yes" or "no"):
> if I perform
> 
> BEGIN;
> INSERT INTO tbl(id, parent_id) VALUES(1, 2);    -- child first!
> INSERT INTO tbl(id, parent_id) VALUES(2, null);  -- parent
> COMMIT;
> 
> on master (assuming that parent_id foreign key is deferrable), DOES Slony
> GUARANTEE that ANY subscriber will receive and SUCESSFULLY process this
> data, or there is a probability that a subscriber will fail to update?

Yes, but I'm not sure whether in your example
statement1(tbl)==statement2(tbl). 

Foreign keys are disabled on the replica on the grounds that they're
enforced on the origin anyway.  But your case isn't a foreign key,
AFAICT, because the tables are the same one in each statement as I
read it.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
The very definition of "news" is "something that hardly ever happens."	
		--Bruce Schneier
From mpartio at gmail.com  Wed Jul 18 21:58:35 2007
From: mpartio at gmail.com (Mikko Partio)
Date: Wed Jul 18 21:59:20 2007
Subject: [Slony1-general] Slave in an insecure location
Message-ID: <2ca799770707182158g235cb29cj6ecbd2237422f3fe@mail.gmail.com>

Hello all,

I have to set up a replica of our production database to a possibly insecure
location (DMZ). I have been using slony in other projects and the log
shipping mode would seem to fit this purpose perfectly, but, alas, it
requires that another slave is configured besides the origin node. This is
not very practical for us since the replicated database is large and we
don't have the hardware to support yet another slave. So, my question is
that is there any way to configure log shipping with just the origin and the
log shipping slave?

On a side note, I saw recently some discussion about the privileges of the
replication user, namely that the superuser mode is only required at the
initial subscription. Can I just ALTER USER x NOSUPERUSER when in "normal"
replication mode?

Regards

MP
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070719/=
4a64f017/attachment.htm
From dmitry at koterov.ru  Thu Jul 19 00:56:19 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Thu Jul 19 00:57:11 2007
Subject: [Slony1-general] Replication and deferrable constraints
In-Reply-To: <20070718195051.GW24908@phlogiston.dyndns.org>
References: <d7df81620707180055y74b7df09n99d2f377f267ead5@mail.gmail.com>
	<60odi9n53b.fsf@dba2.int.libertyrms.com>
	<d7df81620707180911r34f28f4cge18cd04c3049a57e@mail.gmail.com>
	<20070718162251.GR24908@phlogiston.dyndns.org>
	<d7df81620707181206v82e3803r4fedbd554c80d80c@mail.gmail.com>
	<20070718195051.GW24908@phlogiston.dyndns.org>
Message-ID: <d7df81620707190056y6271c049sb70b9e97c313dc1@mail.gmail.com>

> Foreign keys are disabled on the replica
That was a key phrase! Thanks!
So, is foreign keys are disabled on replicas, the answer is "YES, deferred
constraints are okay".

In my case - it is a foreign key referenced to the same table (tbl).

But hos Slony disables foreign keys constraints if corresponding triggers
are enabled on replicas? I watched trigger states using phpPgAdmin, they are
all turned on. Possibly using a blocking slony trigger which it adds to all
tables at the beginning of the trigger sequence? I haven't know if such
trigger blocks internal foreign key constraint trigger too...

On 7/18/07, Andrew Sullivan <ajs@crankycanuck.ca> wrote:
>
> On Wed, Jul 18, 2007 at 11:06:44PM +0400, Dmitry Koterov wrote:
> > Sorry, but I have not undersand this clearly. :-(
> > Could you please answer the single practical question (just "yes" or
> "no"):
> > if I perform
> >
> > BEGIN;
> > INSERT INTO tbl(id, parent_id) VALUES(1, 2);    -- child first!
> > INSERT INTO tbl(id, parent_id) VALUES(2, null);  -- parent
> > COMMIT;
> >
> > on master (assuming that parent_id foreign key is deferrable), DOES
> Slony
> > GUARANTEE that ANY subscriber will receive and SUCESSFULLY process this
> > data, or there is a probability that a subscriber will fail to update?
>
> Yes, but I'm not sure whether in your example
> statement1(tbl)=3D=3Dstatement2(tbl).
>
> Foreign keys are disabled on the replica on the grounds that they're
> enforced on the origin anyway.  But your case isn't a foreign key,
> AFAICT, because the tables are the same one in each statement as I
> read it.
>
> A
>
> --
> Andrew Sullivan  | ajs@crankycanuck.ca
> The very definition of "news" is "something that hardly ever happens."
>                 --Bruce Schneier
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070719/=
77ad457b/attachment.htm
From JanWieck at Yahoo.com  Thu Jul 19 07:31:22 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu Jul 19 07:31:46 2007
Subject: [Slony1-general] Replication and deferrable constraints
In-Reply-To: <20070718195051.GW24908@phlogiston.dyndns.org>
References: <d7df81620707180055y74b7df09n99d2f377f267ead5@mail.gmail.com>	<60odi9n53b.fsf@dba2.int.libertyrms.com>	<d7df81620707180911r34f28f4cge18cd04c3049a57e@mail.gmail.com>	<20070718162251.GR24908@phlogiston.dyndns.org>	<d7df81620707181206v82e3803r4fedbd554c80d80c@mail.gmail.com>
	<20070718195051.GW24908@phlogiston.dyndns.org>
Message-ID: <469F75BA.8090307@Yahoo.com>

On 7/18/2007 3:50 PM, Andrew Sullivan wrote:
> On Wed, Jul 18, 2007 at 11:06:44PM +0400, Dmitry Koterov wrote:
>> Sorry, but I have not undersand this clearly. :-(
>> Could you please answer the single practical question (just "yes" or "no"):
>> if I perform
>> 
>> BEGIN;
>> INSERT INTO tbl(id, parent_id) VALUES(1, 2);    -- child first!
>> INSERT INTO tbl(id, parent_id) VALUES(2, null);  -- parent
>> COMMIT;
>> 
>> on master (assuming that parent_id foreign key is deferrable), DOES Slony
>> GUARANTEE that ANY subscriber will receive and SUCESSFULLY process this
>> data, or there is a probability that a subscriber will fail to update?
> 
> Yes, but I'm not sure whether in your example
> statement1(tbl)==statement2(tbl). 
> 
> Foreign keys are disabled on the replica on the grounds that they're
> enforced on the origin anyway.  But your case isn't a foreign key,
> AFAICT, because the tables are the same one in each statement as I
> read it.

That would be a self referencing table and still a foreign key. And for 
sure can someone use a deferrable constraint in that case to allow 
inserting self referencing rows out of order. Nothing wrong with the 
example.

To answer Dimitry's questions, first all changes done by one transaction 
on the origin will be replicated in the same slon transaction on a 
subscriber. It is possible that multiple origin transactions are grouped 
together into one replication transaction on serializable snapshot 
visibility boundaries. But the actions of a single origin transaction 
are never split into multiple replication transactions. This means that 
there will never be a state on a subscriber that would not have been 
visible under MVCC rules on the origin in precisely a SYNC events 
transaction. Unless one uses READ DIRTY isolation, of course.

Second foreign key constraints are internally implemented with triggers 
in Postgres. Since Slony disables triggers on a replica, the simple 
answer is that foreign keys are not checked on replicated tables on a slave.


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From JanWieck at Yahoo.com  Thu Jul 19 07:41:58 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu Jul 19 07:42:16 2007
Subject: [Slony1-general] adding --version option to binaries
In-Reply-To: <469E5262.8010706@ca.afilias.info>
References: <5a0a9d6f0707181000g787934esd67863106995c23e@mail.gmail.com>
	<469E5262.8010706@ca.afilias.info>
Message-ID: <469F7836.9020408@Yahoo.com>

On 7/18/2007 1:48 PM, Christopher Browne wrote:
> Andrew Hammond wrote:
>> Our binaries (slon and slonik) don't currently support the --version
>> convention. Would it make sense to add -v or --version or -V which
>> returns the version?
> Jan added the "-v" switch almost three years ago; looking at CVS:

I don't mind if it gets aliased to --version though.


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From cbbrowne at ca.afilias.info  Thu Jul 19 08:08:21 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Jul 19 08:08:31 2007
Subject: [Slony1-general] Slave in an insecure location
In-Reply-To: <2ca799770707182158g235cb29cj6ecbd2237422f3fe@mail.gmail.com>
	(Mikko Partio's message of "Thu, 19 Jul 2007 07:58:35 +0300")
References: <2ca799770707182158g235cb29cj6ecbd2237422f3fe@mail.gmail.com>
Message-ID: <608x9cjusa.fsf@dba2.int.libertyrms.com>

"Mikko Partio" <mpartio@gmail.com> writes:
> I have to set up a replica of our production database to a possibly insecure location (DMZ). I have been using slony in other projects and the log shipping mode would
> seem to fit this purpose perfectly, but, alas, it requires that another slave is configured besides the origin node. This is not very practical for us since the
> replicated database is large and we don't have the hardware to support yet another slave. So, my question is that is there any way to configure log shipping with just
> the origin and the log shipping slave?
> On a side note, I saw recently some discussion about the privileges of the replication user, namely that the superuser mode is only required at the initial
> subscription. Can I just ALTER USER x NOSUPERUSER when in "normal" replication mode?

Actually, there's a bit better result that falls out of some further
analysis we have done on this...

You can set up two "slony users" for the two different roles that they
have:

- The DB user that slon uses to connect to the node that it manages
  needs to be a superuser.  (The configuration for this is in the slon
  command line, in the form of the DSN that you pass in.)

- The DB user that slon uses to connect to *other nodes* does NOT need
  to be so exalted in its powers.

  The DSNs for these connections are set up in the table "sl_path",
  and are stored in the DBMS via the slonik "STORE PATH" command.

You can set up two users at each node:

 - "slonysuper" which is a superuser.

 - "slonyweak" which has pretty minimal permissions.

   Most of the time, it needs little more than read permissions on
   tables in the schema that has Slony-I config and data.  It needs
   write access to sequences sl_nodelock and
   sl_nodelock_nl_conncnt_seq on remote nodes.

See recent versions of the "best practices" page; look for "Lowering
Authority."

-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://linuxdatabases.info/info/bestpractices.html>
Christopher Browne
(416) 673-4124 (land)
From JanWieck at Yahoo.com  Thu Jul 19 08:09:49 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu Jul 19 08:10:11 2007
Subject: [Slony1-general] Slave in an insecure location
In-Reply-To: <2ca799770707182158g235cb29cj6ecbd2237422f3fe@mail.gmail.com>
References: <2ca799770707182158g235cb29cj6ecbd2237422f3fe@mail.gmail.com>
Message-ID: <469F7EBD.9090908@Yahoo.com>

On 7/19/2007 12:58 AM, Mikko Partio wrote:
> Hello all,
> 
> I have to set up a replica of our production database to a possibly 
> insecure location (DMZ). I have been using slony in other projects and 
> the log shipping mode would seem to fit this purpose perfectly, but, 
> alas, it requires that another slave is configured besides the origin 
> node. This is not very practical for us since the replicated database is 
> large and we don't have the hardware to support yet another slave. So, 
> my question is that is there any way to configure log shipping with just 
> the origin and the log shipping slave?

No, only the slon of a real replica can actually write the log files. 
However, what might work in your case is to create a "fake" replica. 
Create a replica that has a slave-only trigger (using STORE TRIGGER) on 
every replicated table. The trigger fires BEFORE INSERT and all it does 
is RETURN NULL. And you subscribe the thing with no forwarding. That 
should create an empty replica that now can write the log shipping 
files. To get the initial content for the offline replica you must 
however stop the application and dump from the origin.

> 
> On a side note, I saw recently some discussion about the privileges of 
> the replication user, namely that the superuser mode is only required at 
> the initial subscription. Can I just ALTER USER x NOSUPERUSER when in 
> "normal" replication mode?

Not even that. Each slon only needs superuser access to its local node. 
Never does it need privileged access to the remote side. And all remote 
access is read only.


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From dmitry at koterov.ru  Thu Jul 19 08:24:04 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Thu Jul 19 08:24:16 2007
Subject: [Slony1-general] CRITICAL bugfix: Backslashes inside an update
	statement for slonik are stripped
Message-ID: <d7df81620707190824t637f3165g69ce5990161be15@mail.gmail.com>

Here is the complete bugfix:

1. src/slon/remote_worker.c
- slon_mkquery(&query1, dest);
+ slon_mkquery(&query1, "%s", dest);

2. src/slonik/slonik.c
- slon_mkquery(&query, dest);
+ slon_mkquery(&query, "%s", dest);

I checked all other places where slon_mkquery() is called, they are OK.
Except these two, of course.
Please include this in the next release (possibly - in critical fix release,
because the bug is very dangerous and implicit).

On 6/28/07, Dmitry Koterov <dmitry@koterov.ru> wrote:
>
> Hello.
>
> I feed the slonik with the following SQL:
>
> DDL Statement 2: (299,471) [
>
> CREATE UNIQUE INDEX "i_dictionary_uni_abbr" ON "static"."dictionary"
>   USING btree ((substring(dic_russian, E'^([^(]*[^( ]) *\\('::text)))
>   WHERE (dic_category_id =3D 26);
>
> ] DDL Statement failed - PGRES_FATAL_ERROR
>
>
> You see, it generates an error. Here is a portion of postgres logs:
>
> 2007-06-28 18:56:55 GMT 87.250.244.99(55965)ERROR:  invalid regular
> expression: parentheses () not balanced
> 2007-06-28 18:56:55 GMT 87.250.244.99(55965)STATEMENT:
>         CREATE UNIQUE INDEX "i_dictionary_uni_abbr" ON
> "static"."dictionary"
>           USING btree ((substring(dic_russian, E'^([^(]*[^( ])
> *\('::text)))
>           WHERE (dic_category_id =3D 26);
>
>
> Note the \\( part above: it is sent to the server as \(.
> Seems slonik replaces \\ by \ before sending it to postgres?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070719/=
77c666bd/attachment.htm
From dmitry at koterov.ru  Thu Jul 19 09:51:47 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Thu Jul 19 09:52:02 2007
Subject: [Slony1-general] Replication and deferrable constraints
In-Reply-To: <469F75BA.8090307@Yahoo.com>
References: <d7df81620707180055y74b7df09n99d2f377f267ead5@mail.gmail.com>
	<60odi9n53b.fsf@dba2.int.libertyrms.com>
	<d7df81620707180911r34f28f4cge18cd04c3049a57e@mail.gmail.com>
	<20070718162251.GR24908@phlogiston.dyndns.org>
	<d7df81620707181206v82e3803r4fedbd554c80d80c@mail.gmail.com>
	<20070718195051.GW24908@phlogiston.dyndns.org>
	<469F75BA.8090307@Yahoo.com>
Message-ID: <d7df81620707190951x3802f7d9w3eeebb7dabc96d91@mail.gmail.com>

Jan, thanks for great answers!
Seems all that is for Slony FAQ. Especially - about never-splitable
transactions.

On 7/19/07, Jan Wieck <JanWieck@yahoo.com> wrote:
>
> On 7/18/2007 3:50 PM, Andrew Sullivan wrote:
> > On Wed, Jul 18, 2007 at 11:06:44PM +0400, Dmitry Koterov wrote:
> >> Sorry, but I have not undersand this clearly. :-(
> >> Could you please answer the single practical question (just "yes" or
> "no"):
> >> if I perform
> >>
> >> BEGIN;
> >> INSERT INTO tbl(id, parent_id) VALUES(1, 2);    -- child first!
> >> INSERT INTO tbl(id, parent_id) VALUES(2, null);  -- parent
> >> COMMIT;
> >>
> >> on master (assuming that parent_id foreign key is deferrable), DOES
> Slony
> >> GUARANTEE that ANY subscriber will receive and SUCESSFULLY process this
> >> data, or there is a probability that a subscriber will fail to update?
> >
> > Yes, but I'm not sure whether in your example
> > statement1(tbl)=3D=3Dstatement2(tbl).
> >
> > Foreign keys are disabled on the replica on the grounds that they're
> > enforced on the origin anyway.  But your case isn't a foreign key,
> > AFAICT, because the tables are the same one in each statement as I
> > read it.
>
> That would be a self referencing table and still a foreign key. And for
> sure can someone use a deferrable constraint in that case to allow
> inserting self referencing rows out of order. Nothing wrong with the
> example.
>
> To answer Dimitry's questions, first all changes done by one transaction
> on the origin will be replicated in the same slon transaction on a
> subscriber. It is possible that multiple origin transactions are grouped
> together into one replication transaction on serializable snapshot
> visibility boundaries. But the actions of a single origin transaction
> are never split into multiple replication transactions. This means that
> there will never be a state on a subscriber that would not have been
> visible under MVCC rules on the origin in precisely a SYNC events
> transaction. Unless one uses READ DIRTY isolation, of course.
>
> Second foreign key constraints are internally implemented with triggers
> in Postgres. Since Slony disables triggers on a replica, the simple
> answer is that foreign keys are not checked on replicated tables on a
> slave.
>
>
> Jan
>
> --
> #=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D#
> # It's easier to get forgiveness for being wrong than for being right. #
> # Let's break this rule - forgive me.                                  #
> #=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D JanWieck@Yahoo.com #
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070719/=
c75bfdbe/attachment.htm
From JanWieck at Yahoo.com  Thu Jul 19 13:08:27 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu Jul 19 13:08:57 2007
Subject: [Slony1-general] Replication and deferrable constraints
In-Reply-To: <d7df81620707190951x3802f7d9w3eeebb7dabc96d91@mail.gmail.com>
References: <d7df81620707180055y74b7df09n99d2f377f267ead5@mail.gmail.com>	<60odi9n53b.fsf@dba2.int.libertyrms.com>	<d7df81620707180911r34f28f4cge18cd04c3049a57e@mail.gmail.com>	<20070718162251.GR24908@phlogiston.dyndns.org>	<d7df81620707181206v82e3803r4fedbd554c80d80c@mail.gmail.com>	<20070718195051.GW24908@phlogiston.dyndns.org>	<469F75BA.8090307@Yahoo.com>
	<d7df81620707190951x3802f7d9w3eeebb7dabc96d91@mail.gmail.com>
Message-ID: <469FC4BB.806@Yahoo.com>

On 7/19/2007 12:51 PM, Dmitry Koterov wrote:
> Jan, thanks for great answers!
> Seems all that is for Slony FAQ. Especially - about never-splitable 
> transactions.

Actually, nobody should ever worry about splittable transactions. 
Postgres has been ACID since the dawn of ages and it was the reason for 
a lot of badmouthing we got from the people who don't think ACID is more 
important than speed. So why would someone design a replication system 
for it that makes partial transactions visible on a replica?


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From andrew.george.hammond at gmail.com  Thu Jul 19 15:19:23 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Thu Jul 19 15:19:49 2007
Subject: [Slony1-general] adding --version option to binaries
In-Reply-To: <469F7836.9020408@Yahoo.com>
References: <5a0a9d6f0707181000g787934esd67863106995c23e@mail.gmail.com>
	<469E5262.8010706@ca.afilias.info> <469F7836.9020408@Yahoo.com>
Message-ID: <5a0a9d6f0707191519p40bf1520s9b37669d7c3e0d08@mail.gmail.com>

Totally untested and probably insanely wrong patch attached.

Andrew


On 7/19/07, Jan Wieck <JanWieck@yahoo.com> wrote:
> On 7/18/2007 1:48 PM, Christopher Browne wrote:
> > Andrew Hammond wrote:
> >> Our binaries (slon and slonik) don't currently support the --version
> >> convention. Would it make sense to add -v or --version or -V which
> >> returns the version?
> > Jan added the "-v" switch almost three years ago; looking at CVS:
>
> I don't mind if it gets aliased to --version though.
>
>
> Jan
>
> --
> #=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D#
> # It's easier to get forgiveness for being wrong than for being right. #
> # Let's break this rule - forgive me.                                  #
> #=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D JanWieck@Yahoo.com #
>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: slony_longopts.patch
Type: text/x-diff
Size: 1979 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20070719=
/beaea91a/slony_longopts.bin
From mpartio at gmail.com  Thu Jul 19 21:39:37 2007
From: mpartio at gmail.com (Mikko Partio)
Date: Thu Jul 19 21:40:22 2007
Subject: [Slony1-general] Slave in an insecure location
In-Reply-To: <469F7EBD.9090908@Yahoo.com>
References: <2ca799770707182158g235cb29cj6ecbd2237422f3fe@mail.gmail.com>
	<469F7EBD.9090908@Yahoo.com>
Message-ID: <2ca799770707192139y58575a11lc8a11a793c93f5af@mail.gmail.com>

On 7/19/07, Jan Wieck <JanWieck@yahoo.com> wrote:
>
> On 7/19/2007 12:58 AM, Mikko Partio wrote:
>
> No, only the slon of a real replica can actually write the log files.
> However, what might work in your case is to create a "fake" replica.
> Create a replica that has a slave-only trigger (using STORE TRIGGER) on
> every replicated table. The trigger fires BEFORE INSERT and all it does
> is RETURN NULL. And you subscribe the thing with no forwarding. That
> should create an empty replica that now can write the log shipping
> files. To get the initial content for the offline replica you must
> however stop the application and dump from the origin.



This method seems to fit my purposes nicely. My own thinking was that I'd
periodically truncate the tables at the subscriber but this solution is much
more elegant.


Not even that. Each slon only needs superuser access to its local node.
> Never does it need privileged access to the remote side. And all remote
> access is read only.
>

Hmm this is interesting, perhaps it should be mentioned at the docs. By the
way, the documention link on the web pages leads to an older version of the
docs, since it doesn't have the "lowering authority" -section.

Thanks a lot for the both of you for your help.

Regards

MP
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070720/=
f7f1f16d/attachment.htm
From mpartio at gmail.com  Fri Jul 20 03:06:54 2007
From: mpartio at gmail.com (Mikko Partio)
Date: Fri Jul 20 03:07:57 2007
Subject: [Slony1-general] Slave in an insecure location
In-Reply-To: <469F7EBD.9090908@Yahoo.com>
References: <2ca799770707182158g235cb29cj6ecbd2237422f3fe@mail.gmail.com>
	<469F7EBD.9090908@Yahoo.com>
Message-ID: <2ca799770707200306t76557286xcfa0081f7cfa123b@mail.gmail.com>

On 7/19/07, Jan Wieck <JanWieck@yahoo.com> wrote:
>
> On 7/19/2007 12:58 AM, Mikko Partio wrote:
>
> No, only the slon of a real replica can actually write the log files.
> However, what might work in your case is to create a "fake" replica.
> Create a replica that has a slave-only trigger (using STORE TRIGGER) on
> every replicated table. The trigger fires BEFORE INSERT and all it does
> is RETURN NULL. And you subscribe the thing with no forwarding. That
> should create an empty replica that now can write the log shipping
> files. To get the initial content for the offline replica you must
> however stop the application and dump from the origin.



One question, the script slony1_dump.sh retrieves the sync-value for table
sl_setsync_offline from the sl_setsync -table. I modified the script so that
the data gets dumped from the origin node (since the subscriber is empty),
but the sl_setsync table is empty at the origin, Where can I get the correct
sync number (is select max(con_seqno) from sl_confirm ok) ?

Regards

MP
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070720/=
c02d8639/attachment.htm
From mpartio at gmail.com  Fri Jul 20 06:05:50 2007
From: mpartio at gmail.com (Mikko Partio)
Date: Fri Jul 20 06:05:54 2007
Subject: [Slony1-general] log shipping gone wrong
Message-ID: <2ca799770707200605j1bb37c13o98c86a21834db2ab@mail.gmail.com>

Hello all,

as mentioned in an another thread I have started a log shipping scenario
where there's an origin, a slave and a log shipping slave. The setup went
smoothly and everything worked when the logs produced by the normal
subscriber were applied to the log shipping slave. The problem is that
inevitably the replication to the log shipping slave stops. The reason is
that the syncs in the logfiles have gaps:

test2_log=3D# select * from _test2.sl_setsync_offline ;
 ssy_setid | ssy_seqno |         ssy_synctime
-----------+-----------+-------------------------------
         1 |       523 | 2007-07-20 15:19:54.493855+03
(1 row)


$ psql -f slony1_log_1_00000000000000000523.sql test2_log
START TRANSACTION
psql:slony1_log_1_00000000000000000523.sql:6: ERROR:  Slony-I: set 1 is on
sync 523, this archive log expects 522
ROLLBACK
VACUUM

$ psql -f slony1_log_1_00000000000000000524.sql test2_log
START TRANSACTION
psql:slony1_log_1_00000000000000000524.sql:5: ERROR:  Slony-I: set 1 is on
sync 523, this archive log expects 521
ROLLBACK
VACUUM

$ psql -f slony1_log_1_00000000000000000525.sql test2_log
START TRANSACTION
psql:slony1_log_1_00000000000000000525.sql:6: ERROR:  Slony-I: set 1 is on
sync 523, this archive log expects 524
ROLLBACK
VACUUM

Something has gone wrong between 524 and 525. In fact, 524 file reveals
that:

$ cat slony1_log_1_00000000000000000524.sql
-- Slony-I log shipping archive
-- Node 1, Event 524
start transaction;

select "_test2".setsyncTracking_offline(1, '521', '524', '2007-07-20 15:19:
58.812135');
-- end of log archiving header
------------------------------------------------------------------
-- start of Slony-I data
------------------------------------------------------------------

------------------------------------------------------------------
-- End Of Archive Log
------------------------------------------------------------------
commit;
vacuum analyze "_test2".sl_setsync_offline;

To my eye it seems that file 524 is trying to advance the sync number from
521 to 524, while it has already been incremented to 523 in the previous
file. Why does the 524 file try to advance the sync number in such a way?

This is slony 1.2.10 and pgsql 8.2.4 if it matters.

Regards

MP
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070720/=
38b1d970/attachment.htm
From JanWieck at Yahoo.com  Fri Jul 20 07:25:43 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Fri Jul 20 07:26:00 2007
Subject: [Slony1-general] Slave in an insecure location
In-Reply-To: <2ca799770707200306t76557286xcfa0081f7cfa123b@mail.gmail.com>
References: <2ca799770707182158g235cb29cj6ecbd2237422f3fe@mail.gmail.com>	
	<469F7EBD.9090908@Yahoo.com>
	<2ca799770707200306t76557286xcfa0081f7cfa123b@mail.gmail.com>
Message-ID: <46A0C5E7.7030001@Yahoo.com>

On 7/20/2007 6:06 AM, Mikko Partio wrote:
> 
> 
> On 7/19/07, *Jan Wieck* <JanWieck@yahoo.com <mailto:JanWieck@yahoo.com>> 
> wrote:
> 
>     On 7/19/2007 12:58 AM, Mikko Partio wrote:
> 
>     No, only the slon of a real replica can actually write the log files.
>     However, what might work in your case is to create a "fake" replica.
>     Create a replica that has a slave-only trigger (using STORE TRIGGER) on
>     every replicated table. The trigger fires BEFORE INSERT and all it does
>     is RETURN NULL. And you subscribe the thing with no forwarding. That
>     should create an empty replica that now can write the log shipping
>     files. To get the initial content for the offline replica you must
>     however stop the application and dump from the origin.
> 
> 
> 
> One question, the script slony1_dump.sh retrieves the sync-value for 
> table sl_setsync_offline from the sl_setsync -table. I modified the 
> script so that the data gets dumped from the origin node (since the 
> subscriber is empty), but the sl_setsync table is empty at the origin, 
> Where can I get the correct sync number (is select max(con_seqno) from 
> sl_confirm ok) ?

That is precisely the problem why you have to stop the application at 
least briefly.

   - Stop application
   - Let (dummy) subscriber completely catch up
   - Get number from subscribers sl_setsync
   - Start the dump on the origin

Due to MVCC, the dump will have the data that would have been on the 
subscriber at that setsync. You don't have to wait for the dump to 
finish. Once it has started its serializable transaction (which it does 
right away), the application can be restarted.

All of that can be done in a minute, so any scheduled maintenance time 
will give you an opportunity to do it in production.


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From cbbrowne at ca.afilias.info  Fri Jul 20 13:15:45 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Jul 20 13:15:55 2007
Subject: [Slony1-general] adding --version option to binaries
In-Reply-To: <5a0a9d6f0707191519p40bf1520s9b37669d7c3e0d08@mail.gmail.com>
References: <5a0a9d6f0707181000g787934esd67863106995c23e@mail.gmail.com>	
	<469E5262.8010706@ca.afilias.info> <469F7836.9020408@Yahoo.com>
	<5a0a9d6f0707191519p40bf1520s9b37669d7c3e0d08@mail.gmail.com>
Message-ID: <46A117F1.20501@ca.afilias.info>

Andrew Hammond wrote:
> Totally untested and probably insanely wrong patch attached.
I have committed a version that takes out the long option usage, as that 
seems to require hooking in more #includes, which seems excessive.

It does show more help; "slon -h" didn't use to indicate that it had 
-h/-v options, and it does, now...

I'm going to have to go on record against using getopt_long(); that's 
not available on non-GNU/non-BSD systems without adding in an extra 
library.  I am really disinclined to require this (explained in one 
word:  AIX ;-)).
From by_pacitan at yahoo.com  Fri Jul 20 23:35:11 2007
From: by_pacitan at yahoo.com (angga erwina)
Date: Fri Jul 20 23:35:26 2007
Subject: [Slony1-general] Best interval timeout and check interval
Message-ID: <94305.22301.qm@web56608.mail.re3.yahoo.com>

Hi, All
 i want to replicate my DB with one master and two
Slave over VPN with bandwidth 64Kbps and my DB
125MB..what is the best interval timeout and time to
check interval..for this time, i use default..any one
has the best experience for timeout?? can i use
trigger to make connection timeout?? could you give me
trigger example to make connection timeout.. if i
can't do it,am i only use -t .. i've trying to use
this rule,but i am still couldnt do it..how can i
check for connection timeout?? 
Thanks,
Regards
Angga Bayu


       
____________________________________________________________________________________
Looking for a deal? Find great prices on flights and hotels with Yahoo! FareChase.
http://farechase.yahoo.com/
From mpartio at gmail.com  Sun Jul 22 23:27:46 2007
From: mpartio at gmail.com (Mikko Partio)
Date: Sun Jul 22 23:28:07 2007
Subject: [Slony1-general] Re: log shipping gone wrong
In-Reply-To: <2ca799770707200605j1bb37c13o98c86a21834db2ab@mail.gmail.com>
References: <2ca799770707200605j1bb37c13o98c86a21834db2ab@mail.gmail.com>
Message-ID: <2ca799770707222327y5e5939bg6ccee843d0ef0097@mail.gmail.com>

On 7/20/07, Mikko Partio <mpartio@gmail.com> wrote:
>
> Hello all,
>
> as mentioned in an another thread I have started a log shipping scenario
> where there's an origin, a slave and a log shipping slave. The setup went
> smoothly and everything worked when the logs produced by the normal
> subscriber were applied to the log shipping slave. The problem is that
> inevitably the replication to the log shipping slave stops. The reason is
> that the syncs in the logfiles have gaps:
>
> To my eye it seems that file 524 is trying to advance the sync number from
> 521 to 524, while it has already been incremented to 523 in the previous
> file. Why does the 524 file try to advance the sync number in such a way?



The problem only surfaces when I modify the replication sets, in normal
replication everything works like a charm. The 1.2.10 release notes say that

Fix archive log ship tracking. Slon now tracks the setsync status in memory
and generates a void archive with the correct old,new event seqno for all
events.

but apparently the problem still exists. If I manually change the sequence
numbers in the archive files everything goes ok, but I wish that slon would
create the correct numbers by itself.

Regards

MP
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070723/=
22c91f75/attachment.htm
From JanWieck at Yahoo.com  Mon Jul 23 06:34:07 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon Jul 23 06:34:23 2007
Subject: [Slony1-general] Re: log shipping gone wrong
In-Reply-To: <2ca799770707222327y5e5939bg6ccee843d0ef0097@mail.gmail.com>
References: <2ca799770707200605j1bb37c13o98c86a21834db2ab@mail.gmail.com>
	<2ca799770707222327y5e5939bg6ccee843d0ef0097@mail.gmail.com>
Message-ID: <46A4AE4F.4080304@Yahoo.com>

On 7/23/2007 2:27 AM, Mikko Partio wrote:
> 
> 
> On 7/20/07, *Mikko Partio* <mpartio@gmail.com 
> <mailto:mpartio@gmail.com>> wrote:
> 
>     Hello all,
> 
>     as mentioned in an another thread I have started a log shipping
>     scenario where there's an origin, a slave and a log shipping slave.
>     The setup went smoothly and everything worked when the logs produced
>     by the normal subscriber were applied to the log shipping slave. The
>     problem is that inevitably the replication to the log shipping slave
>     stops. The reason is that the syncs in the logfiles have gaps:
> 
>     To my eye it seems that file 524 is trying to advance the sync
>     number from 521 to 524, while it has already been incremented to 523
>     in the previous file. Why does the 524 file try to advance the sync
>     number in such a way? 
> 
> 
> 
> The problem only surfaces when I modify the replication sets, in normal 
> replication everything works like a charm. The 1.2.10 release notes say that

Modify the replication sets exactly how?


Jan

> 
> Fix archive log ship tracking. Slon now tracks the setsync status in 
> memory and generates a void archive with the correct old,new event seqno 
> for all events.
> 
> but apparently the problem still exists. If I manually change the 
> sequence numbers in the archive files everything goes ok, but I wish 
> that slon would create the correct numbers by itself.
> 
> Regards
> 
> MP
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From mpartio at gmail.com  Mon Jul 23 07:28:54 2007
From: mpartio at gmail.com (Mikko Partio)
Date: Mon Jul 23 07:29:05 2007
Subject: [Slony1-general] Re: log shipping gone wrong
In-Reply-To: <46A4AE4F.4080304@Yahoo.com>
References: <2ca799770707200605j1bb37c13o98c86a21834db2ab@mail.gmail.com>
	<2ca799770707222327y5e5939bg6ccee843d0ef0097@mail.gmail.com>
	<46A4AE4F.4080304@Yahoo.com>
Message-ID: <2ca799770707230728h3e3afa3auefdc1ae8dd429156@mail.gmail.com>

Modify the replication sets exactly how?
>
>
>
Through slonik, what I meant by that statement is that anything done with
slonik (well at least create set and merge set) messes the sequence numbers
somehow.

Regards

MP
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070723/=
cdbe777a/attachment.htm
From JanWieck at Yahoo.com  Mon Jul 23 08:06:15 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon Jul 23 08:06:32 2007
Subject: [Slony1-general] Re: log shipping gone wrong
In-Reply-To: <2ca799770707230728h3e3afa3auefdc1ae8dd429156@mail.gmail.com>
References: <2ca799770707200605j1bb37c13o98c86a21834db2ab@mail.gmail.com>	
	<2ca799770707222327y5e5939bg6ccee843d0ef0097@mail.gmail.com>	
	<46A4AE4F.4080304@Yahoo.com>
	<2ca799770707230728h3e3afa3auefdc1ae8dd429156@mail.gmail.com>
Message-ID: <46A4C3E7.1020107@Yahoo.com>

On 7/23/2007 10:28 AM, Mikko Partio wrote:
> 
>  
> 
>     Modify the replication sets exactly how?
> 
> 
> 
> Through slonik, what I meant by that statement is that anything done 
> with slonik (well at least create set and merge set) messes the sequence 
> numbers somehow.

What I meant is that it'd be good to get enough detail information about 
the specific actions that lead to this problem in order to create a 
standalone test that can reproduce it.


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From rbt at sitesell.com  Wed Jul 11 12:16:34 2007
From: rbt at sitesell.com (Rod Taylor)
Date: Mon Jul 23 08:40:57 2007
Subject: [Slony1-general] Re: Split Set
In-Reply-To: <4695283A.9000904@ca.afilias.info>
References: <6A894B6C-122A-481B-A01F-6D996CAE176D@sitesell.com>
	<4695283A.9000904@ca.afilias.info>
Message-ID: <36D1DF10-E01C-4BA1-86F0-6EAE2C2D934F@sitesell.com>

>
> This is do-able, but it feels mighty clumsy, and even more fragile.
>
> It seems really tempting to think about having a "virtual  
> replication set" for this, and putting that into Slony-I proper.
>
> The notion of the "virtual replication set" still seems pretty clumsy.

Not too sure about hiding it. It is very fragile if it breaks mid-way  
through. If it is hidden it will be difficult to fix, unless you  
think you can make it continue when it finds a virtual set partially  
subscribed. Good transaction boundaries leave the number of possible  
broken states at a small enough count.

> Here's an outline of a very different approach that Jan and I have  
> been talking about that we call "COPY pipelining." It ought to help  
> by parallelizing the data load. It's in the TODO...

Fine for small to mid sized databases. If you're using table  
partitioning because of the size of the structures, and have lots of  
partitions, this isn't going to do much to eliminate the large  
transaction problem.

Granted, it would be very nice if subscribes went a little quicker :)

> ======================================================
> COPY pipelining
>
> - the notion here is to try to parallelize the data load at
> SUBSCRIBE time. Suppose we decide we can process 4 tables at a
> time, we set up 4 threads. We then iterate thus:
>
> For each table
> - acquire a thread (waiting as needed)
> - submit COPY TO stdout to the provider, and feed to
> COPY FROM stdin on the subscriber
> - Submit the REINDEX request on the subscriber
>
> Even with a fairly small number of threads, we should be able to
> process the whole subscription in as long as it takes to process
> the single largest table.
>
> This introduces a risk of locking problems not true at present
> (alas) in that, at present, the subscription process is able to
> demand exclusive locks on all tables up front; that is no longer
> possible if the subscriptions are split across multiple tables.
> In addition, the updates will COMMIT across some period of time on
> the subscriber rather than appearing at one instant in time.
>
> The timing improvement is probably still worthwhile.
>
> http://lists.slony.info/pipermail/slony1-hackers/2007-April/ 
> 000000.html
> ======================================================
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

From rbt at sitesell.com  Wed Jul 11 12:51:25 2007
From: rbt at sitesell.com (Rod Taylor)
Date: Mon Jul 23 08:40:58 2007
Subject: [Slony1-general] Re: Split Set
In-Reply-To: <60vecqwws7.fsf@dba2.int.libertyrms.com>
References: <6A894B6C-122A-481B-A01F-6D996CAE176D@sitesell.com>
	<4695283A.9000904@ca.afilias.info>
	<36D1DF10-E01C-4BA1-86F0-6EAE2C2D934F@sitesell.com>
	<60vecqwws7.fsf@dba2.int.libertyrms.com>
Message-ID: <53AF2BF0-EBBF-43CA-AF60-59BE306F5DAF@sitesell.com>

Your (everyones) efforts are certainly appreciated and I fully intend  
to try out the SET MOVE TABLE strategy in the next week or so on a  
200GB migration.

Catchup isn't so bad since each large block occurs in its own  
transaction. I'm not really concerned with the actual time to  
subscribe but rather than fact my master node is not very happy when  
we add new slaves.

Cheers and I'll buy you a beer at the next GTABug.

- Rod

On 11-Jul-07, at 3:44 PM, Christopher Browne wrote:

> Rod Taylor <rbt@sitesell.com> writes:
>>> This is do-able, but it feels mighty clumsy, and even more fragile.
>>>
>>> It seems really tempting to think about having a "virtual
>>> replication set" for this, and putting that into Slony-I proper.
>>>
>>> The notion of the "virtual replication set" still seems pretty  
>>> clumsy.
>>
>> Not too sure about hiding it. It is very fragile if it breaks mid-way
>> through. If it is hidden it will be difficult to fix, unless you
>> think you can make it continue when it finds a virtual set partially
>> subscribed. Good transaction boundaries leave the number of possible
>> broken states at a small enough count.
>>
>>> Here's an outline of a very different approach that Jan and I have
>>> been talking about that we call "COPY pipelining." It ought to help
>>> by parallelizing the data load. It's in the TODO...
>>
>> Fine for small to mid sized databases. If you're using table
>> partitioning because of the size of the structures, and have lots of
>> partitions, this isn't going to do much to eliminate the large
>> transaction problem.
>>
>> Granted, it would be very nice if subscribes went a little quicker
>> :)
>
> If you set up 4 pipelines, then that should improve the speed by a
> factor of around 4, which should be a not unappreciable improvement
> :-).
>
> FYI, Jan has been trying out an approach that the Skype guys pointed
> out that seems to *massively* improve the ability to catch up after
> large transactions, so I think that may get helped soon enough (e.g. -
> v2.0).
> <http://lists.slony.info/pipermail/slony1-commit/2007-July/ 
> 001854.html>
> That may be the "better change."  Note that it's a very small change;
> it might well work in 1.2, too.
>
> In any case, for a database with a whole lot of partitions, I don't
> think it's too outrageous to directly head to the "subscribe in steps"
> approach.
>
> foreach $table (@tables_in_set_10) {
>    create set (id=999+$table, provider=1, comment='Temporary set';
>    set move table (origin=1, id=$table, new set=999+$table);
>    subscribe set (id=999+$table, provider=1, receiver=2);
> }
> subscribe set (id=10, provider=1, receiver=2);
>
> and then clean up with a bunch of MERGE SET requests afterwards.
> -- 
> "cbbrowne","@","cbbrowne.com"
> http://cbbrowne.com/info/rdbms.html
> Rules of the Evil Overlord #78.  "I will not tell my Legions of Terror
> "And he must  be taken alive!" The command will be:  ``And try to take
> him alive if it is reasonably practical.''"
> <http://www.eviloverlord.com/>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

From cboudjnah at squiz.net  Sun Jul 15 19:07:59 2007
From: cboudjnah at squiz.net (Chmouel Boudjnah)
Date: Mon Jul 23 08:40:58 2007
Subject: [Slony1-general] Log Shipping with slony.
Message-ID: <87fy3pt81s.fsf@paris.squiz.net>

Hi,

I am trying to setup a log shipping replication on a system and i was
wondering if the documentation was up to date. Please let me know if
i miss something of theses steps :

- I need to replicate from Master to SLAVE, slave is the log shipping
  destination.

- To do so i have to setup a real node and i am doing it on the same
  postgresql DB, i am calling it MIRROR.

- I synchronize the schemas from MASTER to SLAVE and MIRROR.

- I setup two slon process one for the SLAVE and one for the MIRROR

- I setup -a on the slon process of the MIRROR to dump the log files.

- I create the sets and subscribe to it (from MASTER TO MIRROR).

- I launch the slony_dump1.sh and dump the sql into the SLAVE.

- I synchronize all theses dump files from the dump directory to slaves
  every few minutes or so.

Everything looks working well my only concerns is the part of the
documentation advising to analyze the first few headers and see if it
is commiting before commiting the rest of the files.

I had no problems to just :

for i in *;do psql -hSLAVEDB -f $i db && rm -f $i;done

without needing to analyze the headers.

So is it still needed to analyze the headers or something got fix in
the latest version of slony 1.2.10

Cheers, Chmouel.
-- 
http://www.squiz.net
From dba at richyen.com  Mon Jul 23 11:23:02 2007
From: dba at richyen.com (Richard Yen)
Date: Mon Jul 23 11:23:18 2007
Subject: [Slony1-general] lag_events v. lag_time
Message-ID: <C9B78BE4-887D-42AD-9A9E-3E7A1870C997@richyen.com>

Hi,

I'm wondering if there's any difference between lag_events and  
lag_time in the sl_status table/view.  There are often times where I  
notice that the lag_events is 0, yet the lag_time is somewhere in the  
range of 20-30 seconds.  Why is this the case?

--Richard
From mpartio at gmail.com  Mon Jul 23 21:29:27 2007
From: mpartio at gmail.com (Mikko Partio)
Date: Mon Jul 23 21:29:45 2007
Subject: [Slony1-general] Re: log shipping gone wrong
In-Reply-To: <46A4C3E7.1020107@Yahoo.com>
References: <2ca799770707200605j1bb37c13o98c86a21834db2ab@mail.gmail.com>
	<2ca799770707222327y5e5939bg6ccee843d0ef0097@mail.gmail.com>
	<46A4AE4F.4080304@Yahoo.com>
	<2ca799770707230728h3e3afa3auefdc1ae8dd429156@mail.gmail.com>
	<46A4C3E7.1020107@Yahoo.com>
Message-ID: <2ca799770707232129p27822251v9947c748d2395630@mail.gmail.com>

>
>
> What I meant is that it'd be good to get enough detail information about
> the specific actions that lead to this problem in order to create a
> standalone test that can reproduce it.
>

Ok here comes:

I have three databases, test3 which is the primary, test3_lo which is the
secondary "loopback" database (only used to make the logshipping logs) and
test3_log, the logshipping database. All reside in the same pg cluster.
Here's exactly what I did, there's a lot of noise but I thought it's better
to be thorough.

1) Create databases and some tables

$ createdb -D data -E utf8 test3
$ psql test3

test3=3D# create table a (id int primary key);
test3=3D# create table b (id int primary key);
test3=3D# create table c (id int primary key);

$ createdb -T test3 test3_lo
$ createdb -T test3 test3_log

2) Create the replication cluster

$ cat test3_preamble.slonik
cluster name =3D test3;

node 1 admin conninfo =3D 'host=3Dlocalhost dbname=3Dtest3 user=3Dpostgres
password=3Dxxx';
node 2 admin conninfo =3D 'host=3Dlocalhost dbname=3Dtest3_lo user=3Dpostgr=
es
password=3Dxxx';

$ cat test3_init.slonik
#!/bin/sh

slonik <<_EOF_

        include <test3_preamble.slonik>;

        init cluster (id=3D1, comment=3D'original database');

        store node (id=3D2, comment =3D 'dummy loopback database');
        store path (server=3D1, client=3D2, conninfo=3D'dbname=3Dtest3
host=3Dlocalhost user=3Dpostgres password=3Dxxx');
        store path (server=3D2, client=3D1, conninfo=3D'dbname=3Dtest3_lo
host=3Dlocalhost user=3Dpostgres password=3Dxxx');
        store listen (origin=3D1, provider=3D1, receiver=3D2);
        store listen (origin=3D2, provider=3D2, receiver=3D1);

_EOF_

$ ./test3_init.slonik

$ slon test3 "dbname=3Dtest3 host=3Dlocalhost user=3Dpostgres password=3Dxx=
x"
$ slon -a /tmp/logshipping/test3 test3 "dbname=3Dtest3_lo host=3Dlocalhost
user=3Dpostgres password=3Dxxx"

$ cat test3_create_set.slonik
#!/bin/sh

if [ $# -ne 3 ]; then
        echo "Usage: $0 SETID TABLEID TABLENAME"
        echo "Example: $0 3 17 public.x"
        exit 1
fi

slonik <<_EOF_

        include <test3_preamble.slonik>;

        create set (id=3D$1, origin=3D1, comment=3D'Set #$1');
        set add table (set id =3D $1, origin =3D 1, id =3D $2, fully qualif=
ied
name =3D '$3');

_EOF_

$ ./test3_create_set.slonik 1 1 public.a
$ cat test3_subscribe.slonik
#!/bin/sh

if [ -z $1 ]; then
        echo "Usage: $0 SETID"
        exit 1;
fi

slonik <<_EOF_

        include <test3_preamble.slonik>;

        subscribe set (id=3D$1, provider=3D1, receiver=3D2, forward=3Dno);

_EOF_

$ ./test3_subscribe.slonik 1
$ psql test3_lo
test3_lo=3D# \d a
       Table "public.a"
 Column |  Type   | Modifiers
--------+---------+-----------
 id     | integer | not null
Indexes:
    "a_pkey" PRIMARY KEY, btree (id)
Triggers:
    _test3_denyaccess_1 BEFORE INSERT OR DELETE OR UPDATE ON a FOR EACH ROW
EXECUTE PROCEDURE _test3.denyaccess('_test3')

$ psql test3
test3=3D# insert into a select * from generate_series(1,100);

3) Dump the state of the cluster to logshipping database

$ ./slony1_dump.sh test3_lo test3 | psql test3_log
$ psql test3_log
test3_log=3D# select count(*) from a;
 count
-------
   100
(1 row)

4) Insert some data to origin db and see if it gets to logshipping db

test3=3D# insert into a select * from generate_series(101,200);

$ psql -d test3_log -f
/tmp/logshipping/test3/slony1_log_1_00000000000000000060.sql
$ psql test3_log
test3_log=3D# select count(*) from a;
 count
-------
   200
(1 row)

5) Create another replication set

$ ./test3_create_set.slonik 10 2 public.b
$ ./test3_subscribe.slonik 10

6) Start applying logfiles to logshipping db

$ psql -d test3_log -f
/tmp/logshipping/test3/slony1_log_1_00000000000000000061.sql
...
$ psql -d test3_log -f
/tmp/logshipping/test3/slony1_log_1_00000000000000000068.sql
$ psql -d test3_log -f
/tmp/logshipping/test3/slony1_log_1_00000000000000000069.sql
START TRANSACTION
psql:/tmp/logshipping/test3/slony1_log_1_00000000000000000069.sql:6: ERROR:
Slony-I: set 1 is on sync 68, this archive log expects 67

$ cat /tmp/logshipping/test3/slony1_log_1_00000000000000000068.sql
-- Slony-I log shipping archive
-- Node 1, Event 68
start transaction;
-- STORE_SET

select "_test3".setsyncTracking_offline(1, '67', '68', '2007-07-24 07:14:
53.592719');
-- end of log archiving header
------------------------------------------------------------------
-- start of Slony-I data
------------------------------------------------------------------

------------------------------------------------------------------
-- End Of Archive Log
------------------------------------------------------------------
commit;
vacuum analyze "_test3".sl_setsync_offline;


$ cat /tmp/logshipping/test3/slony1_log_1_00000000000000000069.sql
-- Slony-I log shipping archive
-- Node 1, Event 69
start transaction;
-- SET_ADD_TABLE

select "_test3".setsyncTracking_offline(1, '67', '69', '2007-07-24 07:14:
53.641896');
-- end of log archiving header
------------------------------------------------------------------
-- start of Slony-I data
------------------------------------------------------------------

------------------------------------------------------------------
-- End Of Archive Log
------------------------------------------------------------------
commit;
vacuum analyze "_test3".sl_setsync_offline;



I hope this was of any help, if you need any more info I'm happy to help.

Regards

MP
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070724/=
40fb82c6/attachment.htm
From dmitry at koterov.ru  Wed Jul 25 13:09:28 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Wed Jul 25 13:10:00 2007
Subject: [Slony1-general] Detection if a subscriber is up-to-date relative
	to an origin at the time X
Message-ID: <d7df81620707251309h1f6befd4v98e5c3064ac3ba69@mail.gmail.com>

Hello.

I have a timestamp (let it be X) on an origin database. I need to determine
if a subscriber have processed ALL updates commited BEFORE the time X (we
will say that in such case a subscriber is "up-to-date relative to X").
Please say, how do I do this correctly?


P.S.
I tried to scan sl_event table on a subscriber checking if we have an event
with ev_timestamp greater than X:

EXISTS (SELECT * FROM sl_event WHERE ev_timestamp > X)   -- run on a
subscriber

But this method is wrong - sometimes it says that a subscriber is
up-to-date, but the data inserted in a table before X is still not appeared
on a subscriber! Experiment shows that if I exclude SYNC events from
checking, all work fine:

EXISTS (SELECT * FROM sl_event WHERE ev_timestamp > X AND ev_type !=3D 'SYN=
C')
-- run on a subscriber

But excluding of SYNC is not a good method, because the next non-SYNC event
may be generated too late (e.g. in 30 minutes).

P.P.S.
In my situation X is calculated as clock_timestamp_before_an_update +
time_took_by_this_update. So, I assign the origin time to X, execute an
UPDATE+COMMIT query and then - adds time spent in this query to X to
determine the time of UPDATE finish for later checking.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070726/=
23ee6d6b/attachment.htm
From cbbrowne at ca.afilias.info  Mon Jul 23 10:01:39 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 25 13:15:53 2007
Subject: [Slony1-general] Log Shipping with slony.
In-Reply-To: <87fy3pt81s.fsf@paris.squiz.net> (Chmouel Boudjnah's message of
	"Mon, 16 Jul 2007 12:07:59 +1000")
References: <87fy3pt81s.fsf@paris.squiz.net>
Message-ID: <60abtnyryk.fsf@dba2.int.libertyrms.com>

Chmouel Boudjnah <cboudjnah@squiz.net> writes:
> Everything looks working well my only concerns is the part of the
> documentation advising to analyze the first few headers and see if it
> is commiting before commiting the rest of the files.

The reason to suggest pulling the first few lines is that if things
do, for whatever reason, break down, it is preferable to handle this
in a controlled way (e.g. - where only a small part of the data gets
loaded in) rather than having large batches of transactions fail.

There may be exogenous reasons for failure that don't have to do with
log shipping itself:

 - Suppose a disk goes bad, corrupting part of a log file
 - Suppose you are copying the files to another server, and something
   breaks down with that copy process
 - Suppose the copy process skips/misses a file

If you aren't doing any pre-examination of the data, the processing
may start failing, and in a way that fills log files with
extraordinary quantities of (pretty useless!) error messages.

The recommendation to do a bit of pre-analysis allows you to detect
these problems and alert a human earlier and without filling up disk
space with millions of lines to the effect of:

ERROR:  current transaction is aborted, commands ignored until end of transaction block
-- 
let name="cbbrowne" and tld="cbbrowne.com" in String.concat "@" [name;tld];;
http://linuxdatabases.info/info/sap.html
"We defeated the enemy with teamwork and the hammer of not bickering."
-- The Shoveller, Mystery Men
From cbbrowne at ca.afilias.info  Mon Jul 23 11:57:38 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 25 13:15:55 2007
Subject: [Slony1-general] lag_events v. lag_time
In-Reply-To: <C9B78BE4-887D-42AD-9A9E-3E7A1870C997@richyen.com> (Richard Yen's
	message of "Mon, 23 Jul 2007 11:23:02 -0700")
References: <C9B78BE4-887D-42AD-9A9E-3E7A1870C997@richyen.com>
Message-ID: <601wezyml9.fsf@dba2.int.libertyrms.com>

Richard Yen <dba@richyen.com> writes:
> I'm wondering if there's any difference between lag_events and
> lag_time in the sl_status table/view.  There are often times where I
> notice that the lag_events is 0, yet the lag_time is somewhere in the
> range of 20-30 seconds.  Why is this the case?

That will typically happen if there are no updates getting submitted
to the origin node.

If the origin is not being updated, then it may take 30 seconds to
generate a SYNC (depending on configuration!).  In that case,
subscribers may sit at the same SYNC number as the origin for that 30
seconds.

After 30s, one of the timeouts is reached, a SYNC (that involves no
updates!) is generated, and so it takes very little time for the
subscribers to process that SYNC, and then sit, lagging by nothing,
for another 30s.

If the application is quiescent, then that's a to-be-expected
behaviour.
-- 
select 'cbbrowne' || '@' || 'cbbrowne.com';
http://linuxfinances.info/info/spiritual.html
"You can measure a programmer's perspective by noting his attitude on
the continuing vitality of FORTRAN." -- Alan J. Perlis
From dba at richyen.com  Wed Jul 25 16:44:44 2007
From: dba at richyen.com (Richard Yen)
Date: Wed Jul 25 16:45:32 2007
Subject: [Slony1-general] lag_events v. lag_time
In-Reply-To: <601wezyml9.fsf@dba2.int.libertyrms.com>
References: <C9B78BE4-887D-42AD-9A9E-3E7A1870C997@richyen.com>
	<601wezyml9.fsf@dba2.int.libertyrms.com>
Message-ID: <4EADA98A-B4CB-49EC-AA26-745E64A6C17B@richyen.com>

> If the origin is not being updated, then it may take 30 seconds to
> generate a SYNC (depending on configuration!).  In that case,
> subscribers may sit at the same SYNC number as the origin for that 30
> seconds.
Are you referring to the sync_interval_timeout parameter in my conf  
file?  I'm set to sync_interval_timeout=3000

I'm also set to sync_interval=500.  Is this supposed to create a SYNC  
event every .5 seconds?  Or perhaps the subscriber is supposed to  
look for a SYNC event every .5 seconds?

Perhaps my understanding of SYNC is misshapen, and it's being exposed  
here...

> After 30s, one of the timeouts is reached, a SYNC (that involves no
> updates!) is generated, and so it takes very little time for the
> subscribers to process that SYNC, and then sit, lagging by nothing,
> for another 30s.
>
> If the application is quiescent, then that's a to-be-expected
> behaviour.
Thanks for the response!
--Richard
From dmitry at koterov.ru  Wed Jul 25 23:19:32 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Wed Jul 25 23:20:15 2007
Subject: [Slony1-general] lag_events v. lag_time
In-Reply-To: <4EADA98A-B4CB-49EC-AA26-745E64A6C17B@richyen.com>
References: <C9B78BE4-887D-42AD-9A9E-3E7A1870C997@richyen.com>
	<601wezyml9.fsf@dba2.int.libertyrms.com>
	<4EADA98A-B4CB-49EC-AA26-745E64A6C17B@richyen.com>
Message-ID: <d7df81620707252319o52c7e28dwb332bf6e382cf880@mail.gmail.com>

Here are some explainations about SYNC:
http://archives.postgresql.org/pgsql-admin/2007-03/msg00109.php


On 7/26/07, Richard Yen <dba@richyen.com> wrote:
>
> > If the origin is not being updated, then it may take 30 seconds to
> > generate a SYNC (depending on configuration!).  In that case,
> > subscribers may sit at the same SYNC number as the origin for that 30
> > seconds.
> Are you referring to the sync_interval_timeout parameter in my conf
> file?  I'm set to sync_interval_timeout=3D3000
>
> I'm also set to sync_interval=3D500.  Is this supposed to create a SYNC
> event every .5 seconds?  Or perhaps the subscriber is supposed to
> look for a SYNC event every .5 seconds?
>
> Perhaps my understanding of SYNC is misshapen, and it's being exposed
> here...
>
> > After 30s, one of the timeouts is reached, a SYNC (that involves no
> > updates!) is generated, and so it takes very little time for the
> > subscribers to process that SYNC, and then sit, lagging by nothing,
> > for another 30s.
> >
> > If the application is quiescent, then that's a to-be-expected
> > behaviour.
> Thanks for the response!
> --Richard
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070726/=
ff61e859/attachment.htm
From adamunno at tele2.it  Thu Jul 26 01:21:15 2007
From: adamunno at tele2.it (adamunno@tele2.it)
Date: Thu Jul 26 01:22:02 2007
Subject: [Slony1-general] trouble with slony 1.2. and postgresql 8.1
Message-ID: <web-73176597@mailbe01.swip.net>

An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070726/5bc5443d/attachment.htm
From plk.zuber at gmail.com  Thu Jul 26 04:14:44 2007
From: plk.zuber at gmail.com (=?UTF-8?Q?Filip_Rembia=C5=82kowski?=)
Date: Thu Jul 26 04:15:31 2007
Subject: [Slony1-general] trouble with slony 1.2. and postgresql 8.1
In-Reply-To: <web-73176597@mailbe01.swip.net>
References: <web-73176597@mailbe01.swip.net>
Message-ID: <92869e660707260414x27b69a32t109f0d1de12a3827@mail.gmail.com>

2007/7/26, adamunno@tele2.it <adamunno@tele2.it>:
> Hi all!
> I'm testing slony 1.2.1 on postgresql 8.1 and ubuntu 8 on a virtual machine
> of vmware following the example on
> http://www.linuxjournal.com/article/7834. I created two db
> on the same machine and I run the script cluster_setup.sh. Then when I run
> slon daemon with
> slon sql_cluster "dbname=contactdb user=postgres" &
> i obtain:
>
> 2007-07-25 22:26:01 CEST DEBUG2 slon_retry() from pid=19480
> 2007-07-25 22:26:01 CEST DEBUG1 slon: retry requested
> 2007-07-25 22:26:01 CEST DEBUG2 slon: notify worker process to shutdown
> 2007-07-25 22:26:01 CEST DEBUG2 slon: worker process created - pid = 19480
> 2007-07-25 22:26:01 CEST DEBUG2 slon: child terminated status: 0; pid:
> 19480, current worker pid: 19480
> 2007-07-25 22:26:01 CEST DEBUG1 slon: restart of worker
> 2007-07-25 22:26:01 CEST CONFIG main: slon version 1.2.1 starting up
> 2007-07-25 22:26:01 CEST DEBUG2 slon: watchdog process started
> 2007-07-25 22:26:01 CEST DEBUG2 slon: watchdog ready - pid = 18968
> 2007-07-25 22:26:01 CEST FATAL  main: Cannot connect to local database -
> could not connect to server: No such file or directory
>         Is the server running locally and accepting
>         connections on Unix domain socket
> "/var/run/postgresql/.s.PGSQL.5432"?
>
> 2007-07-25 22:26:01 CEST DEBUG2 slon_retry() from pid=19481
> 2007-07-25 22:26:01 CEST DEBUG1 slon: retry requested
> 2007-07-25 22:26:01 CEST DEBUG2 slon: notify worker process to shutdown
> 2007-07-25 22:26:01 CEST DEBUG2 slon: worker process created - pid = 19481
> 2007-07-25 22:26:01 CEST DEBUG2 slon: child terminated status: 0; pid:
> 19481, current worker pid: 19481
> 2007-07-25 22:26:01 CEST DEBUG1 slon: restart of worker
> 2007-07-25 22:26:01 CEST CONFIG main: slon version 1.2.1 starting up
> 2007-07-25 22:26:01 CEST FATAL  slon: sched_wakeuppipe create failed -(24)
> Too many open files
> 2007-07-25 22:26:01 CEST DEBUG2 slon: exit(-1)
>
> In my pg_hba.cong there is
> local all all trust
> host all all 127.0.0.1/32 trust
> and postgres run correctly on loacalhost on port 5432.
>
> Any ideas?

try to add "host=localhost port=5432" to connection strings passed to slon.
if you don't, it tries to connect via unix socket and fails.

-- 
Filip Rembia?kowski
From jc at oxado.com  Thu Jul 26 07:22:57 2007
From: jc at oxado.com (Jacques Caron)
Date: Thu Jul 26 07:23:34 2007
Subject: [Slony1-general] Relhasindex and PostgreSQL 8.2
Message-ID: <20070726142315.B669CAA72CB@zeus.fr1.directinfos.com>

Hi all,

It seems that in PostgreSQL 8.2.4 (don't know exactly when the change 
was made) TRUNCATE resets relhasindex to the appropriate value. This 
has the effect that if the TRUNCATE in preparetableforcopy succeeds, 
relhasindex is set back to true, and the indexes are updated during 
the copy, with the well known performance hit that goes with it.

Attached patch apparently fixes the problem (just set relhasindex to 
false _after_ the TRUNCATE):

--- slony1_funcs.v81.sql.orig   Thu Jul 26 16:11:36 2007
+++ slony1_funcs.v81.sql        Thu Jul 26 16:12:28 2007
@@ -40,19 +40,23 @@
         end if;

         -- ----
+       -- Try using truncate to empty the table and fallback to
+       -- delete on error.
+       -- ----
+       execute ''truncate '' || @NAMESPACE@.slon_quote_input(v_tab_fqname);
+       raise notice ''truncate of % succeeded'', v_tab_fqname;
+
+       -- ----
         -- Setting pg_class.relhasindex to false will cause copy not to
         -- maintain any indexes. At the end of the copy we will reenable
         -- them and reindex the table. This bulk creating of indexes is
         -- faster.
+       --
+       -- This must be done _after_ the truncate as since 8.2 truncate
+       -- resets relhasindex!
         -- ----
         update pg_class set relhasindex = ''f'' where oid = v_tab_oid;

-       -- ----
-       -- Try using truncate to empty the table and fallback to
-       -- delete on error.
-       -- ----
-       execute ''truncate '' || @NAMESPACE@.slon_quote_input(v_tab_fqname);
-       raise notice ''truncate of % succeeded'', v_tab_fqname;
         return 1;
         exception when others then
                 raise notice ''truncate of % failed - doing 
delete'', v_tab_fqname;


This is against Slony 1.2.10.

Hope this helps,

Jacques.

From cbbrowne at ca.afilias.info  Thu Jul 26 10:05:29 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Jul 26 10:05:38 2007
Subject: [Slony1-general] lag_events v. lag_time
In-Reply-To: <4EADA98A-B4CB-49EC-AA26-745E64A6C17B@richyen.com> (Richard Yen's
	message of "Wed, 25 Jul 2007 16:44:44 -0700")
References: <C9B78BE4-887D-42AD-9A9E-3E7A1870C997@richyen.com>
	<601wezyml9.fsf@dba2.int.libertyrms.com>
	<4EADA98A-B4CB-49EC-AA26-745E64A6C17B@richyen.com>
Message-ID: <60vec75c4m.fsf@dba2.int.libertyrms.com>

Richard Yen <dba@richyen.com> writes:

>> If the origin is not being updated, then it may take 30 seconds to
>> generate a SYNC (depending on configuration!).  In that case,
>> subscribers may sit at the same SYNC number as the origin for that 30
>> seconds.
> Are you referring to the sync_interval_timeout parameter in my conf
> file?  I'm set to sync_interval_timeout=3000
>
> I'm also set to sync_interval=500.  Is this supposed to create a SYNC
> event every .5 seconds?  Or perhaps the subscriber is supposed to
> look for a SYNC event every .5 seconds?
>
> Perhaps my understanding of SYNC is misshapen, and it's being exposed
> here...

You've got the right parameters...

- sync_interval, defaulting to 2000ms (per 1.2), indicates how often the work
  loop waits to consider doing another SYNC.

  In effect, SYNCs will always be *at least* sync_interval
  milliseconds apart.

- sync_interval_timeout, defaulting to 10000 (per 1.2) indicates the
  longest that the work loop will wait to generate a SYNC, if there has
  been no activity in that interval.

  Thus, SYNCs will be *at most* sync_interval_timeout milliseconds
  apart.

With your figures of 500 and 3000, there will be at most a SYNC every
0.5s, and at least, one every 3.0s (with a bit of fuzziness if things
are pretty busy; the times might be a *little* more).

>> After 30s, one of the timeouts is reached, a SYNC (that involves no
>> updates!) is generated, and so it takes very little time for the
>> subscribers to process that SYNC, and then sit, lagging by nothing,
>> for another 30s.
>>
>> If the application is quiescent, then that's a to-be-expected
>> behaviour.
> Thanks for the response!

If your sync_interval_timeout is 3000, then there shouldn't be 30s
periods; there should only be 3s periods.  So I'm not sure where you'd
get 30 seconds of 'nothing changing.'

Actually, I think you could quite reasonably increase that timeout
interval; note that if there *are* updates, the sync_interval value of
500ms will pick them up and generate a SYNC within 0.5s.  There isn't
any particular risk in increasing sync_interval_timeout to a higher
value; you won't lose table data as a result of that.

The one risk that *would* be taken in increasing sync_interval_timeout
is that if you're doing a lot of sequence updates that *do not* any
involve updates to replicated tables, the sequence updates might not
get replicated for a while.  And I have to really squint at it to
imagine that this is terribly much of a risk.  The usual usage of
sequences to manage values in SERIAL-like columns leads to INSERTs
taking place lock-step with the sequence updates.
-- 
(reverse (concatenate 'string "ofni.sesabatadxunil" "@" "enworbbc"))
http://linuxdatabases.info/info/unix.html
"It is not enough to succeed, others must fail."  -- Gore Vidal
From andrew.george.hammond at gmail.com  Thu Jul 26 16:18:46 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Thu Jul 26 16:19:05 2007
Subject: [Slony1-general] trouble with slony 1.2. and postgresql 8.1
In-Reply-To: <web-73176597@mailbe01.swip.net>
References: <web-73176597@mailbe01.swip.net>
Message-ID: <5a0a9d6f0707261618l48bce25n4e06e6012e69671d@mail.gmail.com>

You almost certainly want to upgrade slony to 1.2.10 before you try
and do anything beyond playing around with it.

Andrew


On 7/26/07, adamunno@tele2.it <adamunno@tele2.it> wrote:
> Hi all!
> I'm testing slony 1.2.1 on postgresql 8.1 and ubuntu 8 on a virtual machine
> of vmware following the example on
> http://www.linuxjournal.com/article/7834. I created two db
> on the same machine and I run the script cluster_setup.sh. Then when I run
> slon daemon with
> slon sql_cluster "dbname=contactdb user=postgres" &
> i obtain:
>
> 2007-07-25 22:26:01 CEST DEBUG2 slon_retry() from pid=19480
> 2007-07-25 22:26:01 CEST DEBUG1 slon: retry requested
> 2007-07-25 22:26:01 CEST DEBUG2 slon: notify worker process to shutdown
> 2007-07-25 22:26:01 CEST DEBUG2 slon: worker process created - pid = 19480
> 2007-07-25 22:26:01 CEST DEBUG2 slon: child terminated status: 0; pid:
> 19480, current worker pid: 19480
> 2007-07-25 22:26:01 CEST DEBUG1 slon: restart of worker
> 2007-07-25 22:26:01 CEST CONFIG main: slon version 1.2.1 starting up
> 2007-07-25 22:26:01 CEST DEBUG2 slon: watchdog process started
> 2007-07-25 22:26:01 CEST DEBUG2 slon: watchdog ready - pid = 18968
> 2007-07-25 22:26:01 CEST FATAL  main: Cannot connect to local database -
> could not connect to server: No such file or directory
>         Is the server running locally and accepting
>         connections on Unix domain socket
> "/var/run/postgresql/.s.PGSQL.5432"?
>
> 2007-07-25 22:26:01 CEST DEBUG2 slon_retry() from pid=19481
> 2007-07-25 22:26:01 CEST DEBUG1 slon: retry requested
> 2007-07-25 22:26:01 CEST DEBUG2 slon: notify worker process to shutdown
> 2007-07-25 22:26:01 CEST DEBUG2 slon: worker process created - pid = 19481
> 2007-07-25 22:26:01 CEST DEBUG2 slon: child terminated status: 0; pid:
> 19481, current worker pid: 19481
> 2007-07-25 22:26:01 CEST DEBUG1 slon: restart of worker
> 2007-07-25 22:26:01 CEST CONFIG main: slon version 1.2.1 starting up
> 2007-07-25 22:26:01 CEST FATAL  slon: sched_wakeuppipe create failed -(24)
> Too many open files
> 2007-07-25 22:26:01 CEST DEBUG2 slon: exit(-1)
>
> In my pg_hba.cong there is
> local all all trust
> host all all 127.0.0.1/32 trust
> and postgres run correctly on loacalhost on port 5432.
>
> Any ideas?
> Thanx in advance.
> ADaM
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
From habibseifzadeh at yahoo.com  Sat Jul 28 20:56:32 2007
From: habibseifzadeh at yahoo.com (Habib Seifzadeh)
Date: Sat Jul 28 20:58:35 2007
Subject: [Slony1-general] About features of Slony-I
Message-ID: <875098.11927.qm@web35601.mail.mud.yahoo.com>

Hi there,
I should setup a system in which some machines with local application and database are connected together via network. All of the machines have the same database and failure of network is very common. 
I would like to setup machines so that each application can work with local database when network fails. when the network goes on, the local database synchronize itself with other databases.
Could Slony-I or other replication systems help me? If not, how can I achieve my goal?
Cheers,
Habib




       
____________________________________________________________________________________
Pinpoint customers who are looking for what you sell. 
http://searchmarketing.yahoo.com/
From ajs at crankycanuck.ca  Sun Jul 29 09:41:12 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Sun Jul 29 09:41:48 2007
Subject: [Slony1-general] About features of Slony-I
In-Reply-To: <875098.11927.qm@web35601.mail.mud.yahoo.com>
References: <875098.11927.qm@web35601.mail.mud.yahoo.com>
Message-ID: <20070729164112.GB3560@phlogiston.dyndns.org>

On Sat, Jul 28, 2007 at 08:56:32PM -0700, Habib Seifzadeh wrote:

> I would like to setup machines so that each application can work
> with local database when network fails. when the network goes on,
> the local database synchronize itself with other databases.

> Could Slony-I or other replication systems help me? If not, how can
> I achieve my goal?

I know of no current system for PostgreSQL that can do this.  There
might be a way to extend Slony in a very hacky way to allow it.  What
you need is a detached-system, async., multimaster replication
system, presumably with some sort of conflict resolution.

If you only need read access during network failure, then you could
do this with slony by putting a replica on every application node.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
The fact that technology doesn't work is no bar to success in the marketplace.
		--Philip Greenspun
From JanWieck at Yahoo.com  Sun Jul 29 10:32:42 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Sun Jul 29 10:33:06 2007
Subject: [Slony1-general] Re: log shipping gone wrong
In-Reply-To: <2ca799770707232129p27822251v9947c748d2395630@mail.gmail.com>
References: <2ca799770707200605j1bb37c13o98c86a21834db2ab@mail.gmail.com>	<2ca799770707222327y5e5939bg6ccee843d0ef0097@mail.gmail.com>	<46A4AE4F.4080304@Yahoo.com>	<2ca799770707230728h3e3afa3auefdc1ae8dd429156@mail.gmail.com>	<46A4C3E7.1020107@Yahoo.com>
	<2ca799770707232129p27822251v9947c748d2395630@mail.gmail.com>
Message-ID: <46ACCF3A.9050005@Yahoo.com>

On 7/24/2007 12:29 AM, Mikko Partio wrote:
> 
>     What I meant is that it'd be good to get enough detail information about
>     the specific actions that lead to this problem in order to create a
>     standalone test that can reproduce it.
> 
> 
> Ok here comes:

This one played quite a bit hide and seek with me. But I think I found 
and fixed the bug.

The problem occurred when some action (like STORE_SET) caused the 
subscriber slon to perform an internal restart. During that, the current 
setsync tracking in the pset structure is reinitialized. However, since 
the sl_setsync table is not updated on events other than SYNC, this 
would lead to the wrong (too low) old sync expected if the last event(s) 
processed from that node where no SYNC events.

Thanks for the detailed example.


Jan

> 
> I have three databases, test3 which is the primary, test3_lo which is 
> the secondary "loopback" database (only used to make the logshipping 
> logs) and test3_log, the logshipping database. All reside in the same pg 
> cluster. Here's exactly what I did, there's a lot of noise but I thought 
> it's better to be thorough.
> 
> 1) Create databases and some tables
> 
> $ createdb -D data -E utf8 test3
> $ psql test3
>    
> test3=# create table a (id int primary key);
> test3=# create table b (id int primary key);
> test3=# create table c (id int primary key);
> 
> $ createdb -T test3 test3_lo
> $ createdb -T test3 test3_log
> 
> 2) Create the replication cluster
> 
> $ cat test3_preamble.slonik
> cluster name = test3;
> 
> node 1 admin conninfo = 'host=localhost dbname=test3 user=postgres 
> password=xxx';
> node 2 admin conninfo = 'host=localhost dbname=test3_lo user=postgres 
> password=xxx';
> 
> $ cat test3_init.slonik
> #!/bin/sh
> 
> slonik <<_EOF_
> 
>         include <test3_preamble.slonik>;
> 
>         init cluster (id=1, comment='original database');
> 
>         store node (id=2, comment = 'dummy loopback database');
>         store path (server=1, client=2, conninfo='dbname=test3 
> host=localhost user=postgres password=xxx');
>         store path (server=2, client=1, conninfo='dbname=test3_lo 
> host=localhost user=postgres password=xxx');
>         store listen (origin=1, provider=1, receiver=2);
>         store listen (origin=2, provider=2, receiver=1);
> 
> _EOF_
> 
> $ ./test3_init.slonik
>    
> $ slon test3 "dbname=test3 host=localhost user=postgres password=xxx"
> $ slon -a /tmp/logshipping/test3 test3 "dbname=test3_lo host=localhost 
> user=postgres password=xxx"
> 
> $ cat test3_create_set.slonik
> #!/bin/sh
> 
> if [ $# -ne 3 ]; then
>         echo "Usage: $0 SETID TABLEID TABLENAME"
>         echo "Example: $0 3 17 public.x"
>         exit 1
> fi
> 
> slonik <<_EOF_
> 
>         include <test3_preamble.slonik>;
> 
>         create set (id=$1, origin=1, comment='Set #$1');
>         set add table (set id = $1, origin = 1, id = $2, fully qualified 
> name = '$3');
> 
> _EOF_
> 
> $ ./test3_create_set.slonik 1 1 public.a
> $ cat test3_subscribe.slonik
> #!/bin/sh
> 
> if [ -z $1 ]; then
>         echo "Usage: $0 SETID"
>         exit 1;
> fi
> 
> slonik <<_EOF_
> 
>         include <test3_preamble.slonik>;
> 
>         subscribe set (id=$1, provider=1, receiver=2, forward=no);
> 
> _EOF_
> 
> $ ./test3_subscribe.slonik 1
> $ psql test3_lo
> test3_lo=# \d a
>        Table " public.a"
>  Column |  Type   | Modifiers
> --------+---------+-----------
>  id     | integer | not null
> Indexes:
>     "a_pkey" PRIMARY KEY, btree (id)
> Triggers:
>     _test3_denyaccess_1 BEFORE INSERT OR DELETE OR UPDATE ON a FOR EACH 
> ROW EXECUTE PROCEDURE _test3.denyaccess('_test3')
> 
> $ psql test3
> test3=# insert into a select * from generate_series(1,100);
> 
> 3) Dump the state of the cluster to logshipping database
> 
> $ ./slony1_dump.sh test3_lo test3 | psql test3_log
> $ psql test3_log
> test3_log=# select count(*) from a;
>  count
> -------
>    100
> (1 row)
> 
> 4) Insert some data to origin db and see if it gets to logshipping db  
>   
> test3=# insert into a select * from generate_series(101,200);
> 
> $ psql -d test3_log -f 
> /tmp/logshipping/test3/slony1_log_1_00000000000000000060.sql    
> $ psql test3_log
> test3_log=# select count(*) from a;
>  count
> -------
>    200
> (1 row)
> 
> 5) Create another replication set
> 
> $ ./test3_create_set.slonik 10 2 public.b
> $ ./test3_subscribe.slonik 10
>    
> 6) Start applying logfiles to logshipping db
> 
> $ psql -d test3_log -f 
> /tmp/logshipping/test3/slony1_log_1_00000000000000000061.sql
> ...
> $ psql -d test3_log -f 
> /tmp/logshipping/test3/slony1_log_1_00000000000000000068.sql
> $ psql -d test3_log -f 
> /tmp/logshipping/test3/slony1_log_1_00000000000000000069.sql
> START TRANSACTION
> psql:/tmp/logshipping/test3/slony1_log_1_00000000000000000069.sql:6: 
> ERROR:  Slony-I: set 1 is on sync 68, this archive log expects 67
> 
> $ cat /tmp/logshipping/test3/slony1_log_1_00000000000000000068.sql
> -- Slony-I log shipping archive
> -- Node 1, Event 68
> start transaction;
> -- STORE_SET
> 
> select "_test3".setsyncTracking_offline(1, '67', '68', '2007-07-24 
> 07:14: 53.592719');
> -- end of log archiving header
> ------------------------------------------------------------------
> -- start of Slony-I data
> ------------------------------------------------------------------
> 
> ------------------------------------------------------------------
> -- End Of Archive Log
> ------------------------------------------------------------------
> commit;
> vacuum analyze "_test3".sl_setsync_offline;
> 
> 
> $ cat /tmp/logshipping/test3/slony1_log_1_00000000000000000069.sql
> -- Slony-I log shipping archive
> -- Node 1, Event 69
> start transaction;
> -- SET_ADD_TABLE
> 
> select "_test3".setsyncTracking_offline(1, '67', '69', '2007-07-24 
> 07:14: 53.641896');
> -- end of log archiving header
> ------------------------------------------------------------------
> -- start of Slony-I data
> ------------------------------------------------------------------
> 
> ------------------------------------------------------------------
> -- End Of Archive Log
> ------------------------------------------------------------------
> commit;
> vacuum analyze "_test3".sl_setsync_offline;
> 
> 
> 
> I hope this was of any help, if you need any more info I'm happy to help.
> 
> Regards
> 
> MP
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From jerry at jerrysievers.com  Mon Jul 30 10:18:42 2007
From: jerry at jerrysievers.com (Jerry Sievers)
Date: Mon Jul 30 10:18:57 2007
Subject: [Slony1-general] Difficulties reshaping cluster
Message-ID: <m3ir81yfm5.fsf@mama.jerrysievers.com>

Greetings;

Slony 1.1.5 on Pg 8.1.4 Solaris/SPARC.

Recently, we discovered a problem on the disk array of a machine
serving as master (Node1) for a large 120G production DB that runs up
to 650 TPS during peak loads.  There were 3 nodes in the cluster in
all configured as such;

Node1
        Node2
        Node3

There are 2 replication sets and we shut down applications, and moved
the 2 sets to Node2 thus making it the master.  To avoid a problem we
experienced earlier by running Slony funcs by hand, a proper Slonik
preamble file was created and Slonik used for all command submission.
Moving the sets took place as follows and no errores were seen.

lock set 1
wait for event ALL
move set 1
wait...

Ditto for set 2

App servers brought up and apparently everything fine.   

Oops!  We might need to pull Node1 and I forgot to change subscription
to make Node3 have new master, Node2 as provider.  So, for the moment
we had this;

Node2
        Node1
                Node3

But everything is up and running here.  I figured all needed was to
change the subscription info for Node3 and the subscribe set Slonik
command was issued on the new master node.

App servers are back live again and the system is busy.

subscribe set1 provider = new master
wait for confirmed ALL wait on new master

Ditto for set2

The script runs and there are no errors.   

What we find however is that the only node which had sl_listen table
entries adjusted was the new master and the node that we were trying
to point to new provider began to fall behind even though querying the
sl_status view was reporting the node up to date with respect to all
other nodes in the system.

Eventually (hours later) , the rebuildlistenentries() function was run
on all nodes and resulted in a correct looking sl_listen table on all
nodes.

Demons were restarted a few times during this process and in fact,
additional non-sync events sent down from the master, including a
complete subscription of a new node are being replicated to the  Node3
machine. 

The tables suggest a configuration like this now;

Node2
        Node1
        Node3

Node3 however is no longer receiving any updates to the DB tables and
may have to be dropped.  The subscription of a new slave due to DB
size and OLTP workload is a 36 hour job and one that we can only do
over a weekend.  Thus, losing slaves is painful and we wish to
understand and avoid this scenario if at all possible.

Please explain what may have gone wrong here.

Many thanks 

-- 
-------------------------------------------------------------------------------
Jerry Sievers   732 365-2844 (work)     Production Database Administrator
                305 321-1144 (mobil	WWW E-Commerce Consultant
From cbbrowne at ca.afilias.info  Tue Jul 31 07:53:31 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul 31 07:53:36 2007
Subject: [Slony1-general] Re: 3 bugs in Slonik: wrong SQL parsing into
	DDL	statements
In-Reply-To: <469801D5.1060309@hypermediasystems.com> (Weslee Bilodeau's
	message of "Fri, 13 Jul 2007 15:51:01 -0700")
References: <d7df81620707120847k72bdf78sc70c47a25b03bac9@mail.gmail.com>
	<d7df81620707120914xdc17c88r5cb1630b4917d886@mail.gmail.com>
	<60bqehweie.fsf@dba2.int.libertyrms.com>
	<469696ED.9040902@hypermediasystems.com>
	<60d4ywx8d9.fsf@dba2.int.libertyrms.com>
	<469801D5.1060309@hypermediasystems.com>
Message-ID: <60abtczkt0.fsf@dba2.int.libertyrms.com>

Weslee Bilodeau <weslee.bilodeau@hypermediasystems.com> writes:
> Christopher Browne wrote:
>> Weslee Bilodeau <weslee.bilodeau@hypermediasystems.com> writes:
>>> Small question on this.
>>>
>>> I would imagine one could easily find a lot of edge cases that can break
>>> the current parser.
>>>
>>> I'm guessing they found them in psql, which is why psql stole the lexer
>>> from the backend itself.
>>>
>>> Is there any reason why slony didn't go the same route?
>>>
>>> http://developer.postgresql.org/cvsweb.cgi/pgsql/src/bin/psql/psqlscan.l?rev=1.21;content-type=text%2Fx-cvsweb-markup
>>>
>>> It was written for the exact same task -
>>>
>>> ------------------------------------------------------------
>>> This code is mainly needed to determine where the end of a SQL statement
>>> is: we are looking for semicolons that are not within quotes, comments,
>>> or parentheses.  The most reliable way to handle this is to borrow the
>>> backend's flex lexer rules, lock, stock, and barrel.  The rules below
>>> are (except for a few) the same as the backend's, but their actions are
>>> just ECHO whereas the backend's actions generally do other things.
>>> ------------------------------------------------------------
>>>
>>>
>>> Or has this already been discussed and dismissed?
>> 
>> If memory serves, I initially tried to introduce a second parser, but
>> ran into trouble with the notion of linking in 2 parsers.
>> 
>> I then reviewed syntax, and figured I had enough of it covered, which
>> was apparently not correct.
>> 
>> I'll take another crack at it...
>
> I figure the $_$, $$, etc edge-casees would be another fun one to roll
> into a custom parser.
>
> CREATE FUNCTION test( ) RETURNS text AS $_$ SELECT ';', E'\';\'',
> '"";""', E'"\';' ; SELECT 'OK'::text ; $_$ LANGUAGE SQL ;
>
> SELECT $_$ hello; this ; - is '\" a '''' test $_$ ;
>
> SELECT $$ $ test ; $ ;  $$ ;
>
> All really funky, but perfectly valid.
>
>
> And yeah, rolling two lexers into one, that does have its own challenges.
>
> Maybe expand the first lexer and add an SQL state, so it can parse SQL
> within the first one directly?

Actually, the existing logic *is* smart enough to cope with all of
these cases; here's the output from the parser test program when I
added these in:

-- Some more torturing per Weslee Bilodeau

-- I figure the $_$, $$, etc edge-casees would be another fun one to roll
-- into a custom parser.

CREATE FUNCTION test( ) RETURNS text AS $_$ SELECT ';', E'\';\'',
'"";""', E'"\';' ; SELECT 'OK'::text ; $_$ LANGUAGE SQL ;
statement 19
-------------------------------------------


SELECT $_$ hello; this ; - is '\" a '''' test $_$ ;
statement 20
-------------------------------------------


SELECT $$ $ test ; $ ;  $$ ;
statement 21
-------------------------------------------


-- All really funky, but perfectly valid.

-- Force a query to be at the end...

create table foo;
statement 22
-------------------------------------------

For now, I'm not inclined to change *everything* around to a new
parser, not when it's only about 3 pages of code...
-- 
output = ("cbbrowne" "@" "acm.org")
http://cbbrowne.com/info/advocacy.html
"End users  are just test loads  for verifying that  the system works,
kind of like resistors in an electrical circuit."
-- Kaz Kylheku in c.o.l.d.s
From jerry at jerrysievers.com  Tue Jul 31 11:45:21 2007
From: jerry at jerrysievers.com (Jerry Sievers)
Date: Tue Jul 31 11:45:32 2007
Subject: [Slony1-general] REshaping on loss of middle tier node?
Message-ID: <m3bqdsv2da.fsf@mama.jerrysievers.com>

Slony 1.1.5

Assuming a cascaded cluster of 3 nodes arranged as follows;

Master
        Slave1
                Slave2

If the middle tier Slave1 is totally lost, is there a way to get
Slave2 pointing to Master without data loss, or at all?

I am unable to do it with Slonik but had thought this was workable. 

Thanks 

-- 
-------------------------------------------------------------------------------
Jerry Sievers   732 365-2844 (work)     Production Database Administrator
                305 321-1144 (mobil	WWW E-Commerce Consultant
From wmoran at collaborativefusion.com  Tue Jul 31 11:49:31 2007
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Tue Jul 31 11:49:41 2007
Subject: [Slony1-general] REshaping on loss of middle tier node?
In-Reply-To: <m3bqdsv2da.fsf@mama.jerrysievers.com>
References: <m3bqdsv2da.fsf@mama.jerrysievers.com>
Message-ID: <20070731144931.37434eee.wmoran@collaborativefusion.com>

In response to Jerry Sievers <jerry@jerrysievers.com>:

> Slony 1.1.5
> 
> Assuming a cascaded cluster of 3 nodes arranged as follows;
> 
> Master
>         Slave1
>                 Slave2
> 
> If the middle tier Slave1 is totally lost, is there a way to get
> Slave2 pointing to Master without data loss, or at all?
> 
> I am unable to do it with Slonik but had thought this was workable. 

What, exactly, did you try to do?

You should be able to do it by simply issuing a new subscribe set ()
command with the correct parameters to subscribe it to Master.

http://slony.info/documentation/reshape.html

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From jerry at jerrysievers.com  Tue Jul 31 13:03:22 2007
From: jerry at jerrysievers.com (Jerry Sievers)
Date: Tue Jul 31 13:03:37 2007
Subject: [Slony1-general] REshaping on loss of middle tier node?
In-Reply-To: <20070731144931.37434eee.wmoran@collaborativefusion.com>
References: <m3bqdsv2da.fsf@mama.jerrysievers.com>
	<20070731144931.37434eee.wmoran@collaborativefusion.com>
Message-ID: <m33az4uyr9.fsf@mama.jerrysievers.com>

Bill Moran <wmoran@collaborativefusion.com> writes:

> In response to Jerry Sievers <jerry@jerrysievers.com>:
> 
> > Slony 1.1.5
> > 
> > Assuming a cascaded cluster of 3 nodes arranged as follows;
> > 
> > Master
> >         Slave1
> >                 Slave2
> > 
> > If the middle tier Slave1 is totally lost, is there a way to get
> > Slave2 pointing to Master without data loss, or at all?
> > 
> > I am unable to do it with Slonik but had thought this was workable. 
> 
> What, exactly, did you try to do?
> 
> You should be able to do it by simply issuing a new subscribe set ()
> command with the correct parameters to subscribe it to Master.

That was my assumption and I guess also believe that Slonik would feed
the new subscription info directly to the now cut=-off slave. .  This
does not seem to be the case.

original subscription looked like 

set 1,
provider 2,
receiver 3

New subscription;

set 1,
provider 1, 
receiver 3

But with the node that was in the middle feeding slave 2 down, I think
there's no way slave 2 ever hears of the new subscription and it keeps
trying to connect to the slave that used to be feeding it.

I've queried the sl_* tables on the  node that I want to reposition
and see that nothing is  changing.  

Any ideas? 

> http://slony.info/documentation/reshape.html
> 
> -- 
> Bill Moran
> Collaborative Fusion Inc.
> http://people.collaborativefusion.com/~wmoran/
> 
> wmoran@collaborativefusion.com
> Phone: 412-422-3463x4023
> 

-- 
-------------------------------------------------------------------------------
Jerry Sievers   732 365-2844 (work)     Production Database Administrator
                305 321-1144 (mobil	WWW E-Commerce Consultant
From wmoran at collaborativefusion.com  Tue Jul 31 13:28:35 2007
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Tue Jul 31 13:28:47 2007
Subject: [Slony1-general] REshaping on loss of middle tier node?
In-Reply-To: <m33az4uyr9.fsf@mama.jerrysievers.com>
References: <m3bqdsv2da.fsf@mama.jerrysievers.com>
	<20070731144931.37434eee.wmoran@collaborativefusion.com>
	<m33az4uyr9.fsf@mama.jerrysievers.com>
Message-ID: <20070731162835.afc914d6.wmoran@collaborativefusion.com>

In response to Jerry Sievers <jerry@jerrysievers.com>:

> Bill Moran <wmoran@collaborativefusion.com> writes:
> 
> > In response to Jerry Sievers <jerry@jerrysievers.com>:
> > 
> > > Slony 1.1.5
> > > 
> > > Assuming a cascaded cluster of 3 nodes arranged as follows;
> > > 
> > > Master
> > >         Slave1
> > >                 Slave2
> > > 
> > > If the middle tier Slave1 is totally lost, is there a way to get
> > > Slave2 pointing to Master without data loss, or at all?
> > > 
> > > I am unable to do it with Slonik but had thought this was workable. 
> > 
> > What, exactly, did you try to do?
> > 
> > You should be able to do it by simply issuing a new subscribe set ()
> > command with the correct parameters to subscribe it to Master.
> 
> That was my assumption and I guess also believe that Slonik would feed
> the new subscription info directly to the now cut=-off slave. .  This
> does not seem to be the case.
> 
> original subscription looked like 
> 
> set 1,
> provider 2,
> receiver 3
> 
> New subscription;
> 
> set 1,
> provider 1, 
> receiver 3
> 
> But with the node that was in the middle feeding slave 2 down, I think
> there's no way slave 2 ever hears of the new subscription and it keeps
> trying to connect to the slave that used to be feeding it.
> 
> I've queried the sl_* tables on the  node that I want to reposition
> and see that nothing is  changing.
> 
> Any ideas?

This may come across harsh.  I hope not, but I fear that there's no way
to describe this without sounding harsh.

If you tell us _what_you_did_ instead of _describing_what_you_think_
you_did_, it's much more likely that people on the list will be able
to help.

This is a common problem.  Lots of people say, "I entered the correct
commands", but refuse (for some unknown reason) to simply cut/paste
the commands into their email.

Unfortunately, this strategy comes up against the fact that 99% of the
problems that people have are operator error, and usually boil down to
a very specific misuse of a command.  Without knowing exactly what those
commands are, we'll never find the problem.

That being said, I'm not sure that this is _supposed_ to work.  It' may
be necessary to "DROP NODE" of node 2 in order for Slony to understand
that it's no longer participating.  Haven't tried this personally, but
Slony should automatically reshape paths when node 2 is officially removed.

Of course, someone who knows better may correct me on this.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From cbbrowne at ca.afilias.info  Tue Jul 31 15:03:04 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul 31 15:03:18 2007
Subject: [Slony1-general] REshaping on loss of middle tier node?
In-Reply-To: <20070731162835.afc914d6.wmoran@collaborativefusion.com>
References: <m3bqdsv2da.fsf@mama.jerrysievers.com>	<20070731144931.37434eee.wmoran@collaborativefusion.com>	<m33az4uyr9.fsf@mama.jerrysievers.com>
	<20070731162835.afc914d6.wmoran@collaborativefusion.com>
Message-ID: <46AFB198.9060205@ca.afilias.info>

Bill Moran wrote:
> In response to Jerry Sievers <jerry@jerrysievers.com>:
>
>   
>> Bill Moran <wmoran@collaborativefusion.com> writes:
>>
>>     
>>> In response to Jerry Sievers <jerry@jerrysievers.com>:
>>>
>>>       
>>>> Slony 1.1.5
>>>>
>>>> Assuming a cascaded cluster of 3 nodes arranged as follows;
>>>>
>>>> Master
>>>>         Slave1
>>>>                 Slave2
>>>>
>>>> If the middle tier Slave1 is totally lost, is there a way to get
>>>> Slave2 pointing to Master without data loss, or at all?
>>>>
>>>> I am unable to do it with Slonik but had thought this was workable. 
>>>>         
>>> What, exactly, did you try to do?
>>>
>>> You should be able to do it by simply issuing a new subscribe set ()
>>> command with the correct parameters to subscribe it to Master.
>>>       
>> That was my assumption and I guess also believe that Slonik would feed
>> the new subscription info directly to the now cut=-off slave. .  This
>> does not seem to be the case.
>>
>> original subscription looked like 
>>
>> set 1,
>> provider 2,
>> receiver 3
>>
>> New subscription;
>>
>> set 1,
>> provider 1, 
>> receiver 3
>>
>> But with the node that was in the middle feeding slave 2 down, I think
>> there's no way slave 2 ever hears of the new subscription and it keeps
>> trying to connect to the slave that used to be feeding it.
>>
>> I've queried the sl_* tables on the  node that I want to reposition
>> and see that nothing is  changing.
>>
>> Any ideas?
>>     
>
> This may come across harsh.  I hope not, but I fear that there's no way
> to describe this without sounding harsh.
>
> If you tell us _what_you_did_ instead of _describing_what_you_think_
> you_did_, it's much more likely that people on the list will be able
> to help.
>
> This is a common problem.  Lots of people say, "I entered the correct
> commands", but refuse (for some unknown reason) to simply cut/paste
> the commands into their email.
>
> Unfortunately, this strategy comes up against the fact that 99% of the
> problems that people have are operator error, and usually boil down to
> a very specific misuse of a command.  Without knowing exactly what those
> commands are, we'll never find the problem.
>
>   
Well put.
> That being said, I'm not sure that this is _supposed_ to work.  It' may
> be necessary to "DROP NODE" of node 2 in order for Slony to understand
> that it's no longer participating.  Haven't tried this personally, but
> Slony should automatically reshape paths when node 2 is officially removed.
>
> Of course, someone who knows better may correct me on this.
>   
Dropping out node #2 will actually NOT work, because node #2 is still 
regarded as data provider for node #3 in some places.

I would expect it to be useful to submit STORE PATH entries for the 
paths that are still functioning, as that would "bump" the remaining 
nodes, telling them all to regenerate listen path (sl_listen) entries.

I think the first thing I'd try to do is to drop whatever PATH 
references to node #2 that I could, and add all the valid ones that I can.

Here are all the possible ones:

  DROP PATH ( SERVER = 1, CLIENT = 2 );
  DROP PATH ( SERVER = 2, CLIENT = 1 );
  DROP PATH ( SERVER = 3, CLIENT = 2 );
  DROP PATH ( SERVER = 2, CLIENT = 3 );

By default, those will be injected at node #1; submitting them to node 
#3 would be a useful alternative:

  DROP PATH ( SERVER = 1, CLIENT = 2, EVENT NODE = 3 );
  DROP PATH ( SERVER = 2, CLIENT = 1, EVENT NODE = 3 );
  DROP PATH ( SERVER = 3, CLIENT = 2, EVENT NODE = 3 );
  DROP PATH ( SERVER = 2, CLIENT = 3, EVENT NODE = 3 );

I expect that some of those would fail due to appearing to cut off 
communications; all the same, the more of them that *do* go away, the 
likelier that useful communications will be re-established between nodes 
1 and 3.

Supposing I have a preamble (e.g. - CLUSTER NAME and ADMIN CONNINFO 
statements) in the file /tmp/preamble.slonik, then the following shell 
script could do the trick:

#!/bin/sh
PREAMBLE=`cat /tmp/preamble.slonik`
for eventnode in 1 3; do
    for server in 1 2 3; do
    for client in 1 2 3; do
        echo "${PREAMBLE}" > /tmp/slonik-script.$$
        echo "drop path (server = ${server}, client = ${client}, event 
node = ${eventnode});" | grep 2 >> /tmp/slonik-script.$$
        slonik /tmp/slonik-script.$$
     done
     done
done

That'll remove all the paths it can, initially, which will trigger 
rebuilding of listen paths.

Submit STORE PATH requests for all the non-related-to-node-2 paths and 
that may also do good.

Restarting the slons may also help.

This is all a bit messy, but the situation is, if anything, that...
From cbbrowne at ca.afilias.info  Tue Jul 31 15:51:07 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul 31 15:51:20 2007
Subject: [Slony1-general] Impending version 1.2.11
Message-ID: <60y7gwb31g.fsf@dba2.int.libertyrms.com>

I spent part of today making sure that the code in what will soon be
1.2.11 works on sundry PostgreSQL versions.  I tested on 8.2 and 7.4;
will need to run thru the others tomorrow.

It would be nice to see other testing, too...
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://www3.sympatico.ca/cbbrowne/unix.html
Signs  of a   Klingon  Programmer  #6: "Debugging?   Klingons  do  not
debug. Our software does not coddle the weak."
From jerry at jerrysievers.com  Tue Jul 31 15:54:39 2007
From: jerry at jerrysievers.com (Jerry Sievers)
Date: Tue Jul 31 15:54:57 2007
Subject: [Slony1-general] REshaping on loss of middle tier node?
In-Reply-To: <46AFB198.9060205@ca.afilias.info>
References: <m3bqdsv2da.fsf@mama.jerrysievers.com>
	<20070731144931.37434eee.wmoran@collaborativefusion.com>
	<m33az4uyr9.fsf@mama.jerrysievers.com>
	<20070731162835.afc914d6.wmoran@collaborativefusion.com>
	<46AFB198.9060205@ca.afilias.info>
Message-ID: <m3ps28tc9c.fsf@mama.jerrysievers.com>

Christopher Browne <cbbrowne@ca.afilias.info> writes:

> Bill Moran wrote:
> 
> > In response to Jerry Sievers <jerry@jerrysievers.com>:
> >
> >
> >> Bill Moran <wmoran@collaborativefusion.com> writes:
> >>
> >>
> >>> In response to Jerry Sievers <jerry@jerrysievers.com>:
> >>>
> >>>
> >>>> Slony 1.1.5
> >>>>
> >>>> Assuming a cascaded cluster of 3 nodes arranged as follows;
> >>>>
> >>>> Master
> >>>>         Slave1
> >>>>                 Slave2
> >>>>
> >>>> If the middle tier Slave1 is totally lost, is there a way to get
> >>>> Slave2 pointing to Master without data loss, or at all?
> >>>>
> >>>> I am unable to do it with Slonik but had thought this was
> >>>> workable.
> >>> What, exactly, did you try to do?
> >>>
> >>> You should be able to do it by simply issuing a new subscribe set ()
> >>> command with the correct parameters to subscribe it to Master.
> >>>
> >> That was my assumption and I guess also believe that Slonik would feed
> >> the new subscription info directly to the now cut=-off slave. .  This
> >> does not seem to be the case.
> >>
> >> original subscription looked like set 1,
> >> provider 2,
> >> receiver 3
> >>
> >> New subscription;
> >>
> >> set 1,
> >> provider 1, receiver 3
> >>
> >> But with the node that was in the middle feeding slave 2 down, I think
> >> there's no way slave 2 ever hears of the new subscription and it keeps
> >> trying to connect to the slave that used to be feeding it.
> >>
> >> I've queried the sl_* tables on the  node that I want to reposition
> >> and see that nothing is  changing.
> >>
> >> Any ideas?
> >>
> >
> > This may come across harsh.  I hope not, but I fear that there's no way
> > to describe this without sounding harsh.
> >
> > If you tell us _what_you_did_ instead of _describing_what_you_think_
> > you_did_, it's much more likely that people on the list will be able
> > to help.
> >
> > This is a common problem.  Lots of people say, "I entered the correct
> > commands", but refuse (for some unknown reason) to simply cut/paste
> > the commands into their email.
> >
> > Unfortunately, this strategy comes up against the fact that 99% of the
> > problems that people have are operator error, and usually boil down to
> > a very specific misuse of a command.  Without knowing exactly what those
> > commands are, we'll never find the problem.
> >
> >
> Well put.
> > That being said, I'm not sure that this is _supposed_ to work.  It' may
> > be necessary to "DROP NODE" of node 2 in order for Slony to understand
> > that it's no longer participating.  Haven't tried this personally, but
> > Slony should automatically reshape paths when node 2 is officially removed.
> >
> > Of course, someone who knows better may correct me on this.
> >
> Dropping out node #2 will actually NOT work, because node #2 is still
> regarded as data provider for node #3 in some places.
> 
> I would expect it to be useful to submit STORE PATH entries for the
> paths that are still functioning, as that would "bump" the remaining
> nodes, telling them all to regenerate listen path (sl_listen) entries.
> 
> I think the first thing I'd try to do is to drop whatever PATH
> references to node #2 that I could, and add all the valid ones that I
> can.
> 
> Here are all the possible ones:
> 
>   DROP PATH ( SERVER = 1, CLIENT = 2 );
>   DROP PATH ( SERVER = 2, CLIENT = 1 );
>   DROP PATH ( SERVER = 3, CLIENT = 2 );
>   DROP PATH ( SERVER = 2, CLIENT = 3 );
> 
> By default, those will be injected at node #1; submitting them to node
> #3 would be a useful alternative:
> 
>   DROP PATH ( SERVER = 1, CLIENT = 2, EVENT NODE = 3 );
>   DROP PATH ( SERVER = 2, CLIENT = 1, EVENT NODE = 3 );
>   DROP PATH ( SERVER = 3, CLIENT = 2, EVENT NODE = 3 );
>   DROP PATH ( SERVER = 2, CLIENT = 3, EVENT NODE = 3 );
> 
> I expect that some of those would fail due to appearing to cut off
> communications; all the same, the more of them that *do* go away, the
> likelier that useful communications will be re-established between
> nodes 1 and 3.
> 
> Supposing I have a preamble (e.g. - CLUSTER NAME and ADMIN CONNINFO
> statements) in the file /tmp/preamble.slonik, then the following shell
> script could do the trick:
> 
> #!/bin/sh
> PREAMBLE=`cat /tmp/preamble.slonik`
> for eventnode in 1 3; do
>     for server in 1 2 3; do
>     for client in 1 2 3; do
>         echo "${PREAMBLE}" > /tmp/slonik-script.$$
>         echo "drop path (server = ${server}, client = ${client}, event
> node = ${eventnode});" | grep 2 >> /tmp/slonik-script.$$
>         slonik /tmp/slonik-script.$$
>      done
>      done
> done
> 
> That'll remove all the paths it can, initially, which will trigger
> rebuilding of listen paths.
> 
> Submit STORE PATH requests for all the non-related-to-node-2 paths and
> that may also do good.
> 
> Restarting the slons may also help.
> 
> This is all a bit messy, but the situation is, if anything, that...

Chris, this is very helpful.  Thanks!> 

Basically, I misunderstood the documentation and believe that what I
was trying was supported and straightforward to do.  You have affirmed
that it is not. 

We're still using Slony 1.1.5 on some large and very busy production
systems due to running EnterpriseDB 8.1 which came bundled with Slony
1.1.5.  Their 8.2 version that we're testing has Slony 1.2.6.

I have one related question here;

I find that even with all nodes running, that on 1.1.5, a new
subscription to do only provider change, does *not* affect the
sl_listen table entries on slaves.  Only the master will be changed to
what should be correct.

In the example of the 3 node cluster being discussed here...

Moving  Slave2 to have Master as provider does indeed update
sl_subscribe tables on all nodes, however the only sl_listen table
then showing   a listen path as;

origin 1, provider 1, receiver 3

...Is Master system.  Both slave nodes continue to have the related
sl_listen record looking as follows.

origin 1, provider 2, receiver 3

Appologies if this is a known defficiency in 1.1.5.

-- 
-------------------------------------------------------------------------------
Jerry Sievers   732 365-2844 (work)     Production Database Administrator
                305 321-1144 (mobil	WWW E-Commerce Consultant
From mpartio at gmail.com  Tue Jul 31 21:05:38 2007
From: mpartio at gmail.com (Mikko Partio)
Date: Tue Jul 31 21:05:54 2007
Subject: [Slony1-general] Re: log shipping gone wrong
In-Reply-To: <46ACCF3A.9050005@Yahoo.com>
References: <2ca799770707200605j1bb37c13o98c86a21834db2ab@mail.gmail.com>
	<2ca799770707222327y5e5939bg6ccee843d0ef0097@mail.gmail.com>
	<46A4AE4F.4080304@Yahoo.com>
	<2ca799770707230728h3e3afa3auefdc1ae8dd429156@mail.gmail.com>
	<46A4C3E7.1020107@Yahoo.com>
	<2ca799770707232129p27822251v9947c748d2395630@mail.gmail.com>
	<46ACCF3A.9050005@Yahoo.com>
Message-ID: <2ca799770707312105h7eb778f2l78c6b6e2885652e1@mail.gmail.com>

On 7/29/07, Jan Wieck <JanWieck@yahoo.com> wrote:
>
> On 7/24/2007 12:29 AM, Mikko Partio wrote:
> >
> >     What I meant is that it'd be good to get enough detail information
> about
> >     the specific actions that lead to this problem in order to create a
> >     standalone test that can reproduce it.
> >
> >
> > Ok here comes:
>
> This one played quite a bit hide and seek with me. But I think I found
> and fixed the bug.
>
> The problem occurred when some action (like STORE_SET) caused the
> subscriber slon to perform an internal restart. During that, the current
> setsync tracking in the pset structure is reinitialized. However, since
> the sl_setsync table is not updated on events other than SYNC, this
> would lead to the wrong (too low) old sync expected if the last event(s)
> processed from that node where no SYNC events.
>
> Thanks for the detailed example.
>


Great that you got it fixed! Will the patch be included in the 1.2.11releas=
e?

Regards

MP
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070801/=
b1cbcf48/attachment-0001.htm
From laurent at over-blog.com  Mon Jul 30 01:48:22 2007
From: laurent at over-blog.com (Laurent Raufaste)
Date: Thu Aug  2 08:11:35 2007
Subject: [Slony1-general] Slony lag times
Message-ID: <46ADA5D6.3080101@over-blog.com>

Hi,

We are using Slony on a production environment and are very pleased by it.

Our cluster is made of 1 master, 4 slaves that needs to be replicated 
fast, and 2 slaves for which the replication speed isn't a problem.

Here's our issue: In the sl_status view I notice that the st_lag_time is 
always between 1 and many seconds: it goes up to 10 seconds regularly, 
and approximatively one time a day, there is always a slave reaching 1 
min, for example while vacuuming.

I tried playing with the folllowing options:
     -s <milliseconds>     SYNC check interval (default 10000)
     -t <milliseconds>     SYNC interval timeout (default 60000)
     -o <milliseconds>     desired subscriber SYNC processing time
     -g <num>              maximum SYNC group size (default 6)

Now on the master I have:
-s 1000 -g 50
On the fast slaves I have:
-s 1000
And on the slow slaves:
-s 10000 -g 10

I tried lowering the SYNC check interval to 500ms with no real effect, 
and the master is already loaded enough anyway ;)

Is there an effective way to shorten the replication lag time ?

A Slony noob.

-- 
Laurent Raufaste
JFG Networks
<http://www.over-blog.com/>
From leeroy.ma at gmail.com  Tue Jul 31 07:46:10 2007
From: leeroy.ma at gmail.com (LeeRoy)
Date: Thu Aug  2 08:11:36 2007
Subject: [Slony1-general] need more details about "NOTICE: failedNode"
Message-ID: <956d33630707310746wf43f306h545cc87cbaedd07b@mail.gmail.com>

Hello,

I have two nodes in my cluster. Everythings is ok.
I have written a program who send some INSERT in my DB and if some command
can't connect to the DB, my program use slonik and do a failover.
In this case the script is execute but don't finish. It's stop on the line
of the failover and in log I see
NOTICE: failedNode: set % has no other direct receivers - move now
I think that's normal to see the message, but why did it block the script?

The things that's very strange is if I execute twice the failover's script
it finish correctly and whitout message.

I had not found informations on the documentation or nowhere on the web,
then i try your mailinglist.
Thank you if you can explain me the phenomenon and thank you to excuse my
english.

LeeRoy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070731/=
ccdca6b0/attachment.htm
