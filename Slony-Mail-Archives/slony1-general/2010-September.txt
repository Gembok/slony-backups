From ssinger at ca.afilias.info  Wed Sep  1 06:44:58 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 01 Sep 2010 09:44:58 -0400
Subject: [Slony1-general] what is happening on the subscriber side?
In-Reply-To: <c617607b5c1e1fb51824c8cb01ab790f@richyen.com>
References: <c617607b5c1e1fb51824c8cb01ab790f@richyen.com>
Message-ID: <4C7E58DA.50306@ca.afilias.info>

dba at richyen.com wrote:
> Hi everyone,
> 
> Just wanted to look for an explanation regarding what happens on the
> subscriber's end of a replication set.  I currently have 4 nodes (1 thru
> 4), and node 4 also has the "-a <dir>" flag turned on for log shipping (but
> I think this is irrelevant)



> 
> Occasionally, I will see in test_slony_state_dbi.pl, that one of the
> subscribers has really old events or that the provider is lagging behind
> the provider, so I decided to harvest some data.  Wrote up a cronjob that
> will fetch the average slony lag on node 4 (I could've picked any of them,
> but just chose this one because load was lowest).
> 
>


> Using Slony 2.0.3, postgres 8.4.2 on CentOS 2.6.18 on all nodes.

1.  You should upgrade right away to 2.0.4.  Slony 2.0.3 has a rather 
serious memory corruption bug.  (the bug was not present in 2.0.2).

2. What you describe sounds like bug 127 
(http://bugs.slony.info/bugzilla/show_bug.cgi?id=127) which is present 
in any recent 1.2.x version and in all currently released 2.0.x versions 
but has been fixed in git and is planned to be included in 2.0.5


> 
> Much appreciated!
> --Richard
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From jeff at pgexperts.com  Thu Sep  2 16:30:05 2010
From: jeff at pgexperts.com (Jeff Frost)
Date: Thu, 2 Sep 2010 16:30:05 -0700
Subject: [Slony1-general] slony1-2.0.x ready for production?
Message-ID: <8728F7F2-9B2D-49A4-B162-49EBB4A6866D@pgexperts.com>

Would you consider slony1-2.0.4 as ready for production use?  i recall a thread from December indicating that 2.0.3 was not ready for production use.

---
Jeff Frost <jeff at pgexperts.com>
CTO, PostgreSQL Experts, Inc.
Phone: 1-888-PG-EXPRT x506
FAX: 415-762-5122
http://www.pgexperts.com/ 







From peter.geoghegan86 at gmail.com  Thu Sep  2 16:39:39 2010
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Fri, 3 Sep 2010 00:39:39 +0100
Subject: [Slony1-general] slony1-2.0.x ready for production?
In-Reply-To: <8728F7F2-9B2D-49A4-B162-49EBB4A6866D@pgexperts.com>
References: <8728F7F2-9B2D-49A4-B162-49EBB4A6866D@pgexperts.com>
Message-ID: <AANLkTinJ840+R2v9RnadCOOswLJA5Bi-grCRwLzKxmAf@mail.gmail.com>

On 3 September 2010 00:30, Jeff Frost <jeff at pgexperts.com> wrote:
> Would you consider slony1-2.0.4 as ready for production use? ?i recall a thread from December indicating that 2.0.3 was not ready for production use.
>

That was due to a bug that broke replication that wasn't caught during
the release candidate process. Slony 2.0.4 is unaffected (in fact,
Slony 2.0.4 is, as far as I know, exactly the same but for the fact
that it's unaffected).
-- 
Regards,
Peter Geoghegan

From ssinger at ca.afilias.info  Fri Sep  3 05:17:38 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 03 Sep 2010 08:17:38 -0400
Subject: [Slony1-general] slony1-2.0.x ready for production?
In-Reply-To: <8728F7F2-9B2D-49A4-B162-49EBB4A6866D@pgexperts.com>
References: <8728F7F2-9B2D-49A4-B162-49EBB4A6866D@pgexperts.com>
Message-ID: <4C80E762.7000306@ca.afilias.info>

Jeff Frost wrote:
> Would you consider slony1-2.0.4 as ready for production use?  i recall a thread from December indicating that 2.0.3 was not ready for production use.
> 

We hope to be able to tag a release candidate for 2.0.5 sometime next 
week.  We plan on giving  2.0.5 (once it is released) a thumbs up for 
production use internally.

During the past few months we have done a fair amount testing of slony 
and review of old bugs.  The release notes in progress for 2.0.5 
(http://git.postgresql.org/gitweb?p=slony1-engine.git;a=blob;f=RELEASE;h=ee95676bb946a9310e189d99ba3828763e1542fd;hb=d747d9c97beeace62a1798e9b81807ca5727de3c)
should give you a sense of what we have fixed since 2.0.4 and that 
should help you make a decision on if you can live with 2.0.4 for your 
purposes.  A lot of the issues listed aren't unique to 2.0.x but are 
also present in 1.2.x if you where to look for them.

I'm going to encourage you (and others) to find some time to do your own 
testing of the release candidate.

Hope that helps.



> ---
> Jeff Frost <jeff at pgexperts.com>
> CTO, PostgreSQL Experts, Inc.
> Phone: 1-888-PG-EXPRT x506
> FAX: 415-762-5122
> http://www.pgexperts.com/ 
> 
> 
> 
> 
> 
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From ulas.albayrak at gmail.com  Wed Sep  8 04:57:03 2010
From: ulas.albayrak at gmail.com (Ulas Albayrak)
Date: Wed, 8 Sep 2010 13:57:03 +0200
Subject: [Slony1-general] setting up replicating system between OS's
Message-ID: <AANLkTi=yoggMOcXh80XzvVS7UQC3icc4Vv3o9Q1E6XL7@mail.gmail.com>

Hi,

so here's the situation: I've got a Slony cluster running between a
unix master and a linux slave. These are running Slony version 1.2.21.
Now I want to add a second slave on a Windows machine as well, but
from looking on the official Slony web page I can see there're no Win
binary installers for version 1.2.21. What should I do? Do I have to
change to another Slony version on the machines already in the Slony
cluster?

-- 
Ulas Albayrak
ulas.albayrak at gmail.com

From rod at iol.ie  Wed Sep  8 05:53:46 2010
From: rod at iol.ie (Raymond O'Donnell)
Date: Wed, 08 Sep 2010 13:53:46 +0100
Subject: [Slony1-general] setting up replicating system between OS's
In-Reply-To: <AANLkTi=yoggMOcXh80XzvVS7UQC3icc4Vv3o9Q1E6XL7@mail.gmail.com>
References: <AANLkTi=yoggMOcXh80XzvVS7UQC3icc4Vv3o9Q1E6XL7@mail.gmail.com>
Message-ID: <4C87875A.7090108@iol.ie>

On 08/09/2010 12:57, Ulas Albayrak wrote:

> so here's the situation: I've got a Slony cluster running between a
> unix master and a linux slave. These are running Slony version 1.2.21.
> Now I want to add a second slave on a Windows machine as well, but
> from looking on the official Slony web page I can see there're no Win
> binary installers for version 1.2.21. What should I do? Do I have to
> change to another Slony version on the machines already in the Slony
> cluster?

The easiest way to get Slony for Windows is probably via the 
StackBuilder application which comes with the One-Click installer for 
Windows, built by EnterpriseDB.

Ray.

-- 
Raymond O'Donnell :: Galway :: Ireland
rod at iol.ie

From ulas.albayrak at gmail.com  Wed Sep  8 06:20:07 2010
From: ulas.albayrak at gmail.com (Ulas Albayrak)
Date: Wed, 8 Sep 2010 15:20:07 +0200
Subject: [Slony1-general] Fwd:  setting up replicating system between OS's
In-Reply-To: <AANLkTikbVZ+u0eTMpJU4Z4m0U5u-9cgbsZwXG-2QGCkQ@mail.gmail.com>
References: <AANLkTi=yoggMOcXh80XzvVS7UQC3icc4Vv3o9Q1E6XL7@mail.gmail.com>
	<4C87875A.7090108@iol.ie>
	<AANLkTikbVZ+u0eTMpJU4Z4m0U5u-9cgbsZwXG-2QGCkQ@mail.gmail.com>
Message-ID: <AANLkTi=e9rLue5mtfDp4eguN7Miw4dvw-e7AmUQmDLt8@mail.gmail.com>

That was my initial idea but Stackbuilder on my Win machine only
offers Slony-I version 2.0.4-1. As far as I know all machines in a
cluster needs to be running the same verision of Slony?

On Wed, Sep 8, 2010 at 2:53 PM, Raymond O'Donnell <rod at iol.ie> wrote:
> On 08/09/2010 12:57, Ulas Albayrak wrote:
>
>> so here's the situation: I've got a Slony cluster running between a
>> unix master and a linux slave. These are running Slony version 1.2.21.
>> Now I want to add a second slave on a Windows machine as well, but
>> from looking on the official Slony web page I can see there're no Win
>> binary installers for version 1.2.21. What should I do? Do I have to
>> change to another Slony version on the machines already in the Slony
>> cluster?
>
> The easiest way to get Slony for Windows is probably via the StackBuilder
> application which comes with the One-Click installer for Windows, built by
> EnterpriseDB.
>
> Ray.
>
> --
> Raymond O'Donnell :: Galway :: Ireland
> rod at iol.ie
>



--
Ulas Albayrak
ulas.albayrak at gmail.com

From ulas.albayrak at gmail.com  Wed Sep  8 06:21:05 2010
From: ulas.albayrak at gmail.com (Ulas Albayrak)
Date: Wed, 8 Sep 2010 15:21:05 +0200
Subject: [Slony1-general] Fwd:  setting up replicating system between OS's
In-Reply-To: <AANLkTikbVZ+u0eTMpJU4Z4m0U5u-9cgbsZwXG-2QGCkQ@mail.gmail.com>
References: <AANLkTi=yoggMOcXh80XzvVS7UQC3icc4Vv3o9Q1E6XL7@mail.gmail.com>
	<4C87875A.7090108@iol.ie>
	<AANLkTikbVZ+u0eTMpJU4Z4m0U5u-9cgbsZwXG-2QGCkQ@mail.gmail.com>
Message-ID: <AANLkTimv-FiW1chQ1XApVAzjmSV4h88-nJ+obezCm=Um@mail.gmail.com>

That was my initial idea but Stackbuilder on my Win machine only
offers Slony-I version 2.0.4-1. As far as I know all machines in a
cluster needs to be running the same verision of Slony?

On Wed, Sep 8, 2010 at 2:53 PM, Raymond O'Donnell <rod at iol.ie> wrote:
> On 08/09/2010 12:57, Ulas Albayrak wrote:
>
>> so here's the situation: I've got a Slony cluster running between a
>> unix master and a linux slave. These are running Slony version 1.2.21.
>> Now I want to add a second slave on a Windows machine as well, but
>> from looking on the official Slony web page I can see there're no Win
>> binary installers for version 1.2.21. What should I do? Do I have to
>> change to another Slony version on the machines already in the Slony
>> cluster?
>
> The easiest way to get Slony for Windows is probably via the StackBuilder
> application which comes with the One-Click installer for Windows, built by
> EnterpriseDB.
>
> Ray.
>
> --
> Raymond O'Donnell :: Galway :: Ireland
> rod at iol.ie
>



--
Ulas Albayrak
ulas.albayrak at gmail.com

From greg at endpoint.com  Wed Sep  8 06:22:53 2010
From: greg at endpoint.com (Greg Sabino Mullane)
Date: Wed, 8 Sep 2010 09:22:53 -0400
Subject: [Slony1-general] setting up replicating system between OS's
In-Reply-To: <4C87875A.7090108@iol.ie>
References: <AANLkTi=yoggMOcXh80XzvVS7UQC3icc4Vv3o9Q1E6XL7@mail.gmail.com>
	<4C87875A.7090108@iol.ie>
Message-ID: <20100908132253.GO3816@core.home>

> The easiest way to get Slony for Windows is probably via the 
> StackBuilder application which comes with the One-Click installer for 
> Windows, built by EnterpriseDB.

That won't solve the version mismatch problem though.

-- 
Greg Sabino Mullane greg at endpoint.com
End Point Corporation
PGP Key: 0x14964AC8
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 163 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20100908/0a4b9a87/attachment.pgp 

From ulas.albayrak at gmail.com  Wed Sep  8 06:20:07 2010
From: ulas.albayrak at gmail.com (Ulas Albayrak)
Date: Wed, 8 Sep 2010 15:20:07 +0200
Subject: [Slony1-general] Fwd:  setting up replicating system between OS's
In-Reply-To: <AANLkTikbVZ+u0eTMpJU4Z4m0U5u-9cgbsZwXG-2QGCkQ@mail.gmail.com>
References: <AANLkTi=yoggMOcXh80XzvVS7UQC3icc4Vv3o9Q1E6XL7@mail.gmail.com>
	<4C87875A.7090108@iol.ie>
	<AANLkTikbVZ+u0eTMpJU4Z4m0U5u-9cgbsZwXG-2QGCkQ@mail.gmail.com>
Message-ID: <AANLkTi=e9rLue5mtfDp4eguN7Miw4dvw-e7AmUQmDLt8@mail.gmail.com>

That was my initial idea but Stackbuilder on my Win machine only
offers Slony-I version 2.0.4-1. As far as I know all machines in a
cluster needs to be running the same verision of Slony?

On Wed, Sep 8, 2010 at 2:53 PM, Raymond O'Donnell <rod at iol.ie> wrote:
> On 08/09/2010 12:57, Ulas Albayrak wrote:
>
>> so here's the situation: I've got a Slony cluster running between a
>> unix master and a linux slave. These are running Slony version 1.2.21.
>> Now I want to add a second slave on a Windows machine as well, but
>> from looking on the official Slony web page I can see there're no Win
>> binary installers for version 1.2.21. What should I do? Do I have to
>> change to another Slony version on the machines already in the Slony
>> cluster?
>
> The easiest way to get Slony for Windows is probably via the StackBuilder
> application which comes with the One-Click installer for Windows, built by
> EnterpriseDB.
>
> Ray.
>
> --
> Raymond O'Donnell :: Galway :: Ireland
> rod at iol.ie
>



--
Ulas Albayrak
ulas.albayrak at gmail.com

From ulas.albayrak at gmail.com  Wed Sep  8 06:30:06 2010
From: ulas.albayrak at gmail.com (Ulas Albayrak)
Date: Wed, 8 Sep 2010 15:30:06 +0200
Subject: [Slony1-general] setting up replicating system between OS's
In-Reply-To: <20100908132253.GO3816@core.home>
References: <AANLkTi=yoggMOcXh80XzvVS7UQC3icc4Vv3o9Q1E6XL7@mail.gmail.com>
	<4C87875A.7090108@iol.ie> <20100908132253.GO3816@core.home>
Message-ID: <AANLkTimJnBzXWL5Or6z98B5R1+v21do9CQfM3fgv0s+D@mail.gmail.com>

My thought exactly. So is there a way to get version 1.2.21 working on
my win machine or do I have to change to version 2.0.4 on the *nix
machines?

On Wed, Sep 8, 2010 at 3:22 PM, Greg Sabino Mullane <greg at endpoint.com> wrote:
>> The easiest way to get Slony for Windows is probably via the
>> StackBuilder application which comes with the One-Click installer for
>> Windows, built by EnterpriseDB.
>
> That won't solve the version mismatch problem though.
>
> --
> Greg Sabino Mullane greg at endpoint.com
> End Point Corporation
> PGP Key: 0x14964AC8
>



-- 
Ulas Albayrak
ulas.albayrak at gmail.com

From greg at endpoint.com  Wed Sep  8 06:53:22 2010
From: greg at endpoint.com (Greg Sabino Mullane)
Date: Wed, 8 Sep 2010 09:53:22 -0400
Subject: [Slony1-general] setting up replicating system between OS's
In-Reply-To: <AANLkTimJnBzXWL5Or6z98B5R1+v21do9CQfM3fgv0s+D@mail.gmail.com>
References: <AANLkTi=yoggMOcXh80XzvVS7UQC3icc4Vv3o9Q1E6XL7@mail.gmail.com>
	<4C87875A.7090108@iol.ie> <20100908132253.GO3816@core.home>
	<AANLkTimJnBzXWL5Or6z98B5R1+v21do9CQfM3fgv0s+D@mail.gmail.com>
Message-ID: <20100908135321.GS3816@core.home>

On Wed, Sep 08, 2010 at 03:30:06PM +0200, Ulas Albayrak wrote:
> My thought exactly. So is there a way to get version 1.2.21 working on
> my win machine or do I have to change to version 2.0.4 on the *nix
> machines?

You can certainly hand-compile 1.2 on Windows, but in my experience 
it will be much easier to switch the Unix box instead. Plus, you then 
get the more modern version.

-- 
Greg Sabino Mullane greg at endpoint.com
End Point Corporation
PGP Key: 0x14964AC8
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 163 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20100908/ec384a39/attachment.pgp 

From ulas.albayrak at gmail.com  Wed Sep  8 07:57:34 2010
From: ulas.albayrak at gmail.com (Ulas Albayrak)
Date: Wed, 8 Sep 2010 16:57:34 +0200
Subject: [Slony1-general] setting up replicating system between OS's
In-Reply-To: <20100908135321.GS3816@core.home>
References: <AANLkTi=yoggMOcXh80XzvVS7UQC3icc4Vv3o9Q1E6XL7@mail.gmail.com>
	<4C87875A.7090108@iol.ie> <20100908132253.GO3816@core.home>
	<AANLkTimJnBzXWL5Or6z98B5R1+v21do9CQfM3fgv0s+D@mail.gmail.com>
	<20100908135321.GS3816@core.home>
Message-ID: <AANLkTinUmVyrrFk0G614wHqaZ2o1ex+LOvnCF1DTQim+@mail.gmail.com>

Ok,

that leads me to another question: is it possible to run multiple
versions of Slony-I on the same machine?

On Wed, Sep 8, 2010 at 3:53 PM, Greg Sabino Mullane <greg at endpoint.com> wrote:
> On Wed, Sep 08, 2010 at 03:30:06PM +0200, Ulas Albayrak wrote:
>> My thought exactly. So is there a way to get version 1.2.21 working on
>> my win machine or do I have to change to version 2.0.4 on the *nix
>> machines?
>
> You can certainly hand-compile 1.2 on Windows, but in my experience
> it will be much easier to switch the Unix box instead. Plus, you then
> get the more modern version.
>
> --
> Greg Sabino Mullane greg at endpoint.com
> End Point Corporation
> PGP Key: 0x14964AC8
>



-- 
Ulas Albayrak
ulas.albayrak at gmail.com

From cbbrowne at ca.afilias.info  Wed Sep  8 07:59:13 2010
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed, 08 Sep 2010 10:59:13 -0400
Subject: [Slony1-general] Fwd: setting up replicating system between OS's
In-Reply-To: <AANLkTi=e9rLue5mtfDp4eguN7Miw4dvw-e7AmUQmDLt8@mail.gmail.com>
	(Ulas Albayrak's message of "Wed, 8 Sep 2010 15:20:07 +0200")
References: <AANLkTi=yoggMOcXh80XzvVS7UQC3icc4Vv3o9Q1E6XL7@mail.gmail.com>
	<4C87875A.7090108@iol.ie>
	<AANLkTikbVZ+u0eTMpJU4Z4m0U5u-9cgbsZwXG-2QGCkQ@mail.gmail.com>
	<AANLkTi=e9rLue5mtfDp4eguN7Miw4dvw-e7AmUQmDLt8@mail.gmail.com>
Message-ID: <87eid4i8n2.fsf@cbbrowne.afilias-int.info>

Ulas Albayrak <ulas.albayrak at gmail.com> writes:
> That was my initial idea but Stackbuilder on my Win machine only
> offers Slony-I version 2.0.4-1. As far as I know all machines in a
> cluster needs to be running the same verision of Slony?

Yep, that's right.  Slon processes will refuse to connect to hosts with
disagreeing versions.

If they've customized to a "2.0.4-1" version (which is not something
that the project released), that may not be treated as the same thing as
"2.0.4", and therefore might cause heartburn.
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"

From greg at endpoint.com  Wed Sep  8 08:01:03 2010
From: greg at endpoint.com (Greg Sabino Mullane)
Date: Wed, 8 Sep 2010 11:01:03 -0400
Subject: [Slony1-general] setting up replicating system between OS's
In-Reply-To: <AANLkTinUmVyrrFk0G614wHqaZ2o1ex+LOvnCF1DTQim+@mail.gmail.com>
References: <AANLkTi=yoggMOcXh80XzvVS7UQC3icc4Vv3o9Q1E6XL7@mail.gmail.com>
	<4C87875A.7090108@iol.ie> <20100908132253.GO3816@core.home>
	<AANLkTimJnBzXWL5Or6z98B5R1+v21do9CQfM3fgv0s+D@mail.gmail.com>
	<20100908135321.GS3816@core.home>
	<AANLkTinUmVyrrFk0G614wHqaZ2o1ex+LOvnCF1DTQim+@mail.gmail.com>
Message-ID: <20100908150102.GU3816@core.home>

> that leads me to another question: is it possible to run multiple
> versions of Slony-I on the same machine?

Possible, but I would not recommend it. For one thing, some of the unix 
rc scripts look for a "slon" process. For another, you'd have to be 
very, very careful about paths; even if it worked initially, you'll have 
a very custom, very fragile system that someone will break someday 
by some minor change to a path or calling the wrong file.

-- 
Greg Sabino Mullane greg at endpoint.com
End Point Corporation
PGP Key: 0x14964AC8
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 163 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20100908/dd65b581/attachment-0001.pgp 

From cbbrowne at ca.afilias.info  Wed Sep  8 08:01:45 2010
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed, 08 Sep 2010 11:01:45 -0400
Subject: [Slony1-general] setting up replicating system between OS's
In-Reply-To: <AANLkTinUmVyrrFk0G614wHqaZ2o1ex+LOvnCF1DTQim+@mail.gmail.com>
	(Ulas Albayrak's message of "Wed, 8 Sep 2010 16:57:34 +0200")
References: <AANLkTi=yoggMOcXh80XzvVS7UQC3icc4Vv3o9Q1E6XL7@mail.gmail.com>
	<4C87875A.7090108@iol.ie> <20100908132253.GO3816@core.home>
	<AANLkTimJnBzXWL5Or6z98B5R1+v21do9CQfM3fgv0s+D@mail.gmail.com>
	<20100908135321.GS3816@core.home>
	<AANLkTinUmVyrrFk0G614wHqaZ2o1ex+LOvnCF1DTQim+@mail.gmail.com>
Message-ID: <87aansi8iu.fsf@cbbrowne.afilias-int.info>

Ulas Albayrak <ulas.albayrak at gmail.com> writes:
> that leads me to another question: is it possible to run multiple
> versions of Slony-I on the same machine?

Possible, as long as they are installed into different instances of the
PostgreSQL binaries.  There are some stored functions implemented in C,
and they are somewhat version-specific.
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"

From tamanna.madan at globallogic.com  Wed Sep  8 13:11:57 2010
From: tamanna.madan at globallogic.com (tamanna madaan)
Date: Thu, 9 Sep 2010 01:41:57 +0530
Subject: [Slony1-general] replication through slony beween the nodes
	having different versions of postgres
In-Reply-To: <20100723145814.GT2847@core.home>
References: <68666423656E1444A011106C4E085F4D96F109@ex3-del1.synapse.com>
	<20100722132415.GG2847@core.home>
	<68666423656E1444A011106C4E085F4D96F10D@ex3-del1.synapse.com>
	<20100723145814.GT2847@core.home>
Message-ID: <68666423656E1444A011106C4E085F4DDDAFA2@ex3-del1.synapse.com>

Hi Greg

I have another query. I have postgres-8.1.2 on one of my cluster setups
.
I am using slony-1.1.5 for replication between servers in that cluster.
Now,  I want to upgrade those servers to latest release of
postgres-8.1.X i.e postgres-8.1.21 .
 
In case I upgrade the servers in a cluster one by one then there wont be

any issue with replication using slony-1.1.5 from postgres-8.1.2 to
postgres-8.1.21 because last digit of postgres version i.e revision is
same . Right ?? 

I had asked it earlier also but that was for slony-2.0.4 . Just wanted
to confirm if its same for slony-1.1.5 also.

Will slony 1.1.5 work with postgres-8.1.21 or I'll have to upgrade slony
as well.

Is there any compatibility matrix between slony and postgres . where can
I find that ??


Please Reply ...

Thanks...
Tamanna 

-----Original Message-----
From: Greg Sabino Mullane [mailto:greg at endpoint.com] 
Sent: Friday, July 23, 2010 8:28 PM
To: tamanna madaan
Cc: slony1-general at lists.slony.info
Subject: Re: [Slony1-general] replication through slony beween the nodes
having different versions of postgres

> postgres-8.4.0 and 8.4.2 are completely compatible. Does it also mean
that
> replication will work fine from postgres-8.4.0 to postgres-8.4.2 means
> from a lower to higher release. Does that holds true for
postgres-8.4.4 as
> well ??

Yes.

> Moreover, in the release notes of 8.4.2 (link below), its mentioned
that 
> "A dump/restore is not required for those running 8.4.X unless you are
having
> hash index" . Does that mean: if I am not using any hash index , then
I 
> can upgrade to postgres-8.4.2 from postgres-8.4.0 simply without any
need of 
> exporting my database schema and data in it before upgrading postgres
and 
> then importing it back after upgrade.

Yes, that is correct. Going from one revision to another (where the last
of the 
three version number changes) never requires a dump and a reload. Even
if you 
have hash indexes, a dump and reload is not required, just a REINDEX
after 
the upgrade has completed.

> won't it corrupt the data if I upgrade postgres db with my database
> schema/data in it ??

No: the only thing that is changing is the Postgres program itself. The 
underlying data is not changed, just the program that reads and writes 
that data is slightly changed, to fix bugs.

If I can further expound to this list some more best practices, you
should 
be subscribed to the pgsql-announce mailing list so you hear about new 
released (of Postgres, Slony, and other Postgres-related programs). You 
may also wish to run check_postgres.pl with the new_version_pg action, 
which will give you a Nagios alert when your version of Postgres becomes

out of date.

-- 
Greg Sabino Mullane greg at endpoint.com
End Point Corporation
PGP Key: 0x14964AC8

From JanWieck at Yahoo.com  Wed Sep  8 12:23:51 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed, 08 Sep 2010 15:23:51 -0400
Subject: [Slony1-general] replication through slony beween the
 nodes	having different versions of postgres
In-Reply-To: <68666423656E1444A011106C4E085F4DDDAFA2@ex3-del1.synapse.com>
References: <68666423656E1444A011106C4E085F4D96F109@ex3-del1.synapse.com>	<20100722132415.GG2847@core.home>	<68666423656E1444A011106C4E085F4D96F10D@ex3-del1.synapse.com>	<20100723145814.GT2847@core.home>
	<68666423656E1444A011106C4E085F4DDDAFA2@ex3-del1.synapse.com>
Message-ID: <4C87E2C7.5090402@Yahoo.com>

On 9/8/2010 4:11 PM, tamanna madaan wrote:
> Hi Greg
>
> I have another query. I have postgres-8.1.2 on one of my cluster setups
> .
> I am using slony-1.1.5 for replication between servers in that cluster.
> Now,  I want to upgrade those servers to latest release of
> postgres-8.1.X i.e postgres-8.1.21 .
>
> In case I upgrade the servers in a cluster one by one then there wont be
>
> any issue with replication using slony-1.1.5 from postgres-8.1.2 to
> postgres-8.1.21 because last digit of postgres version i.e revision is
> same . Right ??

This was one of the original design goals and had been the way since 
Slony-I 1.0.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From greg at endpoint.com  Wed Sep  8 14:57:28 2010
From: greg at endpoint.com (Greg Sabino Mullane)
Date: Wed, 8 Sep 2010 17:57:28 -0400
Subject: [Slony1-general] replication through slony beween the nodes
 having different versions of postgres
In-Reply-To: <68666423656E1444A011106C4E085F4DDDAFA2@ex3-del1.synapse.com>
References: <68666423656E1444A011106C4E085F4D96F109@ex3-del1.synapse.com>
	<20100722132415.GG2847@core.home>
	<68666423656E1444A011106C4E085F4D96F10D@ex3-del1.synapse.com>
	<20100723145814.GT2847@core.home>
	<68666423656E1444A011106C4E085F4DDDAFA2@ex3-del1.synapse.com>
Message-ID: <20100908215727.GC3816@core.home>

> In case I upgrade the servers in a cluster one by one then there wont be
> any issue with replication using slony-1.1.5 from postgres-8.1.2 to
> postgres-8.1.21 because last digit of postgres version i.e revision is
> same . Right ?? 

Yes, revision upgrades are very easy and require no dump and reload nor 
worrying about Slony.

> Will slony 1.1.5 work with postgres-8.1.21 or I'll have to upgrade slony
> as well.

It will work the same.

> Is there any compatibility matrix between slony and postgres . 
> where can I find that ??

I do not know, other than reading the version descriptions on slony.info

-- 
Greg Sabino Mullane greg at endpoint.com
End Point Corporation
PGP Key: 0x14964AC8
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 163 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20100908/0e830202/attachment.pgp 

From ulas.albayrak at gmail.com  Thu Sep  9 01:04:43 2010
From: ulas.albayrak at gmail.com (Ulas Albayrak)
Date: Thu, 9 Sep 2010 10:04:43 +0200
Subject: [Slony1-general] Fwd: setting up replicating system between OS's
In-Reply-To: <87eid4i8n2.fsf@cbbrowne.afilias-int.info>
References: <AANLkTi=yoggMOcXh80XzvVS7UQC3icc4Vv3o9Q1E6XL7@mail.gmail.com>
	<4C87875A.7090108@iol.ie>
	<AANLkTikbVZ+u0eTMpJU4Z4m0U5u-9cgbsZwXG-2QGCkQ@mail.gmail.com>
	<AANLkTi=e9rLue5mtfDp4eguN7Miw4dvw-e7AmUQmDLt8@mail.gmail.com>
	<87eid4i8n2.fsf@cbbrowne.afilias-int.info>
Message-ID: <AANLkTi=1E1mZJ8rjk4395EQYWxSKyFJX5Csc8_mk9LUe@mail.gmail.com>

The win version seems ok, after installing I issue the commands
"slon.exe -v" and "slonik.exe -v" which both return "version 2.0.4",
so I'm hoping this means the versions are compatible. I'll try
installing slony 2.0.4 on the other machines and hope for the best.


On Wed, Sep 8, 2010 at 4:59 PM, Christopher Browne
<cbbrowne at ca.afilias.info> wrote:
> Ulas Albayrak <ulas.albayrak at gmail.com> writes:
>> That was my initial idea but Stackbuilder on my Win machine only
>> offers Slony-I version 2.0.4-1. As far as I know all machines in a
>> cluster needs to be running the same verision of Slony?
>
> Yep, that's right. ?Slon processes will refuse to connect to hosts with
> disagreeing versions.
>
> If they've customized to a "2.0.4-1" version (which is not something
> that the project released), that may not be treated as the same thing as
> "2.0.4", and therefore might cause heartburn.
> --
> select 'cbbrowne' || '@' || 'ca.afilias.info';
> Christopher Browne
> "Bother," ?said Pooh, ?"Eeyore, ready ?two photon ?torpedoes ?and lock
> phasers on the Heffalump, Piglet, meet me in transporter room three"
>



-- 
Ulas Albayrak
ulas.albayrak at gmail.com

From ulas.albayrak at gmail.com  Thu Sep  9 06:26:11 2010
From: ulas.albayrak at gmail.com (Ulas Albayrak)
Date: Thu, 9 Sep 2010 15:26:11 +0200
Subject: [Slony1-general] Upgrading slony
Message-ID: <AANLkTikRMi=ThbedoEU+AMPyZFrekaOhmQaHPedRnqqd@mail.gmail.com>

Hi,

This might be a quite fundamental question, but the consequences of
being wrong could be dire so I'll ratghr be sure: I need to upgrade
Slony and the guide on the official Slony website says I should
terminate all slon processes then install the newer version of Slony.
My question is can I just build and install the newer version of Slony
parallel to the old and after running the "update functions"-script,
just delete the old slony files?

-- 
Ulas Albayrak
ulas.albayrak at gmail.com

From greg at endpoint.com  Thu Sep  9 06:38:38 2010
From: greg at endpoint.com (Greg Sabino Mullane)
Date: Thu, 9 Sep 2010 09:38:38 -0400
Subject: [Slony1-general] Upgrading slony
In-Reply-To: <AANLkTikRMi=ThbedoEU+AMPyZFrekaOhmQaHPedRnqqd@mail.gmail.com>
References: <AANLkTikRMi=ThbedoEU+AMPyZFrekaOhmQaHPedRnqqd@mail.gmail.com>
Message-ID: <20100909133838.GF3816@core.home>

> This might be a quite fundamental question, but the consequences of
> being wrong could be dire so I'll ratghr be sure: I need to upgrade
> Slony and the guide on the official Slony website says I should
> terminate all slon processes then install the newer version of Slony.
> My question is can I just build and install the newer version of Slony
> parallel to the old and after running the "update functions"-script,
> just delete the old slony files?

Broadly speaking, yes, but the process should be ironed out on your 
testing/QA box first, of course. If you don't have such a box that 
closely mirrors production, creating one should be your first step 
in the upgrade. :)

-- 
Greg Sabino Mullane greg at endpoint.com
End Point Corporation
PGP Key: 0x14964AC8
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 163 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20100909/a36f2c85/attachment.pgp 

From ssinger_pg at sympatico.ca  Thu Sep  9 06:34:19 2010
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Thu, 9 Sep 2010 09:34:19 -0400 (EDT)
Subject: [Slony1-general] Upgrading slony
In-Reply-To: <AANLkTikRMi=ThbedoEU+AMPyZFrekaOhmQaHPedRnqqd@mail.gmail.com>
References: <AANLkTikRMi=ThbedoEU+AMPyZFrekaOhmQaHPedRnqqd@mail.gmail.com>
Message-ID: <BLU0-SMTP8522E14F1B42091DDD1F17AC730@phx.gbl>

On Thu, 9 Sep 2010, Ulas Albayrak wrote:

> Hi,
>
> This might be a quite fundamental question, but the consequences of
> being wrong could be dire so I'll ratghr be sure: I need to upgrade
> Slony and the guide on the official Slony website says I should
> terminate all slon processes then install the newer version of Slony.
> My question is can I just build and install the newer version of Slony
> parallel to the old and after running the "update functions"-script,
> just delete the old slony files?

The version you are upgrading from/too controls the answer to this question. 
Perhaps the docs are less clear then they should be.

If you are upgrading from 1.2 to 2.0 you must uninstall slony with 
"uninstall node" on all nodes using the 1.2.x binaries then install the 2.0 
binaries and then re-build (init cluster, store node, subscribe set etc...) 
your cluster.

If your going from 2.0.3 to 2.0.4  or 1.2.x to 1.2.21 then you can use the 
update functions script.  Two versions of slony can't normally be installed 
at the same time under the same postgresql server directory since the slony 
install scripts normally put files in $postgresqldir/lib and share that 
would conflict.

In either case you should setup a test system with your old configuration 
and test your upgrade procedure.



>
> -- 
> Ulas Albayrak
> ulas.albayrak at gmail.com
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From ulas.albayrak at gmail.com  Thu Sep  9 07:02:37 2010
From: ulas.albayrak at gmail.com (Ulas Albayrak)
Date: Thu, 9 Sep 2010 16:02:37 +0200
Subject: [Slony1-general] Upgrading slony
In-Reply-To: <BLU0-SMTP8522E14F1B42091DDD1F17AC730@phx.gbl>
References: <AANLkTikRMi=ThbedoEU+AMPyZFrekaOhmQaHPedRnqqd@mail.gmail.com>
	<BLU0-SMTP8522E14F1B42091DDD1F17AC730@phx.gbl>
Message-ID: <AANLkTimHP_8qZuXXor+ZZNz-MacdQ7fLfM2zYv+wNTMx@mail.gmail.com>

I'm upgrading from 1.2.21 to 2.0.4, which in other words means I need
to uninstall the node and delete the old Slony directory before
installing the new. Setting up a testing environment will be quite a
big job though, what's the worst thing that can happen if I decide to
update the live system straight away without testing first? Could it
interfere with postgresql?

On Thu, Sep 9, 2010 at 3:34 PM, Steve Singer <ssinger_pg at sympatico.ca> wrote:
> On Thu, 9 Sep 2010, Ulas Albayrak wrote:
>
>> Hi,
>>
>> This might be a quite fundamental question, but the consequences of
>> being wrong could be dire so I'll ratghr be sure: I need to upgrade
>> Slony and the guide on the official Slony website says I should
>> terminate all slon processes then install the newer version of Slony.
>> My question is can I just build and install the newer version of Slony
>> parallel to the old and after running the "update functions"-script,
>> just delete the old slony files?
>
> The version you are upgrading from/too controls the answer to this question.
> Perhaps the docs are less clear then they should be.
>
> If you are upgrading from 1.2 to 2.0 you must uninstall slony with
> "uninstall node" on all nodes using the 1.2.x binaries then install the 2.0
> binaries and then re-build (init cluster, store node, subscribe set etc...)
> your cluster.
>
> If your going from 2.0.3 to 2.0.4 ?or 1.2.x to 1.2.21 then you can use the
> update functions script. ?Two versions of slony can't normally be installed
> at the same time under the same postgresql server directory since the slony
> install scripts normally put files in $postgresqldir/lib and share that
> would conflict.
>
> In either case you should setup a test system with your old configuration
> and test your upgrade procedure.
>
>
>
>>
>> --
>> Ulas Albayrak
>> ulas.albayrak at gmail.com
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>
>



-- 
Ulas Albayrak
ulas.albayrak at gmail.com

From ajs at crankycanuck.ca  Thu Sep  9 08:13:17 2010
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu, 9 Sep 2010 11:13:17 -0400
Subject: [Slony1-general] Upgrading slony
In-Reply-To: <AANLkTimHP_8qZuXXor+ZZNz-MacdQ7fLfM2zYv+wNTMx@mail.gmail.com>
References: <AANLkTikRMi=ThbedoEU+AMPyZFrekaOhmQaHPedRnqqd@mail.gmail.com>
	<BLU0-SMTP8522E14F1B42091DDD1F17AC730@phx.gbl>
	<AANLkTimHP_8qZuXXor+ZZNz-MacdQ7fLfM2zYv+wNTMx@mail.gmail.com>
Message-ID: <20100909151317.GA7373@shinkuro.com>

On Thu, Sep 09, 2010 at 04:02:37PM +0200, Ulas Albayrak wrote:

> installing the new. Setting up a testing environment will be quite a
> big job though, what's the worst thing that can happen if I decide to
> update the live system straight away without testing first? Could it
> interfere with postgresql?

The worst thing that can ever happen when you test in production is
that you will screw up, destroy all your data, be unable to recover,
and get fired.  If that risk is acceptable to you, then try it in
production first.  Why you need to have a replica of your database,
however, if you feel that relaxed about the value of your data is a
little beyond me.

A

-- 
Andrew Sullivan
ajs at crankycanuck.ca

From ajs at crankycanuck.ca  Thu Sep  9 10:26:06 2010
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu, 9 Sep 2010 13:26:06 -0400
Subject: [Slony1-general] replication through slony beween the
	nodes	having different versions of postgres
In-Reply-To: <20100908215727.GC3816@core.home>
References: <68666423656E1444A011106C4E085F4D96F109@ex3-del1.synapse.com>
	<20100722132415.GG2847@core.home>
	<68666423656E1444A011106C4E085F4D96F10D@ex3-del1.synapse.com>
	<20100723145814.GT2847@core.home>
	<68666423656E1444A011106C4E085F4DDDAFA2@ex3-del1.synapse.com>
	<20100908215727.GC3816@core.home>
Message-ID: <20100909172605.GI7373@shinkuro.com>

On Wed, Sep 08, 2010 at 05:57:28PM -0400, Greg Sabino Mullane wrote:
> > In case I upgrade the servers in a cluster one by one then there wont be
> > any issue with replication using slony-1.1.5 from postgres-8.1.2 to
> > postgres-8.1.21 because last digit of postgres version i.e revision is
> > same . Right ?? 
> 
> Yes, revision upgrades are very easy and require no dump and reload nor 
> worrying about Slony.

To be clear: if you don't have to do dump-and-reload, Slony doesn't
care.  If you do, you can use Slony to replicate to the new Postgres
version and then do a controlled switchover (in a maintenance
window).

A

-- 
Andrew Sullivan
ajs at crankycanuck.ca

From ssinger_pg at sympatico.ca  Thu Sep  9 20:01:34 2010
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Thu, 9 Sep 2010 23:01:34 -0400 (EDT)
Subject: [Slony1-general] Upgrading slony
In-Reply-To: <AANLkTimHP_8qZuXXor+ZZNz-MacdQ7fLfM2zYv+wNTMx@mail.gmail.com>
References: <AANLkTikRMi=ThbedoEU+AMPyZFrekaOhmQaHPedRnqqd@mail.gmail.com>
	<BLU0-SMTP8522E14F1B42091DDD1F17AC730@phx.gbl>
	<AANLkTimHP_8qZuXXor+ZZNz-MacdQ7fLfM2zYv+wNTMx@mail.gmail.com>
Message-ID: <BLU0-SMTP866A786B8AAA8E4CA29D23AC740@phx.gbl>

On Thu, 9 Sep 2010, Ulas Albayrak wrote:

> I'm upgrading from 1.2.21 to 2.0.4, which in other words means I need
> to uninstall the node and delete the old Slony directory before
> installing the new. Setting up a testing environment will be quite a
> big job though, what's the worst thing that can happen if I decide to
> update the live system straight away without testing first? Could it
> interfere with postgresql?

Testing changes and procedures in a test environment is  far safer than 
trying things for the first time in production. We test things to find the 
issues that you aren't able to forsee.

Slony is not the most forgiving piece of software, there are many places 
where user error can leave you with slony cluster that needs to be 
torn-down.  Slony *shouldn't* interfere with postgresql (other than the 
locks slony takes) but I can't promise you it won't.  Slony 1.1.x and 1.2.x 
munge the postgresql catalog to make things work, in theory the uninstall 
will leave your postgresql instances in the proper state but if practice and 
theory always worked out the same then software would never have bugs.

Another option might be to take backups of your databases before you start 
the process so you have something to fall-back to but I've also seen 
instances (not slony related) where the sysadmin/dba thought they were 
taking a backup but didn't test the restore procedure and later discovered 
that the backups they had been taking weren't usable.

I don't know how much time is involved in testing up a test environment, how 
much downtime would cost you or what would be involved in recovering from 
dataloss but testing would lower the risks of issues.

Steve



> On Thu, Sep 9, 2010 at 3:34 PM, Steve Singer <ssinger_pg at sympatico.ca> wrote:
>> On Thu, 9 Sep 2010, Ulas Albayrak wrote:
>>
>>> Hi,
>>>
>>> This might be a quite fundamental question, but the consequences of
>>> being wrong could be dire so I'll ratghr be sure: I need to upgrade
>>> Slony and the guide on the official Slony website says I should
>>> terminate all slon processes then install the newer version of Slony.
>>> My question is can I just build and install the newer version of Slony
>>> parallel to the old and after running the "update functions"-script,
>>> just delete the old slony files?
>>
>> The version you are upgrading from/too controls the answer to this question.
>> Perhaps the docs are less clear then they should be.
>>
>> If you are upgrading from 1.2 to 2.0 you must uninstall slony with
>> "uninstall node" on all nodes using the 1.2.x binaries then install the 2.0
>> binaries and then re-build (init cluster, store node, subscribe set etc...)
>> your cluster.
>>
>> If your going from 2.0.3 to 2.0.4 ?or 1.2.x to 1.2.21 then you can use the
>> update functions script. ?Two versions of slony can't normally be installed
>> at the same time under the same postgresql server directory since the slony
>> install scripts normally put files in $postgresqldir/lib and share that
>> would conflict.
>>
>> In either case you should setup a test system with your old configuration
>> and test your upgrade procedure.
>>
>>
>>
>>>
>>> --
>>> Ulas Albayrak
>>> ulas.albayrak at gmail.com
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general at lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>
>>
>>
>
>
>
> -- 
> Ulas Albayrak
> ulas.albayrak at gmail.com
>
>

From scott.marlowe at gmail.com  Thu Sep  9 20:39:59 2010
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Thu, 9 Sep 2010 21:39:59 -0600
Subject: [Slony1-general] Upgrading slony
In-Reply-To: <AANLkTimHP_8qZuXXor+ZZNz-MacdQ7fLfM2zYv+wNTMx@mail.gmail.com>
References: <AANLkTikRMi=ThbedoEU+AMPyZFrekaOhmQaHPedRnqqd@mail.gmail.com>
	<BLU0-SMTP8522E14F1B42091DDD1F17AC730@phx.gbl>
	<AANLkTimHP_8qZuXXor+ZZNz-MacdQ7fLfM2zYv+wNTMx@mail.gmail.com>
Message-ID: <AANLkTikJhq3bYsR9Ut=go2hRamtT50gHqCSjW09aTy_3@mail.gmail.com>

On Thu, Sep 9, 2010 at 8:02 AM, Ulas Albayrak <ulas.albayrak at gmail.com> wrote:
> I'm upgrading from 1.2.21 to 2.0.4, which in other words means I need
> to uninstall the node and delete the old Slony directory before
> installing the new. Setting up a testing environment will be quite a
> big job though, what's the worst thing that can happen if I decide to
> update the live system straight away without testing first? Could it
> interfere with postgresql?

OK.  No matter how much testing you do, you can still have things go
to hell in production.  So, you need to first off be prepared.  Have a
backup machine ready to take over with somewhat out of date (minutes,
hours, whatever) data.  Have a hard backup, preferably both on and off
site. Then, after testing it you can try it in production.

I do initial slony testing on my laptop (either in vms or on the bare
OS).  I don't test on a database as big as production, because I've
only got a 500G drive in this thing.  After proving it basically works
we test it on a bigger machine with more memory and drive space on a
real sized database.  Then we run it on production.

So, what can go wrong?  What's most likely to go wrong?  The most
common failure is that slony simply fails to work and you have to
scrub slony and start over on replication.  Next most common is that
somehow slony breaks in such a way that you cannot use your database
while it's on and running.  Could be something like I had last year
with 2.0.2 or so where it ran fine for a week or two, then the whole
cluster just stopped processing any requests, slony or otherwise until
I removed slony.

The worst possibility is that you would lose your main database server
and all backups.  Try to avoid that.

-- 
To understand recursion, one must first understand recursion.

From brianf at consistentstate.com  Thu Sep 16 16:35:53 2010
From: brianf at consistentstate.com (Brian Fehrle)
Date: Thu, 16 Sep 2010 17:35:53 -0600
Subject: [Slony1-general] slony not replicating after re-initializing the
	slave cluster
Message-ID: <4C92A9D9.2030109@consistentstate.com>

Hi all,
    Due to realizing that our 1 master -> 1 slave slony cluster had 
different encodings on each box, we attempted to fix that. Our master 
had encoding of LATIN1 and our slave had the encoding of SQL_ASCII (they 
were initialized so long ago, we don't know who did it or why it was 
done that way).
    Slony worked with this setup, but we wanted to fix it, due to some 
other problems, by moving the slave from SQL_ASCII to LATIN1.

    So we brought down the slon daemons, brought down the slave database 
and rebooted the physical machine the slave is on (dozens of cron jobs 
we commented out and wanted to verify they were all dead).

    When we rebooted the machine, we brought the slave postgres cluster 
online and preformed a pg_dump on the entire database (including the 
_slony schema). Then we brought down the postgres cluster, ran initdb to 
create a new one with LATIN1 encoding, brought the new cluster online 
and ran a pg_restore on it with the dump file we created before.

    After that we restarted our cron jobs, which also started up the two 
slon daemons, we started monitoring the slave and noticed that no 
updates are being applied. We're running the slon daemons with -s 60000 
(force a sync every 60 seconds) and a -x flag to get some slony logs for 
log shipping. These slony logs that are generated with -x are empty 
(they have the slony header and footer, but no insert data).

    On the master, if I do a # select * from _slony.sl_status; I get 
back that there are anywhere between 0 - 2 events, and a lag time no 
greater than 3 minutes. Monitoring the slave slony log output also 
verifies that events are being receved and processed without error every 
minute.

    Again, on the master, # select count(*) from _slony.sl_log_1; 
returns with 12,000 + rows, and it continually grows. So from what I can 
tell, the master is getting events qued up, but not pushing them in the 
events to the slave, each event is completely void of data, and it looks 
like sl_log_1 just keeps building up.

    One theory is that even though we have an exact data dump of the old 
slave cluster restored to to the new slave cluster, since the encoding 
has changed perhaps the master doesn't recognize the slave as the same 
slave it had before. If thats the case, is there any way we can get it 
to recognize it without having to rebuild the slony cluster? (rebuilding 
the cluster would mean a few days of work if not week/s).

    Other than that, I'm unsure what to make of this. I've restarted the 
daemons, and neither the master nor the slave daemon report any errors 
in the logs. I verified that the triggers exist on the master as they 
should (we never touched the master anyways, but still checking 
everything), the path to the slave remained the same as the previous 
slave (same dbname, host, port, user).

    Any thoughts or things I can check would be appreciated. Or if my 
theory about the master not recognizing the new slave cluster as the old 
one is correct, then if we can fix that that would be great.

thanks in advance,
                      Brian F

From ssinger_pg at sympatico.ca  Thu Sep 16 16:12:47 2010
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Thu, 16 Sep 2010 19:12:47 -0400 (EDT)
Subject: [Slony1-general] slony not replicating after re-initializing
 the slave cluster
In-Reply-To: <4C92A9D9.2030109@consistentstate.com>
References: <4C92A9D9.2030109@consistentstate.com>
Message-ID: <BLU0-SMTP83780E6A82D2A7D1454515AC7A0@phx.gbl>

On Thu, 16 Sep 2010, Brian Fehrle wrote:

What are your slon processes logging/printing?   Are the remoteWorker 
threads actually processing events or is the remoteListener the only thread 
logging?  Are rows the events in sl_event being marked as confirmed in 
sl_confirm (on the slave? is the data making it back to the sl_confirm on 
the master? I suspect not otherwise sl_log_1 wouldn't keep growing)

Also remember to run REPAIR CONFIG 
(http://www.slony.info/documentation/stmtrepairconfig.html) after restoring 
from a pg_dump.




> Hi all,
>    Due to realizing that our 1 master -> 1 slave slony cluster had
> different encodings on each box, we attempted to fix that. Our master
> had encoding of LATIN1 and our slave had the encoding of SQL_ASCII (they
> were initialized so long ago, we don't know who did it or why it was
> done that way).
>    Slony worked with this setup, but we wanted to fix it, due to some
> other problems, by moving the slave from SQL_ASCII to LATIN1.
>
>    So we brought down the slon daemons, brought down the slave database
> and rebooted the physical machine the slave is on (dozens of cron jobs
> we commented out and wanted to verify they were all dead).
>
>    When we rebooted the machine, we brought the slave postgres cluster
> online and preformed a pg_dump on the entire database (including the
> _slony schema). Then we brought down the postgres cluster, ran initdb to
> create a new one with LATIN1 encoding, brought the new cluster online
> and ran a pg_restore on it with the dump file we created before.
>
>    After that we restarted our cron jobs, which also started up the two
> slon daemons, we started monitoring the slave and noticed that no
> updates are being applied. We're running the slon daemons with -s 60000
> (force a sync every 60 seconds) and a -x flag to get some slony logs for
> log shipping. These slony logs that are generated with -x are empty
> (they have the slony header and footer, but no insert data).
>
>    On the master, if I do a # select * from _slony.sl_status; I get
> back that there are anywhere between 0 - 2 events, and a lag time no
> greater than 3 minutes. Monitoring the slave slony log output also
> verifies that events are being receved and processed without error every
> minute.
>
>    Again, on the master, # select count(*) from _slony.sl_log_1;
> returns with 12,000 + rows, and it continually grows. So from what I can
> tell, the master is getting events qued up, but not pushing them in the
> events to the slave, each event is completely void of data, and it looks
> like sl_log_1 just keeps building up.
>
>    One theory is that even though we have an exact data dump of the old
> slave cluster restored to to the new slave cluster, since the encoding
> has changed perhaps the master doesn't recognize the slave as the same
> slave it had before. If thats the case, is there any way we can get it
> to recognize it without having to rebuild the slony cluster? (rebuilding
> the cluster would mean a few days of work if not week/s).
>
>    Other than that, I'm unsure what to make of this. I've restarted the
> daemons, and neither the master nor the slave daemon report any errors
> in the logs. I verified that the triggers exist on the master as they
> should (we never touched the master anyways, but still checking
> everything), the path to the slave remained the same as the previous
> slave (same dbname, host, port, user).
>
>    Any thoughts or things I can check would be appreciated. Or if my
> theory about the master not recognizing the new slave cluster as the old
> one is correct, then if we can fix that that would be great.
>
> thanks in advance,
>                      Brian F
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>


From atsaloli.tech at gmail.com  Thu Sep 16 17:19:58 2010
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Thu, 16 Sep 2010 17:19:58 -0700
Subject: [Slony1-general] How to disable PostgreSQL autovacuum of Slony
 tables? (getting error: column "_" does not exist)
Message-ID: <AANLkTin1kcpAagyor_vDyGYM3ynXdz+=o_fuh4X8-bKT@mail.gmail.com>

Hi.

The Slony maintenace docs recommend disabling PostgreSQL autovacuum
on the Slony tables (http://www.slony.info/documentation/maintenance.html)
but the command to identify the tables fails:

mycluster# select oid, relname from pg_class where relnamespace =
(select oid from pg_namespace where nspname = '_' || 'MyCluster') and
relhasindex;
...
ERROR:  column "_" does not exist
LINE 1: ...e = (select oid from pg_namespace where nspname = _ || ...

Has this command changed for 8.4.4 ?  (I'm running slony 1.2.20
against PostgreSQL 8.4.4.)

Best,
-at

From vivek at khera.org  Fri Sep 17 05:25:58 2010
From: vivek at khera.org (Vick Khera)
Date: Fri, 17 Sep 2010 08:25:58 -0400
Subject: [Slony1-general] How to disable PostgreSQL autovacuum of Slony
 tables? (getting error: column "_" does not exist)
In-Reply-To: <AANLkTin1kcpAagyor_vDyGYM3ynXdz+=o_fuh4X8-bKT@mail.gmail.com>
References: <AANLkTin1kcpAagyor_vDyGYM3ynXdz+=o_fuh4X8-bKT@mail.gmail.com>
Message-ID: <AANLkTi=kTuE_sA69b296hJKNKzy2fR09psKB8hdxQNDw@mail.gmail.com>

On Thu, Sep 16, 2010 at 8:19 PM, Aleksey Tsalolikhin
<atsaloli.tech at gmail.com> wrote:
> The Slony maintenace docs recommend disabling PostgreSQL autovacuum
> on the Slony tables (http://www.slony.info/documentation/maintenance.html)
> but the command to identify the tables fails:
>

I'm running an 8.3 cluster and I have not disabled autovacuum on the
slony tables.  I have not had any problems.  I think modern autovacuum
is smart enough to work this way.  I also run a manual vacuum on my
whole DB every night.

From brianf at consistentstate.com  Fri Sep 17 11:46:13 2010
From: brianf at consistentstate.com (Brian Fehrle)
Date: Fri, 17 Sep 2010 12:46:13 -0600
Subject: [Slony1-general] slony not replicating after re-initializing
 the slave cluster
In-Reply-To: <BLU0-SMTP83780E6A82D2A7D1454515AC7A0@phx.gbl>
References: <4C92A9D9.2030109@consistentstate.com>
	<BLU0-SMTP83780E6A82D2A7D1454515AC7A0@phx.gbl>
Message-ID: <4C93B775.2040804@consistentstate.com>

Thanks Steve,
    Unfortunately since this is a very busy production server (with only 
a couple hours of slony downtime, it's taking a while to catch up so we 
didn't want to prolong it even more), we didn't have too much time to 
trouble shoot this so I did not grab the slon daemon logs and back them 
up like I should have (I only looked at them, and noticed events being 
processed but didn't note on whether they were worker or listener 
threads). We restored the original slave cluster's data directory and 
fired it all up, and slony resumed just fine.

We did not run slonik repair config, but now that we know about it I'd 
imagine that would fix the problem, since we didn't include any oid's in 
the pg_dump.

As of now we're running on the old cluster, we'll eventually try again 
but only after testing it on a few test clusters we set up and see if we 
can replicate the "problem", then fix it. I'll report results when we do 
that.

- Brian

Steve Singer wrote:
> On Thu, 16 Sep 2010, Brian Fehrle wrote:
>
> What are your slon processes logging/printing?   Are the remoteWorker 
> threads actually processing events or is the remoteListener the only 
> thread logging?  Are rows the events in sl_event being marked as 
> confirmed in sl_confirm (on the slave? is the data making it back to 
> the sl_confirm on the master? I suspect not otherwise sl_log_1 
> wouldn't keep growing)
>
> Also remember to run REPAIR CONFIG 
> (http://www.slony.info/documentation/stmtrepairconfig.html) after 
> restoring from a pg_dump.
>
>
>
>
>> Hi all,
>>    Due to realizing that our 1 master -> 1 slave slony cluster had
>> different encodings on each box, we attempted to fix that. Our master
>> had encoding of LATIN1 and our slave had the encoding of SQL_ASCII (they
>> were initialized so long ago, we don't know who did it or why it was
>> done that way).
>>    Slony worked with this setup, but we wanted to fix it, due to some
>> other problems, by moving the slave from SQL_ASCII to LATIN1.
>>
>>    So we brought down the slon daemons, brought down the slave database
>> and rebooted the physical machine the slave is on (dozens of cron jobs
>> we commented out and wanted to verify they were all dead).
>>
>>    When we rebooted the machine, we brought the slave postgres cluster
>> online and preformed a pg_dump on the entire database (including the
>> _slony schema). Then we brought down the postgres cluster, ran initdb to
>> create a new one with LATIN1 encoding, brought the new cluster online
>> and ran a pg_restore on it with the dump file we created before.
>>
>>    After that we restarted our cron jobs, which also started up the two
>> slon daemons, we started monitoring the slave and noticed that no
>> updates are being applied. We're running the slon daemons with -s 60000
>> (force a sync every 60 seconds) and a -x flag to get some slony logs for
>> log shipping. These slony logs that are generated with -x are empty
>> (they have the slony header and footer, but no insert data).
>>
>>    On the master, if I do a # select * from _slony.sl_status; I get
>> back that there are anywhere between 0 - 2 events, and a lag time no
>> greater than 3 minutes. Monitoring the slave slony log output also
>> verifies that events are being receved and processed without error every
>> minute.
>>
>>    Again, on the master, # select count(*) from _slony.sl_log_1;
>> returns with 12,000 + rows, and it continually grows. So from what I can
>> tell, the master is getting events qued up, but not pushing them in the
>> events to the slave, each event is completely void of data, and it looks
>> like sl_log_1 just keeps building up.
>>
>>    One theory is that even though we have an exact data dump of the old
>> slave cluster restored to to the new slave cluster, since the encoding
>> has changed perhaps the master doesn't recognize the slave as the same
>> slave it had before. If thats the case, is there any way we can get it
>> to recognize it without having to rebuild the slony cluster? (rebuilding
>> the cluster would mean a few days of work if not week/s).
>>
>>    Other than that, I'm unsure what to make of this. I've restarted the
>> daemons, and neither the master nor the slave daemon report any errors
>> in the logs. I verified that the triggers exist on the master as they
>> should (we never touched the master anyways, but still checking
>> everything), the path to the slave remained the same as the previous
>> slave (same dbname, host, port, user).
>>
>>    Any thoughts or things I can check would be appreciated. Or if my
>> theory about the master not recognizing the new slave cluster as the old
>> one is correct, then if we can fix that that would be great.
>>
>> thanks in advance,
>>                      Brian F
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>


From ssinger at ca.afilias.info  Fri Sep 17 13:56:48 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 17 Sep 2010 16:56:48 -0400
Subject: [Slony1-general] Slony 2.0.5 Release Candidate 1
Message-ID: <4C93D610.6000804@ca.afilias.info>

The slony1 team is pleased to announce a release candidate for version 
2.0.5.


The release candidate is available at

http://www.slony.info/downloads/2.0/source/slony1-2.0.5.rc1.tar.bz2
http://www.slony.info/downloads/2.0/source/slony1-2.0.5.rc1-docs.tar.bz2

or through the 2_0_5_RC1 tag in the git repository.

2.0.5 includes over 24 fixes from 2.0.4 fixing many issues found during 
an extensive testing period.

Changes from 2.0.4 of particular interest are:

-SUBSCRIBE SET is now submitted against the set origin not the provider. 
This means that slonik scripts might need to be modified to do a WAIT 
FOR against the origin instead of the provider. (See bug #121)

-Slony now requires that your libpq be built with --enable-thread-saftey 
on all platforms.  Previously this was only required on some platforms( 
AIX, Solaris).  However we have determined that it is required on all 
platforms.  This should not be an issue for postgresql installs from 
binary packages that are already compiled with --enable-thread-safety 
but might require users who have built their own postgresql from source 
(without --enable-thread-safety) to recompile.  (See bug #40)

A complete list of changes can be found in the RELEASE notes file.

We would ask that people give the release candidate a try and report 
both successes and failures to us (either through the mailing list or 
off-list email).



From cbbrowne at ca.afilias.info  Fri Sep 17 15:21:42 2010
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri, 17 Sep 2010 18:21:42 -0400
Subject: [Slony1-general] Slony 2.0.5 Release Candidate 1
In-Reply-To: <4C93D610.6000804@ca.afilias.info> (Steve Singer's message of
	"Fri, 17 Sep 2010 16:56:48 -0400")
References: <4C93D610.6000804@ca.afilias.info>
Message-ID: <87sk18c8p5.fsf@cbbrowne.afilias-int.info>

Steve Singer <ssinger at ca.afilias.info> writes:
> -Slony now requires that your libpq be built with --enable-thread-saftey 
> on all platforms.  Previously this was only required on some platforms( 
> AIX, Solaris).  However we have determined that it is required on all 
> platforms.  This should not be an issue for postgresql installs from 
> binary packages that are already compiled with --enable-thread-safety 
> but might require users who have built their own postgresql from source 
> (without --enable-thread-safety) to recompile.  (See bug #40)

Just to quibble a little bit on details, I'm not certain that this
necessarily requires that the --enable-thread-safety specifically be
used.

What's necessary is that libpq be thread-safe, which may be validated
via a libpq function, PQisthreadsafe().

On many (perhaps most, perhaps, but not *for certain* all, hence my
quibble) platforms, for PQisthreadsafe() to return true requires that
PostgreSQL (well, libpq, specifically) be configured with
--enable-thread-safety.  But I'm not certain that this is always
necessary.  It appears that some platforms pass PQisthreadsafe() without
"configure --enable-thread-safety" having been explicitly requested.

Quibble done :-).

> A complete list of changes can be found in the RELEASE notes file.
>
> We would ask that people give the release candidate a try and report 
> both successes and failures to us (either through the mailing list or 
> off-list email).

Agreed!

Lots of stuff has gotten cleaned up for a 2.0.5 release; some validation
of success/failure would certainly be appreciated.  It has been a pretty
long road, and I certainly hope 2.0.5 "actual" proceeds mighty soon.
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"

From lai at clustersolutions.net  Sat Sep 18 09:11:41 2010
From: lai at clustersolutions.net (lai at clustersolutions.net)
Date: Sat, 18 Sep 2010 09:11:41 -0700 (PDT)
Subject: [Slony1-general] New index on replicated table
Message-ID: <57085.72.134.111.79.1284826301.squirrel@www.clustersolutions.net>

Hello,

Reading throught the doc for Slony it seems that it is okay to add an
unique index to a replicated table on the master node using psql. We use
our slave for read-only. I would appreciate any help on this issue before
carrying out the task. Oh, can I also get a confirmation that it is okay
to leave out the index on the same table of the slave node?

Thank you so much for your help!

Tim


From jaime at 2ndquadrant.com  Sat Sep 18 09:06:36 2010
From: jaime at 2ndquadrant.com (Jaime Casanova)
Date: Sat, 18 Sep 2010 11:06:36 -0500
Subject: [Slony1-general] New index on replicated table
In-Reply-To: <57085.72.134.111.79.1284826301.squirrel@www.clustersolutions.net>
References: <57085.72.134.111.79.1284826301.squirrel@www.clustersolutions.net>
Message-ID: <AANLkTin5LtUfJ_px5NsQBP2y6OMGZRQ0RZ3svq+WUiyU@mail.gmail.com>

On Sat, Sep 18, 2010 at 11:11 AM,  <lai at clustersolutions.net> wrote:
> Hello,
>
> Reading throught the doc for Slony it seems that it is okay to add an
> unique index to a replicated table on the master node using psql. We use
> our slave for read-only. I would appreciate any help on this issue before
> carrying out the task. Oh, can I also get a confirmation that it is okay
> to leave out the index on the same table of the slave node?
>

yeah! no problem with that... just don't make it your pk


-- 
Jaime Casanova? ? ? ?? www.2ndQuadrant.com
Soporte y capacitaci?n de PostgreSQL

From cbbrowne at ca.afilias.info  Mon Sep 20 08:04:11 2010
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon, 20 Sep 2010 11:04:11 -0400
Subject: [Slony1-general] New index on replicated table
In-Reply-To: <57085.72.134.111.79.1284826301.squirrel@www.clustersolutions.net>
	(lai@clustersolutions.net's message of "Sat, 18 Sep 2010 09:11:41
	-0700 (PDT)")
References: <57085.72.134.111.79.1284826301.squirrel@www.clustersolutions.net>
Message-ID: <87ocbscv84.fsf@cbbrowne.afilias-int.info>

lai at clustersolutions.net writes:
> Reading throught the doc for Slony it seems that it is okay to add an
> unique index to a replicated table on the master node using psql. We use
> our slave for read-only. I would appreciate any help on this issue before
> carrying out the task. Oh, can I also get a confirmation that it is okay
> to leave out the index on the same table of the slave node?
>
> Thank you so much for your help!

Sure, that's perfectly reasonable to do.

Putting a unique index onto a replicated on a subscriber is an entirely
riskier endeavour, as that puts a constraint onto the subscriber which
the provider was not required to satisfy.

*That* would give the risk that an application puts data onto the
"master" node that won't be accepted by the subscriber, thereby causing
replication to fail.

The failure mode isn't completely heinous - replication  stops, and the
subscriber's replication logs will complain about the violation of the
unique constraint.  The sad thing that happens next is that the
subscriber will try this over and over, failing every time :-(.

But that's not the case you're talking about - you're talking about
putting an extra constaint on the "master."  No particular problem with
that.
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"

From bnichols at ca.afilias.info  Mon Sep 20 12:26:33 2010
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Mon, 20 Sep 2010 15:26:33 -0400
Subject: [Slony1-general] How to disable PostgreSQL autovacuum of Slony
 tables? (getting error: column "_" does not exist)
In-Reply-To: <AANLkTi=kTuE_sA69b296hJKNKzy2fR09psKB8hdxQNDw@mail.gmail.com>
References: <AANLkTin1kcpAagyor_vDyGYM3ynXdz+=o_fuh4X8-bKT@mail.gmail.com>
	<AANLkTi=kTuE_sA69b296hJKNKzy2fR09psKB8hdxQNDw@mail.gmail.com>
Message-ID: <4C97B569.5090304@ca.afilias.info>

  On 10-09-17 08:25 AM, Vick Khera wrote:
> On Thu, Sep 16, 2010 at 8:19 PM, Aleksey Tsalolikhin
> <atsaloli.tech at gmail.com>  wrote:
>> The Slony maintenace docs recommend disabling PostgreSQL autovacuum
>> on the Slony tables (http://www.slony.info/documentation/maintenance.html)
>> but the command to identify the tables fails:
>>
> I'm running an 8.3 cluster and I have not disabled autovacuum on the
> slony tables.  I have not had any problems.  I think modern autovacuum
> is smart enough to work this way.  I also run a manual vacuum on my
> whole DB every night.
>

Same here.  We have also disabled the slon triggered vacuums and let 
autovacuum handle everything.

To answer the OP question about pg_autovacuum - yes, the table has gone 
away in Postgres 8.4.  Autovacuum settings on a per-table basis are 
controlled in with the tables storage parameters now via CREATE/ALTER table.

http://www.postgresql.org/docs/9.0/interactive/sql-createtable.html#SQL-CREATETABLE-STORAGE-PARAMETERS

Perhaps the docs should be updated to reflect this.

-- 
Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.



From tore.halvorsen at gmail.com  Tue Sep 21 00:45:24 2010
From: tore.halvorsen at gmail.com (Tore Halvorsen)
Date: Tue, 21 Sep 2010 09:45:24 +0200
Subject: [Slony1-general] Windows and x64
Message-ID: <AANLkTi=q3mm-NEY_PuWvvwraT6yBSkrGvvUh0+2+SRPP@mail.gmail.com>

Is there a plan to get slony running in 64bit mode on windows?

-- 
Eld p? ?ren og sol p? eng gjer mannen fegen og fj?g. [J?tul]
<demo> 2010 Tore Halvorsen || +052 0553034554

From atsaloli.tech at gmail.com  Tue Sep 21 13:08:43 2010
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Tue, 21 Sep 2010 13:08:43 -0700
Subject: [Slony1-general] ERROR: cache lookup failed for type 14237017
Message-ID: <AANLkTimH+9LFigvP0juv6TZpEB6=uWQYbuVrkW8vTFpG@mail.gmail.com>

Yesterday, I had twelve thousand  "cache lookup failed for type N"
messages, like this:

2010-09-20 00:00:00 PDT ERROR:  cache lookup failed for type 14237017
2010-09-20 00:00:00 PDT CONTEXT:  SQL statement "INSERT INTO
mycluster.sl_log_2 (log_origin, log_xid, log_tableid, log_actionseq,
log_cmdtype, log_cmddata) VALUES (1, $1, $2,
nextval('mycluster.sl_action_seq'), $3, $4);"

The context is always Slony sl_log_2 table.  All twelve thousand
errors occurred within 40 minutes.  This did happen right after a
Slony cluster set drop and recreate.

What does "type 14237017" mean?  What cache are we talking about?

I am concerned because of PostgreSQL classification of ERROR rather
than WARNING.  What does this mean?  I want to understand what
happened if there is anything I can do about it.

PostgreSQL 8.4.4
Slony1 1.2.20

Thanks,
Aleksey

From ssinger at ca.afilias.info  Tue Sep 21 13:26:14 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 21 Sep 2010 16:26:14 -0400
Subject: [Slony1-general] ERROR: cache lookup failed for type 14237017
In-Reply-To: <AANLkTimH+9LFigvP0juv6TZpEB6=uWQYbuVrkW8vTFpG@mail.gmail.com>
References: <AANLkTimH+9LFigvP0juv6TZpEB6=uWQYbuVrkW8vTFpG@mail.gmail.com>
Message-ID: <4C9914E6.2040604@ca.afilias.info>

On 10-09-21 04:08 PM, Aleksey Tsalolikhin wrote:
> Yesterday, I had twelve thousand  "cache lookup failed for type N"
> messages, like this:
>
> 2010-09-20 00:00:00 PDT ERROR:  cache lookup failed for type 14237017
> 2010-09-20 00:00:00 PDT CONTEXT:  SQL statement "INSERT INTO
> mycluster.sl_log_2 (log_origin, log_xid, log_tableid, log_actionseq,
> log_cmdtype, log_cmddata) VALUES (1, $1, $2,
> nextval('mycluster.sl_action_seq'), $3, $4);"
>
> The context is always Slony sl_log_2 table.  All twelve thousand
> errors occurred within 40 minutes.  This did happen right after a
> Slony cluster set drop and recreate.
>
> What does "type 14237017" mean?  What cache are we talking about?
>
> I am concerned because of PostgreSQL classification of ERROR rather
> than WARNING.  What does this mean?  I want to understand what
> happened if there is anything I can do about it.
>

I'm assuming you got these errors on the master.


What I suspect happened was this.

1.  Your application created a database connection and did some inserts 
on replicated tables.  This caused the slony log triggers to prepare a 
plan for inserting data into sl_log_1.  This database connection stayed 
open.
2. You uninstalled slony on the database. This dropped sl_log_1
3. You reinstalled slony on this database.  It recreated sl_log_1
but with a different oid.
4. Your application connection from (1) did something else on a
replicated table that caused the log_trigger to fire.
The C shared library was still loaded in the backend and it
tries to re-use the plan from 1 that is pointing at the old
sl_log_1 table (which no longer exists, since you deleted it).

This is an  ERROR because the slony trigger isn't letting the 
transaction proceed.  (I would expect you to also find errors in your 
application logs at this time).

If your application disconnects from the database and reconnects (after 
the slony re-install) you will get a new database connection with a new 
plan that points at the correct sl_log_1.  If your using a connection 
pool you will also need to cycle the connection pool to force reconnections.





> PostgreSQL 8.4.4
> Slony1 1.2.20
>
> Thanks,
> Aleksey
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From atsaloli.tech at gmail.com  Tue Sep 21 14:04:11 2010
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Tue, 21 Sep 2010 14:04:11 -0700
Subject: [Slony1-general] ERROR: cache lookup failed for type 14237017
In-Reply-To: <4C9914E6.2040604@ca.afilias.info>
References: <AANLkTimH+9LFigvP0juv6TZpEB6=uWQYbuVrkW8vTFpG@mail.gmail.com>
	<4C9914E6.2040604@ca.afilias.info>
Message-ID: <AANLkTink+S4q37MXMfWOkFwd-AmFEHyPt5JKLB9LZD-i@mail.gmail.com>

Dear Steve,

  You are right, I saw these errors on the Slony master.  The master
was NOT quiesced for the Slony drop/recreate; data was rolling in.

  So does this mean 12,000 queries failed?  I don't see anything in
the database log like "query cancelled" or anything like that.

 Why does it say "cache lookup failed" as opposed to "insert failed"?
Cache lookup I can ignore (generally speaking, although it might be a
performance hit, lookup can still succeed, from the primary source, even
if it's not in the cache) but a failed query is more serious.

Best,
Aleksey

From atsaloli.tech at gmail.com  Tue Sep 21 14:53:11 2010
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Tue, 21 Sep 2010 14:53:11 -0700
Subject: [Slony1-general] How to disable PostgreSQL autovacuum of Slony
 tables? (getting error: column "_" does not exist)
In-Reply-To: <4C97B569.5090304@ca.afilias.info>
References: <AANLkTin1kcpAagyor_vDyGYM3ynXdz+=o_fuh4X8-bKT@mail.gmail.com>
	<AANLkTi=kTuE_sA69b296hJKNKzy2fR09psKB8hdxQNDw@mail.gmail.com>
	<4C97B569.5090304@ca.afilias.info>
Message-ID: <AANLkTimOg-RZhp04+bfPoq4tqmW8=uNaYLGMAdMDCrgf@mail.gmail.com>

On Mon, Sep 20, 2010 at 12:26 PM, Brad Nicholson
<bnichols at ca.afilias.info> wrote:
> ?On 10-09-17 08:25 AM, Vick Khera wrote:
>> On Thu, Sep 16, 2010 at 8:19 PM, Aleksey Tsalolikhin
>> <atsaloli.tech at gmail.com> ?wrote:
>>> The Slony maintenace docs recommend disabling PostgreSQL autovacuum
>>> on the Slony tables (http://www.slony.info/documentation/maintenance.html)
>>> but the command to identify the tables fails:
>>>
>> I'm running an 8.3 cluster and I have not disabled autovacuum on the
>> slony tables. ?I have not had any problems. ?I think modern autovacuum
>> is smart enough to work this way. ?I also run a manual vacuum on my
>> whole DB every night.
>>
>
> Same here. ?We have also disabled the slon triggered vacuums and let
> autovacuum handle everything.


> To answer the OP question about pg_autovacuum - yes, the table has gone
> away in Postgres 8.4.

Thank you.   I see how to control autovacuuming using ALTER TABLE.
So Vick and Brad both let Postgres autovacuum the Slony tables.

I propose a patch to http://www.slony.info/documentation/maintenance.html :

Change section title from
"6.1. Interaction with PostgreSQL autovacuum"
to
"6.1. Interaction with PostgreSQL 8.1 and 8.2 autovacuum"

Change
"The following query (change the cluster name to match your local
configuration) will identify the tables that autovacuum should be
configured not to process"
to
"The following query (change the cluster name to match your local
configuration) on Postgres 8 versions up to and not including 8.4 will
identify the tables that autovacuum should be configured not to
process"

Change
"The following query will populate pg_catalog.pg_autovacuum with
suitable configuration information:  "
to
"The following query (for Postgres versions up to and not including
8.4) will populate pg_catalog.pg_autovacuum with suitable
configuration information:  "

Best,
-at

From brianf at consistentstate.com  Tue Sep 21 16:23:52 2010
From: brianf at consistentstate.com (Brian Fehrle)
Date: Tue, 21 Sep 2010 17:23:52 -0600
Subject: [Slony1-general] slony not replicating after re-initializing
 the slave cluster
In-Reply-To: <BLU0-SMTP83780E6A82D2A7D1454515AC7A0@phx.gbl>
References: <4C92A9D9.2030109@consistentstate.com>
	<BLU0-SMTP83780E6A82D2A7D1454515AC7A0@phx.gbl>
Message-ID: <4C993E88.8010408@consistentstate.com>

I got some time and decided to test this again on some VM boxes rather 
than our live environment, but had little luck.

Simply so I can have this logged in the mailing list with what was done 
(and hopefully a solution in the near future), here's my process I 
preformed.

I created two clusters that mirror our live boxes as closely as possible.
- - PostgreSQL version 8.4.2
- - Slony version 1.2.20
- - both installed via source

I created the master cluster as:
# initdb -D /usr/local/pgsql/encoding_master/ --locale=C --encoding=LATIN1
I created the slave cluster as:
# initdb -D /usr/local/pgsql/encoding_slave/ --locale=C --encoding=SQL_ASCII

I set up a master -> slave slony cluster and replicated a single table 
in a single replication set, and verified that replication was taking place.

I wrote a small daemon that inserts a row into the table being 
replicated on the master once a minute.

I brought down the slon daemons, and preformed a pg_dump on the slave:
# pg_dump -p 5433 -Fc postgres > /tmp/postgres_dump.sql

I brought down the slave cluster, then created a new one with the LATIN1 
encoding:
# initdb -D /usr/local/pgsql/encoding_slave_latin/ --locale=C 
--encoding=SQL_ASCII

I brought the cluster online and started up the slon daemons. The slave 
slon daemon reported remoteworker and remote listener threads, and 
reported increasing SYNC numbers, however did not actually replicate 
data from the master to the slave, and _slony.sl_log_1 on the master 
grew in numbers with every insert that took place . NOTE: This is the 
same behavior I experienced before on our live servers.

I then executed the following:
#!/bin/bash
. etc/slony.env
echo "Repair config"

slonik <<_EOF_
cluster name = $CLUSTERNAME ;
node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST 
port=$MASTERPORT user=$REPUSER';
node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST 
port=$SLAVEPORT user=$REPUSER';
REPAIR CONFIG (SET ID = 1, EVENT NODE = 1, EXECUTE ONLY ON = 2);
_EOF_

it executed without error, however replication did not start working, 
and the slave daemon started acting weird with the child process being 
terminated constantly, then restarted every 10 seconds just to be 
terminated again.

Monitoring the slave slon log, The following chunk gets repeated every 
10 seconds (with the addition of another que event each time):

2010-09-21 16:39:08 MDT CONFIG main: slon version 1.2.20 starting up
2010-09-21 16:39:08 MDT DEBUG2 slon: watchdog process started
2010-09-21 16:39:08 MDT DEBUG2 slon: watchdog ready - pid = 13845
2010-09-21 16:39:08 MDT DEBUG2 slon: worker process created - pid = 14308
2010-09-21 16:39:08 MDT CONFIG main: local node id = 2
2010-09-21 16:39:08 MDT DEBUG2 main: main process started
2010-09-21 16:39:08 MDT CONFIG main: launching sched_start_mainloop
2010-09-21 16:39:08 MDT CONFIG main: loading current cluster configuration
2010-09-21 16:39:08 MDT CONFIG storeNode: no_id=1 no_comment='Server 1'
2010-09-21 16:39:08 MDT DEBUG2 setNodeLastEvent: no_id=1 event_seq=1008
2010-09-21 16:39:08 MDT CONFIG storePath: pa_server=1 pa_client=2 
pa_conninfo="dbname=postgres host=172.16.44.133 port=5432 user=postgres" 
pa_connretry=10
2010-09-21 16:39:08 MDT CONFIG storeListen: li_origin=1 li_receiver=2 
li_provider=1
2010-09-21 16:39:08 MDT CONFIG storeSet: set_id=1 set_origin=1 
set_comment='logtester'
2010-09-21 16:39:08 MDT WARN   remoteWorker_wakeup: node 1 - no worker 
thread
2010-09-21 16:39:08 MDT DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + 
worker signaled)
2010-09-21 16:39:08 MDT CONFIG storeSubscribe: sub_set=1 sub_provider=1 
sub_forward='f'
2010-09-21 16:39:08 MDT WARN   remoteWorker_wakeup: node 1 - no worker 
thread
2010-09-21 16:39:08 MDT DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + 
worker signaled)
2010-09-21 16:39:08 MDT CONFIG enableSubscription: sub_set=1
2010-09-21 16:39:08 MDT WARN   remoteWorker_wakeup: node 1 - no worker 
thread
2010-09-21 16:39:08 MDT DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + 
worker signaled)
2010-09-21 16:39:08 MDT DEBUG2 main: last local event sequence = 575
2010-09-21 16:39:08 MDT CONFIG main: configuration complete - starting 
threads
2010-09-21 16:39:08 MDT DEBUG1 localListenThread: thread starts
2010-09-21 16:39:08 MDT DEBUG4 version for "dbname=postgres port=5433 
host=172.16.44.133 user=postgres" is 80402
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=14291
2010-09-21 16:39:08 MDT CONFIG enableNode: no_id=1
2010-09-21 16:39:08 MDT DEBUG1 main: running scheduler mainloop
2010-09-21 16:39:08 MDT DEBUG1 remoteWorkerThread_1: thread starts
2010-09-21 16:39:08 MDT DEBUG4 version for "dbname=postgres port=5433 
host=172.16.44.133 user=postgres" is 80402
2010-09-21 16:39:08 MDT DEBUG4 remoteWorkerThread_1: update provider 
configuration
2010-09-21 16:39:08 MDT DEBUG1 remoteWorkerThread_1: helper thread for 
provider 1 created
2010-09-21 16:39:08 MDT DEBUG4 remoteWorkerThread_1: added active set 1 
to provider 1
2010-09-21 16:39:08 MDT DEBUG2 remoteWorkerThread_1: set 1 starts at 
ssy_seqno 1008
2010-09-21 16:39:08 MDT DEBUG1 remoteListenThread_1: thread starts
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: start listening for 
event origin 1
2010-09-21 16:39:08 MDT DEBUG4 version for "dbname=postgres 
host=172.16.44.133 port=5432 user=postgres" is 80402
2010-09-21 16:39:08 MDT DEBUG1 cleanupThread: thread starts
2010-09-21 16:39:08 MDT DEBUG4 cleanupThread: bias = 35383
2010-09-21 16:39:08 MDT DEBUG4 version for "dbname=postgres port=5433 
host=172.16.44.133 user=postgres" is 80402
2010-09-21 16:39:08 MDT DEBUG1 syncThread: thread starts
2010-09-21 16:39:08 MDT DEBUG4 version for "dbname=postgres port=5433 
host=172.16.44.133 user=postgres" is 80402
2010-09-21 16:39:08 MDT DEBUG1 remoteListenThread_1: connected to 
'dbname=postgres host=172.16.44.133 port=5432 user=postgres'
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1009 
RESET_CONFIG
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1010 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1011 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1012 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1013 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1014 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1015 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1016 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1017 
RESET_CONFIG
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1018 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1019 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1020 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1021 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1022 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1023 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1024 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1025 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1026 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1027 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1028 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1029 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1030 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1031 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1032 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1033 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1034 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1035 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1036 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1037 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1038 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1039 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1040 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1041 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1042 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1043 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1044 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1045 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1046 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1047 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1048 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1049 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1050 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1051 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1052 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1053 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1054 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1055 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1056 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1057 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1058 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1059 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1060 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1061 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1062 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1063 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1064 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1065 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1066 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1067 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1068 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1069 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteListenThread_1: queue event 1,1070 SYNC
2010-09-21 16:39:08 MDT DEBUG2 remoteWorkerThread_1: Received event 
1,1009 RESET_CONFIG
2010-09-21 16:39:09 MDT DEBUG2 slon: child terminated status: 11; pid: 
14308, current worker pid: 14308
2010-09-21 16:39:09 MDT DEBUG1 slon: restart of worker in 10 seconds

At this point, slony is not replicating, the slave daemon's child 
process won't stay alive, and restarting the deamons yield nothing.

I stopped the slon daemons, brought down the slave database, restored 
the OLD slave database that replication was working on, and started up 
the daemons. of the events that were qued up in sl_log_1, half of them 
replicated over to the slave before the slave daemon's child process was 
terminated. After this, the slave daemon entered into the same  loop 
that it was doing on the previous slave (with log printed above).

So now at this point, i'm on the old slave cluster, and slony 
replication is dead. I'm not too concerned at the moment to get it 
running again, as it's just a test system. The thing that I need working 
however is replication on the new cluster.


NEXT STEP:
Next I'll be resetting everything and repeating the process. However 
this time around I'll be including OID's in the pg_dump and monitoring 
how slony behaves with that approach.

Any suggestions / error corrections are appreciated.

- Brian F



Steve Singer wrote:
> On Thu, 16 Sep 2010, Brian Fehrle wrote:
>
> What are your slon processes logging/printing?   Are the remoteWorker 
> threads actually processing events or is the remoteListener the only 
> thread logging?  Are rows the events in sl_event being marked as 
> confirmed in sl_confirm (on the slave? is the data making it back to 
> the sl_confirm on the master? I suspect not otherwise sl_log_1 
> wouldn't keep growing)
>
> Also remember to run REPAIR CONFIG 
> (http://www.slony.info/documentation/stmtrepairconfig.html) after 
> restoring from a pg_dump.
>
>
>
>
>> Hi all,
>>    Due to realizing that our 1 master -> 1 slave slony cluster had
>> different encodings on each box, we attempted to fix that. Our master
>> had encoding of LATIN1 and our slave had the encoding of SQL_ASCII (they
>> were initialized so long ago, we don't know who did it or why it was
>> done that way).
>>    Slony worked with this setup, but we wanted to fix it, due to some
>> other problems, by moving the slave from SQL_ASCII to LATIN1.
>>
>>    So we brought down the slon daemons, brought down the slave database
>> and rebooted the physical machine the slave is on (dozens of cron jobs
>> we commented out and wanted to verify they were all dead).
>>
>>    When we rebooted the machine, we brought the slave postgres cluster
>> online and preformed a pg_dump on the entire database (including the
>> _slony schema). Then we brought down the postgres cluster, ran initdb to
>> create a new one with LATIN1 encoding, brought the new cluster online
>> and ran a pg_restore on it with the dump file we created before.
>>
>>    After that we restarted our cron jobs, which also started up the two
>> slon daemons, we started monitoring the slave and noticed that no
>> updates are being applied. We're running the slon daemons with -s 60000
>> (force a sync every 60 seconds) and a -x flag to get some slony logs for
>> log shipping. These slony logs that are generated with -x are empty
>> (they have the slony header and footer, but no insert data).
>>
>>    On the master, if I do a # select * from _slony.sl_status; I get
>> back that there are anywhere between 0 - 2 events, and a lag time no
>> greater than 3 minutes. Monitoring the slave slony log output also
>> verifies that events are being receved and processed without error every
>> minute.
>>
>>    Again, on the master, # select count(*) from _slony.sl_log_1;
>> returns with 12,000 + rows, and it continually grows. So from what I can
>> tell, the master is getting events qued up, but not pushing them in the
>> events to the slave, each event is completely void of data, and it looks
>> like sl_log_1 just keeps building up.
>>
>>    One theory is that even though we have an exact data dump of the old
>> slave cluster restored to to the new slave cluster, since the encoding
>> has changed perhaps the master doesn't recognize the slave as the same
>> slave it had before. If thats the case, is there any way we can get it
>> to recognize it without having to rebuild the slony cluster? (rebuilding
>> the cluster would mean a few days of work if not week/s).
>>
>>    Other than that, I'm unsure what to make of this. I've restarted the
>> daemons, and neither the master nor the slave daemon report any errors
>> in the logs. I verified that the triggers exist on the master as they
>> should (we never touched the master anyways, but still checking
>> everything), the path to the slave remained the same as the previous
>> slave (same dbname, host, port, user).
>>
>>    Any thoughts or things I can check would be appreciated. Or if my
>> theory about the master not recognizing the new slave cluster as the old
>> one is correct, then if we can fix that that would be great.
>>
>> thanks in advance,
>>                      Brian F
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>


From atsaloli.tech at gmail.com  Tue Sep 21 16:54:49 2010
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Tue, 21 Sep 2010 16:54:49 -0700
Subject: [Slony1-general] ERROR: cache lookup failed for type 14237017
In-Reply-To: <AANLkTink+S4q37MXMfWOkFwd-AmFEHyPt5JKLB9LZD-i@mail.gmail.com>
References: <AANLkTimH+9LFigvP0juv6TZpEB6=uWQYbuVrkW8vTFpG@mail.gmail.com>
	<4C9914E6.2040604@ca.afilias.info>
	<AANLkTink+S4q37MXMfWOkFwd-AmFEHyPt5JKLB9LZD-i@mail.gmail.com>
Message-ID: <AANLkTinE-yr9htnvu5kpn6m2e1UinHR8SOV3HG_x4v=u@mail.gmail.com>

On Tue, Sep 21, 2010 at 2:04 PM, Aleksey Tsalolikhin
<atsaloli.tech at gmail.com> wrote:
> Dear Steve,
>
> ?You are right, I saw these errors on the Slony master. ?The master
> was NOT quiesced for the Slony drop/recreate; data was rolling in.
>
> ?So does this mean 12,000 queries failed? ?I don't see anything in
> the database log like "query cancelled" or anything like that.

I just found in my application logs that I had 35 queries fail earlier in the
Slony cluster drop/recreate timeline than the 12000 ERROR's in the
database log.

I guess that means I have to put my application into "maintenance mode"
and quisce it before doing a Slony drop/recreate.  Is that what people
do, commonly?

I don't see this on the "Best Practices" page at
http://www.slony.info/documentation/slonyadmin.html, should it be added?

Thanks,
Aleksey

From cbbrowne at ca.afilias.info  Wed Sep 22 07:35:43 2010
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed, 22 Sep 2010 10:35:43 -0400
Subject: [Slony1-general] ERROR: cache lookup failed for type 14237017
In-Reply-To: <AANLkTinE-yr9htnvu5kpn6m2e1UinHR8SOV3HG_x4v=u@mail.gmail.com>
	(Aleksey Tsalolikhin's message of "Tue, 21 Sep 2010 16:54:49 -0700")
References: <AANLkTimH+9LFigvP0juv6TZpEB6=uWQYbuVrkW8vTFpG@mail.gmail.com>
	<4C9914E6.2040604@ca.afilias.info>
	<AANLkTink+S4q37MXMfWOkFwd-AmFEHyPt5JKLB9LZD-i@mail.gmail.com>
	<AANLkTinE-yr9htnvu5kpn6m2e1UinHR8SOV3HG_x4v=u@mail.gmail.com>
Message-ID: <87wrqdc0cg.fsf@cbbrowne.afilias-int.info>

Aleksey Tsalolikhin <atsaloli.tech at gmail.com> writes:

> On Tue, Sep 21, 2010 at 2:04 PM, Aleksey Tsalolikhin
> <atsaloli.tech at gmail.com> wrote:
>> Dear Steve,
>>
>> ?You are right, I saw these errors on the Slony master. ?The master
>> was NOT quiesced for the Slony drop/recreate; data was rolling in.
>>
>> ?So does this mean 12,000 queries failed? ?I don't see anything in
>> the database log like "query cancelled" or anything like that.
>
> I just found in my application logs that I had 35 queries fail earlier in the
> Slony cluster drop/recreate timeline than the 12000 ERROR's in the
> database log.
>
> I guess that means I have to put my application into "maintenance mode"
> and quisce it before doing a Slony drop/recreate.  Is that what people
> do, commonly?
>
> I don't see this on the "Best Practices" page at
> http://www.slony.info/documentation/slonyadmin.html, should it be added?

It isn't there, but it is in the FAQ...

<http://slony.info/documentation/faq.html#AEN7333>
-- 
let name="cbbrowne" and tld="ca.afilias.info" in name ^ "@" ^ tld;;
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"

From JanWieck at Yahoo.com  Wed Sep 22 08:49:17 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed, 22 Sep 2010 11:49:17 -0400
Subject: [Slony1-general] ERROR: cache lookup failed for type 14237017
In-Reply-To: <AANLkTinE-yr9htnvu5kpn6m2e1UinHR8SOV3HG_x4v=u@mail.gmail.com>
References: <AANLkTimH+9LFigvP0juv6TZpEB6=uWQYbuVrkW8vTFpG@mail.gmail.com>	<4C9914E6.2040604@ca.afilias.info>	<AANLkTink+S4q37MXMfWOkFwd-AmFEHyPt5JKLB9LZD-i@mail.gmail.com>
	<AANLkTinE-yr9htnvu5kpn6m2e1UinHR8SOV3HG_x4v=u@mail.gmail.com>
Message-ID: <4C9A257D.6000306@Yahoo.com>

On 9/21/2010 7:54 PM, Aleksey Tsalolikhin wrote:
> On Tue, Sep 21, 2010 at 2:04 PM, Aleksey Tsalolikhin
> <atsaloli.tech at gmail.com>  wrote:
>>  Dear Steve,
>>
>>    You are right, I saw these errors on the Slony master.  The master
>>  was NOT quiesced for the Slony drop/recreate; data was rolling in.
>>
>>    So does this mean 12,000 queries failed?  I don't see anything in
>>  the database log like "query cancelled" or anything like that.
>
> I just found in my application logs that I had 35 queries fail earlier in the
> Slony cluster drop/recreate timeline than the 12000 ERROR's in the
> database log.
>
> I guess that means I have to put my application into "maintenance mode"
> and quisce it before doing a Slony drop/recreate.  Is that what people
> do, commonly?

What you should do is to restart the application between the Slony-I 
uninstall and the reinstall. That will ensure that all prepared query 
plans are recreated.

The message says "cache lookup" because it is coming from the internal 
query executor, that searches for the object (in this case a data type) 
in the system catalog cache and it isn't there.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From ssinger at ca.afilias.info  Wed Sep 22 12:14:08 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 22 Sep 2010 15:14:08 -0400
Subject: [Slony1-general] slony not replicating after re-initializing
 the slave cluster
In-Reply-To: <4C993E88.8010408@consistentstate.com>
References: <4C92A9D9.2030109@consistentstate.com>	<BLU0-SMTP83780E6A82D2A7D1454515AC7A0@phx.gbl>
	<4C993E88.8010408@consistentstate.com>
Message-ID: <4C9A5580.9010609@ca.afilias.info>

On 10-09-21 07:23 PM, Brian Fehrle wrote:
> I got some time and decided to test this again on some VM boxes rather
> than our live environment, but had little luck.
>
> Simply so I can have this logged in the mailing list with what was done
> (and hopefully a solution in the near future), here's my process I
> preformed.
>
> I created two clusters that mirror our live boxes as closely as possible.
> - - PostgreSQL version 8.4.2
> - - Slony version 1.2.20
> - - both installed via source
>
> I created the master cluster as:
> # initdb -D /usr/local/pgsql/encoding_master/ --locale=C --encoding=LATIN1
> I created the slave cluster as:
> # initdb -D /usr/local/pgsql/encoding_slave/ --locale=C --encoding=SQL_ASCII
>
> I set up a master ->  slave slony cluster and replicated a single table
> in a single replication set, and verified that replication was taking place.
>
> I wrote a small daemon that inserts a row into the table being
> replicated on the master once a minute.
>
> I brought down the slon daemons, and preformed a pg_dump on the slave:
> # pg_dump -p 5433 -Fc postgres>  /tmp/postgres_dump.sql
>
> I brought down the slave cluster, then created a new one with the LATIN1
> encoding:
> # initdb -D /usr/local/pgsql/encoding_slave_latin/ --locale=C
> --encoding=SQL_ASCII
>
> I brought the cluster online and started up the slon daemons. The slave
> slon daemon reported remoteworker and remote listener threads, and
> reported increasing SYNC numbers, however did not actually replicate
> data from the master to the slave, and _slony.sl_log_1 on the master
> grew in numbers with every insert that took place . NOTE: This is the
> same behavior I experienced before on our live servers.
>
> I then executed the following:
> #!/bin/bash
> . etc/slony.env
> echo "Repair config"
>
> slonik<<_EOF_
> cluster name = $CLUSTERNAME ;
> node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST
> port=$MASTERPORT user=$REPUSER';
> node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST
> port=$SLAVEPORT user=$REPUSER';
> REPAIR CONFIG (SET ID = 1, EVENT NODE = 1, EXECUTE ONLY ON = 2);
> _EOF_
>

Try

REPAIR CONFIG (SET ID=1, EVENT NODE=2, EXECUTE ONLY ON=2);

I tried a somewhat simliar sequence to what you described (though with a 
different postgresql and slony version) and the REPAIR CONFIG did not 
seem to do anything on node 2.  Ie the oid values in sl_table did NOT 
match what was in pg_class.  When I ran it with event node=2 then it did 
seem to update sl_table on node 2.




> it executed without error, however replication did not start working,
> and the slave daemon started acting weird with the child process being
> terminated constantly, then restarted every 10 seconds just to be
> terminated again.

From bnichols at ca.afilias.info  Thu Sep 23 05:56:17 2010
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Thu, 23 Sep 2010 08:56:17 -0400
Subject: [Slony1-general] [GENERAL] One warning on migration from 8.4
	--> 9.0
In-Reply-To: <4C9A98C2.3030905@denninger.net>
References: <4C9A98C2.3030905@denninger.net>
Message-ID: <4C9B4E71.4010108@ca.afilias.info>

  On 10-09-22 08:01 PM, Karl Denninger wrote:
>   If you use Slony, expect it to lose the replication status.
>
> I attempted the following:
>
> 1. Master and slaves on 8.4.
>
> 2. Upgrade one slave to 9.0.   Shut it down, used pg_upgrade to perform
> the upgrade.
>
> 3. Restarted the slave.
>
> Slony appeared to come up, but said it was syncing only TWO tables (out
> of the 33 in the working set!)  Of course that didn't work very well at
> all.....
>
> Dropping the replication set and re-adding it appears to be fine, but of
> course that caused a full database copy.  Not too cool.
>
> I have no idea why it only thought there were two tables in the
> replication set.  Very, very odd stuff....
>
> Something to be aware of - I haven't figured out why it did this, nor do
> I know if it will do the same thing to me when I attempt to upgrade the
> master to 9.0 - that's something I won't attempt until the weekend at
> the earliest.
>
> Other than that running with 8.4 for the master and 9.0 for the slaves
> appears to be ok.

What version of Slony was this?

(Note I've moved this over to the Slony list)

-- 
Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.



From karl at denninger.net  Thu Sep 23 06:20:37 2010
From: karl at denninger.net (Karl Denninger)
Date: Thu, 23 Sep 2010 08:20:37 -0500
Subject: [Slony1-general] [GENERAL] One warning on migration from 8.4
 --> 9.0
In-Reply-To: <4C9B4E71.4010108@ca.afilias.info>
References: <4C9A98C2.3030905@denninger.net> <4C9B4E71.4010108@ca.afilias.info>
Message-ID: <4C9B5425.3060407@denninger.net>

 On 9/23/2010 7:56 AM, Brad Nicholson wrote:
>   On 10-09-22 08:01 PM, Karl Denninger wrote:
>>   If you use Slony, expect it to lose the replication status.
>>
>> I attempted the following:
>>
>> 1. Master and slaves on 8.4.
>>
>> 2. Upgrade one slave to 9.0.   Shut it down, used pg_upgrade to perform
>> the upgrade.
>>
>> 3. Restarted the slave.
>>
>> Slony appeared to come up, but said it was syncing only TWO tables (out
>> of the 33 in the working set!)  Of course that didn't work very well at
>> all.....
>>
>> Dropping the replication set and re-adding it appears to be fine, but of
>> course that caused a full database copy.  Not too cool.
>>
>> I have no idea why it only thought there were two tables in the
>> replication set.  Very, very odd stuff....
>>
>> Something to be aware of - I haven't figured out why it did this, nor do
>> I know if it will do the same thing to me when I attempt to upgrade the
>> master to 9.0 - that's something I won't attempt until the weekend at
>> the earliest.
>>
>> Other than that running with 8.4 for the master and 9.0 for the slaves
>> appears to be ok.
> What version of Slony was this?
>
> (Note I've moved this over to the Slony list)
2.0.4 - current stuff.

As I noted, I have NOT yet attempted to upgrade the master, although I
likely will this weekend.

This is a fairly serious issue if you have big databases under
replication!  In my case it caused a ~40gb database to have to be
resynced - a pain in the butt, but not catastrophic.  If it had bit one
of my TBish sized ones I'd be QUITE unhappy....

-- Karl
-------------- next part --------------
A non-text attachment was scrubbed...
Name: karl.vcf
Type: text/x-vcard
Size: 180 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20100923/c4e4cf2c/attachment.vcf 

From vivek at khera.org  Thu Sep 23 06:35:11 2010
From: vivek at khera.org (Vick Khera)
Date: Thu, 23 Sep 2010 09:35:11 -0400
Subject: [Slony1-general] [GENERAL] One warning on migration from 8.4
	--> 9.0
In-Reply-To: <4C9B5425.3060407@denninger.net>
References: <4C9A98C2.3030905@denninger.net> <4C9B4E71.4010108@ca.afilias.info>
	<4C9B5425.3060407@denninger.net>
Message-ID: <AANLkTinoKQjPxK7bu6Jo4uE6f=7dN_122aHgTBtFh9Nj@mail.gmail.com>

Did you attempt to run the slony repair config? Just guessing...

From ssinger at ca.afilias.info  Thu Sep 23 06:46:00 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 23 Sep 2010 09:46:00 -0400
Subject: [Slony1-general] [GENERAL] One warning on migration from 8.4
 --> 9.0
In-Reply-To: <4C9B5425.3060407@denninger.net>
References: <4C9A98C2.3030905@denninger.net> <4C9B4E71.4010108@ca.afilias.info>
	<4C9B5425.3060407@denninger.net>
Message-ID: <4C9B5A18.9080502@ca.afilias.info>

On 10-09-23 09:20 AM, Karl Denninger wrote:
>   On 9/23/2010 7:56 AM, Brad Nicholson wrote:
>>    On 10-09-22 08:01 PM, Karl Denninger wrote:
>>>    If you use Slony, expect it to lose the replication status.
>>>
>>> I attempted the following:
>>>
>>> 1. Master and slaves on 8.4.
>>>
>>> 2. Upgrade one slave to 9.0.   Shut it down, used pg_upgrade to perform
>>> the upgrade.
>>>
>>> 3. Restarted the slave.
>>>
>>> Slony appeared to come up, but said it was syncing only TWO tables (out
>>> of the 33 in the working set!)  Of course that didn't work very well at
>>> all.....
>>>


A few things to confirm

1. You were running 2.0.4 before the upgrade as well
2. The slony binaries your running with on 9.0 were compiled against
9.0.  In particular the slony shared library
3.  If you do select * from _clustername.sl_table and compare the 
tab_reloid values with what is in pg_class do they match?
Ie

select c.relname, t.tab_relname FROM pg_class c, _clustername.sl_table t 
where c.oid=t.tab_reloid

you should see matches if not you need to run REPAIR CONFIG on the slave 
(event node= slave id, only on set to slave id)


>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From karl at denninger.net  Thu Sep 23 07:06:08 2010
From: karl at denninger.net (Karl Denninger)
Date: Thu, 23 Sep 2010 09:06:08 -0500
Subject: [Slony1-general] [GENERAL] One warning on migration from 8.4
 --> 9.0
In-Reply-To: <4C9B5A18.9080502@ca.afilias.info>
References: <4C9A98C2.3030905@denninger.net> <4C9B4E71.4010108@ca.afilias.info>
	<4C9B5425.3060407@denninger.net> <4C9B5A18.9080502@ca.afilias.info>
Message-ID: <4C9B5ED0.5080305@denninger.net>

 On 9/23/2010 8:46 AM, Steve Singer wrote:
> On 10-09-23 09:20 AM, Karl Denninger wrote:
>>   On 9/23/2010 7:56 AM, Brad Nicholson wrote:
>>>    On 10-09-22 08:01 PM, Karl Denninger wrote:
>>>>    If you use Slony, expect it to lose the replication status.
>>>>
>>>> I attempted the following:
>>>>
>>>> 1. Master and slaves on 8.4.
>>>>
>>>> 2. Upgrade one slave to 9.0.   Shut it down, used pg_upgrade to
>>>> perform
>>>> the upgrade.
>>>>
>>>> 3. Restarted the slave.
>>>>
>>>> Slony appeared to come up, but said it was syncing only TWO tables
>>>> (out
>>>> of the 33 in the working set!)  Of course that didn't work very
>>>> well at
>>>> all.....
>>>>
>
> A few things to confirm
>
> 1. You were running 2.0.4 before the upgrade as well
Yes.
> 2. The slony binaries your running with on 9.0 were compiled against
> 9.0.  In particular the slony shared library
Yes.
> 3.  If you do select * from _clustername.sl_table and compare the
> tab_reloid values with what is in pg_class do they match?
Too late.  I dropped the slave and re-inserted it, and it has re-synced.
> Ie
>
> select c.relname, t.tab_relname FROM pg_class c, _clustername.sl_table
> t where c.oid=t.tab_reloid
>
> you should see matches if not you need to run REPAIR CONFIG on the
> slave (event node= slave id, only on set to slave id)

-- Karl
-------------- next part --------------
A non-text attachment was scrubbed...
Name: karl.vcf
Type: text/x-vcard
Size: 180 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20100923/cd9ee07a/attachment.vcf 

From guy.helmer at palisadesystems.com  Thu Sep 23 07:11:22 2010
From: guy.helmer at palisadesystems.com (Guy Helmer)
Date: Thu, 23 Sep 2010 09:11:22 -0500
Subject: [Slony1-general] Rapid-fire updates to table missed by slony
In-Reply-To: <C80821A7-A3D3-4D97-AAD5-2117393E7752@palisadesystems.com>
References: <6B66C950-980F-4608-844D-1A02A95C92CB@palisadesystems.com>
	<4C72CF3D.2000007@ca.afilias.info>
	<C80821A7-A3D3-4D97-AAD5-2117393E7752@palisadesystems.com>
Message-ID: <2491C359-80E9-4F3B-9256-417E83E60A39@palisadesystems.com>

On Aug 26, 2010, at 8:11 AM, Guy Helmer wrote:

> On Aug 23, 2010, at 2:42 PM, Steve Singer wrote:
> 
>> Guy Helmer wrote:
>>> I'm seeing something odd occasionally on a fairly new slony1 (1.2.20) replication set involving one slave.  At times, the application inserts a record to a particular table, updates the record several times, and then deletes the record, sometimes in a fairly quick succession (but not always).
>>> When I run the test-slony-state script, sometimes I find that the replication is failing, and when I look deeper, I find that Slony is having trouble replicating the changes to this table because of rows in the slave table that shouldn't be there.  After I manually remove the conflicting rows, Slony is then able to finish the backlogged replication.
>>> Is there anything in particular I should look for in the log file prior to this problem?
>> 
>> 
>> Shortly after the problem happens your going to want to look at sl_log_1  sl_log_2 and sl_event to figure out what was going on.
>> 
>> You want to find the what sync the delete should have been part of, and what sync the failing insert was part of and try to figure out why the delete wasn't applied to the slave by the time it tried the insert.
>> 
>> You would also want to look at the logs slon generates to see if that sync did get applied and look in sl_confirm to verify that.
>> 
>> 
>> Honestly I am somewhat suspect that something else isn't going on I find your description somewhat hard reconcile with how things work.
>> 
> 
> Thanks for the advice.  It has happened again.  Due to the timing of the issue corresponding somewhat closely with a software update where we took the database & slony down for the maintenance, I am wondering if we might be taking things down in incorrect order...

Just to wrap this thread up cleanly, I found there was a TRUNCATE command issued in our PHP code on the table in question.  I had not expected that, so I didn't look for it until today.

Thanks everyone for the help and suggestions!

Guy--------
This message has been scanned by ComplianceSafe, powered by Palisade's PacketSure.

From bpineau at elma.fr  Thu Sep 23 07:27:06 2010
From: bpineau at elma.fr (Benjamin Pineau)
Date: Thu, 23 Sep 2010 16:27:06 +0200
Subject: [Slony1-general] [GENERAL] One warning on migration from
	8.4	--> 9.0
In-Reply-To: <4C9B5A18.9080502@ca.afilias.info>
References: <4C9A98C2.3030905@denninger.net> <4C9B4E71.4010108@ca.afilias.info>
	<4C9B5425.3060407@denninger.net> <4C9B5A18.9080502@ca.afilias.info>
Message-ID: <20100923142706.GA29105@mailer.elma.fr>

Hi,

On Thu, Sep 23, 2010 at 09:46:00AM -0400, Steve Singer wrote:
> 
> 1. You were running 2.0.4 before the upgrade as well
> 2. The slony binaries your running with on 9.0 were compiled against
> 9.0.  In particular the slony shared library

By the way, is Slony 2.0.4 ready for/working with PostgreSQL 9.0 as is?

I noticed there's no "src/backend/slony1_funcs.v90.sql" and
"slony1_base.v90.sql" yet, even on git master's head ; but this should
be mostly harmless, right?

(sorry for sneacking in this unrelated thread - I'm also looking at
migrations options, but preferably using slony ;).


From dmenichelli at coresecurity.com  Thu Sep 23 10:35:47 2010
From: dmenichelli at coresecurity.com (dario)
Date: Thu, 23 Sep 2010 14:35:47 -0300
Subject: [Slony1-general] Log Question
Message-ID: <4C9B8FF3.8000007@coresecurity.com>

Hi all,

Slony is working well but when i tail the log, the same entries shows up every second:


2010-09-23 14:34:59 ART DEBUG2 syncThread: new sl_action_seq 1 - SYNC 4501018
2010-09-23 14:35:00 ART DEBUG2 localListenThread: Received event 2,4501018 SYNC
2010-09-23 14:35:00 ART DEBUG2 remoteListenThread_1: queue event 1,8268464 SYNC
2010-09-23 14:35:00 ART DEBUG2 remoteWorkerThread_1: Received event 1,8268464 SYNC
2010-09-23 14:35:00 ART DEBUG3 calc sync size - last time: 1 last length: 10104 ideal: 5 proposed size: 3
2010-09-23 14:35:00 ART DEBUG2 remoteWorkerThread_1: SYNC 8268464 processing
2010-09-23 14:35:00 ART DEBUG2 remoteWorkerThread_1: syncing set 1 with 60 table(s) from provider 1
2010-09-23 14:35:00 ART DEBUG4  ssy_action_list value:
2010-09-23 14:35:00 ART DEBUG2  ssy_action_list length: 0
2010-09-23 14:35:00 ART DEBUG2 remoteWorkerThread_1: current local log_status is 2
2010-09-23 14:35:00 ART DEBUG3 remoteWorkerThread_1: activate helper 1
2010-09-23 14:35:00 ART DEBUG4 remoteWorkerThread_1: waiting for log data
2010-09-23 14:35:00 ART DEBUG4 remoteHelperThread_1_1: got work to do
2010-09-23 14:35:00 ART DEBUG2 remoteWorkerThread_1_1: current remote log_status = 3
2010-09-23 14:35:00 ART DEBUG4 remoteHelperThread_1_1: allocate line buffers
2010-09-23 14:35:00 ART DEBUG4 remoteHelperThread_1_1: fetch from cursor
2010-09-23 14:35:04 ART DEBUG2 remoteHelperThread_1_1: 3.736 seconds delay for first row
2010-09-23 14:35:04 ART DEBUG4 remoteHelperThread_1_1: fetched 0 log rows
2010-09-23 14:35:04 ART DEBUG4 remoteHelperThread_1_1: return 10 unused line buffers
2010-09-23 14:35:04 ART DEBUG2 remoteHelperThread_1_1: 3.737 seconds until close cursor
2010-09-23 14:35:04 ART DEBUG2 remoteHelperThread_1_1: inserts=0 updates=0 deletes=0
2010-09-23 14:35:04 ART DEBUG4 remoteHelperThread_1_1: change helper thread status
2010-09-23 14:35:04 ART DEBUG4 remoteHelperThread_1_1: send DONE/ERROR line to worker
2010-09-23 14:35:04 ART DEBUG3 remoteWorkerThread_1: helper 1 finished
2010-09-23 14:35:04 ART DEBUG4 remoteWorkerThread_1: returning lines to pool
2010-09-23 14:35:04 ART DEBUG3 remoteWorkerThread_1: all helpers done.
2010-09-23 14:35:04 ART DEBUG4 remoteWorkerThread_1: changing helper 1 to IDLE
2010-09-23 14:35:04 ART DEBUG4 remoteWorkerThread_1: cleanup
2010-09-23 14:35:04 ART DEBUG4 remoteHelperThread_1_1: waiting for work
2010-09-23 14:35:04 ART DEBUG2 remoteWorkerThread_1: new sl_rowid_seq value: 1000000000000000
2010-09-23 14:35:04 ART DEBUG2 remoteWorkerThread_1: SYNC 8268464 done in 3.787 seconds
2010-09-23 14:35:04 ART DEBUG2 remoteWorkerThread_1: forward confirm 2,4501018 received by 1


Are they ok?? i have started to doubt becuase the process that handles the connection between the servers (primary and secondary) is using 30/40% of the
main cpu.(it was always around 10% of cpu use)

Thanks in advance,
dario

From ssinger at ca.afilias.info  Thu Sep 23 10:52:25 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Thu, 23 Sep 2010 13:52:25 -0400
Subject: [Slony1-general] [GENERAL] One warning on migration from 8.4
 --> 9.0
In-Reply-To: <20100923142706.GA29105@mailer.elma.fr>
References: <4C9A98C2.3030905@denninger.net> <4C9B4E71.4010108@ca.afilias.info>
	<4C9B5425.3060407@denninger.net> <4C9B5A18.9080502@ca.afilias.info>
	<20100923142706.GA29105@mailer.elma.fr>
Message-ID: <4C9B93D9.9040505@ca.afilias.info>

On 10-09-23 10:27 AM, Benjamin Pineau wrote:
> Hi,
>
> On Thu, Sep 23, 2010 at 09:46:00AM -0400, Steve Singer wrote:
>>
>> 1. You were running 2.0.4 before the upgrade as well
>> 2. The slony binaries your running with on 9.0 were compiled against
>> 9.0.  In particular the slony shared library
>
> By the way, is Slony 2.0.4 ready for/working with PostgreSQL 9.0 as is?
>
> I noticed there's no "src/backend/slony1_funcs.v90.sql" and
> "slony1_base.v90.sql" yet, even on git master's head ; but this should
> be mostly harmless, right?
>
> (sorry for sneacking in this unrelated thread - I'm also looking at
> migrations options, but preferably using slony ;).
>

2.0.4 should work with 9.0.  A decent amount of the testing I did in 
getting ready for 2.0.5 was against beta version of 9.0 (and I don't see 
any fixes for 9.0 stuff that went into 2.0.5 above what was already in 
2.0.4).  The 8.4 version of the backend functions seem to work okay
on 9.0.

But please report any problems you encounter.

From brianf at consistentstate.com  Thu Sep 23 14:09:05 2010
From: brianf at consistentstate.com (Brian Fehrle)
Date: Thu, 23 Sep 2010 15:09:05 -0600
Subject: [Slony1-general] slony not replicating after re-initializing
 the slave cluster
In-Reply-To: <4C9A5580.9010609@ca.afilias.info>
References: <4C92A9D9.2030109@consistentstate.com>	<BLU0-SMTP83780E6A82D2A7D1454515AC7A0@phx.gbl>
	<4C993E88.8010408@consistentstate.com>
	<4C9A5580.9010609@ca.afilias.info>
Message-ID: <4C9BC1F1.4010005@consistentstate.com>

Steve Singer wrote:
> On 10-09-21 07:23 PM, Brian Fehrle wrote:
>> I got some time and decided to test this again on some VM boxes rather
>> than our live environment, but had little luck.
>>
>> Simply so I can have this logged in the mailing list with what was done
>> (and hopefully a solution in the near future), here's my process I
>> preformed.
>>
>> I created two clusters that mirror our live boxes as closely as 
>> possible.
>> - - PostgreSQL version 8.4.2
>> - - Slony version 1.2.20
>> - - both installed via source
>>
>> I created the master cluster as:
>> # initdb -D /usr/local/pgsql/encoding_master/ --locale=C 
>> --encoding=LATIN1
>> I created the slave cluster as:
>> # initdb -D /usr/local/pgsql/encoding_slave/ --locale=C 
>> --encoding=SQL_ASCII
>>
>> I set up a master ->  slave slony cluster and replicated a single table
>> in a single replication set, and verified that replication was taking 
>> place.
>>
>> I wrote a small daemon that inserts a row into the table being
>> replicated on the master once a minute.
>>
>> I brought down the slon daemons, and preformed a pg_dump on the slave:
>> # pg_dump -p 5433 -Fc postgres>  /tmp/postgres_dump.sql
>>
>> I brought down the slave cluster, then created a new one with the LATIN1
>> encoding:
>> # initdb -D /usr/local/pgsql/encoding_slave_latin/ --locale=C
>> --encoding=SQL_ASCII
>>
>> I brought the cluster online and started up the slon daemons. The slave
>> slon daemon reported remoteworker and remote listener threads, and
>> reported increasing SYNC numbers, however did not actually replicate
>> data from the master to the slave, and _slony.sl_log_1 on the master
>> grew in numbers with every insert that took place . NOTE: This is the
>> same behavior I experienced before on our live servers.
>>
>> I then executed the following:
>> #!/bin/bash
>> . etc/slony.env
>> echo "Repair config"
>>
>> slonik<<_EOF_
>> cluster name = $CLUSTERNAME ;
>> node 1 admin conninfo = 'dbname=$MASTERDBNAME host=$MASTERHOST
>> port=$MASTERPORT user=$REPUSER';
>> node 2 admin conninfo = 'dbname=$SLAVEDBNAME host=$SLAVEHOST
>> port=$SLAVEPORT user=$REPUSER';
>> REPAIR CONFIG (SET ID = 1, EVENT NODE = 1, EXECUTE ONLY ON = 2);
>> _EOF_
>>
>
> Try
>
> REPAIR CONFIG (SET ID=1, EVENT NODE=2, EXECUTE ONLY ON=2);
>
> I tried a somewhat simliar sequence to what you described (though with 
> a different postgresql and slony version) and the REPAIR CONFIG did 
> not seem to do anything on node 2.  Ie the oid values in sl_table did 
> NOT match what was in pg_class.  When I ran it with event node=2 then 
> it did seem to update sl_table on node 2.
>
Ok, so I tried this and while it did update the row in sl_table to point 
to the correct oid, the slave's slon daemon kills itself and no 
replication takes place. Starting the daemon again results in another 
untimely death. Result from the log when executing REPAIR CONFIG (SET 
ID=1, EVENT NODE=2, EXECUTE ONLY ON=2):

2010-09-23 12:36:55 MDT FATAL  localListenThread: event 31: Unknown 
event type: RESET_CONFIG

Out of just wanting to try EVERYTHING, I set up everything on the new 
encoding server, however instead of running the repair config, and 
before starting up the slon daemons, I manually updated the row in 
sl_table and set the tab_reloid column to the oid of the table in the 
post-pg_recover cluster.

I started up the slon daemons, replication works without any 
warnings/errors, and the daemons stay alive. Any new data inserted into 
that table gets replicated.

I also turned on the query logging in the slave's database and set it to 
log all queries, and monitored them, i see the copy statements that copy 
the data from the master to the slave, and it all matches the data 
received and the data in the log shipping logs that the slave daemon 
generates via the -x command.

So I'm wondering, is updating sl_table myself like that safe? I know 
it's highly discouraged to modify anything in the slony system tables 
myself, but since the repair config command doesn't seem to be working 
for me, not sure if I have another option.

- Brian

>
>
>
>> it executed without error, however replication did not start working,
>> and the slave daemon started acting weird with the child process being
>> terminated constantly, then restarted every 10 seconds just to be
>> terminated again.


From vivek at khera.org  Fri Sep 24 09:35:23 2010
From: vivek at khera.org (Vick Khera)
Date: Fri, 24 Sep 2010 12:35:23 -0400
Subject: [Slony1-general] [GENERAL] One warning on migration from 8.4
	--> 9.0
In-Reply-To: <4C9B93D9.9040505@ca.afilias.info>
References: <4C9A98C2.3030905@denninger.net> <4C9B4E71.4010108@ca.afilias.info>
	<4C9B5425.3060407@denninger.net> <4C9B5A18.9080502@ca.afilias.info>
	<20100923142706.GA29105@mailer.elma.fr>
	<4C9B93D9.9040505@ca.afilias.info>
Message-ID: <AANLkTikiRfvy9gGqnFxVpEMV1OVh9wyn7y0V1AHQ5v8p@mail.gmail.com>

On Thu, Sep 23, 2010 at 1:52 PM, Steve Singer <ssinger at ca.afilias.info> wrote:
> 2.0.4 should work with 9.0. ?A decent amount of the testing I did in
> getting ready for 2.0.5 was against beta version of 9.0 (and I don't see
> any fixes for 9.0 stuff that went into 2.0.5 above what was already in
> 2.0.4). ?The 8.4 version of the backend functions seem to work okay
> on 9.0.
>
> But please report any problems you encounter.

Earlier today I was looking into moving some test servers to 9.0 to
validate my app against it.  I was reading the slony docs and nowhere
in the system requirements page does it say what versions of postgres
are supported.  Seems to be a rather important detail :)  I gleaned
from the recent history items on the slony.info page that 1.2.21 does
support 9.0, which was sufficient for my needs.

Also, the docs indicate that slony is tested against FreeBSD versions
4, 5, and 6.  All of these are EOL.  I personally test and run it on
FreeBSD 7 and 8, and probably not 7 for much longer.

From atsaloli.tech at gmail.com  Fri Sep 24 15:06:32 2010
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Fri, 24 Sep 2010 15:06:32 -0700
Subject: [Slony1-general] ERROR: cache lookup failed for type 14237017
In-Reply-To: <4C9A257D.6000306@Yahoo.com>
References: <AANLkTimH+9LFigvP0juv6TZpEB6=uWQYbuVrkW8vTFpG@mail.gmail.com>
	<4C9914E6.2040604@ca.afilias.info>
	<AANLkTink+S4q37MXMfWOkFwd-AmFEHyPt5JKLB9LZD-i@mail.gmail.com>
	<AANLkTinE-yr9htnvu5kpn6m2e1UinHR8SOV3HG_x4v=u@mail.gmail.com>
	<4C9A257D.6000306@Yahoo.com>
Message-ID: <AANLkTin-oSMYrKenR9nv7CDNTqrWJsPjpdszdoBemHHB@mail.gmail.com>

Thanks Steve, Christopher and Jan for your replies.  I read the FAQ and will
make sure to stop our application next time we drop the Slony cluster.

I might suggest briefly mentioning in the Best Practices section of
the documentation
a reference to this issue.

Thanks again!

Best regards,
Aleksey

From ssinger at ca.afilias.info  Fri Sep 24 16:19:44 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 24 Sep 2010 19:19:44 -0400
Subject: [Slony1-general] How to disable PostgreSQL autovacuum of Slony
 tables? (getting error: column "_" does not exist)
In-Reply-To: <AANLkTimOg-RZhp04+bfPoq4tqmW8=uNaYLGMAdMDCrgf@mail.gmail.com>
References: <AANLkTin1kcpAagyor_vDyGYM3ynXdz+=o_fuh4X8-bKT@mail.gmail.com>	<AANLkTi=kTuE_sA69b296hJKNKzy2fR09psKB8hdxQNDw@mail.gmail.com>	<4C97B569.5090304@ca.afilias.info>
	<AANLkTimOg-RZhp04+bfPoq4tqmW8=uNaYLGMAdMDCrgf@mail.gmail.com>
Message-ID: <4C9D3210.4010604@ca.afilias.info>

On 10-09-21 05:53 PM, Aleksey Tsalolikhin wrote:

>
> Thank you.   I see how to control autovacuuming using ALTER TABLE.
> So Vick and Brad both let Postgres autovacuum the Slony tables.

Patch applied.
Thanks


>
> I propose a patch to http://www.slony.info/documentation/maintenance.html :
>
> Change section title from
> "6.1. Interaction with PostgreSQL autovacuum"
> to
> "6.1. Interaction with PostgreSQL 8.1 and 8.2 autovacuum"
>
> Change
> "The following query (change the cluster name to match your local
> configuration) will identify the tables that autovacuum should be
> configured not to process"
> to
> "The following query (change the cluster name to match your local
> configuration) on Postgres 8 versions up to and not including 8.4 will
> identify the tables that autovacuum should be configured not to
> process"
>
> Change
> "The following query will populate pg_catalog.pg_autovacuum with
> suitable configuration information:  "
> to
> "The following query (for Postgres versions up to and not including
> 8.4) will populate pg_catalog.pg_autovacuum with suitable
> configuration information:  "
>
> Best,
> -at
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From ssinger at ca.afilias.info  Fri Sep 24 16:21:05 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 24 Sep 2010 19:21:05 -0400
Subject: [Slony1-general] ERROR: cache lookup failed for type 14237017
In-Reply-To: <AANLkTin-oSMYrKenR9nv7CDNTqrWJsPjpdszdoBemHHB@mail.gmail.com>
References: <AANLkTimH+9LFigvP0juv6TZpEB6=uWQYbuVrkW8vTFpG@mail.gmail.com>	<4C9914E6.2040604@ca.afilias.info>	<AANLkTink+S4q37MXMfWOkFwd-AmFEHyPt5JKLB9LZD-i@mail.gmail.com>	<AANLkTinE-yr9htnvu5kpn6m2e1UinHR8SOV3HG_x4v=u@mail.gmail.com>	<4C9A257D.6000306@Yahoo.com>
	<AANLkTin-oSMYrKenR9nv7CDNTqrWJsPjpdszdoBemHHB@mail.gmail.com>
Message-ID: <4C9D3261.20701@ca.afilias.info>

On 10-09-24 06:06 PM, Aleksey Tsalolikhin wrote:
> Thanks Steve, Christopher and Jan for your replies.  I read the FAQ and will
> make sure to stop our application next time we drop the Slony cluster.
>
> I might suggest briefly mentioning in the Best Practices section of
> the documentation
> a reference to this issue.
>
> Thanks again!
>
> Best regards,
> Aleksey
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

Do you think it be better to mention that in best practices or the 
section on uninstalling/removing slony?



From atsaloli.tech at gmail.com  Sat Sep 25 23:22:29 2010
From: atsaloli.tech at gmail.com (Aleksey Tsalolikhin)
Date: Sat, 25 Sep 2010 23:22:29 -0700
Subject: [Slony1-general] ERROR: cache lookup failed for type 14237017
In-Reply-To: <4C9D3261.20701@ca.afilias.info>
References: <AANLkTimH+9LFigvP0juv6TZpEB6=uWQYbuVrkW8vTFpG@mail.gmail.com>
	<4C9914E6.2040604@ca.afilias.info>
	<AANLkTink+S4q37MXMfWOkFwd-AmFEHyPt5JKLB9LZD-i@mail.gmail.com>
	<AANLkTinE-yr9htnvu5kpn6m2e1UinHR8SOV3HG_x4v=u@mail.gmail.com>
	<4C9A257D.6000306@Yahoo.com>
	<AANLkTin-oSMYrKenR9nv7CDNTqrWJsPjpdszdoBemHHB@mail.gmail.com>
	<4C9D3261.20701@ca.afilias.info>
Message-ID: <AANLkTikt0sboiZW=dgytHF_Ot-UmREJ7xnohp_NuRYOz@mail.gmail.com>

On Fri, Sep 24, 2010 at 4:21 PM, Steve Singer <ssinger at ca.afilias.info> wrote:
> On 10-09-24 06:06 PM, Aleksey Tsalolikhin wrote:
>>
>>?I read the FAQ and will make sure to stop our application next time we drop the Slony cluster.
>>
>> I might suggest briefly mentioning in the Best Practices section of
>> the documentation
>> a reference to this issue.
>
> Do you think it be better to mention that in best practices or the section
> on uninstalling/removing slony?

Both.  :)    This is really the sort of thing one does not want to
miss / find out the hard way.

Truly,
Aleksey

From karl at flightaware.com  Sun Sep 26 14:51:47 2010
From: karl at flightaware.com (Karl Lehenbauer)
Date: Sun, 26 Sep 2010 21:51:47 +0000
Subject: [Slony1-general] Slony1-general Digest, Vol 43, Issue 18
In-Reply-To: <mailman.0.1285527601.26084.slony1-general@lists.slony.info>
References: <mailman.0.1285527601.26084.slony1-general@lists.slony.info>
Message-ID: <57A56740-72D0-470A-B248-02077337F8E1@flightaware.com>

Lenny,

We had a library version incompatibility on one of the mapservers that caused all the requests it attempted to handle to fail.  We rebuilt and reinstalled the (seemingly) messed up one and it appears to be working properly now.  Could you see if the error rate has dropped to (near) zero?

Thanks in advance and thanks for the vigilance?

Karl

On Sep 26, 2010, at 2:00 PM, <slony1-general-request at lists.slony.info<mailto:slony1-general-request at lists.slony.info>>
 wrote:

Send Slony1-general mailing list submissions to
slony1-general at lists.slony.info<mailto:slony1-general at lists.slony.info>

To subscribe or unsubscribe via the World Wide Web, visit
http://lists.slony.info/mailman/listinfo/slony1-general
or, via email, send a message with subject or body 'help' to
slony1-general-request at lists.slony.info

You can reach the person managing the list at
slony1-general-owner at lists.slony.info

When replying, please edit your Subject line so it is more specific
than "Re: Contents of Slony1-general digest..."
Today's Topics:

  1. Re: ERROR: cache lookup failed for type 14237017
     (Aleksey Tsalolikhin)

From: Aleksey Tsalolikhin <atsaloli.tech at gmail.com>
Date: September 26, 2010 1:22:29 AM CDT
To: <slony1-general at lists.slony.info>
Subject: Re: [Slony1-general] ERROR: cache lookup failed for type 14237017


On Fri, Sep 24, 2010 at 4:21 PM, Steve Singer <ssinger at ca.afilias.info> wrote:
On 10-09-24 06:06 PM, Aleksey Tsalolikhin wrote:

 I read the FAQ and will make sure to stop our application next time we drop the Slony cluster.

I might suggest briefly mentioning in the Best Practices section of
the documentation
a reference to this issue.

Do you think it be better to mention that in best practices or the section
on uninstalling/removing slony?

Both.  :)    This is really the sort of thing one does not want to
miss / find out the hard way.

Truly,
Aleksey



_______________________________________________
Slony1-general mailing list
Slony1-general at lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20100926/66986dc8/attachment.htm 

From ssinger at ca.afilias.info  Tue Sep 28 06:19:55 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 28 Sep 2010 09:19:55 -0400
Subject: [Slony1-general] Admin Guide Updates
Message-ID: <4CA1EB7B.5000000@ca.afilias.info>

I've been working on some updates to the administration guide. Mostly 
organizational in nature but I've rewritted and added a few sections.

I also have tried to remove the documentation from older versions that 
no longer applies to 2.0.x.   Most people I've spoken to feel that this 
was making the documentation too confusing and that you are better off 
using the slony documentation for the version you are using.

You can view a preview here http://www.slony.info/~ssinger/slony2.0.5/

with my changes available 
http://github.com/ssinger/slony1-engine/tree/adminguide_rework

If no one objects I will merge these changes into REL_2_0_STABLE later 
this week so they are included with release 2.0.5


