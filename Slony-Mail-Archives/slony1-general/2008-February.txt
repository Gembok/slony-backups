From cedric.villemain at dalibo.com  Fri Feb  1 03:21:01 2008
From: cedric.villemain at dalibo.com (=?ISO-8859-1?Q?C=E9dric_Villemain?=)
Date: Fri Feb  1 03:22:18 2008
Subject: [Slony1-general] Slony Nagios plugins  - sl_status
In-Reply-To: <905236.39398.qm@web25801.mail.ukl.yahoo.com>
References: <905236.39398.qm@web25801.mail.ukl.yahoo.com>
Message-ID: <47A3009D.5030808@dalibo.com>

Glyn Astill a ?crit :
> Hi C?dric,
>
>
> --- C?dric Villemain <cedric.villemain@dalibo.com> wrote:
>
>   
>> I have just look at it a quick view ... you would use the common 
>> postgresql command line options ....
>>
>>     
>
> You mean I should use the postgresql command line options? ?? I'm not
> sure what you mean here
>   

the command line options for psql. Like
-U user -d dbname ....

>   
>> Else, I suggest you to add a table to your set, like
>> 'last_update_set_X' 
>> then update it every 1 second for ex.
>> And check the last update on all nodes, you will have a real mesure
>> of 
>> the effective lag.
>>     
>
> Yes I agree with you, I will do this and I'll use the script that
> comes with slony when I get around to it.
>
>   
>> I prefer (or also) check replication outside system provided by the
>>
>> replication engine itself ....
>>
>>     
>
> Not sure what you mean?
>   
I mean that I prefer check that the replication is ok with the above table.
I do also with sl_* but then you trust the info provided by the slony 
himself.

It is like ask a banker to check himself if is paying taxes correctly  =)
>
>       __________________________________________________________
> Sent from Yahoo! Mail - a smarter inbox http://uk.mail.yahoo.com
>
>
>   


-- 
C?dric Villemain
Administrateur de Base de Donn?es
Cel: +33 (0)6 74 15 56 53
http://dalibo.com - http://dalibo.org

From glynastill at yahoo.co.uk  Fri Feb  1 09:35:11 2008
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Fri Feb  1 09:35:19 2008
Subject: [Slony1-general] Slony Nagios plugins  - sl_status
In-Reply-To: <47A3009D.5030808@dalibo.com>
Message-ID: <950710.1130.qm@web25808.mail.ukl.yahoo.com>

Aha, I get you C?dric

Thanks for the pointers.

Glyn

--- C?dric Villemain <cedric.villemain@dalibo.com> wrote:

> Glyn Astill a ?crit :
> > Hi C?dric,
> >
> >
> > --- C?dric Villemain <cedric.villemain@dalibo.com> wrote:
> >
> >   
> >> I have just look at it a quick view ... you would use the common
> 
> >> postgresql command line options ....
> >>
> >>     
> >
> > You mean I should use the postgresql command line options? ?? I'm
> not
> > sure what you mean here
> >   
> 
> the command line options for psql. Like
> -U user -d dbname ....
> 
> >   
> >> Else, I suggest you to add a table to your set, like
> >> 'last_update_set_X' 
> >> then update it every 1 second for ex.
> >> And check the last update on all nodes, you will have a real
> mesure
> >> of 
> >> the effective lag.
> >>     
> >
> > Yes I agree with you, I will do this and I'll use the script that
> > comes with slony when I get around to it.
> >
> >   
> >> I prefer (or also) check replication outside system provided by
> the
> >>
> >> replication engine itself ....
> >>
> >>     
> >
> > Not sure what you mean?
> >   
> I mean that I prefer check that the replication is ok with the
> above table.
> I do also with sl_* but then you trust the info provided by the
> slony 
> himself.
> 
> It is like ask a banker to check himself if is paying taxes
> correctly  =)
> >
> >       __________________________________________________________
> > Sent from Yahoo! Mail - a smarter inbox http://uk.mail.yahoo.com
> >
> >
> >   
> 
> 
> -- 
> C?dric Villemain
> Administrateur de Base de Donn?es
> Cel: +33 (0)6 74 15 56 53
> http://dalibo.com - http://dalibo.org
> 
> 



      __________________________________________________________
Sent from Yahoo! Mail - a smarter inbox http://uk.mail.yahoo.com


From jc at praud.com  Sun Feb  3 10:05:59 2008
From: jc at praud.com (Jean-Christophe Praud)
Date: Sun Feb  3 10:06:10 2008
Subject: [Slony1-general] partial replication
Message-ID: <47A60287.8050508@praud.com>

Hi all,

I'm new to Slony, here's my newbie question :

I need to setup a cluster of separate servers, each handling a set of 
users data.
The architecture I'm planning is :

- 1 central server storing my whole users table (id, login, passwd, 
id_of_users_server)
- n users servers containing the part of the central users table
- a user authenticate on the central server, then the application uses 
the users server hosting this user

As each users server will contain a part of the central users table, 
modifications of each record of this table on the central server need to 
be replicated to the good users server.


I don't plan to use inherited partitions for the central users table (I 
need PK, FK and other constraints on this table). Can Slony be used to 
do the partial replication from central to each users server ?


-- 
Jean-Christophe Praud         -      http://shub-niggurath.com
Conseil & D?veloppement Informatique      http://www.praud.com
Ph'nglui mglw'nafh Cthulhu n'gah Bill R'lyeh Wgah'nagl fhtagn!

From drees76 at gmail.com  Mon Feb  4 16:54:27 2008
From: drees76 at gmail.com (David Rees)
Date: Mon Feb  4 16:54:38 2008
Subject: [Slony1-general] 1.2.13?
Message-ID: <72dbd3150802041654k7753c820vd7b9b286b1d8d910@mail.gmail.com>

Now that PostgreSQL 8.3.0 is released, will there be a Slony 1.2.13
since Slony 1.2.12 doesn't build against Pg 8.3?

I could pull from CVS, but just anxious to get some testing in on our
dev servers. :-)

-Dave
From cbbrowne at ca.afilias.info  Mon Feb  4 20:22:06 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Feb  4 20:22:21 2008
Subject: [Slony1-general] 1.2.13?
In-Reply-To: <72dbd3150802041654k7753c820vd7b9b286b1d8d910@mail.gmail.com>
	(David Rees's message of "Mon, 4 Feb 2008 16:54:27 -0800")
References: <72dbd3150802041654k7753c820vd7b9b286b1d8d910@mail.gmail.com>
Message-ID: <60ejbsav81.fsf@dba2.int.libertyrms.com>

"David Rees" <drees76@gmail.com> writes:
> Now that PostgreSQL 8.3.0 is released, will there be a Slony 1.2.13
> since Slony 1.2.12 doesn't build against Pg 8.3?
>
> I could pull from CVS, but just anxious to get some testing in on our
> dev servers. :-)

Evidently it's time for 1.2.13...  I'll see about it tomorrow
afternoon.
-- 
output = ("cbbrowne" "@" "acm.org")
http://linuxfinances.info/info/rdbms.html
"Consistency requires you to be as  ignorant today as  you were a year
ago."  -- Bernard Berenson
From devrim at CommandPrompt.com  Mon Feb  4 20:59:14 2008
From: devrim at CommandPrompt.com (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Mon Feb  4 20:59:42 2008
Subject: [Slony1-general] 1.2.13?
In-Reply-To: <60ejbsav81.fsf@dba2.int.libertyrms.com>
References: <72dbd3150802041654k7753c820vd7b9b286b1d8d910@mail.gmail.com>
	<60ejbsav81.fsf@dba2.int.libertyrms.com>
Message-ID: <1202187554.4042.45.camel@localhost.localdomain>

SGksCgpPbiBUdWUsIDIwMDgtMDItMDUgYXQgMDQ6MjIgKzAwMDAsIENocmlzdG9waGVyIEJyb3du
ZSB3cm90ZToKCj4gRXZpZGVudGx5IGl0J3MgdGltZSBmb3IgMS4yLjEzLi4uICBJJ2xsIHNlZSBh
Ym91dCBpdCB0b21vcnJvdwo+IGFmdGVybm9vbi4KCi9tZSB0aGlua3MgdGhhdCB3ZSBzaG91bGQg
cmVsZWFzZSAxLjIuMTMgKmJlZm9yZSogOC4zLjAgd2FzIG91dC4KClJlZ2FyZHMsCi0tIApEZXZy
aW0gR8OcTkTDnFogLCBSSENFClBvc3RncmVTUUwgUmVwbGljYXRpb24sIENvbnN1bHRpbmcsIEN1
c3RvbSBEZXZlbG9wbWVudCwgMjR4NyBzdXBwb3J0Ck1hbmFnZWQgU2VydmljZXMsIFNoYXJlZCBh
bmQgRGVkaWNhdGVkIEhvc3RpbmcKQ28tQXV0aG9yczogcGxQSFAsIE9EQkNuZyAtIGh0dHA6Ly93
d3cuY29tbWFuZHByb21wdC5jb20vCi0tLS0tLS0tLS0tLS0tIG5leHQgcGFydCAtLS0tLS0tLS0t
LS0tLQpBIG5vbi10ZXh0IGF0dGFjaG1lbnQgd2FzIHNjcnViYmVkLi4uCk5hbWU6IG5vdCBhdmFp
bGFibGUKVHlwZTogYXBwbGljYXRpb24vcGdwLXNpZ25hdHVyZQpTaXplOiAxODkgYnl0ZXMKRGVz
YzogVGhpcyBpcyBhIGRpZ2l0YWxseSBzaWduZWQgbWVzc2FnZSBwYXJ0ClVybCA6IGh0dHA6Ly9s
aXN0cy5zbG9ueS5pbmZvL3BpcGVybWFpbC9zbG9ueTEtZ2VuZXJhbC9hdHRhY2htZW50cy8yMDA4
MDIwNC85NTIzMTBmMC9hdHRhY2htZW50LnBncAo=
From david at fetter.org  Mon Feb  4 21:11:13 2008
From: david at fetter.org (David Fetter)
Date: Mon Feb  4 21:11:22 2008
Subject: [Slony1-general] 1.2.13?
In-Reply-To: <1202187554.4042.45.camel@localhost.localdomain>
References: <72dbd3150802041654k7753c820vd7b9b286b1d8d910@mail.gmail.com>
	<60ejbsav81.fsf@dba2.int.libertyrms.com>
	<1202187554.4042.45.camel@localhost.localdomain>
Message-ID: <20080205051113.GH21040@fetter.org>

On Mon, Feb 04, 2008 at 08:59:14PM -0800, Devrim G?ND?Z wrote:
> Hi,
> 
> On Tue, 2008-02-05 at 04:22 +0000, Christopher Browne wrote:
> 
> > Evidently it's time for 1.2.13...  I'll see about it tomorrow
> > afternoon.
> 
> /me thinks that we should release 1.2.13 *before* 8.3.0 was out.

We won't manage that without a time machine ;)

Cheers,
David (Mr. Fusion)
-- 
David Fetter <david@fetter.org> http://fetter.org/
Phone: +1 415 235 3778  AIM: dfetter666  Yahoo!: dfetter
Skype: davidfetter      XMPP: david.fetter@gmail.com

Remember to vote!
Consider donating to Postgres: http://www.postgresql.org/about/donate
From devrim at CommandPrompt.com  Mon Feb  4 21:14:25 2008
From: devrim at CommandPrompt.com (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Mon Feb  4 21:14:53 2008
Subject: [Slony1-general] 1.2.13?
In-Reply-To: <20080205051113.GH21040@fetter.org>
References: <72dbd3150802041654k7753c820vd7b9b286b1d8d910@mail.gmail.com>
	<60ejbsav81.fsf@dba2.int.libertyrms.com>
	<1202187554.4042.45.camel@localhost.localdomain>
	<20080205051113.GH21040@fetter.org>
Message-ID: <1202188465.4042.48.camel@localhost.localdomain>

SGksCgpPbiBNb24sIDIwMDgtMDItMDQgYXQgMjE6MTEgLTA4MDAsIERhdmlkIEZldHRlciB3cm90
ZToKPiA+IC9tZSB0aGlua3MgdGhhdCB3ZSBzaG91bGQgcmVsZWFzZSAxLjIuMTMgKmJlZm9yZSog
OC4zLjAgd2FzIG91dC4KPiAKPiBXZSB3b24ndCBtYW5hZ2UgdGhhdCB3aXRob3V0IGEgdGltZSBt
YWNoaW5lIDspCgo6KQoKc2hvdWxkIHJlbGVhc2UgLT4gc2hvdWxkIGhhdmUgcmVsZWFzZWQKCkkg
YWxzbyByYWlzZWQgdGhpcyBpZGVhIG9uICNzbG9ueSBvbmUgb3IgdHdvIHdlZWtzIGJlZm9yZS4g
QW55d2F5LCBhcwp5b3Ugc2FpZCwgd2UgZG9uJ3QgaGF2ZSBhIHRpbWUgbWFjaGluZSA6KQoKQ2hl
ZXJzLAotLSAKRGV2cmltIEfDnE5Ew5xaICwgUkhDRQpQb3N0Z3JlU1FMIFJlcGxpY2F0aW9uLCBD
b25zdWx0aW5nLCBDdXN0b20gRGV2ZWxvcG1lbnQsIDI0eDcgc3VwcG9ydApNYW5hZ2VkIFNlcnZp
Y2VzLCBTaGFyZWQgYW5kIERlZGljYXRlZCBIb3N0aW5nCkNvLUF1dGhvcnM6IHBsUEhQLCBPREJD
bmcgLSBodHRwOi8vd3d3LmNvbW1hbmRwcm9tcHQuY29tLwotLS0tLS0tLS0tLS0tLSBuZXh0IHBh
cnQgLS0tLS0tLS0tLS0tLS0KQSBub24tdGV4dCBhdHRhY2htZW50IHdhcyBzY3J1YmJlZC4uLgpO
YW1lOiBub3QgYXZhaWxhYmxlClR5cGU6IGFwcGxpY2F0aW9uL3BncC1zaWduYXR1cmUKU2l6ZTog
MTg5IGJ5dGVzCkRlc2M6IFRoaXMgaXMgYSBkaWdpdGFsbHkgc2lnbmVkIG1lc3NhZ2UgcGFydApV
cmwgOiBodHRwOi8vbGlzdHMuc2xvbnkuaW5mby9waXBlcm1haWwvc2xvbnkxLWdlbmVyYWwvYXR0
YWNobWVudHMvMjAwODAyMDQvMGQzNmU2YTIvYXR0YWNobWVudC5wZ3AK
From dpage at postgresql.org  Tue Feb  5 01:11:04 2008
From: dpage at postgresql.org (Dave Page)
Date: Tue Feb  5 01:11:25 2008
Subject: [Slony1-general] 1.2.13?
In-Reply-To: <60ejbsav81.fsf@dba2.int.libertyrms.com>
References: <72dbd3150802041654k7753c820vd7b9b286b1d8d910@mail.gmail.com>
	<60ejbsav81.fsf@dba2.int.libertyrms.com>
Message-ID: <937d27e10802050111x566a5679l53ae7ecec2018e4d@mail.gmail.com>

On Feb 5, 2008 4:22 AM, Christopher Browne <cbbrowne@ca.afilias.info> wrote:
> "David Rees" <drees76@gmail.com> writes:
> > Now that PostgreSQL 8.3.0 is released, will there be a Slony 1.2.13
> > since Slony 1.2.12 doesn't build against Pg 8.3?
> >
> > I could pull from CVS, but just anxious to get some testing in on our
> > dev servers. :-)
>
> Evidently it's time for 1.2.13...  I'll see about it tomorrow
> afternoon.

<blinks>But it does build doesn't it? We have binaries builds for
Windows downloadable through StackBuilder on pgFoundry:
http://pgfoundry.org/frs/?group_id=1000256, plus the builds of EDB
Postgres 8.3 on Mac, Linux and Win32 include Slony 1.2.12, built
against the GA server.

/D
From bnichols at ca.afilias.info  Tue Feb  5 06:57:20 2008
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Tue Feb  5 06:56:42 2008
Subject: [Slony1-general] 1.2.13?
In-Reply-To: <937d27e10802050111x566a5679l53ae7ecec2018e4d@mail.gmail.com>
References: <72dbd3150802041654k7753c820vd7b9b286b1d8d910@mail.gmail.com>
	<60ejbsav81.fsf@dba2.int.libertyrms.com>
	<937d27e10802050111x566a5679l53ae7ecec2018e4d@mail.gmail.com>
Message-ID: <1202223440.16171.188.camel@bnicholson-desktop>


On Tue, 2008-02-05 at 09:11 +0000, Dave Page wrote:
> On Feb 5, 2008 4:22 AM, Christopher Browne <cbbrowne@ca.afilias.info> wrote:
> > "David Rees" <drees76@gmail.com> writes:
> > > Now that PostgreSQL 8.3.0 is released, will there be a Slony 1.2.13
> > > since Slony 1.2.12 doesn't build against Pg 8.3?
> > >
> > > I could pull from CVS, but just anxious to get some testing in on our
> > > dev servers. :-)
> >
> > Evidently it's time for 1.2.13...  I'll see about it tomorrow
> > afternoon.
> 
> <blinks>But it does build doesn't it? We have binaries builds for
> Windows downloadable through StackBuilder on pgFoundry:
> http://pgfoundry.org/frs/?group_id=1000256, plus the builds of EDB
> Postgres 8.3 on Mac, Linux and Win32 include Slony 1.2.12, built
> against the GA server.

Builds, but doesn't work is my understanding.  Something to do with a
change of the number of arguments passed to an internal PG function.

-- 
Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.


From dpage at postgresql.org  Tue Feb  5 07:05:49 2008
From: dpage at postgresql.org (Dave Page)
Date: Tue Feb  5 07:05:54 2008
Subject: [Slony1-general] 1.2.13?
In-Reply-To: <1202223440.16171.188.camel@bnicholson-desktop>
References: <72dbd3150802041654k7753c820vd7b9b286b1d8d910@mail.gmail.com>
	<60ejbsav81.fsf@dba2.int.libertyrms.com>
	<937d27e10802050111x566a5679l53ae7ecec2018e4d@mail.gmail.com>
	<1202223440.16171.188.camel@bnicholson-desktop>
Message-ID: <937d27e10802050705q3a69d57dia844928060573774@mail.gmail.com>

On Feb 5, 2008 2:57 PM, Brad Nicholson <bnichols@ca.afilias.info> wrote:
>
>
> On Tue, 2008-02-05 at 09:11 +0000, Dave Page wrote:
> > On Feb 5, 2008 4:22 AM, Christopher Browne <cbbrowne@ca.afilias.info> wrote:
> > > "David Rees" <drees76@gmail.com> writes:
> > > > Now that PostgreSQL 8.3.0 is released, will there be a Slony 1.2.13
> > > > since Slony 1.2.12 doesn't build against Pg 8.3?
> > > >
> > > > I could pull from CVS, but just anxious to get some testing in on our
> > > > dev servers. :-)
> > >
> > > Evidently it's time for 1.2.13...  I'll see about it tomorrow
> > > afternoon.
> >
> > <blinks>But it does build doesn't it? We have binaries builds for
> > Windows downloadable through StackBuilder on pgFoundry:
> > http://pgfoundry.org/frs/?group_id=1000256, plus the builds of EDB
> > Postgres 8.3 on Mac, Linux and Win32 include Slony 1.2.12, built
> > against the GA server.
>
> Builds, but doesn't work is my understanding.  Something to do with a
> change of the number of arguments passed to an internal PG function.

That sounds like a bug I fixed a while back - but it didn't build at
all because the link failed because of the function signature change.

http://main.slony.info/viewcvs/viewvc.cgi/slony1-engine/src/backend/slony1_funcs.c?view=log
- rev 1.53.2.3

Unfortunately there don't seem to be any tags for releases :-(. There
isn't any mention of that fix in the 1.2.12 release announcement
either, so maybe I built my own tarball then wiped all memory of the
event from my brain...

/D
From prouxel.pacte at gmail.com  Tue Feb  5 08:19:14 2008
From: prouxel.pacte at gmail.com (philippe rouxel)
Date: Tue Feb  5 08:28:58 2008
Subject: [Slony1-general] logshipping and slonik segmentation error
Message-ID: <4b78ab7d0802050819j48e5e316hf45a43940e605f8@mail.gmail.com>

Hi,

I got a segmentation error when I set spoolnode to true for a store node
that I declare for the logshipping.

I have 2 db that I want to synchronize with logshipping.

What best practices:
1 node/1 slon with -a option
2 node/2 slon (one virtual with -a)
I was testing the second one when I got the error

Thanks.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080205/=
0200df5e/attachment.htm
From stephane.schildknecht at postgresqlfr.org  Tue Feb  5 08:49:52 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Tue Feb  5 08:50:00 2008
Subject: [Slony1-general] partial replication
In-Reply-To: <47A60287.8050508@praud.com>
References: <47A60287.8050508@praud.com>
Message-ID: <47A893B0.2000708@postgresqlfr.org>

Jean-Christophe Praud a ?crit :
> Hi all,
> 
> I'm new to Slony, here's my newbie question :
> 
> I need to setup a cluster of separate servers, each handling a set of
> users data.
> The architecture I'm planning is :
> 
> - 1 central server storing my whole users table (id, login, passwd,
> id_of_users_server)
> - n users servers containing the part of the central users table
> - a user authenticate on the central server, then the application uses
> the users server hosting this user
> 
> As each users server will contain a part of the central users table,
> modifications of each record of this table on the central server need to
> be replicated to the good users server.
> 
> 
> I don't plan to use inherited partitions for the central users table (I
> need PK, FK and other constraints on this table). Can Slony be used to
> do the partial replication from central to each users server ?
> 
> 
Hi,

Not sure I understand everything, but if the question is "can slony
replicate portions of table based on values", the answer is "no".

Regards,
-- 
St?phane SCHILDKNECHT
Pr?sident de PostgreSQLFr
T?l. 09 53 69 97 12
http://www.postgresqlfr.org
From jc at praud.com  Tue Feb  5 08:59:03 2008
From: jc at praud.com (Jean-Christophe Praud)
Date: Tue Feb  5 08:59:13 2008
Subject: [Slony1-general] partial replication
In-Reply-To: <47A893B0.2000708@postgresqlfr.org>
References: <47A60287.8050508@praud.com> <47A893B0.2000708@postgresqlfr.org>
Message-ID: <47A895D7.3050509@praud.com>

St=E9phane A. Schildknecht a =E9crit :
> Jean-Christophe Praud a =E9crit :
>   =

>> Hi all,
>>
>> I'm new to Slony, here's my newbie question :
>>
>> I need to setup a cluster of separate servers, each handling a set of
>> users data.
>> The architecture I'm planning is :
>>
>> - 1 central server storing my whole users table (id, login, passwd,
>> id_of_users_server)
>> - n users servers containing the part of the central users table
>> - a user authenticate on the central server, then the application uses
>> the users server hosting this user
>>
>> As each users server will contain a part of the central users table,
>> modifications of each record of this table on the central server need to
>> be replicated to the good users server.
>>
>>
>> I don't plan to use inherited partitions for the central users table (I
>> need PK, FK and other constraints on this table). Can Slony be used to
>> do the partial replication from central to each users server ?
>>
>>
>>     =

> Hi,
>
> Not sure I understand everything, but if the question is "can slony
> replicate portions of table based on values", the answer is "no".
>
> Regards,
>   =

That's what I wanted to know. Thank you :)

Regards,

-- =

Jean-Christophe Praud         -      http://shub-niggurath.com
Conseil & D=E9veloppement Informatique      http://www.praud.com
Ph'nglui mglw'nafh Cthulhu n'gah Bill R'lyeh Wgah'nagl fhtagn!

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080205/=
f18e5b72/attachment-0001.htm
From drees76 at gmail.com  Tue Feb  5 12:48:33 2008
From: drees76 at gmail.com (David Rees)
Date: Tue Feb  5 12:48:43 2008
Subject: [Slony1-general] 1.2.13?
In-Reply-To: <1202223440.16171.188.camel@bnicholson-desktop>
References: <72dbd3150802041654k7753c820vd7b9b286b1d8d910@mail.gmail.com>
	<60ejbsav81.fsf@dba2.int.libertyrms.com>
	<937d27e10802050111x566a5679l53ae7ecec2018e4d@mail.gmail.com>
	<1202223440.16171.188.camel@bnicholson-desktop>
Message-ID: <72dbd3150802051248v1b52e128g4a9cecaba01d6247@mail.gmail.com>

On Feb 5, 2008 6:57 AM, Brad Nicholson <bnichols@ca.afilias.info> wrote:
> Builds, but doesn't work is my understanding.  Something to do with a
> change of the number of arguments passed to an internal PG function.

Doesn't even build for me on my Fedora 8 test system against 8.3:

make[2]: Entering directory `/tmp/slony1-1.2.12/src/backend'
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../..
-fpic -I/usr/local/pgsql-8.3.0-2008020400/include/
-I/usr/local/pgsql-8.3.0-2008020400/include/server/  -c -o
slony1_funcs.o slony1_funcs.c
slony1_funcs.c: In function 'slon_quote_literal':
slony1_funcs.c:1106: warning: pointer targets in passing argument 1 of
'pg_mblen' differ in signedness
slony1_funcs.c: In function 'getClusterStatus':
slony1_funcs.c:1357: warning: passing argument 1 of 'typenameTypeId'
from incompatible pointer type
slony1_funcs.c:1357: error: too few arguments to function 'typenameTypeId'
slony1_funcs.c:1440: warning: passing argument 1 of 'typenameTypeId'
from incompatible pointer type
slony1_funcs.c:1440: error: too few arguments to function 'typenameTypeId'
make[2]: *** [slony1_funcs.o] Error 1
make[2]: Leaving directory `/tmp/slony1-1.2.12/src/backend'
make[1]: *** [all] Error 2
make[1]: Leaving directory `/tmp/slony1-1.2.12/src'
make: *** [all] Error 2

Pulling the HEAD of CVS does build as does the REL_1_2_STABLE branch,
but I have not installed it or ran any tests.

-Dave
From JanWieck at Yahoo.com  Tue Feb  5 13:45:24 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Tue Feb  5 13:45:42 2008
Subject: [Slony1-general] 1.2.13?
In-Reply-To: <72dbd3150802051248v1b52e128g4a9cecaba01d6247@mail.gmail.com>
References: <72dbd3150802041654k7753c820vd7b9b286b1d8d910@mail.gmail.com>	<60ejbsav81.fsf@dba2.int.libertyrms.com>	<937d27e10802050111x566a5679l53ae7ecec2018e4d@mail.gmail.com>	<1202223440.16171.188.camel@bnicholson-desktop>
	<72dbd3150802051248v1b52e128g4a9cecaba01d6247@mail.gmail.com>
Message-ID: <47A8D8F4.1080201@Yahoo.com>

On 2/5/2008 3:48 PM, David Rees wrote:
> On Feb 5, 2008 6:57 AM, Brad Nicholson <bnichols@ca.afilias.info> wrote:
>> Builds, but doesn't work is my understanding.  Something to do with a
>> change of the number of arguments passed to an internal PG function.
> 
> Doesn't even build for me on my Fedora 8 test system against 8.3:
> 
> make[2]: Entering directory `/tmp/slony1-1.2.12/src/backend'
> gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../..
> -fpic -I/usr/local/pgsql-8.3.0-2008020400/include/
> -I/usr/local/pgsql-8.3.0-2008020400/include/server/  -c -o
> slony1_funcs.o slony1_funcs.c
> slony1_funcs.c: In function 'slon_quote_literal':
> slony1_funcs.c:1106: warning: pointer targets in passing argument 1 of
> 'pg_mblen' differ in signedness
> slony1_funcs.c: In function 'getClusterStatus':
> slony1_funcs.c:1357: warning: passing argument 1 of 'typenameTypeId'

The problem is that 8.3's typenameTypeId() has changed to 3 call 
arguments. The config/acx_libpq.m4 of REL_1_2_STABLE does check for that 
correctly. The relevant change was committed on November 22., which was 
after 1.2.12 was released.

I kinda disagree that we should have released 1.2.13 BEFORE 8.3 was 
actually out. As long as something is a release candidate, there is 
still a faint chance that some API may change (however small that might 
be). So what people are actually asking for is to release with the first 
RC, and rerelease with every subsequent RC that might break anything in 
Slony. I don't think we have the resources to do that.


Jan


> Pulling the HEAD of CVS does build as does the REL_1_2_STABLE branch,
> but I have not installed it or ran any tests.
> 
> -Dave

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From bill.footcow at gmail.com  Tue Feb  5 14:21:11 2008
From: bill.footcow at gmail.com (=?utf-8?q?Herv=C3=A9_Piedvache?=)
Date: Tue Feb  5 14:21:24 2008
Subject: [Slony1-general] Migration to 8.3 throught Slony ?
Message-ID: <200802052321.11770.bill.footcow@gmail.com>

Hi,

I would like to know if a PostgreSQL database v8.1.10 using at the moment 
Slony-I with 3 replicated nodes could be migrated to a v8.3 of PostgreSQL ... 
using Slony ?

My problem is that making the dump of my production server for the classical 
migration should take a too big interruption of the service ...

Any procedure for this ?

Thanks for your answers ...

Regards,
-- 
Herv? Piedvache
From rod at iol.ie  Tue Feb  5 14:26:42 2008
From: rod at iol.ie (Raymond O'Donnell)
Date: Tue Feb  5 14:26:54 2008
Subject: [Slony1-general] Migration to 8.3 throught Slony ?
In-Reply-To: <200802052321.11770.bill.footcow@gmail.com>
References: <200802052321.11770.bill.footcow@gmail.com>
Message-ID: <47A8E2A2.30704@iol.ie>

On 05/02/2008 22:21, Herv? Piedvache wrote:

> I would like to know if a PostgreSQL database v8.1.10 using at the moment 
> Slony-I with 3 replicated nodes could be migrated to a v8.3 of PostgreSQL ... 
> using Slony ?
> 
> My problem is that making the dump of my production server for the classical 
> migration should take a too big interruption of the service ...

Absolutely - that's one of the things Slony is intended to be used for. 
You need to make sure that the versions of Slony on each node are 
exactly the same; but the versions of Postgres don't have to be.

Ray.


---------------------------------------------------------------
Raymond O'Donnell, Director of Music, Galway Cathedral, Ireland
rod@iol.ie
---------------------------------------------------------------
From bill.footcow at gmail.com  Tue Feb  5 14:35:43 2008
From: bill.footcow at gmail.com (=?utf-8?q?Herv=C3=A9_Piedvache?=)
Date: Tue Feb  5 14:36:03 2008
Subject: [Slony1-general] Migration to 8.3 throught Slony ?
In-Reply-To: <47A8E2A2.30704@iol.ie>
References: <200802052321.11770.bill.footcow@gmail.com> <47A8E2A2.30704@iol.ie>
Message-ID: <200802052335.43801.bill.footcow@gmail.com>

Ok thanks, and there is nothing specific to prepare or configure ? 
I mean I just create a new database in v8.3, add it to the cluster as a 
classical slave ... then make the synchronisation ... then I'll need to use 
this new node as my new Master ... and that's it ?

Le mardi 05 f?vrier 2008, Raymond O'Donnell a ?crit :
> On 05/02/2008 22:21, Herv? Piedvache wrote:
> > I would like to know if a PostgreSQL database v8.1.10 using at the moment
> > Slony-I with 3 replicated nodes could be migrated to a v8.3 of PostgreSQL
> > ... using Slony ?
> >
> > My problem is that making the dump of my production server for the
> > classical migration should take a too big interruption of the service ...
>
> Absolutely - that's one of the things Slony is intended to be used for.
> You need to make sure that the versions of Slony on each node are
> exactly the same; but the versions of Postgres don't have to be.
>
> Ray.
>
>
> ---------------------------------------------------------------
> Raymond O'Donnell, Director of Music, Galway Cathedral, Ireland
> rod@iol.ie
> ---------------------------------------------------------------



-- 
Herv? Piedvache
From rod at iol.ie  Tue Feb  5 14:38:31 2008
From: rod at iol.ie (Raymond O'Donnell)
Date: Tue Feb  5 14:38:43 2008
Subject: [Slony1-general] Migration to 8.3 throught Slony ?
In-Reply-To: <200802052335.43801.bill.footcow@gmail.com>
References: <200802052321.11770.bill.footcow@gmail.com> <47A8E2A2.30704@iol.ie>
	<200802052335.43801.bill.footcow@gmail.com>
Message-ID: <47A8E567.4090601@iol.ie>

On 05/02/2008 22:35, Herv? Piedvache wrote:
> Ok thanks, and there is nothing specific to prepare or configure ? 
> I mean I just create a new database in v8.3, add it to the cluster as a 
> classical slave ... then make the synchronisation ... then I'll need to use 
> this new node as my new Master ... and that's it ?

As far as I know, yes - but I've never done it myself :-)

Why not try it out and let us know how you get on.....

Ray.


---------------------------------------------------------------
Raymond O'Donnell, Director of Music, Galway Cathedral, Ireland
rod@iol.ie
---------------------------------------------------------------
From bnichols at ca.afilias.info  Tue Feb  5 19:14:05 2008
From: bnichols at ca.afilias.info (bnichols@ca.afilias.info)
Date: Tue Feb  5 19:14:19 2008
Subject: [Slony1-general] 1.2.13?
In-Reply-To: <47A8D8F4.1080201@Yahoo.com>
References: <72dbd3150802041654k7753c820vd7b9b286b1d8d910@mail.gmail.com>
	<60ejbsav81.fsf@dba2.int.libertyrms.com>
	<937d27e10802050111x566a5679l53ae7ecec2018e4d@mail.gmail.com>
	<1202223440.16171.188.camel@bnicholson-desktop>
	<72dbd3150802051248v1b52e128g4a9cecaba01d6247@mail.gmail.com>
	<47A8D8F4.1080201@Yahoo.com>
Message-ID: <62993.99.233.73.94.1202267645.squirrel@look.libertyrms.com>

> On 2/5/2008 3:48 PM, David Rees wrote:
>> On Feb 5, 2008 6:57 AM, Brad Nicholson <bnichols@ca.afilias.info> wrote:
>>> Builds, but doesn't work is my understanding.  Something to do with a
>>> change of the number of arguments passed to an internal PG function.
>>
>> Doesn't even build for me on my Fedora 8 test system against 8.3:
>>
>> make[2]: Entering directory `/tmp/slony1-1.2.12/src/backend'
>> gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../..
>> -fpic -I/usr/local/pgsql-8.3.0-2008020400/include/
>> -I/usr/local/pgsql-8.3.0-2008020400/include/server/  -c -o
>> slony1_funcs.o slony1_funcs.c
>> slony1_funcs.c: In function 'slon_quote_literal':
>> slony1_funcs.c:1106: warning: pointer targets in passing argument 1 of
>> 'pg_mblen' differ in signedness
>> slony1_funcs.c: In function 'getClusterStatus':
>> slony1_funcs.c:1357: warning: passing argument 1 of 'typenameTypeId'
>
> The problem is that 8.3's typenameTypeId() has changed to 3 call
> arguments. The config/acx_libpq.m4 of REL_1_2_STABLE does check for that
> correctly. The relevant change was committed on November 22., which was
> after 1.2.12 was released.
>
> I kinda disagree that we should have released 1.2.13 BEFORE 8.3 was
> actually out. As long as something is a release candidate, there is
> still a faint chance that some API may change (however small that might
> be). So what people are actually asking for is to release with the first
> RC, and rerelease with every subsequent RC that might break anything in
> Slony. I don't think we have the resources to do that.
>
>

I agree with Jan here about not releasing before the final version. 
However, since the patch was ready, a RC for Slony would have made sense.

From satya461 at gmail.com  Tue Feb  5 23:00:43 2008
From: satya461 at gmail.com (Satya)
Date: Tue Feb  5 23:00:59 2008
Subject: [Slony1-general] 1.2.13?
In-Reply-To: <6ccff2720802052259i3e7e015eha0d02eb7c9368dd5@mail.gmail.com>
References: <72dbd3150802041654k7753c820vd7b9b286b1d8d910@mail.gmail.com>
	<60ejbsav81.fsf@dba2.int.libertyrms.com>
	<937d27e10802050111x566a5679l53ae7ecec2018e4d@mail.gmail.com>
	<1202223440.16171.188.camel@bnicholson-desktop>
	<6ccff2720802052259i3e7e015eha0d02eb7c9368dd5@mail.gmail.com>
Message-ID: <6ccff2720802052300y22bb8cedkf8b0b0b3e23d4963@mail.gmail.com>

On Feb 6, 2008 12:29 PM, Satya <satya461@gmail.com> wrote:

>
>
> On Feb 5, 2008 8:27 PM, Brad Nicholson <bnichols@ca.afilias.info> wrote:
>
> >
> > On Tue, 2008-02-05 at 09:11 +0000, Dave Page wrote:
> > > On Feb 5, 2008 4:22 AM, Christopher Browne <cbbrowne@ca.afilias.info>
> > wrote:
> > > > "David Rees" <drees76@gmail.com> writes:
> > > > > Now that PostgreSQL 8.3.0 is released, will there be a Slony
> > 1.2.13
> > > > > since Slony 1.2.12 doesn't build against Pg 8.3?
> > > > >
> > > > > I could pull from CVS, but just anxious to get some testing in on
> > our
> > > > > dev servers. :-)
> > > >
> > > > Evidently it's time for 1.2.13...  I'll see about it tomorrow
> > > > afternoon.
> > >
> > > <blinks>But it does build doesn't it? We have binaries builds for
> > > Windows downloadable through StackBuilder on pgFoundry:
> > > http://pgfoundry.org/frs/?group_id=3D1000256, plus the builds of EDB
> > > Postgres 8.3 on Mac, Linux and Win32 include Slony 1.2.12, built
> > > against the GA server.
> >
> > Builds, but doesn't work is my understanding.  Something to do with a
> > change of the number of arguments passed to an internal PG function.
> >
>
>     Yes I had faced the same problem with Slony 1.2.12 version. There is a
> mismatch in the number of arguments to some Postgres function.
>
>     With 1.2.13(from cvs) I was able to build Slony but the tests are
> still failing. I think the tests must be fixed. I am seeing  the below
> errors .
> ERROR:  relation "public.table5" does not exist
> ERROR:  relation "public.table5_id_seq" does not exist
> ERROR:  relation "_slony_regress1.sl_trigger" does not exist
>
> ./poll_cluster.sh: line 50: [: -eq: unary operator expected
>  rows+events behind
>
> WARNING:  nonstandard use of \\ in a string literal at character 47
> and many more...
>
> Regards,
> Satya.
>
> >
> > --
> > Brad Nicholson  416-673-4106
> > Database Administrator, Afilias Canada Corp.
> >
> >
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general@lists.slony.info
> > http://lists.slony.info/mailman/listinfo/slony1-general
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080206/=
2893d287/attachment-0001.htm
From satya461 at gmail.com  Wed Feb  6 00:04:20 2008
From: satya461 at gmail.com (Satya)
Date: Wed Feb  6 00:04:38 2008
Subject: [Slony1-general] Re: Slony-1 1.12.13 tests failing with PostgreSQL
	8.3 RC2
In-Reply-To: <6ccff2720801300251xa10297eg284b7654a049b0a5@mail.gmail.com>
References: <6ccff2720801300251xa10297eg284b7654a049b0a5@mail.gmail.com>
Message-ID: <6ccff2720802060004h63baee95m5b19a83736a9a45b@mail.gmail.com>

Resending this mail as I think my previous mail has not reached the alias.

On Jan 30, 2008 4:21 PM, Satya <satya461@gmail.com> wrote:

> Hi,
>
> I have checked out slony-1 source code from cvs and tried compiling with
> PostgreSQL 8.3 RC2(checkout version). I was able to compile it but slony
> tests are failing.
> Has anyone faced this problem? Is there some issue with Slony code? Please
> let me know if I need to attach any logs.
> All the slony tests are failing but I am attaching the logs for only one
> test(testmergeset).
>
>
>
> SLONY TEST LOG:
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
> bash-3.2$ PGBINDIR=3D/space/WORK/postgres/bin/  PGUSER=3Dsatya ./run_test=
.sh
> testmergeset
> test: testmergeset
> ----------------------------------------------------
> $Id: README,v 1.2 2007-10-26 14:47:24 cbbrowne Exp $
>
> testmergeset sets up a series of replication sets, progressively
> merging them together.
>
> This was written based on "testpartition", the difference being that
> in this test, tables are added to replication via explicitly creating
> a set (a temporary set #999), and then merging that set back into the
> main set.
>
> This test also takes a fairly aggressive tack on usage of WAIT FOR
> EVENT; when it gets into the "merge phase," it submits a SYNC followed
> by waiting for that SYNC to be confirmed on all relevant nodes before
> submitting a MERGE SET request to eliminate the extra set.  It does
> not use any SLEEP requests; concurrency control is expected to be
> controlled by WAIT FOR EVENT.
> ----------------------------------------------------
> Test by j.random.luser@example.net to be summarized in /tmp/Slony-
> I-test-results.log
> creating origin DB: satya -h localhost -U satya -p 5432 slonyregress1
> add plpgsql to Origin
> loading origin DB with testmergeset/init_schema.sql
> setting up user satya to have weak access to data
> done
> creating subscriber 2 DB: satya -h localhost -U satya -p 5432
> slonyregress2
> add plpgsql to subscriber
> loading subscriber 2 DB from slonyregress1
> done
> creating subscriber 3 DB: satya -h localhost -U satya -p 5432
> slonyregress3
> add plpgsql to subscriber
> loading subscriber 3 DB from slonyregress1
> done
> creating subscriber 4 DB: satya -h localhost -U satya -p 5432
> slonyregress4
> add plpgsql to subscriber
> loading subscriber 4 DB from slonyregress1
> done
> creating cluster
> done
> storing nodes
> done
> Granting weak access on Slony-I schema
> done
> storing paths
> done
> launching originnode : /space/WORK/postgres/bin//slon  -s500 -g10 -d2
> slony_regress1 "dbname=3Dslonyregress1 host=3Dlocalhost user=3Dsatya port=
=3D5432"
> Archive node:
> Considering node 2
> launching: /space/WORK/postgres/bin//slon -s500 -g10 -d2  slony_regress1
> "dbname=3Dslonyregress2 host=3Dlocalhost user=3Dsatya port=3D5432"
> Considering node 3
> launching: /space/WORK/postgres/bin//slon -s500 -g10 -d2  slony_regress1
> "dbname=3Dslonyregress3 host=3Dlocalhost user=3Dsatya port=3D5432"
> Considering node 4
> launching: /space/WORK/postgres/bin//slon -s500 -g10 -d2  slony_regress1
> "dbname=3Dslonyregress4 host=3Dlocalhost user=3Dsatya port=3D5432"
> subscribing
>
> and HANGS at this stage..
>
> Postgres Log
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
> LOG:  database system was shut down at 2008-01-30 16:12:30 IST
> LOG:  autovacuum launcher started
> LOG:  database system is ready to accept connections
> NOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index
> "regions_pkey" for table "regions"
> NOTICE:  CREATE TABLE will create implicit sequence
> "products_product_id_seq" for serial column "products.product_id"
> NOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index
> "products_pkey" for table "products"
> NOTICE:  CREATE TABLE / UNIQUE will create implicit index
> "products_name_key" for table "products"
> NOTICE:  CREATE TABLE will create implicit sequence "sales_txns_id_seq"
> for serial column "sales_txns.id"
> NOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index
> "sales_txns_pkey" for table "sales_txns"
> ERROR:  relation "public.table1" does not exist
> STATEMENT:  grant select on table public.table1 to satya;
> ERROR:  relation "public.table1_id_seq" does not exist
> STATEMENT:  grant select on table public.table1_id_seq to satya;
> ERROR:  relation "public.table2" does not exist
> STATEMENT:  grant select on table public.table2 to satya;
> ERROR:  relation "public.table2_id_seq" does not exist
> STATEMENT:  grant select on table public.table2_id_seq to satya;
> ERROR:  relation "public.table1a" does not exist
> STATEMENT:  grant select on table public.table1a to satya;
> ERROR:  relation "public.table1a_id_seq" does not exist
> STATEMENT:  grant select on table public.table1a_id_seq to satya;
> ERROR:  relation "public.table2a" does not exist
> STATEMENT:  grant select on table public.table2a to satya;
> ERROR:  relation "public.table2a_id_seq" does not exist
> STATEMENT:  grant select on table public.table2a_id_seq to satya;
> ERROR:  Missing sales_txns partition for date 2006-01-15 00:00:00+05:30
> CONTEXT:  SQL statement "INSERT INTO sales_txns (region_code, product_id,
> quantity, amount, trans_on) values ( $1 ,  $2 ,  $3 ,  $4 ,  $5 )"
>         PL/pgSQL function "purchase_product" line 11 at SQL statement
> STATEMENT:  select purchase_product( region_code, product_id,
> (random()*5+random()*8+random()*7)::integer, '2006-01-15') from regions,
> products order by random() limit 3;
> ERROR:  Missing sales_txns partition for date 2006-01-16 00:00:00+05:30
> CONTEXT:  SQL statement "INSERT INTO sales_txns (region_code, product_id,
> quantity, amount, trans_on) values ( $1 ,  $2 ,  $3 ,  $4 ,  $5 )"
>         PL/pgSQL function "purchase_product" line 11 at SQL statement
> STATEMENT:  select purchase_product( region_code, product_id,
> (random()*5+random()*8+random()*7)::integer, '2006-01-16') from regions,
> products order by random() limit 3;
> ERROR:  Missing sales_txns partition for date 2006-01-17 00:00:00+05:30
> CONTEXT:  SQL statement "INSERT INTO sales_txns (region_code, product_id,
> quantity, amount, trans_on) values ( $1 ,  $2 ,  $3 ,  $4 ,  $5 )"
>         PL/pgSQL function "purchase_product" line 11 at SQL statement
> STATEMENT:  select purchase_product( region_code, product_id,
> (random()*5+random()*8+random()*7)::integer, '2006-01-17') from regions,
> products order by random() limit 3;
> ERROR:  Missing sales_txns partition for date 2006-01-18 00:00:00+05:30
> CONTEXT:  SQL statement "INSERT INTO sales_txns (region_code, product_id,
> quantity, amount, trans_on) values ( $1 ,  $2 ,  $3 ,  $4 ,  $5 )"
>         PL/pgSQL function "purchase_product" line 11 at SQL statement
> STATEMENT:  select purchase_product( region_code, product_id,
> (random()*5+random()*8+random()*7)::integer, '2006-01-18') from regions,
> products order by random() limit 3;
> ERROR:  Missing sales_txns partition for date 2006-01-19 00:00:00+05:30
> CONTEXT:  SQL statement "INSERT INTO sales_txns (region_code, product_id,
> quantity, amount, trans_on) values ( $1 ,  $2 ,  $3 ,  $4 ,  $5 )"
>         PL/pgSQL function "purchase_product" line 11 at SQL statement
> STATEMENT:  select purchase_product( region_code, product_id,
> (random()*5+random()*8+random()*7)::integer, '2006-01-19') from regions,
> products order by random() limit 3;
> ERROR:  Missing sales_txns partition for date 2006-01-20 00:00:00+05:30
> CONTEXT:  SQL statement "INSERT INTO sales_txns (region_code, product_id,
> quantity, amount, trans_on) values ( $1 ,  $2 ,  $3 ,  $4 ,  $5 )"
>         PL/pgSQL function "purchase_product" line 11 at SQL statement
> STATEMENT:  select purchase_product( region_code, product_id,
> (random()*5+random()*8+random()*7)::integer, '2006-01-20') from regions,
> products order by random() limit 3;
> ERROR:  language "plpgsql" already exists
> STATEMENT:  CREATE PROCEDURAL LANGUAGE plpgsql;
> NOTICE:  ALTER TABLE / ADD UNIQUE will create implicit index
> "products_name_key" for table "products"
> NOTICE:  ALTER TABLE / ADD PRIMARY KEY will create implicit index
> "products_pkey" for table "products"
> NOTICE:  ALTER TABLE / ADD PRIMARY KEY will create implicit index
> "regions_pkey" for table "regions"
> NOTICE:  ALTER TABLE / ADD PRIMARY KEY will create implicit index
> "sales_txns_pkey" for table "sales_txns"
> ERROR:  language "plpgsql" already exists
> STATEMENT:  CREATE PROCEDURAL LANGUAGE plpgsql;
> NOTICE:  ALTER TABLE / ADD UNIQUE will create implicit index
> "products_name_key" for table "products"
> NOTICE:  ALTER TABLE / ADD PRIMARY KEY will create implicit index
> "products_pkey" for table "products"
> NOTICE:  ALTER TABLE / ADD PRIMARY KEY will create implicit index
> "regions_pkey" for table "regions"
> NOTICE:  ALTER TABLE / ADD PRIMARY KEY will create implicit index
> "sales_txns_pkey" for table "sales_txns"
> ERROR:  language "plpgsql" already exists
>
>
> Regards,
> Satya
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080206/=
52ea1b7a/attachment.htm
From stephane.schildknecht at postgresqlfr.org  Wed Feb  6 00:24:36 2008
From: stephane.schildknecht at postgresqlfr.org (=?UTF-8?B?IlN0w6lwaGFuZSBBLiBTY2hpbGRrbmVjaHQi?=)
Date: Wed Feb  6 00:24:57 2008
Subject: [Slony1-general] Migration to 8.3 throught Slony ?
In-Reply-To: <200802052321.11770.bill.footcow@gmail.com>
References: <200802052321.11770.bill.footcow@gmail.com>
Message-ID: <47A96EC4.6020508@postgresqlfr.org>

Herv? Piedvache a ?crit :
> Hi,
> 
> I would like to know if a PostgreSQL database v8.1.10 using at the moment 
> Slony-I with 3 replicated nodes could be migrated to a v8.3 of PostgreSQL ... 
> using Slony ?
> 
> My problem is that making the dump of my production server for the classical 
> migration should take a too big interruption of the service ...
> 
> Any procedure for this ?
> 
> Thanks for your answers ...
> 
> Regards,
Hi,

Reading the previous thread let me think it won't be so easy as
Slony-1.2.13 which is 8.3 compliant isn't published yet.

I then wonder if you don't have to wait until 1.2.13, upgrade slony on
all nodes and then create the new node with PG8.3 and slony.

Regards,

-- 
St?phane SCHILDKNECHT
Pr?sident de PostgreSQLFr
T?l. 09 53 69 97 12
http://www.postgresqlfr.org
From vanderleeden at logicunited.com  Wed Feb  6 02:08:03 2008
From: vanderleeden at logicunited.com (Rudolf van der Leeden)
Date: Wed Feb  6 02:08:25 2008
Subject: [Slony1-general] slon daemon SEGV on Linux (i686)
Message-ID: <91214C7F-E66C-43C8-9B4B-C592944EFEDD@logicunited.com>

I'm running Slony 1.2.12 along with PG 8.2.4  successfully on Mac OSX  
10.4 (PPC).

Currently I'm testing the same Slony version on
   - Gentoo 2.6.20.6 SMP,  Apr 10  2007,  i686 AMD Athlon(tm) MP 1800 
+ AuthenticAMD GNU/Linux
   - PG 8.1.3
   - Slony master and slave on the same machine
It builds successfully but as soon as slon gets something to do it  
aborts with SIGSEGV.

Further investigation shows (after adding more debug output to  
remote_worker.c):
   - DEBUG2 remoteWorkerThread_1: Received event 1,2 STORE_NODE
   - event message data:
     -- event->ev_data1 = '2'
     -- event->ev_data2 = 'node2'
     -- event->ev_data3 = '(NULL)'     <--- this is WRONG!
The event message does not contain enough arguments for further  
processing and causes slon to abort.
The third argument is supposed to be 'f'  (false).

Does anybody know what's wrong or has run into the same problem?
Is it a BUILD problem (wrong libraries?), a Postgres problem, a big/ 
little endian problem, ... ?

Thanks for your help.
Rudolf VanderLeeden
Logic United GmbH
Munich, Germany

From prouxel.pacte at gmail.com  Wed Feb  6 02:45:18 2008
From: prouxel.pacte at gmail.com (philippe rouxel)
Date: Wed Feb  6 02:45:40 2008
Subject: [Slony1-general] Re: logshipping and slonik segmentation error
In-Reply-To: <4b78ab7d0802050819j48e5e316hf45a43940e605f8@mail.gmail.com>
References: <4b78ab7d0802050819j48e5e316hf45a43940e605f8@mail.gmail.com>
Message-ID: <4b78ab7d0802060245t16a4ae5ahb5f54fe1d4781b8f@mail.gmail.com>

the segmentation is because I put no conninfo  in the node admin

 node 1 admin conninfo='host=127.0.0.1 dbname=fb7 user=pgslony port=5433';
 node 2 admin conninfo='';
  init cluster (id = 1, comment = 'Node 1 - fb7@127.0.0.1');

# STORE NODE
  store node (id = 2, spoolnode = true, comment = 'Node 2 ');

But I see in Slony.c

 2011 : cbbrowne 1.54         /* Eliminate no_spool evaluation - all
nodes are "real nodes" */  /* if (!stmt->no_spool) */
 2012 :     		

 2013 :     	if (db_begin_xact((SlonikStmt *) stmt, adminfo1) < 0)
 2014 :

So is this spoolnode really active or each node must have its own db ?
From lists at serioustechnology.com  Wed Feb  6 05:13:11 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb  6 05:13:15 2008
Subject: [Slony1-general] Slony-II documentation
Message-ID: <47A9B267.4040902@serioustechnology.com>

I'm trying to locate Slony-II documentation, but having a hard time 
doing so.  Is Slony-II real, or is it still in development?

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From cbbrowne at ca.afilias.info  Wed Feb  6 06:49:36 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Feb  6 06:49:41 2008
Subject: [Slony1-general] Slony-II documentation
In-Reply-To: <47A9B267.4040902@serioustechnology.com> (Geoffrey's message of
	"Wed, 06 Feb 2008 08:13:11 -0500")
References: <47A9B267.4040902@serioustechnology.com>
Message-ID: <60myqe87i7.fsf@dba2.int.libertyrms.com>

Geoffrey <lists@serioustechnology.com> writes:
> I'm trying to locate Slony-II documentation, but having a hard time
> doing so.  Is Slony-II real, or is it still in development?

It's Resting In Peace.

A number of not-readily-surmountable problems arose...

- There was a new error condition that would have been inherent to it
  (e.g. - not avoidable without significant redesign to our
  applications) that would have, in a 3 node scenario, led to our
  having ~2/3 of transactions failing, where none would have
  failed with 1 node.  (Issue:  Fighting over a single tuple.)

- Lock management turned out to be more expensive than
  expected

- There isn't a really good, really high performance, BSD-licensed
  group communications system.  (Spread isn't quite faultless...)

Those things seem to have been the essential discouragements.

There were also numerous issues such as the handling of schema updates
where there were still large, nebulous problems to be properly
characterized, let alone resolved.

If Theo Schlossnagle's idea of building a successor to Spread (which
hasn't moved in over a year) turned into something, it might be worth
re-exploring the path.
-- 
"cbbrowne","@","linuxfinances.info"
http://linuxfinances.info/info/lsf.html
"Never insult seven men, when all you're packin' is a six gun"
--- Zane Gray
From lists at serioustechnology.com  Wed Feb  6 06:54:58 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb  6 06:55:04 2008
Subject: [Slony1-general] Slony-II documentation
In-Reply-To: <60myqe87i7.fsf@dba2.int.libertyrms.com>
References: <47A9B267.4040902@serioustechnology.com>
	<60myqe87i7.fsf@dba2.int.libertyrms.com>
Message-ID: <47A9CA42.10100@serioustechnology.com>

Christopher Browne wrote:
> Geoffrey <lists@serioustechnology.com> writes:
>> I'm trying to locate Slony-II documentation, but having a hard time
>> doing so.  Is Slony-II real, or is it still in development?
> 
> It's Resting In Peace.

Thanks for the info.  So, am I clear in my understanding that the 
primary (only?) difference between Slony-I and Slony-II was multi-master 
replication.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From ajs at crankycanuck.ca  Wed Feb  6 08:45:44 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Feb  6 08:45:59 2008
Subject: [Slony1-general] Migration to 8.3 throught Slony ?
In-Reply-To: <200802052335.43801.bill.footcow@gmail.com>
References: <200802052321.11770.bill.footcow@gmail.com> <47A8E2A2.30704@iol.ie>
	<200802052335.43801.bill.footcow@gmail.com>
Message-ID: <20080206164544.GA5918@crankycanuck.ca>

On Tue, Feb 05, 2008 at 11:35:43PM +0100, Herv? Piedvache wrote:
> Ok thanks, and there is nothing specific to prepare or configure ? 
> I mean I just create a new database in v8.3, add it to the cluster as a 
> classical slave ... then make the synchronisation ... then I'll need to use 
> this new node as my new Master ... and that's it ?

You'll need to install the very latest (I'm not even sure it's released yet)
Slony version, because there's an incompatibility with former Slony versions
and 8.3.  But yes.  Controlled switchover is your friend.

A
From ajs at crankycanuck.ca  Wed Feb  6 09:13:57 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Feb  6 09:14:07 2008
Subject: [Slony1-general] Slony-II documentation
In-Reply-To: <47A9CA42.10100@serioustechnology.com>
References: <47A9B267.4040902@serioustechnology.com>
	<60myqe87i7.fsf@dba2.int.libertyrms.com>
	<47A9CA42.10100@serioustechnology.com>
Message-ID: <20080206171357.GB5918@crankycanuck.ca>

On Wed, Feb 06, 2008 at 09:54:58AM -0500, Geoffrey wrote:
> Thanks for the info.  So, am I clear in my understanding that the 
> primary (only?) difference between Slony-I and Slony-II was multi-master 
> replication.

No.  Slony-II was sort of an unfortunate name.  It was really taking off
from the work done in Postgres-R.  If you want to pursue that line of
thinking, Markus is looking for someone to sponsor his work.

A

From cbbrowne at ca.afilias.info  Wed Feb  6 10:05:39 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Feb  6 10:05:53 2008
Subject: [Slony1-general] Modifying SLON_DATA_FETCH_SIZE
In-Reply-To: <4795E901.8070001@echo.fr> (Cyril SCETBON's message of "Tue,
	22 Jan 2008 14:00:49 +0100")
References: <601wavf4kp.fsf@dba2.int.libertyrms.com>
	<47396E63.8080106@free.fr> <1195748137.4246.173.camel@ebony.site>
	<478F6BA9.7090102@free.fr> <60lk6oqwat.fsf_-_@dba2.int.libertyrms.com>
	<4795E901.8070001@echo.fr>
Message-ID: <60ejbq7yfg.fsf@dba2.int.libertyrms.com>

Cyril SCETBON <scetbon@echo.fr> writes:
> I told it in a previous mail :
>
> /" I've modified the value of SLON_DATA_FETCH_SIZE in src/slon/slon.h
> and rebuilt it. I've changed it from 10 to 50 to make fetchs of 500
> lines. "
> /

I have just committed the above change to HEAD.

<http://lists.slony.info/pipermail/slony1-commit/2008-February/002160.html>

It warrants more testing, with the totality of the other relevant
changes in HEAD (e.g. - eliminating VACUUM on sl_log_?, and such), but
it doesn't seem too likely that this would worsen overall performance.
-- 
let name="cbbrowne" and tld="cbbrowne.com" in String.concat "@" [name;tld];;
http://linuxfinances.info/info/rdbms.html
I always try to do things in chronological order. 
From JanWieck at Yahoo.com  Wed Feb  6 11:07:57 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed Feb  6 12:19:36 2008
Subject: [Slony1-general] Migration to 8.3 throught Slony ?
In-Reply-To: <20080206164544.GA5918@crankycanuck.ca>
References: <200802052321.11770.bill.footcow@gmail.com>
	<47A8E2A2.30704@iol.ie>	<200802052335.43801.bill.footcow@gmail.com>
	<20080206164544.GA5918@crankycanuck.ca>
Message-ID: <47AA058D.10108@Yahoo.com>

On 2/6/2008 11:45 AM, Andrew Sullivan wrote:
> On Tue, Feb 05, 2008 at 11:35:43PM +0100, Herv? Piedvache wrote:
>> Ok thanks, and there is nothing specific to prepare or configure ? 
>> I mean I just create a new database in v8.3, add it to the cluster as a 
>> classical slave ... then make the synchronisation ... then I'll need to use 
>> this new node as my new Master ... and that's it ?
> 
> You'll need to install the very latest (I'm not even sure it's released yet)
> Slony version, because there's an incompatibility with former Slony versions
> and 8.3.  But yes.  Controlled switchover is your friend.

The very latest is 1.2.12 and it does NOT support 8.3. So he will indeed 
have to wait for 1.2.13.


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From cbbrowne at ca.afilias.info  Wed Feb  6 13:22:26 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Feb  6 13:22:37 2008
Subject: [Slony1-general] Migration to 8.3 through Slony ?
In-Reply-To: <47AA058D.10108@Yahoo.com>
References: <200802052321.11770.bill.footcow@gmail.com>	<47A8E2A2.30704@iol.ie>	<200802052335.43801.bill.footcow@gmail.com>	<20080206164544.GA5918@crankycanuck.ca>
	<47AA058D.10108@Yahoo.com>
Message-ID: <47AA2512.7060406@ca.afilias.info>

Jan Wieck wrote:
> On 2/6/2008 11:45 AM, Andrew Sullivan wrote:
>> On Tue, Feb 05, 2008 at 11:35:43PM +0100, Herv=E9 Piedvache wrote:
>>> Ok thanks, and there is nothing specific to prepare or configure ? I =

>>> mean I just create a new database in v8.3, add it to the cluster as =

>>> a classical slave ... then make the synchronisation ... then I'll =

>>> need to use this new node as my new Master ... and that's it ?
>>
>> You'll need to install the very latest (I'm not even sure it's =

>> released yet)
>> Slony version, because there's an incompatibility with former Slony =

>> versions
>> and 8.3.  But yes.  Controlled switchover is your friend.
>
> The very latest is 1.2.12 and it does NOT support 8.3. So he will =

> indeed have to wait for 1.2.13.
Note: I have been going through all of the recent bug reports to see =

about any impact on 1.2.13.  There has been a bit; I dropped out some =

spurious NOTIFY requests that have been obsolete throughout the 1.2 branch.

The fallout looks to be that I ought to run through a full set of builds =

against the various PG versions, and see how tests work out.  I'll get =

on that...

-- =

(format nil "~S@~S" "cbbrowne" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)

-------------- next part --------------
A non-text attachment was scrubbed...
Name: cbbrowne.vcf
Type: text/x-vcard
Size: 286 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080206=
/8f43df1a/cbbrowne.vcf
From cbbrowne at ca.afilias.info  Wed Feb  6 13:38:24 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Feb  6 13:38:34 2008
Subject: [Slony1-general] Impending 1.2.13
In-Reply-To: <47A8D8F4.1080201@Yahoo.com>
References: <72dbd3150802041654k7753c820vd7b9b286b1d8d910@mail.gmail.com>	<60ejbsav81.fsf@dba2.int.libertyrms.com>	<937d27e10802050111x566a5679l53ae7ecec2018e4d@mail.gmail.com>	<1202223440.16171.188.camel@bnicholson-desktop>	<72dbd3150802051248v1b52e128g4a9cecaba01d6247@mail.gmail.com>
	<47A8D8F4.1080201@Yahoo.com>
Message-ID: <47AA28D0.9040605@ca.afilias.info>

Jan Wieck wrote:
> I kinda disagree that we should have released 1.2.13 BEFORE 8.3 was =

> actually out. As long as something is a release candidate, there is =

> still a faint chance that some API may change (however small that =

> might be). So what people are actually asking for is to release with =

> the first RC, and rerelease with every subsequent RC that might break =

> anything in Slony. I don't think we have the resources to do that.
I mentioned this approach to several people if not as an "at large =

announcement;" if people had been really, really keen on having 1.2.13 =

available *instantly* upon 8.3, there was time to speak up...

At any rate, I have had a few small changes fall in as a result of =

reviewing the outstanding bug list, so that seems to warrant my doing a =

wide set of builds to verify that we haven't regressed.

People that are interested, please do builds of -rREL_1_2_STABLE.  =

Devrim Gunduz sometimes has RPM packaging changes to make at the last =

minute...

Expect a release of 1.2.13 by the end of the week...

-- =

"cbbrowne","@","ca.afilias.info"
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)

-------------- next part --------------
A non-text attachment was scrubbed...
Name: cbbrowne.vcf
Type: text/x-vcard
Size: 286 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080206=
/f82bb767/cbbrowne.vcf
From drees76 at gmail.com  Wed Feb  6 14:08:07 2008
From: drees76 at gmail.com (David Rees)
Date: Wed Feb  6 14:08:19 2008
Subject: [Slony1-general] Impending 1.2.13
In-Reply-To: <47AA28D0.9040605@ca.afilias.info>
References: <72dbd3150802041654k7753c820vd7b9b286b1d8d910@mail.gmail.com>
	<60ejbsav81.fsf@dba2.int.libertyrms.com>
	<937d27e10802050111x566a5679l53ae7ecec2018e4d@mail.gmail.com>
	<1202223440.16171.188.camel@bnicholson-desktop>
	<72dbd3150802051248v1b52e128g4a9cecaba01d6247@mail.gmail.com>
	<47A8D8F4.1080201@Yahoo.com> <47AA28D0.9040605@ca.afilias.info>
Message-ID: <72dbd3150802061408t2c445c37mf57b08dd6cf6f6dd@mail.gmail.com>

On Feb 6, 2008 1:38 PM, Christopher Browne <cbbrowne@ca.afilias.info> wrote:
> Jan Wieck wrote:
> > I kinda disagree that we should have released 1.2.13 BEFORE 8.3 was
> > actually out. As long as something is a release candidate, there is
> > still a faint chance that some API may change (however small that
> > might be). So what people are actually asking for is to release with
> > the first RC, and rerelease with every subsequent RC that might break
> > anything in Slony. I don't think we have the resources to do that.
>
> I mentioned this approach to several people if not as an "at large
> announcement;" if people had been really, really keen on having 1.2.13
> available *instantly* upon 8.3, there was time to speak up...

I agree. I think I was the first to mention getting a new release out
there for 8.3, and that was after the release. While getting a release
that works with Pg 8.3 out the door ahead of time would have been
nice, obviously not that many people were looking for it. :-)

> People that are interested, please do builds of -rREL_1_2_STABLE.
> Devrim Gunduz sometimes has RPM packaging changes to make at the last
> minute...

I'll try to grab it today and see if I can use it to migrate my test
environment from 8.2 to 8.3.

> Expect a release of 1.2.13 by the end of the week...

Thanks!

-Dave
From victor.aluko at gmail.com  Thu Feb  7 06:48:56 2008
From: victor.aluko at gmail.com (ajcity)
Date: Thu Feb  7 06:49:00 2008
Subject: [Slony1-general] Slony setup over a LAN but not replicating
Message-ID: <15308421.post@talk.nabble.com>


Hi all,
  I setup Slony over a LAN following all the stated procedures i.e. Create
similar schema,create service on both machines and add both engines, Create
the cluster, create set, add tables, add path & listens and subscribe
set......But with all these its not replicating even after I restarted the
slon services on both machines. What am I supposed to do next? 
  And there was something I wasnt sure of when I setup the '.conf' files on
both machines...Am I supposed to specify 'localhost' as the host on the
first machine (say NY) and specify the IP address of the second machine (say
TX) or should I specify their IP addresses?
  Much love.

                           Victor
-- 
View this message in context: http://www.nabble.com/Slony-setup-over-a-LAN-but-not-replicating-tp15308421p15308421.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From storm at iparadigms.com  Thu Feb  7 16:16:06 2008
From: storm at iparadigms.com (Christian Storm)
Date: Thu Feb  7 16:16:17 2008
Subject: [Slony1-general] Dead tuples on the slave
Message-ID: <71274A3E-0964-4A3D-9AC1-74DFEBAAAF5B@iparadigms.com>

I was hoping someone could help shed some light on whether dead tuples  
reduce slave
performance in terms of keeping up to date with the master.

I read Shoaib's article

http://people.planetpostgresql.org/shoaibmir/index.php?/archives/9-Schema-specific-dead-tuples-report.html#extended

and decided to take a look at our Slony schema.  We get once in a  
while lags of 30 seconds or more and wondered
if it had to do with table bloat.  We are running Slony 1.2.10 on  
PostgreSQL 8.2.4.

The results on the master seemed reasonable every time I looked, i.e.,  
routinely vacuumed.

However, on one of the slaves I saw a lot of table bloat.  Can this  
hurt performance?

tii=# select * from deadtup_report;
-[ RECORD 2 ]------+--------------------
table_name         | _tii.sl_confirm
table_len          | 11214848
tuple_count        | 7762
tuple_len          | 403624
tuple_percent      | 3.6
dead_tuple_count   | 186290
dead_tuple_len     | 9687080
dead_tuple_percent | 86.38
free_space         | 315080
free_percent       | 2.81
-[ RECORD 3 ]------+--------------------
table_name         | _tii.sl_event
table_len          | 10371072
tuple_count        | 2915
tuple_len          | 243012
tuple_percent      | 2.34
dead_tuple_count   | 70657
dead_tuple_len     | 5936368
dead_tuple_percent | 57.24
free_space         | 3640376
free_percent       | 35.1
-[ RECORD 6 ]------+--------------------
table_name         | _tii.sl_log_2
table_len          | 148004864
tuple_count        | 38796
tuple_len          | 5069174
tuple_percent      | 3.43
dead_tuple_count   | 1017334
dead_tuple_len     | 134977401
dead_tuple_percent | 91.2
free_space         | 1742612
free_percent       | 1.18
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080207/2d61355d/attachment.htm
From drees76 at gmail.com  Thu Feb  7 17:26:42 2008
From: drees76 at gmail.com (David Rees)
Date: Thu Feb  7 17:26:52 2008
Subject: [Slony1-general] Dead tuples on the slave
In-Reply-To: <71274A3E-0964-4A3D-9AC1-74DFEBAAAF5B@iparadigms.com>
References: <71274A3E-0964-4A3D-9AC1-74DFEBAAAF5B@iparadigms.com>
Message-ID: <72dbd3150802071726y4672855dt1fdd63843c8de4e7@mail.gmail.com>

On Feb 7, 2008 4:16 PM, Christian Storm <storm@iparadigms.com> wrote:
> However, on one of the slaves I saw a lot of table bloat.  Can this hurt
> performance?

It can. Do  you not vacuum the slave database? Autovacuum would help
you out here at a minimum on the slaves if you didn't want to bother
with doing a full vacuum analysis.

-Dave
From JanWieck at Yahoo.com  Thu Feb  7 19:01:00 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu Feb  7 19:01:33 2008
Subject: [Slony1-general] Dead tuples on the slave
Message-ID: <200802080301.m1831DYV072916@jupiter.jannicash.info>

It not only can, it definitely will hurt performance. A Slony replica experiences the same updates, row per row, as the origin did. This means that the need for vacuuming is exactly the same.


Jan

--
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin


-----Original Message-----

From:  "David Rees" <drees76@gmail.com>
Subj:  Re: [Slony1-general] Dead tuples on the slave
Date:  Thu Feb 7, 2008 20:26
Size:  504 bytes
To:  "Christian Storm" <storm@iparadigms.com>
cc:  slony1-general@lists.slony.info

On Feb 7, 2008 4:16 PM, Christian Storm <storm@iparadigms.com> wrote:
> However, on one of the slaves I saw a lot of table bloat.  Can this hurt
> performance?

It can. Do  you not vacuum the slave database? Autovacuum would help
you out here at a minimum on the slaves if you didn't want to bother
with doing a full vacuum analysis.

-Dave
_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general

From termeau at gmail.com  Fri Feb  8 06:21:12 2008
From: termeau at gmail.com (termeau sebastien)
Date: Fri Feb  8 06:21:15 2008
Subject: [Slony1-general] Slony setup over a LAN but not replicating
In-Reply-To: <15308421.post@talk.nabble.com>
References: <15308421.post@talk.nabble.com>
Message-ID: <8466e51e0802080621v51d4088drf4ed260f9bcbdf1b@mail.gmail.com>

2008/2/7, ajcity <victor.aluko@-----l.com>:
>
>
> Hi all,
>   I setup Slony over a LAN following all the stated procedures i.e. Create
> similar schema,create service on both machines and add both engines,
> Create
> the cluster, create set, add tables, add path & listens and subscribe
> set......But with all these its not replicating even after I restarted the
> slon services on both machines. What am I supposed to do next?
>   And there was something I wasnt sure of when I setup the '.conf' files
> on
> both machines...Am I supposed to specify 'localhost' as the host on the
> first machine (say NY) and specify the IP address of the second machine
> (say
> TX) or should I specify their IP addresses?
>   Much love.
>
>                            Victor
>
>

Hello,

You should specify both IP addresses or names if their names could be
resolved thanks to the DNS.
Please give us more details about you configuration and error messages if
you want some help.
Regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080208/=
013e9583/attachment.htm
From mweber at aip.de  Fri Feb  8 06:26:16 2008
From: mweber at aip.de (Michael Weber)
Date: Fri Feb  8 06:26:22 2008
Subject: [Slony1-general] Slony setup over a LAN but not replicating
In-Reply-To: <8466e51e0802080621v51d4088drf4ed260f9bcbdf1b@mail.gmail.com>
References: <15308421.post@talk.nabble.com>
	<8466e51e0802080621v51d4088drf4ed260f9bcbdf1b@mail.gmail.com>
Message-ID: <4b15b0190802080626qe7d1503ld65c2be1de69dbf8@mail.gmail.com>

Hi,
and all you need is one config file on the host you run the daemon/service
on. I use a VPN to get the postgresql port of the remote machine onto the
local one, but I think nowadays there is methods of direct encrypted
postgresql port connections.

Or you can use an ssh tunnel, and get the postgresql port on the remote
machine to a local port (with different number).

Michael


On Feb 8, 2008 3:21 PM, termeau sebastien <termeau@gmail.com> wrote:

> 2008/2/7, ajcity <victor.aluko@-----l.com>:
> >
> >
> > Hi all,
> >   I setup Slony over a LAN following all the stated procedures i.e.
> > Create
> > similar schema,create service on both machines and add both engines,
> > Create
> > the cluster, create set, add tables, add path & listens and subscribe
> > set......But with all these its not replicating even after I restarted
> > the
> > slon services on both machines. What am I supposed to do next?
> >   And there was something I wasnt sure of when I setup the '.conf' files
> > on
> > both machines...Am I supposed to specify 'localhost' as the host on the
> > first machine (say NY) and specify the IP address of the second machine
> > (say
> > TX) or should I specify their IP addresses?
> >   Much love.
> >
> >                            Victor
> >
> >
>
> Hello,
>
> You should specify both IP addresses or names if their names could be
> resolved thanks to the DNS.
> Please give us more details about you configuration and error messages if
> you want some help.
> Regards
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>


-- =

Dr. Michael Weber | +49 331 7499 - 632 | fax - 200 | mweber@aip.de
Astrophysikalisches Institut Potsdam (AIP)
An der Sternwarte 16, D-14482 Potsdam
Vorstand: Prof. Dr. Matthias Steinmetz, Peter A. Stolz
Stiftung privaten Rechts, Stiftungsverzeichnis Brandenburg: III/7-71-026
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080208/=
26a17cad/attachment.htm
From cbbrowne at ca.afilias.info  Fri Feb  8 09:00:30 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Feb  8 09:00:40 2008
Subject: [Slony1-general] Dead tuples on the slave
In-Reply-To: <200802080301.m1831DYV072916@jupiter.jannicash.info> (Jan Wieck's
	message of "Thu, 7 Feb 2008 22:01:00 -0500")
References: <200802080301.m1831DYV072916@jupiter.jannicash.info>
Message-ID: <60ir0z758x.fsf@dba2.int.libertyrms.com>

"Jan Wieck" <JanWieck@Yahoo.com> writes:
> It not only can, it definitely will hurt performance. A Slony
> replica experiences the same updates, row per row, as the origin
> did. This means that the need for vacuuming is exactly the same.

Well, there are *some* differences.

Any transactions that roll back on the origin won't be reflected at
all on subscribers, so if a system sees a lot of transaction
ROLLBACKs, that will a point of non-correspondance.

But Christian pointed out three specific Slony-I-managed tables that
are worth at least a quick look...

- sl_log_2 consisted 91% of dead tuples
- sl_event had 57% dead tuples
- sl_confirm was composed of 86% dead tuples

In the 1.2 branch, we do periodically rotate between sl_log_1 and
sl_log_2 so that *that* space will eventually get reclaimed via a
TRUNCATE.  In the 2.0 branch, we'll be moving to a strategy of solely
running TRUNCATE against sl_log_{*}, so there should never be any dead
tuples except insofar as some replicable activity got recently rolled
back.

The same is not true for the other two tables.

It seems to me that sl_confirm and sl_confirm could really use a
CLUSTER (or similar), here, as the tables are mostly comprised of
"dead space."

Mind you:

 - They are regularly vacuumed by the cleanup loop, probably every 10
   minutes

 - Access usually offers reasonable possibilities for index usage,
   which should allow access to be reasonably efficient even if they
   have some "bloat."

The existing structure of Slony-I tries to vacuum them immediately
after purging data from them; I can't see autovacuum doing a better
job than that.
-- 
(format nil "~S@~S" "cbbrowne" "linuxfinances.info")
http://cbbrowne.com/info/multiplexor.html
"Nondeterminism means never having to say you're wrong."  -- Unknown
From storm at iparadigms.com  Fri Feb  8 13:55:25 2008
From: storm at iparadigms.com (Christian Storm)
Date: Fri Feb  8 13:55:36 2008
Subject: [Slony1-general] Dead tuples on the slave
In-Reply-To: <60ir0z758x.fsf@dba2.int.libertyrms.com>
References: <200802080301.m1831DYV072916@jupiter.jannicash.info>
	<60ir0z758x.fsf@dba2.int.libertyrms.com>
Message-ID: <17B39CA6-C527-451F-A072-158BC0864DE2@iparadigms.com>

Why not have sl_event_[1,2] and sl_confirm_[1,2]?  They seem
to have similar levels of churn.

SkyTools PGQ rotates their queues between three tables every 5 minutes.
See do_maintenance function:
http://cvs.pgfoundry.org/cgi-bin/cvsweb.cgi/skytools/skytools/python/pgq/maint.py?rev=1.2&content-type=text/x-cvsweb-markup

The do_maintenance function calls these plpgsql functions  
(maint_rotate_tables_step1, maint_rotate_tables_step2,  
maint_tables_to_vacuum) defined here:
http://cvs.pgfoundry.org/cgi-bin/cvsweb.cgi/skytools/skytools/sql/pgq/functions/pgq.maint_rotate_tables.sql?rev=1.3&content-type=text/x-cvsweb-markup

They're design was to "We use table rotation to decrease hard disk io."


On Feb 8, 2008, at 9:00 AM, Christopher Browne wrote:

> "Jan Wieck" <JanWieck@Yahoo.com> writes:
>> It not only can, it definitely will hurt performance. A Slony
>> replica experiences the same updates, row per row, as the origin
>> did. This means that the need for vacuuming is exactly the same.
>
> Well, there are *some* differences.
>
> Any transactions that roll back on the origin won't be reflected at
> all on subscribers, so if a system sees a lot of transaction
> ROLLBACKs, that will a point of non-correspondance.
>
> But Christian pointed out three specific Slony-I-managed tables that
> are worth at least a quick look...
>
> - sl_log_2 consisted 91% of dead tuples
> - sl_event had 57% dead tuples
> - sl_confirm was composed of 86% dead tuples
>
> In the 1.2 branch, we do periodically rotate between sl_log_1 and
> sl_log_2 so that *that* space will eventually get reclaimed via a
> TRUNCATE.  In the 2.0 branch, we'll be moving to a strategy of solely
> running TRUNCATE against sl_log_{*}, so there should never be any dead
> tuples except insofar as some replicable activity got recently rolled
> back.
>
> The same is not true for the other two tables.
>
> It seems to me that sl_confirm and sl_confirm could really use a
> CLUSTER (or similar), here, as the tables are mostly comprised of
> "dead space."
>
> Mind you:
>
> - They are regularly vacuumed by the cleanup loop, probably every 10
>   minutes
>
> - Access usually offers reasonable possibilities for index usage,
>   which should allow access to be reasonably efficient even if they
>   have some "bloat."
>
> The existing structure of Slony-I tries to vacuum them immediately
> after purging data from them; I can't see autovacuum doing a better
> job than that.
> -- 
> (format nil "~S@~S" "cbbrowne" "linuxfinances.info")
> http://cbbrowne.com/info/multiplexor.html
> "Nondeterminism means never having to say you're wrong."  -- Unknown

From storm at iparadigms.com  Fri Feb  8 14:08:11 2008
From: storm at iparadigms.com (Christian Storm)
Date: Fri Feb  8 14:08:20 2008
Subject: [Slony1-general] Dead tuples on the slave
In-Reply-To: <72dbd3150802071726y4672855dt1fdd63843c8de4e7@mail.gmail.com>
References: <71274A3E-0964-4A3D-9AC1-74DFEBAAAF5B@iparadigms.com>
	<72dbd3150802071726y4672855dt1fdd63843c8de4e7@mail.gmail.com>
Message-ID: <9E379B52-B113-4F6F-B891-060AA8220AF7@iparadigms.com>

The strange thing is that we have autovacuum set up on this database  
and we allow Slony to vacuum as well.
It seems that this level of bloat is normal?!

I've talked to Josh about this, DB tables make bad queues.

When we use to use DBMirror we use to do this to get around this  
problem.  While in a access exclusive
lock we'd rebuild the table in place every 1/2 hour.  It appears that  
PgQ (SkyTools) is doing something similar, however,
they do it every 5 minutes.

BEGIN;

SELECT now() AS "ts", 'locking tables' AS "step";

LOCK TABLE "Pending" IN ACCESS EXCLUSIVE MODE;
LOCK TABLE "PendingData" IN ACCESS EXCLUSIVE MODE;
LOCK TABLE "MirroredTransaction" IN ACCESS EXCLUSIVE MODE;


ALTER TABLE "Pending" RENAME TO p_old;
ALTER TABLE "PendingData" RENAME TO pd_old;
ALTER TABLE "MirroredTransaction" RENAME TO mt_old;

UPDATE pg_class SET relname = 'p_old_id_seq' WHERE relname =  
'Pending_SeqId_seq';

UPDATE pg_type  SET typname = 'p_old_id_seq' WHERE typname =  
'Pending_SeqId_seq';

ALTER TABLE p_old ALTER COLUMN "SeqId" DROP DEFAULT;
ALTER TABLE p_old ALTER COLUMN "SeqId" SET DEFAULT  
NEXTVAL('p_old_id_seq'::TEXT);


ALTER TABLE pd_old DROP CONSTRAINT "$1";
ALTER TABLE mt_old DROP CONSTRAINT "$2";

ALTER TABLE p_old  DROP CONSTRAINT "Pending_pkey";
ALTER TABLE pd_old DROP CONSTRAINT "PendingData_pkey";
ALTER TABLE mt_old DROP CONSTRAINT "MirroredTransaction_pkey";

DROP INDEX "Pending_XID_Index";

CREATE TABLE "Pending" (
        "SeqId" serial,
        "TableName" varchar NOT NULL,
        "Op" character,
        "XID" int4 NOT NULL,
        PRIMARY KEY ("SeqId")
);
LOCK TABLE "Pending" IN ACCESS EXCLUSIVE MODE;

CREATE TABLE "PendingData" (
        "SeqId" int4 NOT NULL,
        "IsKey" bool NOT NULL,
        "Data" varchar,
       PRIMARY KEY ("SeqId", "IsKey") ,
       FOREIGN KEY ("SeqId") REFERENCES "Pending" ("SeqId") ON UPDATE  
CASCADE  ON DELETE CASCADE
);
LOCK TABLE "PendingData" IN ACCESS EXCLUSIVE MODE;

CREATE TABLE "MirroredTransaction" (
        "XID" int4 NOT NULL,
        "LastSeqId" int4 NOT NULL,
        "MirrorHostId" int4 NOT NULL,
       PRIMARY KEY  ("XID","MirrorHostId"),
       FOREIGN KEY ("MirrorHostId") REFERENCES  
"MirrorHost" ("MirrorHostId") ON UPDATE CASCADE ON DELETE CASCADE,
       FOREIGN KEY ("LastSeqId") REFERENCES "Pending" ("SeqId")  ON  
UPDATE CASCADE ON DELETE CASCADE
);
LOCK TABLE "MirroredTransaction" IN ACCESS EXCLUSIVE MODE;
CREATE INDEX "Pending_XID_Index" ON "Pending" ("XID");
SELECT SETVAL('"Pending_SeqId_seq"', (SELECT MAX("SeqId") FROM p_old)  
+ 1);
INSERT INTO "Pending" SELECT * FROM p_old;
INSERT INTO "PendingData" SELECT * FROM pd_old;
INSERT INTO "MirroredTransaction" SELECT * FROM mt_old;

GRANT SELECT, INSERT, UPDATE, DELETE ON "Pending" TO www_master;
GRANT SELECT, INSERT, UPDATE, DELETE ON "PendingData" TO www_master;
GRANT SELECT, INSERT, UPDATE, DELETE ON "MirroredTransaction" TO  
www_master;
GRANT ALL ON "Pending_SeqId_seq" TO www_master;

GRANT SELECT ON "Pending" TO www;
GRANT SELECT ON "PendingData" TO www;
GRANT SELECT ON "MirroredTransaction" TO www;

COMMIT;

VACUUM ANALYZE pg_class;
VACUUM ANALYZE pg_type;
VACUUM ANALYZE "Pending";
VACUUM ANALYZE "PendingData";
VACUUM ANALYZE "MirroredTransaction";



On Feb 7, 2008, at 5:26 PM, David Rees wrote:

> On Feb 7, 2008 4:16 PM, Christian Storm <storm@iparadigms.com> wrote:
>> However, on one of the slaves I saw a lot of table bloat.  Can this  
>> hurt
>> performance?
>
> It can. Do  you not vacuum the slave database? Autovacuum would help
> you out here at a minimum on the slaves if you didn't want to bother
> with doing a full vacuum analysis.
>
> -Dave

From scetbon at echo.fr  Sat Feb  9 11:00:42 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Sat Feb  9 11:00:53 2008
Subject: [Fwd: Re: [Slony1-general] Configuration on 2 sites / RESOLVED]
Message-ID: <47ADF85A.9010601@echo.fr>

Hi,

I did not get answers about this issue I resolved, and I wanted to know =

if now it's corrected or if the behaviour is simply normal.

Regards.

FYI, the thread was created on  Wed, 29 Aug 2007 11:17:49 +0200 (CEST)

-- =

Cyril SCETBON
-------------- next part --------------
An embedded message was scrubbed...
From: Cyril SCETBON <cscetbon.ext@orange-ftgroup.com>
Subject: Re: [Slony1-general] Configuration on 2 sites / RESOLVED
Date: Fri, 07 Sep 2007 16:23:32 +0200
Size: 9359
Url: http://lists.slony.info/pipermail/slony1-general/attachments/20080209/=
e67b2499/RESOLVED.eml
From devrim at CommandPrompt.com  Sun Feb 10 22:02:17 2008
From: devrim at CommandPrompt.com (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Sun Feb 10 22:03:01 2008
Subject: [Slony1-general] Impending 1.2.13
In-Reply-To: <47AA28D0.9040605@ca.afilias.info>
References: <72dbd3150802041654k7753c820vd7b9b286b1d8d910@mail.gmail.com>
	<60ejbsav81.fsf@dba2.int.libertyrms.com>
	<937d27e10802050111x566a5679l53ae7ecec2018e4d@mail.gmail.com>
	<1202223440.16171.188.camel@bnicholson-desktop>
	<72dbd3150802051248v1b52e128g4a9cecaba01d6247@mail.gmail.com>
	<47A8D8F4.1080201@Yahoo.com>  <47AA28D0.9040605@ca.afilias.info>
Message-ID: <1202709737.24118.57.camel@localhost.localdomain>

SGksCgpPbiBXZWQsIDIwMDgtMDItMDYgYXQgMjE6MzggKzAwMDAsIENocmlzdG9waGVyIEJyb3du
ZSB3cm90ZToKPiBEZXZyaW0gR3VuZHV6IHNvbWV0aW1lcyBoYXMgUlBNIHBhY2thZ2luZyBjaGFu
Z2VzIHRvIG1ha2UgYXQgdGhlIGxhc3QgCj4gbWludXRlLi4uCgpMb29rcyBsaWtlIEkgd2FzIGxh
dGUgYWdhaW4gOi0pCgpSZWdhcmRzLAotLSAKRGV2cmltIEfDnE5Ew5xaICwgUkhDRQpQb3N0Z3Jl
U1FMIFJlcGxpY2F0aW9uLCBDb25zdWx0aW5nLCBDdXN0b20gRGV2ZWxvcG1lbnQsIDI0eDcgc3Vw
cG9ydApNYW5hZ2VkIFNlcnZpY2VzLCBTaGFyZWQgYW5kIERlZGljYXRlZCBIb3N0aW5nCkNvLUF1
dGhvcnM6IHBsUEhQLCBPREJDbmcgLSBodHRwOi8vd3d3LmNvbW1hbmRwcm9tcHQuY29tLwotLS0t
LS0tLS0tLS0tLSBuZXh0IHBhcnQgLS0tLS0tLS0tLS0tLS0KQSBub24tdGV4dCBhdHRhY2htZW50
IHdhcyBzY3J1YmJlZC4uLgpOYW1lOiBub3QgYXZhaWxhYmxlClR5cGU6IGFwcGxpY2F0aW9uL3Bn
cC1zaWduYXR1cmUKU2l6ZTogMTg5IGJ5dGVzCkRlc2M6IFRoaXMgaXMgYSBkaWdpdGFsbHkgc2ln
bmVkIG1lc3NhZ2UgcGFydApVcmwgOiBodHRwOi8vbGlzdHMuc2xvbnkuaW5mby9waXBlcm1haWwv
c2xvbnkxLWdlbmVyYWwvYXR0YWNobWVudHMvMjAwODAyMTAvYjgwMWFhYTAvYXR0YWNobWVudC5w
Z3AK
From vanderleeden at logicunited.com  Mon Feb 11 01:20:57 2008
From: vanderleeden at logicunited.com (Rudolf van der Leeden)
Date: Mon Feb 11 01:21:19 2008
Subject: [Slony1-general] Building Slony 1.2.13 on MacOSX 10.4 (PPC)
Message-ID: <EBE586B4-972E-418F-8856-33C646A781DF@logicunited.com>

I just tested building Slony 1.2.13 and PG 8.3.0 on MacOSX 10.4 (PPC).
I had a minor error building slonik:

gcc -I/fink/include -Wall -Wmissing-prototypes -Wmissing-declarations  
-I../.. -DPGSHARE="\"/fink/share/postgresql/\""  \
   slonik.o dbutil.o parser.o  ../parsestatements/scanner.o -L/fink/ 
lib/ -L/fink/lib/postgresql/ -lpq   -lpgport  -o slonik
/usr/bin/ld: table of contents for archive: /fink/lib//libpgport.a is  
out of date; rerun ranlib(1) (can't load from it)

Running ranlib as suggested:    ranlib /fink/lib/libpgport.a      
solves the problem.

I don't know why this happens. With Slony 1.2.12 I haven't seen this  
problem with libpgport.a.
Anyway, it works fine now.  Many thanks to the team.

Regards,
Rudolf VanderLeeden
Logic United GmbH
Munich, Germany



From lists at serioustechnology.com  Mon Feb 11 05:26:29 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Mon Feb 11 05:26:35 2008
Subject: [Slony1-general] recovery approaches using slony
Message-ID: <47B04D05.3000309@serioustechnology.com>

I've noted that our restore procedure is flawed now that we've started 
replicating our data.  Apparently our backup procedure picks up a schema 
in our primary node that was created by slony.  Thus, when I attempted 
to restore into a database that had a schema created prior to our slony 
implementation, I received an error because the backup data contained 
the slony created schema and the empty database I was attempting to load 
did not.

Question is, what is the best solution here?  Should I permit the 
restore to include the slony schema?  What do other folks do in this 
situation?

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From lists at serioustechnology.com  Mon Feb 11 05:43:36 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Mon Feb 11 05:43:42 2008
Subject: [Slony1-general] recovery approaches using slony
Message-ID: <47B05108.1080401@serioustechnology.com>

I've noted that our restore procedure is flawed now that we've started 
replicating our data.  Apparently our backup procedure picks up a schema 
in our primary node that was created by slony.  Thus, when I attempted 
to restore into a database that had a schema created prior to our slony 
implementation, I received an error because the backup data contained 
the slony created schema and the empty database I was attempting to load 
did not.

Question is, what is the best solution here?  Should I permit the 
restore to include the slony schema?  What do other folks do in this 
situation?

My next question is, did the restore exit when it encountered this 
error, or did it complete?  I'm not sure if it might be the case that 
the slony schema would be restored after the rest of the database.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From wmoran at collaborativefusion.com  Mon Feb 11 06:38:38 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Mon Feb 11 06:38:43 2008
Subject: [Slony1-general] recovery approaches using slony
In-Reply-To: <47B04D05.3000309@serioustechnology.com>
References: <47B04D05.3000309@serioustechnology.com>
Message-ID: <20080211093838.e689e768.wmoran@collaborativefusion.com>

In response to Geoffrey <lists@serioustechnology.com>:

> I've noted that our restore procedure is flawed now that we've started 
> replicating our data.  Apparently our backup procedure picks up a schema 
> in our primary node that was created by slony.  Thus, when I attempted 
> to restore into a database that had a schema created prior to our slony 
> implementation, I received an error because the backup data contained 
> the slony created schema and the empty database I was attempting to load 
> did not.
> 
> Question is, what is the best solution here?  Should I permit the 
> restore to include the slony schema?  What do other folks do in this 
> situation?

There are different approaches possible.

Sometimes I do dump/restores to get a copy of live data in our testing
environment.  When I do that, I just do a full dump from the primary
system (including the slony schema) then restore that to testing systems,
then jump in and drop the slony schema.

For backup purposes, I use the -N option to pg_dump to exclude the Slony
schema.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From vivek at khera.org  Mon Feb 11 07:52:35 2008
From: vivek at khera.org (Vivek Khera)
Date: Mon Feb 11 07:52:40 2008
Subject: [Slony1-general] Dead tuples on the slave
In-Reply-To: <200802080301.m1831DYV072916@jupiter.jannicash.info>
References: <200802080301.m1831DYV072916@jupiter.jannicash.info>
Message-ID: <00AB38DE-3550-4503-83B4-4947102AD426@khera.org>


On Feb 7, 2008, at 10:01 PM, Jan Wieck wrote:

> It not only can, it definitely will hurt performance. A Slony  
> replica experiences the same updates, row per row, as the origin  
> did. This means that the need for vacuuming is exactly the same.

Additionally, if your bloat makes your PK indexes bigger (and more  
sparse) then you also magnify the effect of updates that affect many  
rows.  On the master, it would be one query but on the replica it  
would be expanded to individual row updates.  Having to look up each  
PK in the index will be slowed down by the bloat.

I'd recommend taking a snapshot of your relpages numbers for each  
table and index from the pg_class table, running vacuum and then re- 
index on the big tables, then compare the relpages numbers again.   
Then compare performance and see how the bloat was affecting you.

From lists at serioustechnology.com  Mon Feb 11 11:20:54 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Mon Feb 11 11:21:04 2008
Subject: [Slony1-general] stop/restarting postmaster on replication machine
Message-ID: <47B0A016.604@serioustechnology.com>

If I need to restart the postmaster on my slony replication slave, 
should I shut down the slony daemons first?

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From wmoran at collaborativefusion.com  Mon Feb 11 11:51:54 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Mon Feb 11 11:52:02 2008
Subject: [Slony1-general] stop/restarting postmaster on replication machine
In-Reply-To: <47B0A016.604@serioustechnology.com>
References: <47B0A016.604@serioustechnology.com>
Message-ID: <20080211145154.63b89a08.wmoran@collaborativefusion.com>

In response to Geoffrey <lists@serioustechnology.com>:

> If I need to restart the postmaster on my slony replication slave, 
> should I shut down the slony daemons first?

If you restart the postmaster without restarting the slons, the slons
will be disconnected, sleep for a bit, then reconnect and go back to
work.

So, no, you don't have to manually restart the slons.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From ajs at crankycanuck.ca  Mon Feb 11 11:35:35 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon Feb 11 12:04:47 2008
Subject: [Slony1-general] stop/restarting postmaster on replication machine
In-Reply-To: <47B0A016.604@serioustechnology.com>
References: <47B0A016.604@serioustechnology.com>
Message-ID: <20080211193535.GE953@crankycanuck.ca>

On Mon, Feb 11, 2008 at 02:20:54PM -0500, Geoffrey wrote:
> If I need to restart the postmaster on my slony replication slave, 
> should I shut down the slony daemons first?

They'll complain if not.  But it won't do any harm.

A

From lists at serioustechnology.com  Mon Feb 11 12:39:38 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Mon Feb 11 12:39:50 2008
Subject: [Slony1-general] stop/restarting postmaster on replication machine
In-Reply-To: <20080211145154.63b89a08.wmoran@collaborativefusion.com>
References: <47B0A016.604@serioustechnology.com>
	<20080211145154.63b89a08.wmoran@collaborativefusion.com>
Message-ID: <47B0B28A.4030401@serioustechnology.com>

Bill Moran wrote:
> In response to Geoffrey <lists@serioustechnology.com>:
> 
>> If I need to restart the postmaster on my slony replication slave, 
>> should I shut down the slony daemons first?
> 
> If you restart the postmaster without restarting the slons, the slons
> will be disconnected, sleep for a bit, then reconnect and go back to
> work.
> 
> So, no, you don't have to manually restart the slons.

So I guess this makes me wonder why the slony documents warn you about 
connections that are not reliable.  What happens in a situation where, 
say the master node goes off line temporarily?  Don't the daemons keep 
retrying?

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From cbbrowne at ca.afilias.info  Mon Feb 11 12:42:25 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Feb 11 12:42:33 2008
Subject: [Slony1-general] stop/restarting postmaster on replication machine
In-Reply-To: <47B0B28A.4030401@serioustechnology.com> (Geoffrey's message of
	"Mon, 11 Feb 2008 15:39:38 -0500")
References: <47B0A016.604@serioustechnology.com>
	<20080211145154.63b89a08.wmoran@collaborativefusion.com>
	<47B0B28A.4030401@serioustechnology.com>
Message-ID: <60ejbj6x8u.fsf@dba2.int.libertyrms.com>

Geoffrey <lists@serioustechnology.com> writes:
> Bill Moran wrote:
>> In response to Geoffrey <lists@serioustechnology.com>:
>>
>>> If I need to restart the postmaster on my slony replication slave,
>>> should I shut down the slony daemons first?
>> If you restart the postmaster without restarting the slons, the slons
>> will be disconnected, sleep for a bit, then reconnect and go back to
>> work.
>> So, no, you don't have to manually restart the slons.
>
> So I guess this makes me wonder why the slony documents warn you about
> connections that are not reliable.  What happens in a situation where,
> say the master node goes off line temporarily?  Don't the daemons keep
> retrying?

They will retry, and this may fill your logs with error messages.

The scenario of "unreliable connections" is a bit different; in that
case, the trouble is that the slon considers the connection to be
"live" even though it isn't.  That causes problems.  If slon considers
a dead connection to be dead, there is no confusion :-).
-- 
(reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From lists at serioustechnology.com  Mon Feb 11 12:47:38 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Mon Feb 11 12:47:48 2008
Subject: [Slony1-general] partial replication of a set
Message-ID: <47B0B46A.3090203@serioustechnology.com>

We've found where we are getting a partial replication of a set.  That 
is to say, about half the tables in the set are being replicated and the 
rest are not.  It's like the set was partially processed.  We see the 
first 15 tables in the set replicated, but any tables after those are 
not.  Further, none of the sequences defined in that set are being 
replicated.  It's like it got to a table in the set and ignored 
everything after that.

Is there max limit on the number of tables in a set?  It appears the set 
stopped at 30 table entries.

Clues would be appreciated.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From wmoran at collaborativefusion.com  Mon Feb 11 12:48:04 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Mon Feb 11 12:48:13 2008
Subject: [Slony1-general] stop/restarting postmaster on replication machine
In-Reply-To: <47B0B28A.4030401@serioustechnology.com>
References: <47B0A016.604@serioustechnology.com>
	<20080211145154.63b89a08.wmoran@collaborativefusion.com>
	<47B0B28A.4030401@serioustechnology.com>
Message-ID: <20080211154804.8f1734ee.wmoran@collaborativefusion.com>

In response to Geoffrey <lists@serioustechnology.com>:

> Bill Moran wrote:
> > In response to Geoffrey <lists@serioustechnology.com>:
> > 
> >> If I need to restart the postmaster on my slony replication slave, 
> >> should I shut down the slony daemons first?
> > 
> > If you restart the postmaster without restarting the slons, the slons
> > will be disconnected, sleep for a bit, then reconnect and go back to
> > work.
> > 
> > So, no, you don't have to manually restart the slons.
> 
> So I guess this makes me wonder why the slony documents warn you about 
> connections that are not reliable.  What happens in a situation where, 
> say the master node goes off line temporarily?  Don't the daemons keep 
> retrying?

Because you need to carefully investigate your requirements in the
event that an "unreliable" connection is involved.

Keep in mind that Slony was not _designed_ to work over unreliable
connections.  The fact that it does a pretty good job anyway is a
testament to good design.

However, a very busy database will have a hellatius time keeping in
sync over a flakey connection.  Also, the term "unreliable" means
different things to different people: some people consider a connection
merely "unreliable" if it drops less than 20% of packets on a regular
basis, whereas I would call such a connection outright "broken".

So the point is that Slony is _not_ designed to handle flakey connections.
If the amount of flake on your connection is low enough to allow Slony
to keep up with your volume of data change, then count your blessings.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From wmoran at collaborativefusion.com  Mon Feb 11 12:51:36 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Mon Feb 11 12:51:45 2008
Subject: [Slony1-general] partial replication of a set
In-Reply-To: <47B0B46A.3090203@serioustechnology.com>
References: <47B0B46A.3090203@serioustechnology.com>
Message-ID: <20080211155136.fa3df66a.wmoran@collaborativefusion.com>

In response to Geoffrey <lists@serioustechnology.com>:

> We've found where we are getting a partial replication of a set.  That 
> is to say, about half the tables in the set are being replicated and the 
> rest are not.  It's like the set was partially processed.  We see the 
> first 15 tables in the set replicated, but any tables after those are 
> not.  Further, none of the sequences defined in that set are being 
> replicated.  It's like it got to a table in the set and ignored 
> everything after that.
> 
> Is there max limit on the number of tables in a set?  It appears the set 
> stopped at 30 table entries.

I can assure you that you can replicate over 30 tables.  We have ~150
in one of our databases.

What's in the logs?  Is it possible that Slony is hung up on some sort
of error or other problem on one of those tables?  If you look at
sl_table, are all the tables listed?  What does the sl_status view
say?

Did this do the initial sync and then stop syncing, or did it fail to
do the initial sync?  What kind of activity is on the server: are the
slons doing anything at all?

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From lists at serioustechnology.com  Mon Feb 11 13:06:49 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Mon Feb 11 13:06:58 2008
Subject: [Slony1-general] partial replication of a set
In-Reply-To: <20080211155136.fa3df66a.wmoran@collaborativefusion.com>
References: <47B0B46A.3090203@serioustechnology.com>
	<20080211155136.fa3df66a.wmoran@collaborativefusion.com>
Message-ID: <47B0B8E9.5000309@serioustechnology.com>

Bill Moran wrote:
> In response to Geoffrey <lists@serioustechnology.com>:
> 
>> We've found where we are getting a partial replication of a set.  That 
>> is to say, about half the tables in the set are being replicated and the 
>> rest are not.  It's like the set was partially processed.  We see the 
>> first 15 tables in the set replicated, but any tables after those are 
>> not.  Further, none of the sequences defined in that set are being 
>> replicated.  It's like it got to a table in the set and ignored 
>> everything after that.
>>
>> Is there max limit on the number of tables in a set?  It appears the set 
>> stopped at 30 table entries.
> 
> I can assure you that you can replicate over 30 tables.  We have ~150
> in one of our databases.
> 
> What's in the logs?  Is it possible that Slony is hung up on some sort
> of error or other problem on one of those tables?  If you look at
> sl_table, are all the tables listed?  What does the sl_status view
> say?

I'm digging throught the logs now.  I only see the tables that are being 
replicated listed in sl_table.

> Did this do the initial sync and then stop syncing, or did it fail to
> do the initial sync?  What kind of activity is on the server: are the
> slons doing anything at all?

Unfortunately I just noticed this.  We are replicating 13 databases.  12 
have identical schemas and the same problem exists in them all.

Is it possible for table ids and sequence ids to collide?  That is will 
table id 1 collided with sequence id 1?

I'm now looking at the output from when I tried to create the second 
set, sure enough, the last entry is:

<stdin>:137: Add primary keyed table public.edic

This is the last table in the set that is actually being replicated.


Yet here is an excerpt from the config file, note

"pkeyedtables" => [
.
.
             "edi214r", "edi990", "edi_210_qual", "edi_214_qual", "edic",
             "esc_class", "esc_popup", "etrans", "etransc", "fuel", 

.
.
         ],

So it's like it stopped in the middle of the set.  edic is the 30th 
entry in that pkeyedtable entry.


-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From jpfletch at ca.afilias.info  Mon Feb 11 13:51:28 2008
From: jpfletch at ca.afilias.info (JP Fletcher)
Date: Mon Feb 11 13:50:47 2008
Subject: [Slony1-general] partial replication of a set
In-Reply-To: <47B0B8E9.5000309@serioustechnology.com>
References: <47B0B46A.3090203@serioustechnology.com>	<20080211155136.fa3df66a.wmoran@collaborativefusion.com>
	<47B0B8E9.5000309@serioustechnology.com>
Message-ID: <47B0C360.2050002@ca.afilias.info>

Geoffrey wrote:
> Bill Moran wrote:
>> In response to Geoffrey <lists@serioustechnology.com>:
>>
>>> We've found where we are getting a partial replication of a set.  
>>> That is to say, about half the tables in the set are being 
>>> replicated and the rest are not.  It's like the set was partially 
>>> processed.  We see the first 15 tables in the set replicated, but 
>>> any tables after those are not.  Further, none of the sequences 
>>> defined in that set are being replicated.  It's like it got to a 
>>> table in the set and ignored everything after that.
>>>
>>> Is there max limit on the number of tables in a set?  It appears the 
>>> set stopped at 30 table entries.
>>
>> I can assure you that you can replicate over 30 tables.  We have ~150
>> in one of our databases.
>>
>> What's in the logs?  Is it possible that Slony is hung up on some sort
>> of error or other problem on one of those tables?  If you look at
>> sl_table, are all the tables listed?  What does the sl_status view
>> say?
>
> I'm digging throught the logs now.  I only see the tables that are 
> being replicated listed in sl_table.
>
>> Did this do the initial sync and then stop syncing, or did it fail to
>> do the initial sync?  What kind of activity is on the server: are the
>> slons doing anything at all?
>
> Unfortunately I just noticed this.  We are replicating 13 databases.  
> 12 have identical schemas and the same problem exists in them all.
>
> Is it possible for table ids and sequence ids to collide?  That is 
> will table id 1 collided with sequence id 1?
>
> I'm now looking at the output from when I tried to create the second 
> set, sure enough, the last entry is:
>
> <stdin>:137: Add primary keyed table public.edic
>
> This is the last table in the set that is actually being replicated.
>
>
> Yet here is an excerpt from the config file, note
>
> "pkeyedtables" => [
> .
> .
>             "edi214r", "edi990", "edi_210_qual", "edi_214_qual", "edic",
>             "esc_class", "esc_popup", "etrans", "etransc", "fuel",
> .
> .
>         ],
>
> So it's like it stopped in the middle of the set.  edic is the 30th 
> entry in that pkeyedtable entry.

Do all of the tables in "pkeyedtables" have actual primary keys?  If 
not, you need to specify a candidate unique index, under 'keyedtables' 
in slon_tools.conf if you're using altperl tools.  It's also helpful if 
you wrap the CREATE SET and  SET ADD TABLE commands in a 'try' block, so 
that if there's a problem, everything rolls back...


>
>
>


-- 
JP Fletcher
Database Administrator
Afilias Canada
voice: 416.646.3304 ext. 4123
fax: 416.646.3305
mobile: 416.561.4763
jpfletch@ca.afilias.info


From lists at serioustechnology.com  Mon Feb 11 15:56:25 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Mon Feb 11 15:56:38 2008
Subject: [Slony1-general] partial replication of a set
In-Reply-To: <47B0C360.2050002@ca.afilias.info>
References: <47B0B46A.3090203@serioustechnology.com>	<20080211155136.fa3df66a.wmoran@collaborativefusion.com>
	<47B0B8E9.5000309@serioustechnology.com>
	<47B0C360.2050002@ca.afilias.info>
Message-ID: <47B0E0A9.7040109@serioustechnology.com>

JP Fletcher wrote:

> Do all of the tables in "pkeyedtables" have actual primary keys?

This is the first place I starting looking and sure enough, the table in 
the set that was the first table that did not get replicated (esc_class) 
does not have a primary key.  What amazes me is there was no error 
message, it simply stopped at that point.  My bad, as I didn't catch it 
in my logged output, but still, I'm surprised it doesn't give some 
indication of a problem.

> If 
> not, you need to specify a candidate unique index, under 'keyedtables' 
> in slon_tools.conf if you're using altperl tools.  It's also helpful if 
> you wrap the CREATE SET and  SET ADD TABLE commands in a 'try' block, so 
> that if there's a problem, everything rolls back...

Yeah, we are using the altperl tools.  Thanks for the followup.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From JanWieck at Yahoo.com  Mon Feb 11 17:05:56 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon Feb 11 17:06:39 2008
Subject: [Slony1-general] stop/restarting postmaster on replication machine
In-Reply-To: <20080211154804.8f1734ee.wmoran@collaborativefusion.com>
References: <47B0A016.604@serioustechnology.com>	<20080211145154.63b89a08.wmoran@collaborativefusion.com>	<47B0B28A.4030401@serioustechnology.com>
	<20080211154804.8f1734ee.wmoran@collaborativefusion.com>
Message-ID: <47B0F0F4.4030000@Yahoo.com>

On 2/11/2008 3:48 PM, Bill Moran wrote:
> Keep in mind that Slony was not _designed_ to work over unreliable
> connections.  The fact that it does a pretty good job anyway is a
> testament to good design.

Thanks for that endorsement.

> However, a very busy database will have a hellatius time keeping in
> sync over a flakey connection.  Also, the term "unreliable" means
> different things to different people: some people consider a connection
> merely "unreliable" if it drops less than 20% of packets on a regular
> basis, whereas I would call such a connection outright "broken".

At least you can rely on the fact that the ack and retrans efforts done 
by the TCP stack aren't for nothing ... one might call that "reliable".

Jokes aside, what is more important than packet loss by itself is how 
the connection handles loss of connection. There are many cheap routers 
out there that, when doing some crappy sort of network address 
translation, they not only drop idle connections. I have seen enough of 
these things really dropping them literally, without even bothering to 
send the appropriate TCP notification that the connection was reset.

Slony does not use TCP_KEEPALIVE on its sockets (after all, they are 
standard libpq connections). So if a connection is entirely used to 
listen on a backend for NOTIFY, a thusly dropped connection will make 
that particular listen wait forever.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin
From liobod.slony at gmail.com  Tue Feb 12 05:58:44 2008
From: liobod.slony at gmail.com (lio bod)
Date: Tue Feb 12 05:58:49 2008
Subject: [Slony1-general] compilation : cannot find -lpgport
Message-ID: <d4f444290802120558t51764cf2y5540393af1c4001d@mail.gmail.com>

Hello world,

For the first time of my life i'm trying to compile slony and i'm facing an
issue i haven't seen in mailing list archive...

The step ./configure     --with-pgconfigdir=3D$PGMAIN/bin is ok.

On gmake all, i've got an error message telling : '/usr/bin/ld: cannot
find -lpgport'

My env is :

gcc 3.4.3 20041212 (Red Hat 3.4.3-9.EL4)
slony1-1.2.13
postgresql - v7.4.19 (installed rpms are postgresql-libs, postgresql ,
postgresql-server, postgresql-devel)
Red Hat Enterprise Linux ES release 4 (Nahant) 2.6.9-5

Any hint is welcome,

lio

-----

Here's the end of gmake output (sorry it's french) :

.../...
scan.c:2310: attention : =C2=AB yy_flex_realloc =C2=BB d=C3(c)fini mais n'a=
 pas =C3(c)t=C3(c)
utilis=C3(c)
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../..
-DPGSHARE=3D"\"/usr/share/pgsql/\""  slonik.o dbutil.o parser.o
../parsestatements/scanner.o -L/usr/lib/ -L/usr/lib/pgsql/ -lpq
-Wl,-rpath,/usr/lib/ -lpgport  -o slonik
/usr/bin/ld: ne peut trouver -lpgport
collect2: ld a retourn=C3(c) 1 code d'=C3(c)tat d'ex=C3(c)cution
gmake[2]: *** [slonik] Erreur 1
gmake[2]: Leaving directory `/home/postgresql/slony1-1.2.13/src/slonik'
gmake[1]: *** [all] Erreur 2
gmake[1]: Leaving directory `/home/postgresql/slony1-1.2.13/src'
gmake: *** [all] Erreur 2
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080212/=
5aa9d9dc/attachment.htm
From liobod.slony at gmail.com  Tue Feb 12 08:53:15 2008
From: liobod.slony at gmail.com (lio bod)
Date: Tue Feb 12 08:53:24 2008
Subject: [Slony1-general] Re: compilation : cannot find -lpgport
In-Reply-To: <d4f444290802120558t51764cf2y5540393af1c4001d@mail.gmail.com>
References: <d4f444290802120558t51764cf2y5540393af1c4001d@mail.gmail.com>
Message-ID: <d4f444290802120853v590a3d56g79b603d8039f9164@mail.gmail.com>

is it in relation with Bug 159382 (For RHEL)
<https://bugzilla.redhat.com/bugzilla/show_bug.cgi?id=3D159382>?
https://bugzilla.redhat.com/show_bug.cgi?id=3D159382


Dave Botsch said :

'Stuff such as slony can't be built on rhel4 due to this issue. Please
fix in RHEL4.'

Is it true?

Any known workaround?




2008/2/12, lio bod <liobod.slony@gmail.com>:
>
> Hello world,
>
> For the first time of my life i'm trying to compile slony and i'm facing
> an issue i haven't seen in mailing list archive...
>
> The step ./configure     --with-pgconfigdir=3D$PGMAIN/bin is ok.
>
> On gmake all, i've got an error message telling : '/usr/bin/ld: cannot
> find -lpgport'
>
> My env is :
>
> gcc 3.4.3 20041212 (Red Hat 3.4.3-9.EL4)
> slony1-1.2.13
> postgresql - v7.4.19 (installed rpms are postgresql-libs, postgresql ,
> postgresql-server, postgresql-devel)
> Red Hat Enterprise Linux ES release 4 (Nahant) 2.6.9-5
>
> Any hint is welcome,
>
> lio
>
> -----
>
> Here's the end of gmake output (sorry it's french) :
>
> .../...
> scan.c:2310: attention : =C2=AB yy_flex_realloc =C2=BB d=C3(c)fini mais n=
'a pas =C3(c)t=C3(c)
> utilis=C3(c)
> gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../..
> -DPGSHARE=3D"\"/usr/share/pgsql/\""  slonik.o dbutil.o parser.o
> ../parsestatements/scanner.o -L/usr/lib/ -L/usr/lib/pgsql/ -lpq
> -Wl,-rpath,/usr/lib/ -lpgport  -o slonik
> /usr/bin/ld: ne peut trouver -lpgport
> collect2: ld a retourn=C3(c) 1 code d'=C3(c)tat d'ex=C3(c)cution
> gmake[2]: *** [slonik] Erreur 1
> gmake[2]: Leaving directory `/home/postgresql/slony1-1.2.13/src/slonik'
> gmake[1]: *** [all] Erreur 2
> gmake[1]: Leaving directory `/home/postgresql/slony1-1.2.13/src'
> gmake: *** [all] Erreur 2
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080212/=
92d04758/attachment.htm
From qnex42 at gmail.com  Tue Feb 12 10:55:53 2008
From: qnex42 at gmail.com (Dawid Kuroczko)
Date: Tue Feb 12 10:56:01 2008
Subject: [Slony1-general] Slony1 rollbacks and 8.3.x
Message-ID: <758d5e7f0802121055y2a27d8abp8fcb1cb8c15d8ae0@mail.gmail.com>

PostgreSQL version 8.3 got "lazy xid allocation", I think it is no
longer desirable
for slony to issue ROLLBACK for its read-only queries.

I think it might be very helpful to have a switch which could turn off
this feature. [1]

Just an idea.

   Regards,
     Dawid Kuroczko

[1]: Well, even without lazy xids, I think such a parameter would be
useful.  If you
are monitoring the rollback counter of your databse, you can spot the
application
problem which is not solved elsewhere.  Now if your database is rollbacking most
every second, you are more likely to not notice it.  I know it has a
price with it,
but sometimes it is worth it.  By the way, how much do we really gain by
rollbacking?  I am curious if they really save our day. :-)
From JanWieck at Yahoo.com  Tue Feb 12 11:59:01 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Tue Feb 12 15:16:04 2008
Subject: [Slony1-general] Slony1 rollbacks and 8.3.x
In-Reply-To: <758d5e7f0802121055y2a27d8abp8fcb1cb8c15d8ae0@mail.gmail.com>
References: <758d5e7f0802121055y2a27d8abp8fcb1cb8c15d8ae0@mail.gmail.com>
Message-ID: <47B1FA85.4040504@Yahoo.com>

On 2/12/2008 1:55 PM, Dawid Kuroczko wrote:
> PostgreSQL version 8.3 got "lazy xid allocation", I think it is no
> longer desirable
> for slony to issue ROLLBACK for its read-only queries.
> 
> I think it might be very helpful to have a switch which could turn off
> this feature. [1]

What is the advantage of committing instead of rolling back?


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From qnex42 at gmail.com  Tue Feb 12 15:52:49 2008
From: qnex42 at gmail.com (Dawid Kuroczko)
Date: Tue Feb 12 15:53:00 2008
Subject: [Slony1-general] Slony1 rollbacks and 8.3.x
In-Reply-To: <47B1FA85.4040504@Yahoo.com>
References: <758d5e7f0802121055y2a27d8abp8fcb1cb8c15d8ae0@mail.gmail.com>
	<47B1FA85.4040504@Yahoo.com>
Message-ID: <758d5e7f0802121552r72a50d11g37ce611da33cb757@mail.gmail.com>

On Feb 12, 2008 8:59 PM, Jan Wieck <JanWieck@yahoo.com> wrote:
> On 2/12/2008 1:55 PM, Dawid Kuroczko wrote:
> > PostgreSQL version 8.3 got "lazy xid allocation", I think it is no
> > longer desirable
> > for slony to issue ROLLBACK for its read-only queries.
> >
> > I think it might be very helpful to have a switch which could turn off
> > this feature. [1]
> What is the advantage of committing instead of rolling back?

Assumption: you set up a simple monitoring for PostgreSQL database.
Something as simple as graphing xact_commit and xact_rollback from
pg_stat_database using tool such as RRDTool.

For a slony-less database you usually get flat-zero line for rollbacks
unless there is something wrong (deadlocks, constraint violations, etc).

For slony-I enabled database you will always get many rollbacks
a minute.

Now, intuitively ROLLBACK is something application does when there is
something wrong.  And even though slony's rollbacks are harmless and
a form of optimalisation, I feel that we artificially pump rollback counter up.

In short: slony's commiting instead of rolling back helps database monitoring.

Regards,
    Dawid
From satya461 at gmail.com  Wed Feb 13 00:06:08 2008
From: satya461 at gmail.com (Satya)
Date: Wed Feb 13 00:06:23 2008
Subject: [Slony1-general] Slony-1 stability?
Message-ID: <6ccff2720802130006t4ccdd00fn5b6322a03445c52d@mail.gmail.com>

Hi,


   Can anyone tell me how frequently the community releases Slony-1 newer
versions? Is it related to Postgres releases or independent of it ??


Regards,
Satya.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080213/=
5e0f63b6/attachment.htm
From liobod.slony at gmail.com  Wed Feb 13 00:41:33 2008
From: liobod.slony at gmail.com (lio bod)
Date: Wed Feb 13 00:41:50 2008
Subject: [Slony1-general] Re: compilation : cannot find -lpgport
In-Reply-To: <d4f444290802120853v590a3d56g79b603d8039f9164@mail.gmail.com>
References: <d4f444290802120558t51764cf2y5540393af1c4001d@mail.gmail.com>
	<d4f444290802120853v590a3d56g79b603d8039f9164@mail.gmail.com>
Message-ID: <d4f444290802130041g2937ecb9o43b9d06c4351ec40@mail.gmail.com>

OK. My mistake.
Bug 159382  <https://bugzilla.redhat.com/bugzilla/show_bug.cgi?id=3D159382>=
 is
not compromised.

I made a few tests aroud and the configuration/compilation steps are
succesful for the following version of slony :

slony1-1.1.9
slony1-1.2.0
slony1-1.2.12

Not for slony1-1.2.13. Regression? My understanding?

Hope this info will be useful.
Anyway, I'm thankful for what i've got.

Cheers,


2008/2/12, lio bod <liobod.slony@gmail.com>:
>
> is it in relation with Bug 159382 (For RHEL)
> <https://bugzilla.redhat.com/bugzilla/show_bug.cgi?id=3D159382>?
> https://bugzilla.redhat.com/show_bug.cgi?id=3D159382
>
>
> Dave Botsch said :
>
> 'Stuff such as slony can't be built on rhel4 due to this issue. Please fi=
x in RHEL4.'
>
> Is it true?
>
> Any known workaround?
>
>
>
>
> 2008/2/12, lio bod <liobod.slony@gmail.com>:
> >
> > Hello world,
> >
> > For the first time of my life i'm trying to compile slony and i'm facing
> > an issue i haven't seen in mailing list archive...
> >
> > The step ./configure     --with-pgconfigdir=3D$PGMAIN/bin is ok.
> >
> > On gmake all, i've got an error message telling : '/usr/bin/ld: cannot
> > find -lpgport'
> >
> > My env is :
> >
> > gcc 3.4.3 20041212 (Red Hat 3.4.3-9.EL4)
> > slony1-1.2.13
> > postgresql - v7.4.19 (installed rpms are postgresql-libs, postgresql ,
> > postgresql-server, postgresql-devel)
> > Red Hat Enterprise Linux ES release 4 (Nahant) 2.6.9-5
> >
> > Any hint is welcome,
> >
> > lio
> >
> > -----
> >
> > Here's the end of gmake output (sorry it's french) :
> >
> > .../...
> > scan.c:2310: attention : =C2=AB yy_flex_realloc =C2=BB d=C3(c)fini mais=
 n'a pas
> > =C3(c)t=C3(c) utilis=C3(c)
> > gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../..
> > -DPGSHARE=3D"\"/usr/share/pgsql/\""  slonik.o dbutil.o parser.o
> > ../parsestatements/scanner.o -L/usr/lib/ -L/usr/lib/pgsql/ -lpq
> > -Wl,-rpath,/usr/lib/ -lpgport  -o slonik
> > /usr/bin/ld: ne peut trouver -lpgport
> > collect2: ld a retourn=C3(c) 1 code d'=C3(c)tat d'ex=C3(c)cution
> > gmake[2]: *** [slonik] Erreur 1
> > gmake[2]: Leaving directory `/home/postgresql/slony1-1.2.13/src/slonik'
> > gmake[1]: *** [all] Erreur 2
> > gmake[1]: Leaving directory `/home/postgresql/slony1-1.2.13/src'
> > gmake: *** [all] Erreur 2
> >
> >
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080213/=
45e9f4cb/attachment.htm
From liobod.slony at gmail.com  Wed Feb 13 04:22:18 2008
From: liobod.slony at gmail.com (lio bod)
Date: Wed Feb 13 04:22:38 2008
Subject: [Slony1-general] gmake rpm vs tarball
Message-ID: <d4f444290802130422k2bb187d4xd29649be8ab9604a@mail.gmail.com>

Hello world,

Once i succeed on compiling slony with gmake, the install procedure is a bit
bothering me.
I explain : instead of running 'gmake install', id like a 'gmake rpm' so i
can deploy rpm files on my prod target.
Indeed gmake is not invailable on my production host...

Note I noticed there is a 'rpm' target for gmake that generate a tarball
file (postgresql-slony1-1.2.12.tar.bz2).
Is this supposed to be compliant with rpm command?
I tried * rpm -ivh  postgresql-slony1-1.2.12.tar.bz2*
Does not work.

I Guess this my might be a frequently asked question. But nothing in
mailling list archive.
Any help?

Thx,

----------
my env :

 gcc 3.4.3 20041212 (Red Hat 3.4.3-9.EL4)
slony1-1.2.12
postgresql - v7.4.19 (installed rpms are postgresql-libs, postgresql ,
postgresql-server, postgresql-devel)
Red Hat Enterprise Linux ES release 4 (Nahant) 2.6.9-5
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080213/=
e340f8bb/attachment.htm
From ajs at crankycanuck.ca  Wed Feb 13 07:07:21 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Feb 13 07:07:34 2008
Subject: [Slony1-general] Slony1 rollbacks and 8.3.x
In-Reply-To: <758d5e7f0802121552r72a50d11g37ce611da33cb757@mail.gmail.com>
References: <758d5e7f0802121055y2a27d8abp8fcb1cb8c15d8ae0@mail.gmail.com>
	<47B1FA85.4040504@Yahoo.com>
	<758d5e7f0802121552r72a50d11g37ce611da33cb757@mail.gmail.com>
Message-ID: <20080213150721.GA10082@crankycanuck.ca>

On Wed, Feb 13, 2008 at 12:52:49AM +0100, Dawid Kuroczko wrote:

> For slony-I enabled database you will always get many rollbacks
> a minute.

> In short: slony's commiting instead of rolling back helps database
> monitoring.

If you don't collect baselines of your application for what is its normal
behaviour, I suggest your monitoring plan needs rework.  You still ought to
be able to see unusual numbers of rollbacks with the tool you want.  It just
isn't "0".  But your baseline is whatever it is, and if your monitor tool
can't compare the current behavior to some arbitrary baseline, your tool
needs work.

A
From qnex42 at gmail.com  Wed Feb 13 07:40:00 2008
From: qnex42 at gmail.com (Dawid Kuroczko)
Date: Wed Feb 13 07:40:06 2008
Subject: [Slony1-general] Slony1 rollbacks and 8.3.x
In-Reply-To: <20080213150721.GA10082@crankycanuck.ca>
References: <758d5e7f0802121055y2a27d8abp8fcb1cb8c15d8ae0@mail.gmail.com>
	<47B1FA85.4040504@Yahoo.com>
	<758d5e7f0802121552r72a50d11g37ce611da33cb757@mail.gmail.com>
	<20080213150721.GA10082@crankycanuck.ca>
Message-ID: <758d5e7f0802130740n290b0c14l92e7b8987484a1c7@mail.gmail.com>

On Feb 13, 2008 4:07 PM, Andrew Sullivan <ajs@crankycanuck.ca> wrote:
> On Wed, Feb 13, 2008 at 12:52:49AM +0100, Dawid Kuroczko wrote:
>
> > For slony-I enabled database you will always get many rollbacks
> > a minute.
>
> > In short: slony's commiting instead of rolling back helps database
> > monitoring.
>
> If you don't collect baselines of your application for what is its normal
> behaviour, I suggest your monitoring plan needs rework.  You still ought to
> be able to see unusual numbers of rollbacks with the tool you want.  It just
> isn't "0".  But your baseline is whatever it is, and if your monitor tool
> can't compare the current behavior to some arbitrary baseline, your tool
> needs work.

Of course my tool has a baseline.  The slaves have constant rate between
39 to 41 rollbacks per minute.  The master calls issues ROLLBACK
between 45 and 60, with average of 50 per minute.  Alarm levels are set
accordingly.

Now, the number of rollbacks on master is closely related to number of
DMLs issued (45-50 : quiet database) (60 -- DML load).

But still I feel this is more like a workaround, especially that since 8.3.x
there should be no difference between commit and rollback for read-only
queries.  Or am I wrong?

   Regards,
      Dawid
From cbbrowne at ca.afilias.info  Wed Feb 13 07:47:39 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Feb 13 07:47:44 2008
Subject: [Slony1-general] Slony-1 stability?
In-Reply-To: <6ccff2720802130006t4ccdd00fn5b6322a03445c52d@mail.gmail.com>
	(Satya's message of "Wed, 13 Feb 2008 13:36:08 +0530")
References: <6ccff2720802130006t4ccdd00fn5b6322a03445c52d@mail.gmail.com>
Message-ID: <60wsp86ep0.fsf@dba2.int.libertyrms.com>

Satya <satya461@gmail.com> writes:
> ?? Can anyone tell me how frequently the community releases Slony-1
> newer versions? Is it related to Postgres releases or independent of
> it ??

When changes to PostgreSQL affect how Slony-I needs to work, that
leads to releases of Slony-I.

For instance, the recent 1.2.13 release was needed in order to support
PostgreSQL 8.3, because of changes to some system calls.

But the release cycles have otherwise generally been largely
independent of one another.
-- 
output = reverse("moc.enworbbc" "@" "enworbbc")
http://cbbrowne.com/info/multiplexor.html
"I visited  a company  that was doing  programming in BASIC  in Panama
City and I asked them if they resented that the BASIC keywords were in
English.   The answer  was:  ``Do  you resent  that  the keywords  for
control of actions in music are in Italian?''"  -- Kent M Pitman
From JanWieck at Yahoo.com  Wed Feb 13 11:11:06 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed Feb 13 15:24:55 2008
Subject: [Slony1-general] Slony1 rollbacks and 8.3.x
In-Reply-To: <758d5e7f0802130740n290b0c14l92e7b8987484a1c7@mail.gmail.com>
References: <758d5e7f0802121055y2a27d8abp8fcb1cb8c15d8ae0@mail.gmail.com>	<47B1FA85.4040504@Yahoo.com>	<758d5e7f0802121552r72a50d11g37ce611da33cb757@mail.gmail.com>	<20080213150721.GA10082@crankycanuck.ca>
	<758d5e7f0802130740n290b0c14l92e7b8987484a1c7@mail.gmail.com>
Message-ID: <47B340CA.7060405@Yahoo.com>

On 2/13/2008 10:40 AM, Dawid Kuroczko wrote:
> On Feb 13, 2008 4:07 PM, Andrew Sullivan <ajs@crankycanuck.ca> wrote:
>> On Wed, Feb 13, 2008 at 12:52:49AM +0100, Dawid Kuroczko wrote:
>>
>> > For slony-I enabled database you will always get many rollbacks
>> > a minute.
>>
>> > In short: slony's commiting instead of rolling back helps database
>> > monitoring.
>>
>> If you don't collect baselines of your application for what is its normal
>> behaviour, I suggest your monitoring plan needs rework.  You still ought to
>> be able to see unusual numbers of rollbacks with the tool you want.  It just
>> isn't "0".  But your baseline is whatever it is, and if your monitor tool
>> can't compare the current behavior to some arbitrary baseline, your tool
>> needs work.
> 
> Of course my tool has a baseline.  The slaves have constant rate between
> 39 to 41 rollbacks per minute.  The master calls issues ROLLBACK
> between 45 and 60, with average of 50 per minute.  Alarm levels are set
> accordingly.
> 
> Now, the number of rollbacks on master is closely related to number of
> DMLs issued (45-50 : quiet database) (60 -- DML load).
> 
> But still I feel this is more like a workaround, especially that since 8.3.x
> there should be no difference between commit and rollback for read-only
> queries.  Or am I wrong?

Which means that this might be an option for Slony-I version 2.0. I will 
look over it.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From satya461 at gmail.com  Wed Feb 13 20:30:28 2008
From: satya461 at gmail.com (Satya)
Date: Wed Feb 13 20:30:43 2008
Subject: [Slony1-general] Slony-1 stability?
In-Reply-To: <60wsp86ep0.fsf@dba2.int.libertyrms.com>
References: <6ccff2720802130006t4ccdd00fn5b6322a03445c52d@mail.gmail.com>
	<60wsp86ep0.fsf@dba2.int.libertyrms.com>
Message-ID: <6ccff2720802132030v212ee4f7mc56e9aa4d446807@mail.gmail.com>

Thanks for the reply. So if their nothing required to change to support
newer version of PostgreSQL, then how frequent are the releases?
I mean, how frequently slony-1 is released if it there are no
incompatibilities with  PostgreSQL?  Is it  like every  3months or 6months
or 12months??

Thanks,
Satya

On Feb 13, 2008 9:17 PM, Christopher Browne <cbbrowne@ca.afilias.info>
wrote:

> Satya <satya461@gmail.com> writes:
> >    Can anyone tell me how frequently the community releases Slony-1
> > newer versions? Is it related to Postgres releases or independent of
> > it ??
>
> When changes to PostgreSQL affect how Slony-I needs to work, that
> leads to releases of Slony-I.
>
> For instance, the recent 1.2.13 release was needed in order to support
> PostgreSQL 8.3, because of changes to some system calls.
>
> But the release cycles have otherwise generally been largely
> independent of one another.
> --
> output =3D reverse("moc.enworbbc" "@" "enworbbc")
> http://cbbrowne.com/info/multiplexor.html
> "I visited  a company  that was doing  programming in BASIC  in Panama
> City and I asked them if they resented that the BASIC keywords were in
> English.   The answer  was:  ``Do  you resent  that  the keywords  for
> control of actions in music are in Italian?''"  -- Kent M Pitman
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080214/=
a1c4887c/attachment.htm
From mike at openbunker.org  Thu Feb 14 02:38:14 2008
From: mike at openbunker.org (Mikhail Kolesnik)
Date: Thu Feb 14 02:38:38 2008
Subject: [Slony1-general] few typos in documentaion
Message-ID: <20080214123814.093202d2@amilo.home>

I've noticed a few typos in admin guide, though I haven't read it all
over.

[1] 'restart' describes synopsis like:
 RESTART NODE (options);
should be:
 RESTART NODE options;

[2] 'wait for event'/oddities section has: 
 WAIT FOR EVENT ([...] WAIT FOR=1);
should be: 
 WAIT FOR EVENT ([...] WAIT ON = 1);

[1] http://slony.info/adminguide/current/doc/adminguide/stmtrestartnode.html
[2] http://slony.info/adminguide/current/doc/adminguide/stmtwaitevent.html

-- 
Mikhail Kolesnik
ICQ: 260259143
IRC: mike_k at freenode/#crux, rusnet/#yalta
From mike at openbunker.org  Thu Feb 14 02:40:58 2008
From: mike at openbunker.org (Mikhail Kolesnik)
Date: Thu Feb 14 02:41:24 2008
Subject: [Slony1-general] periodic problems with 3 nodes and controlled
	failover
Message-ID: <20080214124058.45a5e42f@amilo.home>

Hello.

I am using the following configuration: 2 slaves, directly subscribed
to one master. The problem sometimes shows up during controlled
failover with 'move set' (doing it as described in [1]). When doing
move set from one node to another the third node sometimes does not
follow the configuration changes. It keeps complaining about: 
'ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep'
and still points to the wrong origin. This happens nearly in every
third case.

Sending 'restart node 3' or killing slon processes directly helps
_sometimes_.

I've played with 'sync' and 'wait for event' a bit in different places
to make the third node receive that 'move set', but with no luck. 

Script-generated input for slon, which is used to setup, and failover
is attached. Set is a sample pgbench database with one additional
explicit primary key added.

Any suggestions? What can trigger such a behavior?

Here is a piece of log with successful 'move set':
2008-02-14 09:45:21 EET DEBUG2 remoteWorkerThread_2: SYNC 134 done in 0.018 seconds
2008-02-14 09:45:21 EET DEBUG2 remoteWorkerThread_2: Received event 2,135 ACCEPT_SET
2008-02-14 09:45:21 EET DEBUG2 start processing ACCEPT_SET
2008-02-14 09:45:21 EET DEBUG2 ACCEPT: set=1
2008-02-14 09:45:21 EET DEBUG2 ACCEPT: old origin=1
2008-02-14 09:45:21 EET DEBUG2 ACCEPT: new origin=2
2008-02-14 09:45:21 EET DEBUG2 ACCEPT: move set seq=147
2008-02-14 09:45:21 EET DEBUG2 got parms ACCEPT_SET
2008-02-14 09:45:21 EET DEBUG2 ACCEPT_SET - node not origin
2008-02-14 09:45:21 EET DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET exists - adjusting setsync status
2008-02-14 09:45:21 EET DEBUG2 ACCEPT_SET - done
2008-02-14 09:45:21 EET DEBUG2 slon_retry() from pid=18367
2008-02-14 09:45:21 EET DEBUG1 slon: retry requested
2008-02-14 09:45:21 EET DEBUG2 slon: notify worker process to shutdown
2008-02-14 09:45:21 EET INFO   remoteListenThread_2: disconnecting from 'dbname=pgb host=192.168.0.202 port=5432 user=slony password=sln'
2008-02-14 09:45:21 EET DEBUG1 localListenThread: thread done
2008-02-14 09:45:21 EET DEBUG1 remoteListenThread_2: thread done
2008-02-14 09:45:21 EET INFO   remoteListenThread_1: disconnecting from 'dbname=pgb host=192.168.0.201 port=5432 user=slony password=sln'
2008-02-14 09:45:21 EET DEBUG1 cleanupThread: thread done
2008-02-14 09:45:21 EET DEBUG1 remoteListenThread_1: thread done
2008-02-14 09:45:21 EET DEBUG1 syncThread: thread done
2008-02-14 09:45:21 EET DEBUG1 main: scheduler mainloop returned
2008-02-14 09:45:21 EET DEBUG2 main: wait for remote threads

Here is unsuccessful one:
2008-02-14 09:58:24 EET DEBUG2 remoteWorkerThread_2: SYNC 11 processing
2008-02-14 09:58:24 EET DEBUG2 remoteWorkerThread_2: no sets need syncing for this event
2008-02-14 09:58:24 EET DEBUG2 remoteWorkerThread_2: Received event 2,12 ACCEPT_SET
2008-02-14 09:58:24 EET DEBUG2 start processing ACCEPT_SET
2008-02-14 09:58:24 EET DEBUG2 ACCEPT: set=1
2008-02-14 09:58:24 EET DEBUG2 ACCEPT: old origin=1
2008-02-14 09:58:24 EET DEBUG2 ACCEPT: new origin=2
2008-02-14 09:58:24 EET DEBUG2 ACCEPT: move set seq=24
2008-02-14 09:58:24 EET DEBUG2 got parms ACCEPT_SET
2008-02-14 09:58:24 EET DEBUG2 ACCEPT_SET - node not origin
2008-02-14 09:58:24 EET DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep
2008-02-14 09:58:26 EET DEBUG2 remoteListenThread_1: queue event 1,23 SYNC
2008-02-14 09:58:26 EET DEBUG2 remoteWorkerThread_1: Received event 1,23 SYNC
2008-02-14 09:58:26 EET DEBUG2 remoteListenThread_1: queue event 1,24 MOVE_SET
2008-02-14 09:58:26 EET DEBUG2 remoteListenThread_1: queue event 1,25 SYNC
2008-02-14 09:58:26 EET DEBUG2 calc sync size - last time: 1 last length: 2001 ideal: 29 proposed size: 3
2008-02-14 09:58:26 EET DEBUG2 remoteWorkerThread_1: SYNC 23 processing
2008-02-14 09:58:26 EET DEBUG2 remoteWorkerThread_1: syncing set 1 with 4 table(s) from provider 1
2008-02-14 09:58:26 EET DEBUG4  ssy_action_list value:
2008-02-14 09:58:26 EET DEBUG2  ssy_action_list length: 0
2008-02-14 09:58:26 EET DEBUG2 remoteWorkerThread_1: current local log_status is 0
2008-02-14 09:58:26 EET DEBUG3 remoteWorkerThread_1: activate helper 1
2008-02-14 09:58:26 EET DEBUG4 remoteWorkerThread_1: waiting for log data

More logs attached.

Slony-I 1.2.13
postgresql 8.2.6
# gcc -v
Using built-in specs.
Target: i686-pc-linux-gnu
Configured with: ../gcc-4.1.2/configure --prefix=/usr
--libexecdir=/usr/lib --enable-languages=c,c++,objc
--enable-threads=posix --enable-__cxa_atexit --enable-clocale=gnu
--enable-shared --disable-nls --with-x=no Thread model: posix gcc
version 4.1.2 (CRUX)

[1] http://slony.info/adminguide/current/doc/adminguide/failover.html

-- 
Mikhail Kolesnik
ICQ: 260259143
IRC: mike_k at freenode/#crux, rusnet/#yalta
-------------- next part --------------
A non-text attachment was scrubbed...
Name: move_set_FAIL.log
Type: text/x-log
Size: 4877 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080214/f0c5596f/move_set_FAIL-0001.bin
-------------- next part --------------
A non-text attachment was scrubbed...
Name: move_set_OK.log
Type: text/x-log
Size: 14054 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080214/f0c5596f/move_set_OK-0001.bin
-------------- next part --------------
A non-text attachment was scrubbed...
Name: initial_setup.slonik
Type: application/octet-stream
Size: 2228 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080214/f0c5596f/initial_setup-0001.obj
-------------- next part --------------
A non-text attachment was scrubbed...
Name: controlled_failover.slonik
Type: application/octet-stream
Size: 169 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080214/f0c5596f/controlled_failover-0001.obj
From craig_james at emolecules.com  Thu Feb 14 07:52:53 2008
From: craig_james at emolecules.com (Craig James)
Date: Thu Feb 14 07:52:55 2008
Subject: [Slony1-general] runaway vacuum
Message-ID: <47B463D5.90605@emolecules.com>

Every 60 minutes, a vacuum process starts on my master node.  Since I am not running AutoVacuum at all, and have no cron jobs, that only leaves Slony ...

The problem is that this vacuum is now taking over 5 minutes per hour to complete, and makes the system virtually unusable (CPU load goes to 400%).  I don't understand why this should happen -- the Slony replication set isn't very large, and the only tables that change much are log tables that are only appended.  And this is the master, not the slave...

Questions:

1. Does Slony run vacuum every hour?
2. If so, is it a requirement?
3. Any ideas why the vacuum is taking longer and longer?
4.I don't even know what else to ask???

This is PG 8.1, Slony 1.2.9.  (I know, I need to upgrade, we're getting new servers soon and will upgrade then.)


Thanks,
Craig
From craig_james at emolecules.com  Thu Feb 14 08:01:31 2008
From: craig_james at emolecules.com (Craig James)
Date: Thu Feb 14 08:01:32 2008
Subject: [Slony1-general] adding index without telling Slony
Message-ID: <47B465DB.8070805@emolecules.com>

On a replicated table, can I add an index without telling Slony?  Say if I have a replicated table foo(a,b,c,...), could I do "create index ifoo on foo(a)" on the master node without using Slony's EXECUTE SCRIPT?

The reason for this is to experiment with performance.  I'm not sure which index might be best, and I don't want to propagate the index to the slave node until I've tested several solutions.

Thanks,
Craig
From cedric.villemain at dalibo.com  Thu Feb 14 08:05:16 2008
From: cedric.villemain at dalibo.com (=?ISO-8859-1?Q?C=E9dric_Villemain?=)
Date: Thu Feb 14 08:06:35 2008
Subject: [Slony1-general] adding index without telling Slony
In-Reply-To: <47B465DB.8070805@emolecules.com>
References: <47B465DB.8070805@emolecules.com>
Message-ID: <47B466BC.7060203@dalibo.com>

Craig James a ?crit :
> On a replicated table, can I add an index without telling Slony?  Say 
> if I have a replicated table foo(a,b,c,...), could I do "create index 
> ifoo on foo(a)" on the master node without using Slony's EXECUTE SCRIPT?
>
> The reason for this is to experiment with performance.  I'm not sure 
> which index might be best, and I don't want to propagate the index to 
> the slave node until I've tested several solutions.
>
> Thanks,
> Craig
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

Slony doesn't care of index, feel free to adjust index on specific node 
to specific usage

-- 
C?dric Villemain
Administrateur de Base de Donn?es
Cel: +33 (0)6 74 15 56 53
http://dalibo.com - http://dalibo.org

From cbbrowne at ca.afilias.info  Thu Feb 14 08:34:51 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Feb 14 08:34:59 2008
Subject: [Slony1-general] Slony-1 stability?
In-Reply-To: <6ccff2720802132030v212ee4f7mc56e9aa4d446807@mail.gmail.com>
	(Satya's message of "Thu, 14 Feb 2008 10:00:28 +0530")
References: <6ccff2720802130006t4ccdd00fn5b6322a03445c52d@mail.gmail.com>
	<60wsp86ep0.fsf@dba2.int.libertyrms.com>
	<6ccff2720802132030v212ee4f7mc56e9aa4d446807@mail.gmail.com>
Message-ID: <60k5l75wes.fsf@dba2.int.libertyrms.com>

Satya <satya461@gmail.com> writes:
> Thanks for the reply. So if their nothing required to change to
> support newer version of PostgreSQL, then how frequent are the
> releases?  I mean, how frequently slony-1 is released if it there
> are no incompatibilities with? PostgreSQL?? Is it? like every?
> 3months or 6months or 12months??

Release of Slony-I have taken place as a result of:

 - Major enhancements
   - 1.0 was the first version, first released in 2004
   - 1.1 was released in 2005
   - 1.2 was released in 2006
   - Version 2.0 should be forthcoming pretty soon

   These versions have been driven by having done development for a
   period of time, and then releasing a new version.

 - Bugs found
   - Each minor version has had subsequent releases due to us finding 
     problems

 - Incompatibilities with new PG versions
   - This has tended to be addressed by issuing minor releases
     (e.g. - 1.2.13 was a minor release that addressed compatibility
     with PostgreSQL 8.3)
-- 
let name="cbbrowne" and tld="linuxdatabases.info" in name ^ "@" ^ tld;;
http://www3.sympatico.ca/cbbrowne/wp.html
In MDDT, no one can hear you scream...
But everybody can hear you say "whoops!"
From cbbrowne at ca.afilias.info  Thu Feb 14 08:38:57 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Feb 14 08:39:03 2008
Subject: [Slony1-general] few typos in documentaion
In-Reply-To: <20080214123814.093202d2@amilo.home> (Mikhail Kolesnik's message
	of "Thu, 14 Feb 2008 12:38:14 +0200")
References: <20080214123814.093202d2@amilo.home>
Message-ID: <60fxvv5w7y.fsf@dba2.int.libertyrms.com>

Mikhail Kolesnik <mike@openbunker.org> writes:
[some typos in docs]

Fixed in -HEAD, thanks!

http://lists.slony.info/pipermail/slony1-commit/2008-February/002174.html
-- 
output = ("cbbrowne" "@" "linuxdatabases.info")
http://www3.sympatico.ca/cbbrowne/spiritual.html
Wow!  Windows  now can do  everything using shared library  DLLs, just
like Multics  did back in  the 1960s!  Maybe someday  they'll discover
separate processes and pipes, which came out in the 1970s!
From cbbrowne at ca.afilias.info  Thu Feb 14 08:54:29 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Feb 14 08:54:35 2008
Subject: [Slony1-general] runaway vacuum
In-Reply-To: <47B463D5.90605@emolecules.com> (Craig James's message of "Thu,
	14 Feb 2008 07:52:53 -0800")
References: <47B463D5.90605@emolecules.com>
Message-ID: <60bq6j5vi2.fsf@dba2.int.libertyrms.com>

Craig James <craig_james@emolecules.com> writes:
> Every 60 minutes, a vacuum process starts on my master node.  Since
> I am not running AutoVacuum at all, and have no cron jobs, that only
> leaves Slony ...
>
> The problem is that this vacuum is now taking over 5 minutes per
> hour to complete, and makes the system virtually unusable (CPU load
> goes to 400%).  I don't understand why this should happen -- the
> Slony replication set isn't very large, and the only tables that
> change much are log tables that are only appended.  And this is the
> master, not the slave...
>
> Questions:
>
> 1. Does Slony run vacuum every hour?

It shouldn't be exactly hourly, I wouldn't think; Slony-I *does* have
a cleanup thread which runs periodically.

In src/slon/slon.h, the #define SLON_CLEANUP_SLEEP, with default of
600 (indicating 600 seconds), combined with SLON_VACUUM_FREQUENCY
(default 3) combine so that I'd expect to see the cleanup thread
vacuum certain Slony-I tables roughly every 1/2 hour.

I say "roughly" because there is some conscious use of random values
to make it vary a bit.

> 2. If so, is it a requirement?

Probably.  If slon doesn't vacuum, and you don't have AutoVacuum doing
vacuums, and you don't have some custom scripts doing vacuums, then
there are a number of tables that would eventually bloat up into
ridiculous size.

Notably:
 - pg_catalog.pg_listener - which has some quantity of dead tuples
   created in it

 - sl_log_1/sl_log_2 - which collect all the tuples that are updated.
   
   These tables periodically (every couple of days, by default) get
   TRUNCATEd, as there is some tendancy for them to arbitrarily bloat
   up.

   In Slony-I 2.0 (e.g. - CVS HEAD), we have eliminated all DELETEs on
   these tables; the cleanup thread will periodically simply rotate
   between sl_log_1 and sl_log_2, and, once the "not-in-use" table
   has no data that needs to be kept, it will TRUNCATE that table.
   This makes the need to VACUUM disappear.

 - sl_event, sl_confirm - these tables have tuples deleted from them; 
   over time, without VACUUM, they would certainly "bloat" up.

> 3. Any ideas why the vacuum is taking longer and longer?

  Probably one of the above tables has bloated, and needs a CLUSTER to
  reorganize it completely.  You might run a VACUUM VERBOSE by hand on
  each table to see which ones may be bloated.

> 4.I don't even know what else to ask???
>
> This is PG 8.1, Slony 1.2.9.  (I know, I need to upgrade, we're
> getting new servers soon and will upgrade then.)

[We've got some elder versions still in use ;-)]
-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@" [name;tld];;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From devrim at CommandPrompt.com  Thu Feb 14 11:18:34 2008
From: devrim at CommandPrompt.com (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Thu Feb 14 11:18:49 2008
Subject: [Slony1-general] gmake rpm vs tarball
In-Reply-To: <d4f444290802130422k2bb187d4xd29649be8ab9604a@mail.gmail.com>
References: <d4f444290802130422k2bb187d4xd29649be8ab9604a@mail.gmail.com>
Message-ID: <1203016714.24118.245.camel@localhost.localdomain>

SGksCgpPbiBXZWQsIDIwMDgtMDItMTMgYXQgMTM6MjIgKzAxMDAsIGxpbyBib2Qgd3JvdGU6Cj4g
SSB0cmllZCAgcnBtIC1pdmggIHBvc3RncmVzcWwtc2xvbnkxLTEuMi4xMi50YXIuYnoyCj4gRG9l
cyBub3Qgd29yay4KClVzZSBvdXIgeXVtIHJlcG9zaXRvcnkgdG8gaW5zdGFsbCBTbG9ueS1JIG9u
IHlvdXIgUkhFTC00IGJveDoKCmh0dHA6Ly95dW0ucGdzcWxycG1zLm9yZy8KCmh0dHA6Ly95dW0u
cGdzcWxycG1zLm9yZy83LjQvcmVkaGF0L3JoZWwtNC1pMzg2L3Nsb255MS0xLjIuMTItMi5yaGVs
NC5pNjg2LnJwbQoKKG9yIHRoZXJlIGlzIGFsc28gYW4gNjQgYml0IHZlcnNpb24sIGlmIHlvdSBu
ZWVkKS4gVGhpcyBpcyBidWlsdCBhZ2FpbnN0ClBvc3RncmVTUUwgNy40LCBzbyB5b3Ugc2hvdWxk
IG5vdCBoYXZlIGFueSBwcm9ibGVtcy4KCklmIHlvdSBkb24ndCBoYXZlIHl1bSBpbnN0YWxsZWQg
b24geW91ciBzZXJ2ZXIgYW5kIHlvdSBoYXZlIG9ubHkKdXAyZGF0ZSwgdGhlcmUgaXMgbm93IHdh
eSB0byB1c2UgdGhpcyByZXBvc2l0b3J5IChuZXdlciByZXBvIGNyZWF0aW9uCnNjcmlwdHMgZG9u
J3Qgd29yayB3aXRoICB1cDJkYXRlKS4gWW91IGNhbiBpbnN0YWxsIHBhY2thZ2VzCmluZGl2aWR1
YWxseSwgb2YgY291cnNlLgoKUmVnYXJkcywKCi0tIApEZXZyaW0gR8OcTkTDnFogLCBSSENFClBv
c3RncmVTUUwgUmVwbGljYXRpb24sIENvbnN1bHRpbmcsIEN1c3RvbSBEZXZlbG9wbWVudCwgMjR4
NyBzdXBwb3J0Ck1hbmFnZWQgU2VydmljZXMsIFNoYXJlZCBhbmQgRGVkaWNhdGVkIEhvc3RpbmcK
Q28tQXV0aG9yczogcGxQSFAsIE9EQkNuZyAtIGh0dHA6Ly93d3cuY29tbWFuZHByb21wdC5jb20v
Ci0tLS0tLS0tLS0tLS0tIG5leHQgcGFydCAtLS0tLS0tLS0tLS0tLQpBIG5vbi10ZXh0IGF0dGFj
aG1lbnQgd2FzIHNjcnViYmVkLi4uCk5hbWU6IG5vdCBhdmFpbGFibGUKVHlwZTogYXBwbGljYXRp
b24vcGdwLXNpZ25hdHVyZQpTaXplOiAxODkgYnl0ZXMKRGVzYzogVGhpcyBpcyBhIGRpZ2l0YWxs
eSBzaWduZWQgbWVzc2FnZSBwYXJ0ClVybCA6IGh0dHA6Ly9saXN0cy5zbG9ueS5pbmZvL3BpcGVy
bWFpbC9zbG9ueTEtZ2VuZXJhbC9hdHRhY2htZW50cy8yMDA4MDIxNC9iYWNiNjE3ZS9hdHRhY2ht
ZW50LnBncAo=
From Ow.Mun.Heng at wdc.com  Thu Feb 14 11:21:21 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Thu Feb 14 11:21:37 2008
Subject: [Slony1-general] force log_switching to occur
Message-ID: <1203016881.16694.1.camel@neuromancer.home.net>

Is there a way to manually force slony to switch from log_1 to log_2?
The table is getting way big (~1+G) and I think that it's causing the
"fetch 100 from log" to become slow ~60-150secs.


From cbbrowne at ca.afilias.info  Thu Feb 14 11:45:45 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Feb 14 11:45:52 2008
Subject: [Slony1-general] force log_switching to occur
In-Reply-To: <1203016881.16694.1.camel@neuromancer.home.net>
References: <1203016881.16694.1.camel@neuromancer.home.net>
Message-ID: <47B49A69.5020502@ca.afilias.info>

Ow Mun Heng wrote:
> Is there a way to manually force slony to switch from log_1 to log_2?
> The table is getting way big (~1+G) and I think that it's causing the
> "fetch 100 from log" to become slow ~60-150secs.
>   =

You can request this from SQL prompt via:

select _my_slony_schema.logswitch_start();

It may fail with the error:
ERROR: Previous logswitch still in progress

It will not immediately lead to cleaning out sl_log_1; that is handled =

as part of the cleanup thread, which normally runs every 10 minutes =

(controlled by a parameter compiled into slon).

In versions 1.0 thru 1.2, further control of this takes place within the =

cleanup thread, which is almost exclusively C code in =

src/slon/cleanup_thread.c.

In CVS HEAD, nearly all of this code has been folded out into a pl/pgsql =

stored procedure, cleanupevent(interval, boolean), which would allow a =

script to invoke cleanup.

-- =

(reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)

-------------- next part --------------
A non-text attachment was scrubbed...
Name: cbbrowne.vcf
Type: text/x-vcard
Size: 286 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20080214=
/d8046777/cbbrowne.vcf
From Ow.Mun.Heng at wdc.com  Thu Feb 14 12:10:27 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Thu Feb 14 12:10:44 2008
Subject: [Slony1-general] force log_switching to occur
In-Reply-To: <47B49A69.5020502@ca.afilias.info>
References: <1203016881.16694.1.camel@neuromancer.home.net>
	<47B49A69.5020502@ca.afilias.info>
Message-ID: <1203019827.16694.4.camel@neuromancer.home.net>


On Thu, 2008-02-14 at 19:45 +0000, Christopher Browne wrote:
> Ow Mun Heng wrote:
> > Is there a way to manually force slony to switch from log_1 to log_2?
> > The table is getting way big (~1+G) and I think that it's causing the
> > "fetch 100 from log" to become slow ~60-150secs.
> >   
> You can request this from SQL prompt via:
> 
> select _my_slony_schema.logswitch_start();

hmxmms=# select _hmxmms_my_cluster.logswitch_start();
NOTICE:  Slony-I: Logswitch to sl_log_2 initiated
 logswitch_start
-----------------
               2
(1 row)

Time: 1149.556 ms

cool, it has switched.

> 
> It may fail with the error:
> ERROR: Previous logswitch still in progress
> 
> It will not immediately lead to cleaning out sl_log_1; that is handled 
> as part of the cleanup thread, which normally runs every 10 minutes 
> (controlled by a parameter compiled into slon).

I've put this onthe CLI option slon -c2

I'm using version 1.2
> 
> In versions 1.0 thru 1.2, further control of this takes place within the 
> cleanup thread, which is almost exclusively C code in 
> src/slon/cleanup_thread.c.
> 
> In CVS HEAD, nearly all of this code has been folded out into a pl/pgsql 
> stored procedure, cleanupevent(interval, boolean), which would allow a 
> script to invoke cleanup.

Nice to know, but as of right now, way over my head.

Thanks
From craig_james at emolecules.com  Thu Feb 14 18:51:29 2008
From: craig_james at emolecules.com (Craig James)
Date: Thu Feb 14 18:51:46 2008
Subject: [Slony1-general] runaway vacuum
In-Reply-To: <60bq6j5vi2.fsf@dba2.int.libertyrms.com>
References: <47B463D5.90605@emolecules.com>
	<60bq6j5vi2.fsf@dba2.int.libertyrms.com>
Message-ID: <47B4FE31.7080206@emolecules.com>

Christopher Browne wrote:
> Craig James <craig_james@emolecules.com> writes:
>> Questions:
>>
>> 1. Does Slony run vacuum every hour?
> 
> It shouldn't be exactly hourly, I wouldn't think; Slony-I *does* have
> a cleanup thread which runs periodically.
> 
> In src/slon/slon.h, the #define SLON_CLEANUP_SLEEP, with default of
> 600 (indicating 600 seconds), combined with SLON_VACUUM_FREQUENCY
> (default 3) combine so that I'd expect to see the cleanup thread
> vacuum certain Slony-I tables roughly every 1/2 hour.
> 
> I say "roughly" because there is some conscious use of random values
> to make it vary a bit.
> 
>> 2. If so, is it a requirement?
> 
> Probably.  If slon doesn't vacuum, and you don't have AutoVacuum doing
> vacuums, and you don't have some custom scripts doing vacuums, then
> there are a number of tables that would eventually bloat up into
> ridiculous size.

Does Slony only vacuum the tables you mention, or does it launch a general VACUUM of the whole database?  In my case, a general VACUUM statement is inappropriate - most of the database is static.

Thanks,
Craig
From henry at zen.co.za  Thu Feb 14 19:28:45 2008
From: henry at zen.co.za (henry)
Date: Thu Feb 14 22:06:59 2008
Subject: [Slony1-general] Speeding up replication
Message-ID: <64082.196.23.181.69.1203046125.squirrel@zenmail.co.za>

Hello all,

My replication cluster periodically lags behind due to load, etc.  This is
a problem for other systems.

I'm using "slon -g256 -o1000" on the slaves to try and speed up
replication, or to force it to replicate larger chunks of rows at a time.

Is this the best way to do this, and if so, how much larger can I safely
make those values?

If not, is there some other parameter I can set to force slony to
replicate larger chunks of data at a time (obviously at the cost of sys
and network load )?  ... or am I totally off the mark here?

Replication lag is a real deal-killer in my case.

Thanks
Henry

From Ow.Mun.Heng at wdc.com  Fri Feb 15 11:28:21 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Fri Feb 15 11:28:43 2008
Subject: [Slony1-general] logswitch and deleting entries in the other log
	table
Message-ID: <1203103701.28800.13.camel@neuromancer.home.net>

I learned how to manually force a logswitch. Then I wait, and wait for
the delete to kick in so that my original log_* table gets the contents
deleted.

What's the algo behind this cleanup anyway?

I have autovacuum and also set the slon_cleanup thread as c3

From satya461 at gmail.com  Sun Feb 17 21:24:53 2008
From: satya461 at gmail.com (Satya)
Date: Sun Feb 17 21:25:07 2008
Subject: [Slony1-general] Slony-1 stability?
In-Reply-To: <60k5l75wes.fsf@dba2.int.libertyrms.com>
References: <6ccff2720802130006t4ccdd00fn5b6322a03445c52d@mail.gmail.com>
	<60wsp86ep0.fsf@dba2.int.libertyrms.com>
	<6ccff2720802132030v212ee4f7mc56e9aa4d446807@mail.gmail.com>
	<60k5l75wes.fsf@dba2.int.libertyrms.com>
Message-ID: <6ccff2720802172124w23d88c7bi62b06d9c7141e064@mail.gmail.com>

Thanks Browne for the clear picture of the Slony-1 release cycle.

-Regards,
 Satya.

On Feb 14, 2008 10:04 PM, Christopher Browne <cbbrowne@ca.afilias.info>
wrote:

> Satya <satya461@gmail.com> writes:
> > Thanks for the reply. So if their nothing required to change to
> > support newer version of PostgreSQL, then how frequent are the
> > releases?  I mean, how frequently slony-1 is released if it there
> > are no incompatibilities with  PostgreSQL?  Is it  like every
> > 3months or 6months or 12months??
>
> Release of Slony-I have taken place as a result of:
>
>  - Major enhancements
>   - 1.0 was the first version, first released in 2004
>   - 1.1 was released in 2005
>   - 1.2 was released in 2006
>   - Version 2.0 should be forthcoming pretty soon
>
>   These versions have been driven by having done development for a
>   period of time, and then releasing a new version.
>
>  - Bugs found
>   - Each minor version has had subsequent releases due to us finding
>     problems
>
>  - Incompatibilities with new PG versions
>   - This has tended to be addressed by issuing minor releases
>     (e.g. - 1.2.13 was a minor release that addressed compatibility
>     with PostgreSQL 8.3)
> --
> let name=3D"cbbrowne" and tld=3D"linuxdatabases.info" in name ^ "@" ^ tld=
;;
> http://www3.sympatico.ca/cbbrowne/wp.html
> In MDDT, no one can hear you scream...
> But everybody can hear you say "whoops!"
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080218/=
dc15f263/attachment.htm
From scetbon at echo.fr  Mon Feb 18 01:23:48 2008
From: scetbon at echo.fr (Cyril SCETBON)
Date: Mon Feb 18 01:24:13 2008
Subject: [Slony1-general] Slony1 rollbacks and 8.3.x
In-Reply-To: <47B340CA.7060405@Yahoo.com>
References: <758d5e7f0802121055y2a27d8abp8fcb1cb8c15d8ae0@mail.gmail.com>	<47B1FA85.4040504@Yahoo.com>	<758d5e7f0802121552r72a50d11g37ce611da33cb757@mail.gmail.com>	<20080213150721.GA10082@crankycanuck.ca>	<758d5e7f0802130740n290b0c14l92e7b8987484a1c7@mail.gmail.com>
	<47B340CA.7060405@Yahoo.com>
Message-ID: <47B94EA4.8080402@echo.fr>



Jan Wieck wrote:
> On 2/13/2008 10:40 AM, Dawid Kuroczko wrote:
>> On Feb 13, 2008 4:07 PM, Andrew Sullivan <ajs@crankycanuck.ca> wrote:
>>> On Wed, Feb 13, 2008 at 12:52:49AM +0100, Dawid Kuroczko wrote:
>>>
>>> > For slony-I enabled database you will always get many rollbacks
>>> > a minute.
>>>
>>> > In short: slony's commiting instead of rolling back helps database
>>> > monitoring.
>>>
>>> If you don't collect baselines of your application for what is its 
>>> normal
>>> behaviour, I suggest your monitoring plan needs rework.  You still 
>>> ought to
>>> be able to see unusual numbers of rollbacks with the tool you want.  
>>> It just
>>> isn't "0".  But your baseline is whatever it is, and if your monitor 
>>> tool
>>> can't compare the current behavior to some arbitrary baseline, your 
>>> tool
>>> needs work.
>>
>> Of course my tool has a baseline.  The slaves have constant rate between
>> 39 to 41 rollbacks per minute.  The master calls issues ROLLBACK
>> between 45 and 60, with average of 50 per minute.  Alarm levels are set
>> accordingly.
>>
>> Now, the number of rollbacks on master is closely related to number of
>> DMLs issued (45-50 : quiet database) (60 -- DML load).
>>
>> But still I feel this is more like a workaround, especially that 
>> since 8.3.x
>> there should be no difference between commit and rollback for read-only
>> queries.  Or am I wrong?
>
> Which means that this might be an option for Slony-I version 2.0. I 
> will look over it.
When this version is announced to be delivered ?
>
>
> Jan
>

-- 
Cyril SCETBON
From ajs at crankycanuck.ca  Mon Feb 18 07:40:43 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon Feb 18 07:40:58 2008
Subject: [Slony1-general] runaway vacuum
In-Reply-To: <47B4FE31.7080206@emolecules.com>
References: <47B463D5.90605@emolecules.com>
	<60bq6j5vi2.fsf@dba2.int.libertyrms.com>
	<47B4FE31.7080206@emolecules.com>
Message-ID: <20080218154043.GA977@crankycanuck.ca>

On Thu, Feb 14, 2008 at 06:51:29PM -0800, Craig James wrote:
> 
> Does Slony only vacuum the tables you mention, or does it launch a general 
> VACUUM of the whole database?  In my case, a general VACUUM statement is 
> inappropriate - most of the database is static.

Only some tables.  You are vacuuming the tables that are getting changes,
though, right?  (And vacuuming at least once every billion or so
transactions, correct?)

A
From ajs at crankycanuck.ca  Mon Feb 18 08:05:21 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon Feb 18 08:05:31 2008
Subject: [Slony1-general] Speeding up replication
In-Reply-To: <64082.196.23.181.69.1203046125.squirrel@zenmail.co.za>
References: <64082.196.23.181.69.1203046125.squirrel@zenmail.co.za>
Message-ID: <20080218160521.GC977@crankycanuck.ca>

On Fri, Feb 15, 2008 at 05:28:45AM +0200, henry wrote:
> I'm using "slon -g256 -o1000" on the slaves to try and speed up
> replication, or to force it to replicate larger chunks of rows at a time.

The disadvantage is that that may actually increase the apparent lag, since
the chunks all appear in one go (so an older transaction doesn't show up
until the big lump comes in).

> Replication lag is a real deal-killer in my case.

If you need no lag at all, then an async system is probably the wrong choice
for your case.

A

From cbbrowne at ca.afilias.info  Mon Feb 18 11:23:42 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Feb 18 11:23:57 2008
Subject: [Slony1-general] runaway vacuum
In-Reply-To: <47B4FE31.7080206@emolecules.com> (Craig James's message of "Thu,
	14 Feb 2008 18:51:29 -0800")
References: <47B463D5.90605@emolecules.com>
	<60bq6j5vi2.fsf@dba2.int.libertyrms.com>
	<47B4FE31.7080206@emolecules.com>
Message-ID: <6063wmhxvl.fsf@dba2.int.libertyrms.com>

Craig James <craig_james@emolecules.com> writes:
> Christopher Browne wrote:
>> Craig James <craig_james@emolecules.com> writes:
>>> Questions:
>>>
>>> 1. Does Slony run vacuum every hour?
>> It shouldn't be exactly hourly, I wouldn't think; Slony-I *does* have
>> a cleanup thread which runs periodically.
>> In src/slon/slon.h, the #define SLON_CLEANUP_SLEEP, with default of
>> 600 (indicating 600 seconds), combined with SLON_VACUUM_FREQUENCY
>> (default 3) combine so that I'd expect to see the cleanup thread
>> vacuum certain Slony-I tables roughly every 1/2 hour.
>> I say "roughly" because there is some conscious use of random values
>> to make it vary a bit.
>>
>>> 2. If so, is it a requirement?
>> Probably.  If slon doesn't vacuum, and you don't have AutoVacuum
>> doing
>> vacuums, and you don't have some custom scripts doing vacuums, then
>> there are a number of tables that would eventually bloat up into
>> ridiculous size.

> Does Slony only vacuum the tables you mention, or does it launch a
> general VACUUM of the whole database?  In my case, a general VACUUM
> statement is inappropriate - most of the database is static.

Slony-I only vacuums the specific tables that it directly uses; it
will never (barring bugs, of course!) vacuum the entire database.
-- 
output = reverse("ofni.sesabatadxunil" "@" "enworbbc")
http://www3.sympatico.ca/cbbrowne/spiritual.html
"Bawden is  misinformed.  Common Lisp  has no philosophy.  We are held
together only by a shared disgust for all the alternatives."
-- Scott Fahlman, explaining why Common Lisp is the way it is....
From cbbrowne at ca.afilias.info  Mon Feb 18 11:39:08 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Feb 18 11:39:32 2008
Subject: [Slony1-general] Speeding up replication
In-Reply-To: <64082.196.23.181.69.1203046125.squirrel@zenmail.co.za>
	(henry@zen.co.za's message of "Fri,
	15 Feb 2008 05:28:45 +0200 (SAST)")
References: <64082.196.23.181.69.1203046125.squirrel@zenmail.co.za>
Message-ID: <601w7ahx5v.fsf@dba2.int.libertyrms.com>

"henry" <henry@zen.co.za> writes:
> My replication cluster periodically lags behind due to load, etc.  This is
> a problem for other systems.
>
> I'm using "slon -g256 -o1000" on the slaves to try and speed up
> replication, or to force it to replicate larger chunks of rows at a time.
>
> Is this the best way to do this, and if so, how much larger can I safely
> make those values?

There has been a discussion about setting a slon.h parameter,
SLON_DATA_FETCH_SIZE, higher, which might be of some assistance.  That
change causes slon to process more INSERT/UPDATE/DELETE queries in one
request.

> If not, is there some other parameter I can set to force slony to
> replicate larger chunks of data at a time (obviously at the cost of sys
> and network load )?  ... or am I totally off the mark here?
>
> Replication lag is a real deal-killer in my case.

"Larger chunks of data" is likely to be at odds with "reducing
replication lag."

The least lag requires processing only the few latest updates
together.

--> The only way that you're processing more than 1 SYNC at a time is
    if replication is lagging behind.

    In that case, the two options are at odds with one another:

     - Processing a bunch of SYNCs together should churn through
       the changes quicker, but still means you're accepting a
       possibly substantial lag time.

     - Processing small numbers of SYNCs at a time means that
       replication won't catch up as quickly, but does mean
       lower lag between processing SYNCs.

    Realistically, the usual answer is to process a bunch of SYNCs to
    catch up ASAP.  But that *is* at odds with your requirement.

--> If replication is up to date, then no grouping of SYNCs is done;
    they are processed as soon as they are received.
-- 
let name="cbbrowne" and tld="linuxdatabases.info" in String.concat "@" [name;tld];;
http://linuxfinances.info/info/
Q: What does FAQ stand for?
A: We are Frequently Asked this Question, and thus far have no idea.
From cbbrowne at ca.afilias.info  Mon Feb 18 11:43:19 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Feb 18 11:43:27 2008
Subject: [Slony1-general] logswitch and deleting entries in the other log
	table
In-Reply-To: <1203103701.28800.13.camel@neuromancer.home.net> (Ow Mun Heng's
	message of "Sat, 16 Feb 2008 03:28:21 +0800")
References: <1203103701.28800.13.camel@neuromancer.home.net>
Message-ID: <60wsp2gieg.fsf@dba2.int.libertyrms.com>

Ow Mun Heng <Ow.Mun.Heng@wdc.com> writes:
> I learned how to manually force a logswitch. Then I wait, and wait for
> the delete to kick in so that my original log_* table gets the contents
> deleted.
>
> What's the algo behind this cleanup anyway?

It's in the code; generally, we consider switching logs if that seems
practical.  (If we are still processing a previous log switch, then it
ISN'T practical ;-)!)

It gets handled in the fairly asynchronous fashion so that we can be
pretty sure that it is safe to TRUNCATE the empty table, and don't
need to worry about that TRUNCATE causing other things to block.

This bit us, occasionally, with the predecessor eRServer system.  It
was not as careful about it's approach to using TRUNCATE, and that
would sometimes lead to application outages when a log table would be
locked by the TRUNCATE :-(.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://cbbrowne.com/info/lisp.html
Space Corps Directive #997: Work done  by an officer's doppleganger in
a parallel universe cannot be claimed as overtime.  -- Red Dwarf
From cbbrowne at ca.afilias.info  Mon Feb 18 11:52:03 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Feb 18 11:52:18 2008
Subject: [Slony1-general] Slony1 rollbacks and 8.3.x
In-Reply-To: <47B94EA4.8080402@echo.fr> (Cyril SCETBON's message of "Mon,
	18 Feb 2008 10:23:48 +0100")
References: <758d5e7f0802121055y2a27d8abp8fcb1cb8c15d8ae0@mail.gmail.com>
	<47B1FA85.4040504@Yahoo.com>
	<758d5e7f0802121552r72a50d11g37ce611da33cb757@mail.gmail.com>
	<20080213150721.GA10082@crankycanuck.ca>
	<758d5e7f0802130740n290b0c14l92e7b8987484a1c7@mail.gmail.com>
	<47B340CA.7060405@Yahoo.com> <47B94EA4.8080402@echo.fr>
Message-ID: <60skzqghzw.fsf@dba2.int.libertyrms.com>

Cyril SCETBON <scetbon@echo.fr> writes:
>> Which means that this might be an option for Slony-I version 2.0. I
>> will look over it.
> When this version is announced to be delivered ?

It hasn't been announced; the focus has been on 1.2.13 (which,
regrettably, will probably turn into 1.2.14, due to a controversial
change...  <http://bugs.slony.info/bugzilla/show_bug.cgi?id=35>).

I think it's probably pretty close to the point where we should tag a
tarball for testing.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://linuxdatabases.info/info/spreadsheets.html
"Microsoft has world class quality control" -- Arthur Norman
From lists at serioustechnology.com  Mon Feb 18 12:19:59 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Mon Feb 18 12:20:16 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
Message-ID: <47B9E86F.2040907@serioustechnology.com>

I want to make sure I have a good handle on this issue.  We currently 
have a master/slave configuration.  In the event the slave must be 
rebooted, what are the proper steps to insure that slony picks up from 
where it left off.

What we are currently doing is simply restart the slon daemons for each 
database.

For the most part this appears to work, but what concerns me is that I 
have one table on one database where the number of records on the master 
node has not increased in a while and the slave does not appear to be 
'trying to catch up.'  That is to say, the slave has fewer records in 
that table then the master and the slave table is not growing.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From henry at zen.co.za  Mon Feb 18 10:00:04 2008
From: henry at zen.co.za (Henry)
Date: Mon Feb 18 13:18:35 2008
Subject: [Slony1-general] Speeding up replication
In-Reply-To: <601w7ahx5v.fsf@dba2.int.libertyrms.com>
References: <64082.196.23.181.69.1203046125.squirrel@zenmail.co.za>
	<601w7ahx5v.fsf@dba2.int.libertyrms.com>
Message-ID: <64745.196.23.181.69.1203357604.squirrel@zenmail.co.za>

On Mon, February 18, 2008 9:39 pm, Christopher Browne wrote:
> ...

Thanks for the comments, Christopher.  I accidentally posted to Andrew
only, so here's my response to the list:

I've tackled the lag problem from another angle:  refactored my cluster
systems to sleep whenever the lag status on master exceeds a threshold
(and keep sleeping until things are back under control).

When things go quiet (system activity-wise) to allow replication catch-up,
it *is* a waste of resources, but this is the lesser of two evils.

btw, I've occasionally noticed the following slony activity on the slaves:

"update table set id=100 where id=100"

(where id is the primary key)

wtf?  Presumably this is some kind of replication artifact or -something-.
Problem is, this kind of weird update on *HUGE* tables on the slaves could
be what sometimes contributes to lag...

Any idea what this is about?

Regards
Henry

From cbbrowne at ca.afilias.info  Mon Feb 18 13:19:59 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Feb 18 13:20:07 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47B9E86F.2040907@serioustechnology.com> (Geoffrey's message of
	"Mon, 18 Feb 2008 15:19:59 -0500")
References: <47B9E86F.2040907@serioustechnology.com>
Message-ID: <60odaegdxc.fsf@dba2.int.libertyrms.com>

Geoffrey <lists@serioustechnology.com> writes:
> I want to make sure I have a good handle on this issue.  We currently
> have a master/slave configuration.  In the event the slave must be
> rebooted, what are the proper steps to insure that slony picks up from
> where it left off.
>
> What we are currently doing is simply restart the slon daemons for
> each database.

That seems apropos.

> For the most part this appears to work, but what concerns me is that
> I have one table on one database where the number of records on the
> master node has not increased in a while and the slave does not
> appear to be 'trying to catch up.'  That is to say, the slave has
> fewer records in that table then the master and the slave table is
> not growing.

Well, then "get thee to the slon logs..."

-> Do they indicate, for the subscriber, that errors are being experienced?

-> Do they indicate that SYNCs are being processed, and data applied?

-> Is the subscriber in question behind (according to the view
   sl_status) by an increasing amount of time?

Error messages in the slon logs should give some idea of what is going on.
-- 
let name="cbbrowne" and tld="cbbrowne.com" in name ^ "@" ^ tld;;
http://linuxfinances.info/info/advocacy.html
If a stealth bomber crashes in a forest, will it make a sound?
From lists at serioustechnology.com  Mon Feb 18 13:46:52 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Mon Feb 18 13:47:10 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <60odaegdxc.fsf@dba2.int.libertyrms.com>
References: <47B9E86F.2040907@serioustechnology.com>
	<60odaegdxc.fsf@dba2.int.libertyrms.com>
Message-ID: <47B9FCCC.5030004@serioustechnology.com>

Christopher Browne wrote:
> Geoffrey <lists@serioustechnology.com> writes:
>> I want to make sure I have a good handle on this issue.  We currently
>> have a master/slave configuration.  In the event the slave must be
>> rebooted, what are the proper steps to insure that slony picks up from
>> where it left off.
>>
>> What we are currently doing is simply restart the slon daemons for
>> each database.
> 
> That seems apropos.
> 
>> For the most part this appears to work, but what concerns me is that
>> I have one table on one database where the number of records on the
>> master node has not increased in a while and the slave does not
>> appear to be 'trying to catch up.'  That is to say, the slave has
>> fewer records in that table then the master and the slave table is
>> not growing.
> 
> Well, then "get thee to the slon logs..."
> 
> -> Do they indicate, for the subscriber, that errors are being experienced?

Not that I can tell.  That is the first placed I looked.

> -> Do they indicate that SYNCs are being processed, and data applied?

SYNCs processed, but says nothing to process:

2008-02-18 16:39:27 EST DEBUG2 localListenThread: Received event 1,10345 
SYNC
2008-02-18 16:39:32 EST DEBUG2 remoteListenThread_2: queue event 2,7343 SYNC
2008-02-18 16:39:32 EST DEBUG2 remoteWorkerThread_2: Received event 
2,7343 SYNC
2008-02-18 16:39:32 EST DEBUG2 remoteWorkerThread_2: SYNC 7343 processing
2008-02-18 16:39:32 EST DEBUG2 remoteWorkerThread_2: no sets need 
syncing for this event

> -> Is the subscriber in question behind (according to the view
>    sl_status) by an increasing amount of time?

I'm not sure what I'm looking for here.  From the slave:

master=# select * from  _master_cluster.sl_status;
  st_origin | st_received | st_last_event |      st_last_event_ts      | 
st_last_received |    st_last_received_ts     | 
st_last_received_event_ts  | st_lag_num_events | st_lag_time
-----------+-------------+---------------+----------------------------+------------------+----------------------------+----------------------------+-------------------+-------------
          2 |           1 |          7377 | 02/18/2008 16:45:11.298502 | 
     7377 | 02/18/2008 16:45:11.728581 | 02/18/2008 16:45:11.298502 | 
    0 | @ 4.40 secs


> Error messages in the slon logs should give some idea of what is going on.


-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From cbbrowne at ca.afilias.info  Mon Feb 18 17:33:54 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Feb 18 17:34:15 2008
Subject: [Slony1-general] Speeding up replication
In-Reply-To: <64745.196.23.181.69.1203357604.squirrel@zenmail.co.za>
	(henry@zen.co.za's message of "Mon,
	18 Feb 2008 20:00:04 +0200 (SAST)")
References: <64082.196.23.181.69.1203046125.squirrel@zenmail.co.za>
	<601w7ahx5v.fsf@dba2.int.libertyrms.com>
	<64745.196.23.181.69.1203357604.squirrel@zenmail.co.za>
Message-ID: <60k5l1hgql.fsf@dba2.int.libertyrms.com>

"Henry" <henry@zen.co.za> writes:
> btw, I've occasionally noticed the following slony activity on the slaves:
>
> "update table set id=100 where id=100"
>
> (where id is the primary key)
>
> wtf?  Presumably this is some kind of replication artifact or -something-.
> Problem is, this kind of weird update on *HUGE* tables on the slaves could
> be what sometimes contributes to lag...
>
> Any idea what this is about?

This would be the case where something is updating tuples on the
origin, and not even actually changing any data.

Thus, perhaps, 
   update table set status = 'done' where created_on > '2007-08-31';

where there are a bunch of tuples with "created_on > '2007-08-31'"
which have status = 'done' already.

That would be a case where I'd sorta like to change the query to:
   update table set status = 'done' where created_on > '2007-08-31' and status <> 'done';

The latter query would not cause Slony-I "logtrigger()" triggers to
fire in cases where the status was already 'done'.

This is by design; note that...

- On the origin, all the tuples did get touched, which would mean that
  "ON UPDATE" triggers on the table would fire;

- We preserve that semantic on the subscriber.
-- 
(format nil "~S@~S" "cbbrowne" "linuxfinances.info")
http://linuxdatabases.info/info/linux.html
Developmental Psychology
"Schoolyard behavior resembles adult primate behavior because "Ontogeny
Recapitulates Phylogeny" doesn't stop at birth."
-- Mark Miller
From henry at zen.co.za  Tue Feb 19 01:14:39 2008
From: henry at zen.co.za (Henry)
Date: Tue Feb 19 04:40:44 2008
Subject: [Slony1-general] Speeding up replication
In-Reply-To: <60k5l1hgql.fsf@dba2.int.libertyrms.com>
References: <64082.196.23.181.69.1203046125.squirrel@zenmail.co.za>
	<601w7ahx5v.fsf@dba2.int.libertyrms.com>
	<64745.196.23.181.69.1203357604.squirrel@zenmail.co.za>
	<60k5l1hgql.fsf@dba2.int.libertyrms.com>
Message-ID: <63439.196.23.181.69.1203412479.squirrel@zenmail.co.za>

On Tue, February 19, 2008 3:33 am, Christopher Browne wrote:
> This would be the case where something is updating tuples on the
> origin, and not even actually changing any data.
>
> Thus, perhaps,
>    update table set status = 'done' where created_on > '2007-08-31';
>
> where there are a bunch of tuples with "created_on > '2007-08-31'"
> which have status = 'done' already.

Ah, ok, that makes sense.  Thank you for the clarification.

Regards
Henry

From lists at serioustechnology.com  Tue Feb 19 05:46:49 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Tue Feb 19 05:46:54 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47B9FCCC.5030004@serioustechnology.com>
References: <47B9E86F.2040907@serioustechnology.com>	<60odaegdxc.fsf@dba2.int.libertyrms.com>
	<47B9FCCC.5030004@serioustechnology.com>
Message-ID: <47BADDC9.8050606@serioustechnology.com>

It's now obvious that there's a problem with slony for this one 
database.  For example, on one table, the primary node has grown in the 
number of records by about 6000 records, still the slave is sitting at 
the same value from yesterday.

Should I restart the daemons?  Do I need to start over?  I don't seen 
anything in the logs that tells me there's a problem.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From lists at serioustechnology.com  Tue Feb 19 05:50:54 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Tue Feb 19 05:50:58 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BADDC9.8050606@serioustechnology.com>
References: <47B9E86F.2040907@serioustechnology.com>	<60odaegdxc.fsf@dba2.int.libertyrms.com>	<47B9FCCC.5030004@serioustechnology.com>
	<47BADDC9.8050606@serioustechnology.com>
Message-ID: <47BADEBE.9000608@serioustechnology.com>

Okay, more research has found a problem, but I don't know how this 
happened, and I don't know how to fix it.

Looking at the list of schemas for the primary node database, the slony 
schema is not listed.

So, why is it I'm not seeing an error regarding this issue?

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From lists at serioustechnology.com  Tue Feb 19 05:56:06 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Tue Feb 19 05:56:15 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BADEBE.9000608@serioustechnology.com>
References: <47B9E86F.2040907@serioustechnology.com>	<60odaegdxc.fsf@dba2.int.libertyrms.com>	<47B9FCCC.5030004@serioustechnology.com>	<47BADDC9.8050606@serioustechnology.com>
	<47BADEBE.9000608@serioustechnology.com>
Message-ID: <47BADFF6.7000104@serioustechnology.com>

Belay that post, I was on the wrong box when I checked this.  The schema 
does show up in the database on the primary node.

Sorry, need more coffee..

Geoffrey wrote:
> Okay, more research has found a problem, but I don't know how this 
> happened, and I don't know how to fix it.
> 
> Looking at the list of schemas for the primary node database, the slony 
> schema is not listed.
> 
> So, why is it I'm not seeing an error regarding this issue?
> 


-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From ajs at crankycanuck.ca  Tue Feb 19 07:02:54 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue Feb 19 07:03:04 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BADDC9.8050606@serioustechnology.com>
References: <47B9E86F.2040907@serioustechnology.com>
	<60odaegdxc.fsf@dba2.int.libertyrms.com>
	<47B9FCCC.5030004@serioustechnology.com>
	<47BADDC9.8050606@serioustechnology.com>
Message-ID: <20080219150254.GA6148@crankycanuck.ca>

On Tue, Feb 19, 2008 at 08:46:49AM -0500, Geoffrey wrote:
> It's now obvious that there's a problem with slony for this one 
> database.  For example, on one table, the primary node has grown in the 
> number of records by about 6000 records, still the slave is sitting at 
> the same value from yesterday.
> 
> Should I restart the daemons?  Do I need to start over?  I don't seen 
> anything in the logs that tells me there's a problem.

I would restart the daemons and see if that fixes it, yes (although I'd
expect an error).  But I wonder whether your slave is properly configured. 
Are you sure you have fsync enabled and write cacheing turned off?  Was this
a controlled reboot, or did it reboot itself?  If you have lost cached data,
then your replica could indeed be missing data: Slony's only going to be as
reliable as the underlying PostgreSQL installation.

A
From lists at serioustechnology.com  Tue Feb 19 07:30:44 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Tue Feb 19 07:30:53 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <20080219150254.GA6148@crankycanuck.ca>
References: <47B9E86F.2040907@serioustechnology.com>	<60odaegdxc.fsf@dba2.int.libertyrms.com>	<47B9FCCC.5030004@serioustechnology.com>	<47BADDC9.8050606@serioustechnology.com>
	<20080219150254.GA6148@crankycanuck.ca>
Message-ID: <47BAF624.2020907@serioustechnology.com>

Andrew Sullivan wrote:
> On Tue, Feb 19, 2008 at 08:46:49AM -0500, Geoffrey wrote:
>> It's now obvious that there's a problem with slony for this one 
>> database.  For example, on one table, the primary node has grown in the 
>> number of records by about 6000 records, still the slave is sitting at 
>> the same value from yesterday.
>>
>> Should I restart the daemons?  Do I need to start over?  I don't seen 
>> anything in the logs that tells me there's a problem.
> 
> I would restart the daemons and see if that fixes it, yes (although I'd
> expect an error).  But I wonder whether your slave is properly configured. 
> Are you sure you have fsync enabled and write cacheing turned off?  Was this
> a controlled reboot, or did it reboot itself?  If you have lost cached data,
> then your replica could indeed be missing data: Slony's only going to be as
> reliable as the underlying PostgreSQL installation.

We never actually rebooted the box, we stop/started the database by 
stopping the daemons, restart the postmaster and start the daemons.

What concerns me now is I'm finding other inconsistencies in other 
databases where the postmaster has been up and running since replication 
was initiated.

Now that I'm starting to research this, I see that there are a number of 
databases with inconsistencies.  We are replicating 12 databases.

While I was checking the count(*) of one table in another database, 
there was a difference of a couple 100 records between the primary and 
slave.  I saw the primary node count go up by one and the slave did the 
same.  Thus it appears that the difference is going to stay the same.

When comparing the differences in the data, it's quite irregular.  That 
is, it does not appear to be a chunk of data from a specific time frame 
that was missed, it's from different days.

The postmasters for this database on both the primary and slave have 
been running since before replication was started, so that is apparently 
not the issue at all.

I don't understand how this is even possible.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From lists at serioustechnology.com  Tue Feb 19 08:13:58 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Tue Feb 19 08:14:05 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BAF624.2020907@serioustechnology.com>
References: <47B9E86F.2040907@serioustechnology.com>	<60odaegdxc.fsf@dba2.int.libertyrms.com>	<47B9FCCC.5030004@serioustechnology.com>	<47BADDC9.8050606@serioustechnology.com>	<20080219150254.GA6148@crankycanuck.ca>
	<47BAF624.2020907@serioustechnology.com>
Message-ID: <47BB0046.10109@serioustechnology.com>

Still digging through the log files.  I find no errors, but I find the 
following warning a handful of times in various logs for the slave node:

master-2008-02-18_14:36:45.log:2008-02-18 14:36:45 EST WARN 
remoteWorker_wakeup: node 1 - no worker thread

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From lists at serioustechnology.com  Tue Feb 19 08:24:10 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Tue Feb 19 08:24:19 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BB0046.10109@serioustechnology.com>
References: <47B9E86F.2040907@serioustechnology.com>	<60odaegdxc.fsf@dba2.int.libertyrms.com>	<47B9FCCC.5030004@serioustechnology.com>	<47BADDC9.8050606@serioustechnology.com>	<20080219150254.GA6148@crankycanuck.ca>	<47BAF624.2020907@serioustechnology.com>
	<47BB0046.10109@serioustechnology.com>
Message-ID: <47BB02AA.2060902@serioustechnology.com>

I'm finding some records that exist in the master that do not exist in 
the slave.  I'm also finding records that exist in the slave, that don't 
exist in the master.

I simply don't understand how this is possible.

I'm assuming the first issue is a record that was never populated to the 
slave, whereas the second would be a delete that was never populated to 
the slave.

Further, these examples are 4 days old, so it's not the case that the 
slave just hasn't caught up.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From cbbrowne at ca.afilias.info  Tue Feb 19 11:10:25 2008
From: cbbrowne at ca.afilias.info (cbbrowne@ca.afilias.info)
Date: Tue Feb 19 11:10:32 2008
Subject: [Slony1-general] proper procedure for re-starting slony after 
	replication slave reboots
In-Reply-To: <47BB02AA.2060902@serioustechnology.com>
References: <47B9E86F.2040907@serioustechnology.com>
	<60odaegdxc.fsf@dba2.int.libertyrms.com>
	<47B9FCCC.5030004@serioustechnology.com>
	<47BADDC9.8050606@serioustechnology.com>
	<20080219150254.GA6148@crankycanuck.ca>
	<47BAF624.2020907@serioustechnology.com>
	<47BB0046.10109@serioustechnology.com>
	<47BB02AA.2060902@serioustechnology.com>
Message-ID: <62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>

> I'm finding some records that exist in the master that do not exist in
> the slave.  I'm also finding records that exist in the slave, that don't
> exist in the master.
>
> I simply don't understand how this is possible.
>
> I'm assuming the first issue is a record that was never populated to the
> slave, whereas the second would be a delete that was never populated to
> the slave.
>
> Further, these examples are 4 days old, so it's not the case that the
> slave just hasn't caught up.

The more you describe this, the more this is sounding like a case where
the databases may have (somehow?) gotten corrupted.

It is not self-evident whether it is the origin or subscribers or both
that have gotten corrupted.

If that is the case, then it is not evident that there is any problem with
Slony-I in this.  This isn't something Slony-I should have caused, and,
alas, it is not something it can necessarily protect you from.  Notably,
if both DBs have been corrupted, all bets are off altogether.

I think you need to look into your backups :-(.

From lists at serioustechnology.com  Tue Feb 19 12:25:56 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Tue Feb 19 12:26:09 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>
References: <47B9E86F.2040907@serioustechnology.com>
	<60odaegdxc.fsf@dba2.int.libertyrms.com>
	<47B9FCCC.5030004@serioustechnology.com>
	<47BADDC9.8050606@serioustechnology.com>
	<20080219150254.GA6148@crankycanuck.ca>
	<47BAF624.2020907@serioustechnology.com>
	<47BB0046.10109@serioustechnology.com>
	<47BB02AA.2060902@serioustechnology.com>
	<62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>
Message-ID: <47BB3B54.7050306@serioustechnology.com>

cbbrowne@ca.afilias.info wrote:

> The more you describe this, the more this is sounding like a case where
> the databases may have (somehow?) gotten corrupted.
> 
> It is not self-evident whether it is the origin or subscribers or both
> that have gotten corrupted.
> 
> If that is the case, then it is not evident that there is any problem with
> Slony-I in this.  This isn't something Slony-I should have caused, and,
> alas, it is not something it can necessarily protect you from.  Notably,
> if both DBs have been corrupted, all bets are off altogether.
> 
> I think you need to look into your backups :-(.

And fortunately we have those.

I'm assuming that triggers on my replication database would be disabled 
by slony.  Is this correct?

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From vivek at khera.org  Tue Feb 19 12:40:29 2008
From: vivek at khera.org (Vivek Khera)
Date: Tue Feb 19 12:40:39 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BB3B54.7050306@serioustechnology.com>
References: <47B9E86F.2040907@serioustechnology.com>
	<60odaegdxc.fsf@dba2.int.libertyrms.com>
	<47B9FCCC.5030004@serioustechnology.com>
	<47BADDC9.8050606@serioustechnology.com>
	<20080219150254.GA6148@crankycanuck.ca>
	<47BAF624.2020907@serioustechnology.com>
	<47BB0046.10109@serioustechnology.com>
	<47BB02AA.2060902@serioustechnology.com>
	<62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>
	<47BB3B54.7050306@serioustechnology.com>
Message-ID: <5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>


On Feb 19, 2008, at 3:25 PM, Geoffrey wrote:

> I'm assuming that triggers on my replication database would be  
> disabled by slony.  Is this correct?

Yes, they will be disabled on the replica.  However, if you have  
tables that are not part of the slony set that have triggers that  
alter tables that are in your slony set, then all bets are off.  You'd  
probably get some error on that condition, though, when the trigger  
fired.


From lists at serioustechnology.com  Tue Feb 19 13:07:52 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Tue Feb 19 13:08:04 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>
References: <47B9E86F.2040907@serioustechnology.com>	<60odaegdxc.fsf@dba2.int.libertyrms.com>	<47B9FCCC.5030004@serioustechnology.com>	<47BADDC9.8050606@serioustechnology.com>	<20080219150254.GA6148@crankycanuck.ca>	<47BAF624.2020907@serioustechnology.com>	<47BB0046.10109@serioustechnology.com>	<47BB02AA.2060902@serioustechnology.com>	<62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>	<47BB3B54.7050306@serioustechnology.com>
	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>
Message-ID: <47BB4528.6090505@serioustechnology.com>

We have come to the conclusion that our replication data is useless, so 
we are going to drop the replication databases and revisit this from 
scratch.

There are a couple of folks working to validate the integrity of the 
production data.

Once this is done, we will re-visit replication.  We are going to start 
with a single database and monitor it very closely.

Thanks for all the assistance from this list.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From ajs at crankycanuck.ca  Tue Feb 19 15:23:09 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue Feb 19 15:23:30 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BB3B54.7050306@serioustechnology.com>
References: <47B9E86F.2040907@serioustechnology.com>
	<60odaegdxc.fsf@dba2.int.libertyrms.com>
	<47B9FCCC.5030004@serioustechnology.com>
	<47BADDC9.8050606@serioustechnology.com>
	<20080219150254.GA6148@crankycanuck.ca>
	<47BAF624.2020907@serioustechnology.com>
	<47BB0046.10109@serioustechnology.com>
	<47BB02AA.2060902@serioustechnology.com>
	<62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>
	<47BB3B54.7050306@serioustechnology.com>
Message-ID: <20080219232309.GG6148@crankycanuck.ca>

On Tue, Feb 19, 2008 at 03:25:56PM -0500, Geoffrey wrote:
> 
> I'm assuming that triggers on my replication database would be disabled 
> by slony.  Is this correct?

Depends on how they got there.

If you added them by hand after you set Slony up, then no.

If you added them by STORE TRIGGER, then no.  

If they were part of the schema when you started, then yes.  Also, if you
added them by some DDL via EXECUTE SCRIPT, then it will be disabled.

A

From ajs at crankycanuck.ca  Tue Feb 19 15:24:27 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue Feb 19 15:24:42 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BB4528.6090505@serioustechnology.com>
References: <47B9FCCC.5030004@serioustechnology.com>
	<47BADDC9.8050606@serioustechnology.com>
	<20080219150254.GA6148@crankycanuck.ca>
	<47BAF624.2020907@serioustechnology.com>
	<47BB0046.10109@serioustechnology.com>
	<47BB02AA.2060902@serioustechnology.com>
	<62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>
	<47BB3B54.7050306@serioustechnology.com>
	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>
	<47BB4528.6090505@serioustechnology.com>
Message-ID: <20080219232427.GH6148@crankycanuck.ca>

On Tue, Feb 19, 2008 at 04:07:52PM -0500, Geoffrey wrote:
> We have come to the conclusion that our replication data is useless, so 
> we are going to drop the replication databases and revisit this from 
> scratch.

The big puzzle is how it got that way.  If you have anything like a history
of things done on these systems, and are willing to share the schema and its
history for debugging purposes, I'd be very interested in seeing it (and I
bet others would be too).

A

From lists at serioustechnology.com  Tue Feb 19 15:57:20 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Tue Feb 19 15:57:32 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <20080219232427.GH6148@crankycanuck.ca>
References: <47B9FCCC.5030004@serioustechnology.com>	<47BADDC9.8050606@serioustechnology.com>	<20080219150254.GA6148@crankycanuck.ca>	<47BAF624.2020907@serioustechnology.com>	<47BB0046.10109@serioustechnology.com>	<47BB02AA.2060902@serioustechnology.com>	<62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>	<47BB3B54.7050306@serioustechnology.com>	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>	<47BB4528.6090505@serioustechnology.com>
	<20080219232427.GH6148@crankycanuck.ca>
Message-ID: <47BB6CE0.7030502@serioustechnology.com>

Andrew Sullivan wrote:
> On Tue, Feb 19, 2008 at 04:07:52PM -0500, Geoffrey wrote:
>> We have come to the conclusion that our replication data is useless, so 
>> we are going to drop the replication databases and revisit this from 
>> scratch.
> 
> The big puzzle is how it got that way.  If you have anything like a history
> of things done on these systems, and are willing to share the schema and its
> history for debugging purposes, I'd be very interested in seeing it (and I
> bet others would be too).

Yes, the big puzzle is terribly bothersome.  Particularly if once we 
re-replicate, we don't see the problem again.  I really want to know 
what caused the problem and therefore, how to avoid it in the future.

The production environment is a logistics tracking system.  There are 12 
separate databases running on 12 different ports, and therefore, 12 
different postmasters.  There is one 'master' database where data is 
archived to from the other 11 databases once a load is complete.

I am certainly willing to share more info with anyone who might be 
interested in this issue.

I don't know that I can share the schema, as it's likely considered 
proprietary.  I'll check though.

We did have some bizarre errors last Friday following initial 
replication of our databases.  The errors were generated by a process 
that communicates with all the databases and shuffles some data around. 
    Researching the error, I found references to problems with temp 
tables, although I'm told that this process does not use temp tables.

Apparently there were 100's of these errors generated, yet, when 
researching the production data, it does not appear that there was any 
corruption.  Here is the text of one of the errors:

relation with OID 394006 does not exist

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From lists at serioustechnology.com  Tue Feb 19 16:28:54 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Tue Feb 19 16:29:12 2008
Subject: [Slony1-general] what user id do you start slony with?
Message-ID: <47BB7446.7070303@serioustechnology.com>

I"m curious as to how folks start the slony processes.  I've been using 
root since the processes need to write to the /usr/log directory.  Do 
folks do this a different way?  I would prefer it to be the postgres 
user, but that doesn't seem to be possible.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From craig_james at emolecules.com  Tue Feb 19 16:30:48 2008
From: craig_james at emolecules.com (Craig James)
Date: Tue Feb 19 16:31:01 2008
Subject: [Slony1-general] runaway vacuum
In-Reply-To: <47B463D5.90605@emolecules.com>
References: <47B463D5.90605@emolecules.com>
Message-ID: <47BB74B8.2030808@emolecules.com>

I think I found the problem with the "runaway vacuum"...

Craig James wrote:
> Every 60 minutes, a vacuum process starts on my master node.  Since I am 
> not running AutoVacuum at all, and have no cron jobs, that only leaves 
> Slony ...

One of our maintenance applications that runs occasionally used a "truncate table ..." on one of the replicated tables.  Since TRUNCATE can't be handled by Slony, this lead to a duplicate-key violation in the slave database, so replication had stopped for a couple weeks (I guess I should check the log files more regularly, Slony has worked well for a while so I got lazy...).  That meant the queue of changes had built up to millions of rows... hence the long-running vacuum of the Slony schema.

Mystery solved.

To cure it, I had to drop the primary key constraint on the slave table until Slony caught up.

Is there any way at all to block, trap, or otherwise prevent TRUNCATE of certain tables?  This problem is solved for now, but who knows when some unsuspecting app developer who hasn't read the Slony manual in detail will make this same mistake again?

Thanks,
Craig
From Ow.Mun.Heng at wdc.com  Tue Feb 19 16:31:27 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Tue Feb 19 16:31:59 2008
Subject: [Slony1-general] what user id do you start slony with?
In-Reply-To: <47BB7446.7070303@serioustechnology.com>
References: <47BB7446.7070303@serioustechnology.com>
Message-ID: <1203467487.18310.21.camel@neuromancer.home.net>


On Tue, 2008-02-19 at 19:28 -0500, Geoffrey wrote:
> I"m curious as to how folks start the slony processes.  I've been using 
> root since the processes need to write to the /usr/log directory.  Do 
> folks do this a different way?  I would prefer it to be the postgres 
> user, but that doesn't seem to be possible.
> 
I start it as user "pg_operator"

I'm writing the logs to an alternate location hence the "no issue"

From drees76 at gmail.com  Tue Feb 19 16:32:11 2008
From: drees76 at gmail.com (David Rees)
Date: Tue Feb 19 16:32:22 2008
Subject: [Slony1-general] what user id do you start slony with?
In-Reply-To: <47BB7446.7070303@serioustechnology.com>
References: <47BB7446.7070303@serioustechnology.com>
Message-ID: <72dbd3150802191632k1aee0fd2w245a5cb5ec969d2c@mail.gmail.com>

On Feb 19, 2008 4:28 PM, Geoffrey <lists@serioustechnology.com> wrote:
> I"m curious as to how folks start the slony processes.  I've been using
> root since the processes need to write to the /usr/log directory.  Do
> folks do this a different way?  I would prefer it to be the postgres
> user, but that doesn't seem to be possible.

All our slon daemons are managed by daemontools, and also run under
the postgres user. Logging is spit to stdout where daemontools logging
takes care of archiving logs for us.

-Dave
From lists at serioustechnology.com  Tue Feb 19 16:44:50 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Tue Feb 19 16:45:04 2008
Subject: [Slony1-general] restarted slony on one of our databases and all
	looks good
Message-ID: <47BB7802.10509@serioustechnology.com>

I've restarted my replication on one of our databases from scratch and 
all is looking well.  I wrote a script that runs 'select count(*)' on 
every replicated table in the database and runs 'select last_value' on 
every replicated sequence on both the primary and slave.  The output was 
identical.

I'll watch it a bit and update the list as to our progress.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From lists at serioustechnology.com  Tue Feb 19 16:48:33 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Tue Feb 19 16:48:56 2008
Subject: [Slony1-general] question about notice message
Message-ID: <47BB78E1.6050109@serioustechnology.com>

I asked this before, but I don't know if I specified the specifics of 
the issue.  I just received the following messages:

NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=2632
CONTEXT:  PL/pgSQL function "cleanupevent" line 77 at perform
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=9725
CONTEXT:  PL/pgSQL function "cleanupevent" line 77 at perform

This was from the database I just started replicating.  I don't 
understand why there would be a 'stale' anything when first starting 
replication.  The message appears about the time the the initial 
replication is complete.

So, this is not something I should be concerned with?

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From craig_james at emolecules.com  Tue Feb 19 17:05:43 2008
From: craig_james at emolecules.com (Craig James)
Date: Tue Feb 19 17:05:57 2008
Subject: [Slony1-general] what user id do you start slony with?
In-Reply-To: <47BB7446.7070303@serioustechnology.com>
References: <47BB7446.7070303@serioustechnology.com>
Message-ID: <47BB7CE7.1090202@emolecules.com>

Geoffrey wrote:
> I"m curious as to how folks start the slony processes.  I've been using 
> root since the processes need to write to the /usr/log directory.  Do 
> folks do this a different way?  I would prefer it to be the postgres 
> user, but that doesn't seem to be possible.

It's never a good idea to run any daemon as root unless there's a genuine reason.  Slony is great, but do you trust your entire system to it?  Running Slon as "postgres" is also a bad idea, since that user has access to ALL postgres data.

I don't think there are any known hacks/exploits for Slony, but if one turned up, it would give a hacker free access.

I run the slon daemon under my own userid, and I have a separate slon logfile.

Craig
From lists at serioustechnology.com  Tue Feb 19 17:27:58 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Tue Feb 19 17:28:18 2008
Subject: [Slony1-general] what user id do you start slony with?
In-Reply-To: <47BB7CE7.1090202@emolecules.com>
References: <47BB7446.7070303@serioustechnology.com>
	<47BB7CE7.1090202@emolecules.com>
Message-ID: <47BB821E.9000103@serioustechnology.com>

Craig James wrote:
> Geoffrey wrote:
>> I"m curious as to how folks start the slony processes.  I've been 
>> using root since the processes need to write to the /usr/log 
>> directory.  Do folks do this a different way?  I would prefer it to be 
>> the postgres user, but that doesn't seem to be possible.
> 
> It's never a good idea to run any daemon as root unless there's a 
> genuine reason.  Slony is great, but do you trust your entire system to 
> it?  Running Slon as "postgres" is also a bad idea, since that user has 
> access to ALL postgres data.

Agreed, regarding root, although, any other user id I would run it as 
would have access to the data as well.

I'll likely hack the scripts so that I can write the logs to where they 
want them and run the daemons as postgres.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From drees76 at gmail.com  Tue Feb 19 18:38:28 2008
From: drees76 at gmail.com (David Rees)
Date: Tue Feb 19 18:38:41 2008
Subject: [Slony1-general] question about notice message
In-Reply-To: <47BB78E1.6050109@serioustechnology.com>
References: <47BB78E1.6050109@serioustechnology.com>
Message-ID: <72dbd3150802191838o569b0bfdufc5786c02a827568@mail.gmail.com>

On Feb 19, 2008 4:48 PM, Geoffrey <lists@serioustechnology.com> wrote:
> I asked this before, but I don't know if I specified the specifics of
> the issue.  I just received the following messages:
>
> NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=2632
> CONTEXT:  PL/pgSQL function "cleanupevent" line 77 at perform
> NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=9725
> CONTEXT:  PL/pgSQL function "cleanupevent" line 77 at perform

You'll get these messages any time a slon daemon restarts.

Completely normal and can be ignored.

-Dave
From victor.aluko at gmail.com  Wed Feb 20 01:59:14 2008
From: victor.aluko at gmail.com (ajcity)
Date: Wed Feb 20 01:59:33 2008
Subject: [Slony1-general] Slony setup over a LAN but not replicating
In-Reply-To: <4b15b0190802080626qe7d1503ld65c2be1de69dbf8@mail.gmail.com>
References: <15308421.post@talk.nabble.com>
	<8466e51e0802080621v51d4088drf4ed260f9bcbdf1b@mail.gmail.com>
	<4b15b0190802080626qe7d1503ld65c2be1de69dbf8@mail.gmail.com>
Message-ID: <15585775.post@talk.nabble.com>


Hi guys,
Thanks for the advice. I've gotten it fixed
 I specified 'localhost' for the .conf files on each machine and the IP of
the other machine on the second .conf files 
  i.e for NY: Ny.conf : localhost
                 Tx.conf : its IP
       for TX: Ny.con : its IP
                 Tx.conf : localhost

and then it worked after I used the slonik scripts to execute (instead of
pgAdmin that I was using before)

 Thanks once again

-- 
View this message in context: http://www.nabble.com/Slony-setup-over-a-LAN-but-not-replicating-tp15308421p15585775.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From victor.aluko at gmail.com  Wed Feb 20 02:17:33 2008
From: victor.aluko at gmail.com (ajcity)
Date: Wed Feb 20 02:17:52 2008
Subject: [Slony1-general] Slony-I build cant find postgresql include folder
Message-ID: <15586012.post@talk.nabble.com>


Hi all,
  I imagine this question would have popped up before but I just wanna ask
and wont mind being referred to an earlier post.
  I wanna setup Slony-I on a remote RedHat Linux machine which uses the
postgresql that comes distributed with the Redhat and the Slony doc doesn't
cover that kind of thing. 
  When I gmake Slony it complains that it can't find the include/server
folder for the postgresql even after I specified the /usr/include directory
where the files that are in the .../include directory of postgresql should
be and it still gives that error during build although it builds.
  So do I ignore it or where can I find the /include/server directory
located?
-- 
View this message in context: http://www.nabble.com/Slony-I-build-cant-find-postgresql-include-folder-tp15586012p15586012.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From lists at serioustechnology.com  Wed Feb 20 05:08:47 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb 20 05:08:53 2008
Subject: [Slony1-general] oids required by slony?
Message-ID: <47BC265F.2010900@serioustechnology.com>

Are you required to use oids in a database that is replicated by slony?
-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From lists at serioustechnology.com  Wed Feb 20 05:15:48 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb 20 05:15:53 2008
Subject: [Slony1-general] question about notice message
In-Reply-To: <72dbd3150802191838o569b0bfdufc5786c02a827568@mail.gmail.com>
References: <47BB78E1.6050109@serioustechnology.com>
	<72dbd3150802191838o569b0bfdufc5786c02a827568@mail.gmail.com>
Message-ID: <47BC2804.2000602@serioustechnology.com>

David Rees wrote:
> On Feb 19, 2008 4:48 PM, Geoffrey <lists@serioustechnology.com> wrote:
>> I asked this before, but I don't know if I specified the specifics of
>> the issue.  I just received the following messages:
>>
>> NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=2632
>> CONTEXT:  PL/pgSQL function "cleanupevent" line 77 at perform
>> NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=9725
>> CONTEXT:  PL/pgSQL function "cleanupevent" line 77 at perform
> 
> You'll get these messages any time a slon daemon restarts.
> 
> Completely normal and can be ignored.
> 
> -Dave

Thanks Dave, but one quick clarification.  I don't restart the daemon. 
I see this message after first starting replication about the time that 
the replication database 'catches up' with the primary node.  Still normal?

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From ajs at crankycanuck.ca  Wed Feb 20 05:31:11 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Feb 20 05:31:19 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BB6CE0.7030502@serioustechnology.com>
References: <20080219150254.GA6148@crankycanuck.ca>
	<47BAF624.2020907@serioustechnology.com>
	<47BB0046.10109@serioustechnology.com>
	<47BB02AA.2060902@serioustechnology.com>
	<62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>
	<47BB3B54.7050306@serioustechnology.com>
	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>
	<47BB4528.6090505@serioustechnology.com>
	<20080219232427.GH6148@crankycanuck.ca>
	<47BB6CE0.7030502@serioustechnology.com>
Message-ID: <20080220133111.GB12005@crankycanuck.ca>

On Tue, Feb 19, 2008 at 06:57:20PM -0500, Geoffrey wrote:
> Apparently there were 100's of these errors generated, yet, when 
> researching the production data, it does not appear that there was any 
> corruption.  Here is the text of one of the errors:
> 
> relation with OID 394006 does not exist

Aha!  I'll bet lunch someone was doing DDL in their "shuffling data around". 
That is an error message telling you that a previously-planned query is
looking for a now-dropped relation.  One way to get this is indeed with temp
tables, but you can get it by dropping regular tables &c.  I'd have a very
good look at how that procedure from last Friday worked.

If you do DDL under Slony without passing it through EXECUTE SCRIPT, you
_will_ get very strange results.

A
From lists at serioustechnology.com  Wed Feb 20 05:53:32 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb 20 05:53:37 2008
Subject: [Slony1-general] should initial replication be done when no one has
 access to the database?
Message-ID: <47BC30DC.9090705@serioustechnology.com>

Subject says it all.  We are grasping at straws to resolve this issue. 
We again had problems with replication over night.  We have a single 
table that does not appear to be properly replicating.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From lists at serioustechnology.com  Wed Feb 20 05:55:39 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb 20 05:55:45 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <20080220133111.GB12005@crankycanuck.ca>
References: <20080219150254.GA6148@crankycanuck.ca>	<47BAF624.2020907@serioustechnology.com>	<47BB0046.10109@serioustechnology.com>	<47BB02AA.2060902@serioustechnology.com>	<62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>	<47BB3B54.7050306@serioustechnology.com>	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>	<47BB4528.6090505@serioustechnology.com>	<20080219232427.GH6148@crankycanuck.ca>	<47BB6CE0.7030502@serioustechnology.com>
	<20080220133111.GB12005@crankycanuck.ca>
Message-ID: <47BC315B.3060708@serioustechnology.com>

Andrew Sullivan wrote:
> On Tue, Feb 19, 2008 at 06:57:20PM -0500, Geoffrey wrote:
>> Apparently there were 100's of these errors generated, yet, when 
>> researching the production data, it does not appear that there was any 
>> corruption.  Here is the text of one of the errors:
>>
>> relation with OID 394006 does not exist
> 
> Aha!  I'll bet lunch someone was doing DDL in their "shuffling data around". 
> That is an error message telling you that a previously-planned query is
> looking for a now-dropped relation.  One way to get this is indeed with temp
> tables, but you can get it by dropping regular tables &c.  I'd have a very
> good look at how that procedure from last Friday worked.
> 
> If you do DDL under Slony without passing it through EXECUTE SCRIPT, you
> _will_ get very strange results.

I only wish that was the problem.  Our application does not remove any 
tables.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From lists at serioustechnology.com  Wed Feb 20 06:09:01 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb 20 06:09:07 2008
Subject: [Slony1-general] narrowing down this problem
Message-ID: <47BC347D.7090709@serioustechnology.com>

Okay, so we have one table that appears to be having a problem in 
replication.  The unique thing about this table is that we have a field 
that often times is null, yet it has in index as follows:

"tract_order_num_key" unique, btree (order_num)

So, this application has been running along just fine like this for a 
couple of years.  Is it possible that the above index is causing the 
problem?

The difference between the data in this table on the primary and slave 
does appear to be related to rows where the order_num field is null.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From cedric.villemain at dalibo.com  Wed Feb 20 07:09:35 2008
From: cedric.villemain at dalibo.com (=?ISO-8859-1?Q?C=E9dric_Villemain?=)
Date: Wed Feb 20 07:09:45 2008
Subject: [Slony1-general] narrowing down this problem
In-Reply-To: <47BC347D.7090709@serioustechnology.com>
References: <47BC347D.7090709@serioustechnology.com>
Message-ID: <47BC42AF.8010004@dalibo.com>

Geoffrey a ?crit :
> Okay, so we have one table that appears to be having a problem in 
> replication.  The unique thing about this table is that we have a 
> field that often times is null, yet it has in index as follows:
>
> "tract_order_num_key" unique, btree (order_num)
>
> So, this application has been running along just fine like this for a 
> couple of years.  Is it possible that the above index is causing the 
> problem?
for me yes. unique contraint does not check the null values. 
http://www.postgresql.org/docs/current/static/ddl-constraints.html
>
> The difference between the data in this table on the primary and slave 
> does appear to be related to rows where the order_num field is null.
>

yes, slony check the first index matching primary key criteria , and use 
it if none specified.
You have probably explicitely tell slony the Primary key to use ? and 
Slony didn't check that you lies :


      if p_idx_name isnull then
                select PGXC.relname
                                into v_idxrow
                                from "pg_catalog".pg_class PGC,
                                        "pg_catalog".pg_namespace PGN,
                                        "pg_catalog".pg_index PGX,
                                        "pg_catalog".pg_class PGXC
                                where 
@NAMESPACE@.slon_quote_brute(PGN.nspname) || ''.'' ||
                                        
@NAMESPACE@.slon_quote_brute(PGC.relname) = v_tab_fqname_quoted
                                        and PGN.oid = PGC.relnamespace
                                        and PGX.indrelid = PGC.oid
                                        and PGX.indexrelid = PGXC.oid
                                        and PGX.indisprimary;
                if not found then
                        raise exception ''Slony-I: table % has no 
primary key'',
                                        v_tab_fqname_quoted;
                end if;
        else
                select PGXC.relname
                                into v_idxrow
                                from "pg_catalog".pg_class PGC,
                                        "pg_catalog".pg_namespace PGN,
                                        "pg_catalog".pg_index PGX,
                                        "pg_catalog".pg_class PGXC
                                where 
@NAMESPACE@.slon_quote_brute(PGN.nspname) || ''.'' ||
                                        
@NAMESPACE@.slon_quote_brute(PGC.relname) = v_tab_fqname_quoted
                                        and PGN.oid = PGC.relnamespace
                                        and PGX.indrelid = PGC.oid
                                        and PGX.indexrelid = PGXC.oid
                                        and PGX.indisunique
                                        and 
@NAMESPACE@.slon_quote_brute(PGXC.relname) = 
@NAMESPACE@.slon_quote_input(p_idx_name);
                if not found then
                        raise exception ''Slony-I: table % has no unique 
index %'',
                                        v_tab_fqname_quoted, p_idx_name;
                end if;
        end if;


in src/backend/slony1_funcs.sql

see the 'and PGX.indisprimary;' vs 'and PGX.indisunique'

Perhaps some bugs around ? Slony MUST check it is a PK, not a unique 
contraint only.

-- 
C?dric Villemain
Administrateur de Base de Donn?es
Cel: +33 (0)6 74 15 56 53
http://dalibo.com - http://dalibo.org

From lists at serioustechnology.com  Wed Feb 20 07:21:50 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb 20 07:21:58 2008
Subject: [Slony1-general] narrowing down this problem
In-Reply-To: <47BC42AF.8010004@dalibo.com>
References: <47BC347D.7090709@serioustechnology.com>
	<47BC42AF.8010004@dalibo.com>
Message-ID: <47BC458E.1000807@serioustechnology.com>

C?dric Villemain wrote:
> Geoffrey a ?crit :
>> Okay, so we have one table that appears to be having a problem in 
>> replication.  The unique thing about this table is that we have a 
>> field that often times is null, yet it has in index as follows:
>>
>> "tract_order_num_key" unique, btree (order_num)
>>
>> So, this application has been running along just fine like this for a 
>> couple of years.  Is it possible that the above index is causing the 
>> problem?
> for me yes. unique contraint does not check the null values. 
> http://www.postgresql.org/docs/current/static/ddl-constraints.html
>>
>> The difference between the data in this table on the primary and slave 
>> does appear to be related to rows where the order_num field is null.
>>
> 
> yes, slony check the first index matching primary key criteria , and use 
> it if none specified.

But this is not the primary key.  There is a primary key specified. 
Here are the indices on this table:

Indexes:

     "tract_pkey" primary key, btree (recid)
     "tract_order_num_key" unique, btree (order_num)
     "tract_assigned" btree (assigned)
     "tract_code" btree (code)
     "tract_comments" btree (comments)
     "tract_compound_1" btree (code, old_order_num)
     "tract_date_avail" btree (date_avail)
     "tract_dest_state" btree (dest_state)
     "tract_dest_zone" btree (dest_zone)
     "tract_driver" btree (driver)
     "tract_orig_state" btree (orig_state)
     "tract_orig_zone" btree (orig_zone)
     "tract_prebooked" btree (prebooked)
     "tract_tractor_num" btree (tractor_num)
     "tract_trailer_num" btree (trailer_num)

Note the primary key is 'tract_pkey'

> You have probably explicitely tell slony the Primary key to use ? and 
> Slony didn't check that you lies :
> 
> 
>      if p_idx_name isnull then
>                select PGXC.relname
>                                into v_idxrow
>                                from "pg_catalog".pg_class PGC,
>                                        "pg_catalog".pg_namespace PGN,
>                                        "pg_catalog".pg_index PGX,
>                                        "pg_catalog".pg_class PGXC
>                                where 
> @NAMESPACE@.slon_quote_brute(PGN.nspname) || ''.'' ||
>                                        
> @NAMESPACE@.slon_quote_brute(PGC.relname) = v_tab_fqname_quoted
>                                        and PGN.oid = PGC.relnamespace
>                                        and PGX.indrelid = PGC.oid
>                                        and PGX.indexrelid = PGXC.oid
>                                        and PGX.indisprimary;
>                if not found then
>                        raise exception ''Slony-I: table % has no primary 
> key'',
>                                        v_tab_fqname_quoted;
>                end if;
>        else
>                select PGXC.relname
>                                into v_idxrow
>                                from "pg_catalog".pg_class PGC,
>                                        "pg_catalog".pg_namespace PGN,
>                                        "pg_catalog".pg_index PGX,
>                                        "pg_catalog".pg_class PGXC
>                                where 
> @NAMESPACE@.slon_quote_brute(PGN.nspname) || ''.'' ||
>                                        
> @NAMESPACE@.slon_quote_brute(PGC.relname) = v_tab_fqname_quoted
>                                        and PGN.oid = PGC.relnamespace
>                                        and PGX.indrelid = PGC.oid
>                                        and PGX.indexrelid = PGXC.oid
>                                        and PGX.indisunique
>                                        and 
> @NAMESPACE@.slon_quote_brute(PGXC.relname) = 
> @NAMESPACE@.slon_quote_input(p_idx_name);
>                if not found then
>                        raise exception ''Slony-I: table % has no unique 
> index %'',
>                                        v_tab_fqname_quoted, p_idx_name;
>                end if;
>        end if;
> 
> 
> in src/backend/slony1_funcs.sql
> 
> see the 'and PGX.indisprimary;' vs 'and PGX.indisunique'
> 
> Perhaps some bugs around ? Slony MUST check it is a PK, not a unique 
> contraint only.
> 


-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From vivek at khera.org  Wed Feb 20 07:33:04 2008
From: vivek at khera.org (Vivek Khera)
Date: Wed Feb 20 07:33:09 2008
Subject: [Slony1-general] what user id do you start slony with?
In-Reply-To: <47BB7446.7070303@serioustechnology.com>
References: <47BB7446.7070303@serioustechnology.com>
Message-ID: <878ED217-A27D-48F3-8707-677948BE709A@khera.org>


On Feb 19, 2008, at 7:28 PM, Geoffrey wrote:

> I"m curious as to how folks start the slony processes.  I've been  
> using root since the processes need to write to the /usr/log  
> directory.  Do folks do this a different way?  I would prefer it to  
> be the postgres user, but that doesn't seem to be possible.

I run it as the same user my postgres runs as.  What is this mythical  
"/usr/log" directory of which you speak?  I have no such directory,  
nor would I ever store user generated data into the /usr partition.

From vivek at khera.org  Wed Feb 20 07:34:13 2008
From: vivek at khera.org (Vivek Khera)
Date: Wed Feb 20 07:34:17 2008
Subject: [Slony1-general] oids required by slony?
In-Reply-To: <47BC265F.2010900@serioustechnology.com>
References: <47BC265F.2010900@serioustechnology.com>
Message-ID: <2DFC2223-F70C-408A-8BCE-395722143F7E@khera.org>


On Feb 20, 2008, at 8:08 AM, Geoffrey wrote:

> Are you required to use oids in a database that is replicated by  
> slony?
no.

From vivek at khera.org  Wed Feb 20 07:35:48 2008
From: vivek at khera.org (Vivek Khera)
Date: Wed Feb 20 07:35:52 2008
Subject: [Slony1-general] should initial replication be done when no one
	has access to the database?
In-Reply-To: <47BC30DC.9090705@serioustechnology.com>
References: <47BC30DC.9090705@serioustechnology.com>
Message-ID: <42F13EC8-8AE5-4DDF-8A07-94ACE27DB722@khera.org>


On Feb 20, 2008, at 8:53 AM, Geoffrey wrote:

> Subject says it all.  We are grasping at straws to resolve this  
> issue. We again had problems with replication over night.  We have a  
> single table that does not appear to be properly replicating.

no, this is not required.  we try to do it during low volume times,  
because otherwise we'd crush the log tables with the massive amount of  
change we go through daily.
From vivek at khera.org  Wed Feb 20 07:41:36 2008
From: vivek at khera.org (Vivek Khera)
Date: Wed Feb 20 07:41:46 2008
Subject: [Slony1-general] narrowing down this problem
In-Reply-To: <47BC347D.7090709@serioustechnology.com>
References: <47BC347D.7090709@serioustechnology.com>
Message-ID: <3AED03A3-12F5-4877-9F3E-F24E539F2739@khera.org>


On Feb 20, 2008, at 9:09 AM, Geoffrey wrote:

> "tract_order_num_key" unique, btree (order_num)
>
> So, this application has been running along just fine like this for  
> a couple of years.  Is it possible that the above index is causing  
> the problem?

slony won't care about this index.  all it cares is that you have a  
primary key defined for the table.  your above key does not qualify as  
a primary key.

From lists at serioustechnology.com  Wed Feb 20 07:46:30 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb 20 07:46:38 2008
Subject: [Slony1-general] logtrigger
Message-ID: <47BC4B56.8070205@serioustechnology.com>

I see the following trigger that was added by slony:

     _mwr_cluster_logtrigger_1 AFTER INSERT OR DELETE OR UPDATE ON avlds 
FOR EACH ROW EXECUTE PROCEDURE _mwr_cluster.logtrigger('_mwr_cluster', 
'1', 'kvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv')

Is that correct?  That last field looks kind of like 'controlled junk.'

Still trying to track down our slony problems.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From lists at serioustechnology.com  Wed Feb 20 07:56:04 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb 20 07:56:10 2008
Subject: [Slony1-general] narrowing down this problem
In-Reply-To: <3AED03A3-12F5-4877-9F3E-F24E539F2739@khera.org>
References: <47BC347D.7090709@serioustechnology.com>
	<3AED03A3-12F5-4877-9F3E-F24E539F2739@khera.org>
Message-ID: <47BC4D94.90007@serioustechnology.com>

Vivek Khera wrote:
> 
> On Feb 20, 2008, at 9:09 AM, Geoffrey wrote:
> 
>> "tract_order_num_key" unique, btree (order_num)
>>
>> So, this application has been running along just fine like this for a 
>> couple of years.  Is it possible that the above index is causing the 
>> problem?
> 
> slony won't care about this index.  all it cares is that you have a 
> primary key defined for the table.  your above key does not qualify as a 
> primary key.

Understood.  We do have a primary key on this index:

Indexes:
     "tract_pkey" primary key, btree (recid)
     "tract_order_num_key" unique, btree (order_num)
     "tract_assigned" btree (assigned)
     "tract_code" btree (code)
     "tract_comments" btree (comments)
     "tract_compound_1" btree (code, old_order_num)
     "tract_date_avail" btree (date_avail)
     "tract_dest_state" btree (dest_state)
     "tract_dest_zone" btree (dest_zone)
     "tract_driver" btree (driver)
     "tract_orig_state" btree (orig_state)
     "tract_orig_zone" btree (orig_zone)
     "tract_prebooked" btree (prebooked)
     "tract_tractor_num" btree (tractor_num)
     "tract_trailer_num" btree (trailer_num)

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From cbbrowne at ca.afilias.info  Wed Feb 20 08:06:26 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Feb 20 08:06:31 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BC315B.3060708@serioustechnology.com> (Geoffrey's message of
	"Wed, 20 Feb 2008 08:55:39 -0500")
References: <20080219150254.GA6148@crankycanuck.ca>
	<47BAF624.2020907@serioustechnology.com>
	<47BB0046.10109@serioustechnology.com>
	<47BB02AA.2060902@serioustechnology.com>
	<62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>
	<47BB3B54.7050306@serioustechnology.com>
	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>
	<47BB4528.6090505@serioustechnology.com>
	<20080219232427.GH6148@crankycanuck.ca>
	<47BB6CE0.7030502@serioustechnology.com>
	<20080220133111.GB12005@crankycanuck.ca>
	<47BC315B.3060708@serioustechnology.com>
Message-ID: <607igzhat9.fsf@dba2.int.libertyrms.com>

Geoffrey <lists@serioustechnology.com> writes:
> Andrew Sullivan wrote:
>> On Tue, Feb 19, 2008 at 06:57:20PM -0500, Geoffrey wrote:
>>> Apparently there were 100's of these errors generated, yet, when
>>> researching the production data, it does not appear that there was
>>> any corruption.  Here is the text of one of the errors:
>>>
>>> relation with OID 394006 does not exist
>> Aha!  I'll bet lunch someone was doing DDL in their "shuffling data
>> around". That is an error message telling you that a
>> previously-planned query is
>> looking for a now-dropped relation.  One way to get this is indeed with temp
>> tables, but you can get it by dropping regular tables &c.  I'd have a very
>> good look at how that procedure from last Friday worked.
>> If you do DDL under Slony without passing it through EXECUTE SCRIPT,
>> you
>> _will_ get very strange results.
>
> I only wish that was the problem.  Our application does not remove any
> tables.

That's not the exact implication, though.  I know we ran into this
sort of thing on an occasion where we dropped replication from an
"origin" node; the trouble was that query plans were expecting the
Slony-I-defined tables to still be there.

The missing relation might not be one of your tables; it might be
something else about the schema.
-- 
"cbbrowne","@","cbbrowne.com"
http://cbbrowne.com/info/lisp.html
Rules of the Evil Overlord #229. "If I have several diabolical schemes
to destroy the hero,  I will set all of them in  motion at once rather
than wait for them to fail and launch them successively."
<http://www.eviloverlord.com/>
From lists at serioustechnology.com  Wed Feb 20 08:24:15 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb 20 08:24:25 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <607igzhat9.fsf@dba2.int.libertyrms.com>
References: <20080219150254.GA6148@crankycanuck.ca>	<47BAF624.2020907@serioustechnology.com>	<47BB0046.10109@serioustechnology.com>	<47BB02AA.2060902@serioustechnology.com>	<62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>	<47BB3B54.7050306@serioustechnology.com>	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>	<47BB4528.6090505@serioustechnology.com>	<20080219232427.GH6148@crankycanuck.ca>	<47BB6CE0.7030502@serioustechnology.com>	<20080220133111.GB12005@crankycanuck.ca>	<47BC315B.3060708@serioustechnology.com>
	<607igzhat9.fsf@dba2.int.libertyrms.com>
Message-ID: <47BC542F.6060508@serioustechnology.com>

Christopher Browne wrote:
> Geoffrey <lists@serioustechnology.com> writes:
>> Andrew Sullivan wrote:
>>> On Tue, Feb 19, 2008 at 06:57:20PM -0500, Geoffrey wrote:
>>>> Apparently there were 100's of these errors generated, yet, when
>>>> researching the production data, it does not appear that there was
>>>> any corruption.  Here is the text of one of the errors:
>>>>
>>>> relation with OID 394006 does not exist
>>> Aha!  I'll bet lunch someone was doing DDL in their "shuffling data
>>> around". That is an error message telling you that a
>>> previously-planned query is
>>> looking for a now-dropped relation.  One way to get this is indeed with temp
>>> tables, but you can get it by dropping regular tables &c.  I'd have a very
>>> good look at how that procedure from last Friday worked.
>>> If you do DDL under Slony without passing it through EXECUTE SCRIPT,
>>> you
>>> _will_ get very strange results.
>> I only wish that was the problem.  Our application does not remove any
>> tables.
> 
> That's not the exact implication, though.  I know we ran into this
> sort of thing on an occasion where we dropped replication from an
> "origin" node; the trouble was that query plans were expecting the
> Slony-I-defined tables to still be there.
> 
> The missing relation might not be one of your tables; it might be
> something else about the schema.

So how does one track this down?  We were betting on slony for 
replication.  As it stands, it's not going to happen.  Right now, we are 
replicating one database in the hopes of figuring out what is going on.

More importantly, already, we have at least two tables that are out of 
sync, and don't appear that they will ever get back in sync.  One table 
has more records in the replication database then the primary.  I know 
this is possible, but there's a difference of over 300 records and I 
don't see it making up the difference.

Is it possible these are two totally different issues?

This is terribly frustrating in that it appears that many folks use 
slony successfully, but we just can't get it to work and it's starting 
to look like we'll have to look at other solutions.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From drees76 at gmail.com  Wed Feb 20 09:52:13 2008
From: drees76 at gmail.com (David Rees)
Date: Wed Feb 20 09:52:21 2008
Subject: [Slony1-general] question about notice message
In-Reply-To: <47BC2804.2000602@serioustechnology.com>
References: <47BB78E1.6050109@serioustechnology.com>
	<72dbd3150802191838o569b0bfdufc5786c02a827568@mail.gmail.com>
	<47BC2804.2000602@serioustechnology.com>
Message-ID: <72dbd3150802200952k1693419bp55c79e7c64827dad@mail.gmail.com>

On Wed, Feb 20, 2008 at 5:15 AM, Geoffrey <lists@serioustechnology.com> wrote:
> David Rees wrote:
>  > On Feb 19, 2008 4:48 PM, Geoffrey <lists@serioustechnology.com> wrote:
>  >> I asked this before, but I don't know if I specified the specifics of
>  >> the issue.  I just received the following messages:
>  >>
>  >> NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=2632
>  >> CONTEXT:  PL/pgSQL function "cleanupevent" line 77 at perform
>  >> NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=9725
>  >> CONTEXT:  PL/pgSQL function "cleanupevent" line 77 at perform
>  >
>  > You'll get these messages any time a slon daemon restarts.
>  >
>  > Completely normal and can be ignored.
>
>  Thanks Dave, but one quick clarification.  I don't restart the daemon.
>  I see this message after first starting replication about the time that
>  the replication database 'catches up' with the primary node.  Still normal?

Yep.

-Dave
From ajs at crankycanuck.ca  Wed Feb 20 11:23:44 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Feb 20 11:24:02 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BC315B.3060708@serioustechnology.com>
References: <47BB0046.10109@serioustechnology.com>
	<47BB02AA.2060902@serioustechnology.com>
	<62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>
	<47BB3B54.7050306@serioustechnology.com>
	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>
	<47BB4528.6090505@serioustechnology.com>
	<20080219232427.GH6148@crankycanuck.ca>
	<47BB6CE0.7030502@serioustechnology.com>
	<20080220133111.GB12005@crankycanuck.ca>
	<47BC315B.3060708@serioustechnology.com>
Message-ID: <20080220192344.GC13020@crankycanuck.ca>

On Wed, Feb 20, 2008 at 08:55:39AM -0500, Geoffrey wrote:

> >>relation with OID 394006 does not exist
> 
> I only wish that was the problem.  Our application does not remove any 
> tables.

Something removed a table.  That's what that error message means.  I can
think of a couple possibilities:

1.  Somewhere, your application or some person got in and removed (or maybe
renamed and re-created) a table that was referenced by _something_ that was
still open.

2.  Slony was dropped from the node without some set of your connections
having disconnected, and they're still expecting the triggers they can still
see to be able to write into that table.

I am by no means willing to dismiss the suggestion that there are bugs in
Slony; but this still looks to me very much like there's something we don't
know about what happened, that explains the errors you're seeing.

A

From lists at serioustechnology.com  Wed Feb 20 11:30:20 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb 20 11:30:40 2008
Subject: [Slony1-general] other replication solutions
In-Reply-To: <47BC4D94.90007@serioustechnology.com>
References: <47BC347D.7090709@serioustechnology.com>	<3AED03A3-12F5-4877-9F3E-F24E539F2739@khera.org>
	<47BC4D94.90007@serioustechnology.com>
Message-ID: <47BC7FCC.3090704@serioustechnology.com>

As we apparently can not determine what the problem is with our slony 
implementation and therefore will not be using slony, I'm hoping there 
might be other replication solutions out there.  Anyone know of any?

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From lists at serioustechnology.com  Wed Feb 20 11:55:32 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb 20 11:55:43 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <20080220192344.GC13020@crankycanuck.ca>
References: <47BB0046.10109@serioustechnology.com>	<47BB02AA.2060902@serioustechnology.com>	<62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>	<47BB3B54.7050306@serioustechnology.com>	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>	<47BB4528.6090505@serioustechnology.com>	<20080219232427.GH6148@crankycanuck.ca>	<47BB6CE0.7030502@serioustechnology.com>	<20080220133111.GB12005@crankycanuck.ca>	<47BC315B.3060708@serioustechnology.com>
	<20080220192344.GC13020@crankycanuck.ca>
Message-ID: <47BC85B4.2000608@serioustechnology.com>

Andrew Sullivan wrote:
> On Wed, Feb 20, 2008 at 08:55:39AM -0500, Geoffrey wrote:
> 
>>>> relation with OID 394006 does not exist
>> I only wish that was the problem.  Our application does not remove any 
>> tables.
> 
> Something removed a table.  That's what that error message means.  I can
> think of a couple possibilities:
> 
> 1.  Somewhere, your application or some person got in and removed (or maybe
> renamed and re-created) a table that was referenced by _something_ that was
> still open.

The only tables that could possibly be removed would be temp tables.  I 
assure you, none of the tables that are being replicated are being 
removed by anyone.  The application is not designed that way.

> 2.  Slony was dropped from the node without some set of your connections
> having disconnected, and they're still expecting the triggers they can still
> see to be able to write into that table.

Can you define 'dropped from the node?'

> I am by no means willing to dismiss the suggestion that there are bugs in
> Slony; but this still looks to me very much like there's something we don't
> know about what happened, that explains the errors you're seeing.

I would so love to figure out this issue.  I appreciate your efforts.

I simply don't understand how one table inparticular could get so far 
out of sync.  We're talking 300 records.

I can't imagine that slony is that fragile.  There's got to be something 
going on that we don't see.

I started the replication of this database last night.  Neither machine 
has been rebooted and neither postmaster was restarted.

Is it possible I should be tweaking the configuration in some way?  I 
see a default value for SYNC_CHECK_INTERNAL.  Is 1000 a good value?

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From drees76 at gmail.com  Wed Feb 20 12:34:51 2008
From: drees76 at gmail.com (David Rees)
Date: Wed Feb 20 12:35:03 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BC85B4.2000608@serioustechnology.com>
References: <47BB0046.10109@serioustechnology.com>
	<47BB3B54.7050306@serioustechnology.com>
	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>
	<47BB4528.6090505@serioustechnology.com>
	<20080219232427.GH6148@crankycanuck.ca>
	<47BB6CE0.7030502@serioustechnology.com>
	<20080220133111.GB12005@crankycanuck.ca>
	<47BC315B.3060708@serioustechnology.com>
	<20080220192344.GC13020@crankycanuck.ca>
	<47BC85B4.2000608@serioustechnology.com>
Message-ID: <72dbd3150802201234n3e7e3ff0oa82a48f0c9a5951@mail.gmail.com>

On Wed, Feb 20, 2008 at 11:55 AM, Geoffrey <lists@serioustechnology.com> wrote:
>  I simply don't understand how one table inparticular could get so far
>  out of sync.  We're talking 300 records.

Have you checked that replication is up to date (look at the sl_status
view on the master)?

What do the slon logs say? Post them up using pastbin or something for review.

>  Is it possible I should be tweaking the configuration in some way?  I
>  see a default value for SYNC_CHECK_INTERNAL.  Is 1000 a good value?

The default values are usually fine. 1000 is fine for sync check
interval, though a bit overkill for most scenarios, the default of
2000 is usually OK.

-Dave
From lists at serioustechnology.com  Wed Feb 20 12:37:53 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb 20 12:38:11 2008
Subject: [Slony1-general] still straw grasping
Message-ID: <47BC8FA1.2040903@serioustechnology.com>

I want to make sure that I'm not missing something in my configuration. 
A quick summary of my config and my startup procedures.

My nodes are defined as follows:

$CLUSTER_NAME = 'mwr_cluster';
$MASTERNODE = 1;
add_node(node     => 1,
          host     => 'mwr',
          dbname   => 'mwr',
          port     => 5436,
          user     => 'postgres',
              password => '');

add_node(node     => 2,
          host     => 'earth',
          dbname   => 'mwr',
          port     => 5436,
          user     => 'postgres',
              password => '');

My two sets are defined as follows:

$SLONY_SETS = {
     "avldsSet" => {
         "set_id" => 1,
         "table_id"    => 1,
         "sequence_id" => 1,
         "pkeyedtables" => [
		.
		.
	]
         "keyedtables" => {},
         "serialtables" => [],
         "sequences" => [
         	.
		.
         ],
     },
     "nonavldsSet" => {
         "set_id"       => 2,
         "table_id"     => 100,
         "sequence_id"  => 100,
         "pkeyedtables" => [
		.
		.
	]
         "keyedtables" => {},
         "serialtables" => [],
         "sequences" => [
		.
		.
	],
     },
}

I have no keyedtables, serialtables defined.  Note, the second set has:

"table_id"     => 100,
  "sequence_id"  => 100,

to insure they don't collide with the previous set.  The previous set 
has less the 100 tables and sequences, so this should not be an issue.

Does this look okay from a configuration perspective?

When I first start slony, I do the following:

slonik_init_cluster --config $CONFIG | slonik

slon_start --config $CONFIG 1
slon_start --config $CONFIG 2

slonik_create_set --config $CONFIG 1 | slonik
slonik_create_set --config $CONFIG 2 | slonik

slonik_subscribe_set --config $CONFIG 1 2 | slonik
slonik_subscribe_set --config $CONFIG 2 2 | slonik

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From lists at serioustechnology.com  Wed Feb 20 12:48:35 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb 20 12:48:48 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <72dbd3150802201234n3e7e3ff0oa82a48f0c9a5951@mail.gmail.com>
References: <47BB0046.10109@serioustechnology.com>	
	<47BB3B54.7050306@serioustechnology.com>	
	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>	
	<47BB4528.6090505@serioustechnology.com>	
	<20080219232427.GH6148@crankycanuck.ca>	
	<47BB6CE0.7030502@serioustechnology.com>	
	<20080220133111.GB12005@crankycanuck.ca>	
	<47BC315B.3060708@serioustechnology.com>	
	<20080220192344.GC13020@crankycanuck.ca>	
	<47BC85B4.2000608@serioustechnology.com>
	<72dbd3150802201234n3e7e3ff0oa82a48f0c9a5951@mail.gmail.com>
Message-ID: <47BC9223.30402@serioustechnology.com>

David Rees wrote:
> On Wed, Feb 20, 2008 at 11:55 AM, Geoffrey <lists@serioustechnology.com> wrote:
>>  I simply don't understand how one table inparticular could get so far
>>  out of sync.  We're talking 300 records.
> 
> Have you checked that replication is up to date (look at the sl_status
> view on the master)?

I'm assuming I should be looking at st_last_event and st_last_received? 
  They are within one on both machines. (9303, 9302 and 7278, 7277).

This would indicate they are keeping up pretty well.  Still, one table 
is out of sync by over 300 records.

> What do the slon logs say? Post them up using pastbin or something for review.

They are huge, so I'm not sure what I should be looking for.  I don't 
see any warnings or errors, only CONFIG, DEBUG1, DEBUG2 references.

>>  Is it possible I should be tweaking the configuration in some way?  I
>>  see a default value for SYNC_CHECK_INTERNAL.  Is 1000 a good value?
> 
> The default values are usually fine. 1000 is fine for sync check
> interval, though a bit overkill for most scenarios, the default of
> 2000 is usually OK.
> 
> -Dave
> 


-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From admin at kilnar.com  Wed Feb 20 14:10:04 2008
From: admin at kilnar.com (John Evans)
Date: Wed Feb 20 14:09:59 2008
Subject: [Slony1-general] Slony Docs For Multiple Databases
Message-ID: <Pine.LNX.4.64.0802201503190.18630@.>

All,

I've been searching for Slony documentation for several days, and I've
found some great documentation out there. However, there is one bit that
I seem to be missing. Let me explain my environment first:

Linux server running Postgres 8.1.11 with 20 databases running under a
single postmaster instance. We just built a new backup server that we
would like to replicate everything to (and keep the replication running
at all times) to ensure data integrity in case of hardware failure.

All of the documentation that I've found is for replicating a single
database to another postmaster instance (locally or remotely). I have
been unable to find anything that allows me to extrapolate how to do 20
databases from a postmaster to 20 databases on another postmaster.

If anyone has any pointers on the best way to do this, I would
appreciate it.

Also, from my reading, I have been lead to believe that a single instance
of 'slon' will be running for each database/node. Is this correct? Does
this mean that I'll have 20 instances of slon running on each server? If
this is the case, is the memory footprint of the process known? I can
find out on my own, but if someone knows off the top of the head, I'd
appreciate the information.

Thanks!

-- 
John Evans
Administrator of kilnar.com
From cbbrowne at ca.afilias.info  Wed Feb 20 15:21:57 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Feb 20 15:22:18 2008
Subject: [Slony1-general] logtrigger
In-Reply-To: <47BC4B56.8070205@serioustechnology.com> (Geoffrey's message of
	"Wed, 20 Feb 2008 10:46:30 -0500")
References: <47BC4B56.8070205@serioustechnology.com>
Message-ID: <60y79ffc2y.fsf@dba2.int.libertyrms.com>

Geoffrey <lists@serioustechnology.com> writes:
> I see the following trigger that was added by slony:
>
>     _mwr_cluster_logtrigger_1 AFTER INSERT OR DELETE OR UPDATE ON
> avlds FOR EACH ROW EXECUTE PROCEDURE
> _mwr_cluster.logtrigger('_mwr_cluster', '1',
> 'kvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv')
>
> Is that correct?  That last field looks kind of like 'controlled junk.'

That "morse code" is normal.  I'd say that that particular table has
its primary key in the first column, which is then followed by, um,
"probably 30-40 columns."  However many "just data" columns as there
are "v"'s in that string.
-- 
(format nil "~S@~S" "cbbrowne" "acm.org")
http://www3.sympatico.ca/cbbrowne/linux.html
Blame it on the *-Property.
From cbbrowne at ca.afilias.info  Wed Feb 20 15:33:07 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Feb 20 15:33:24 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BC85B4.2000608@serioustechnology.com> (Geoffrey's message of
	"Wed, 20 Feb 2008 14:55:32 -0500")
References: <47BB0046.10109@serioustechnology.com>
	<47BB02AA.2060902@serioustechnology.com>
	<62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>
	<47BB3B54.7050306@serioustechnology.com>
	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>
	<47BB4528.6090505@serioustechnology.com>
	<20080219232427.GH6148@crankycanuck.ca>
	<47BB6CE0.7030502@serioustechnology.com>
	<20080220133111.GB12005@crankycanuck.ca>
	<47BC315B.3060708@serioustechnology.com>
	<20080220192344.GC13020@crankycanuck.ca>
	<47BC85B4.2000608@serioustechnology.com>
Message-ID: <60tzk3fbkc.fsf@dba2.int.libertyrms.com>

Geoffrey <lists@serioustechnology.com> writes:
> Andrew Sullivan wrote:
>> I am by no means willing to dismiss the suggestion that there are bugs in
>> Slony; but this still looks to me very much like there's something we don't
>> know about what happened, that explains the errors you're seeing.
>
> I would so love to figure out this issue.  I appreciate your efforts.
>
> I simply don't understand how one table inparticular could get so far
> out of sync.  We're talking 300 records.
>
> I can't imagine that slony is that fragile.  There's got to be
> something going on that we don't see.

I agree.  From what I have heard, it doesn't sound like you have
experienced anything that should be scratching any of the edge points
of Slony-I.

300 records don't just disappear.

When I put this all together, I'm increasingly suspicious that you may
have experienced hardware problems or some such thing that might cause
data loss that Slony-I would have no way to address.

> I started the replication of this database last night.  Neither
> machine has been rebooted and neither postmaster was restarted.
>
> Is it possible I should be tweaking the configuration in some way?  I
> see a default value for SYNC_CHECK_INTERNAL.  Is 1000 a good value?

That makes it try to do a SYNC each second, so that the granularity of
possible data loss is, well, 1000ms.

Reducing that to 100ms would tend to lead to somewhat more
aggressively-quick replication, though it is not obvious that the
system would necessarily replicate much faster.

I don't see fiddling with that being a particularly useful thing to
do.  It's "grasping at straws."

You've grown suspicious about *every* component, which, on the one
hand, is unsurprising, but on the other, not much useful.  I haven't
heard you mention anything that would cause me to expect Slony-I to
have eaten data, or to have even "started to look hungrily at the
data."

The notices you have mentioned are all benign things.  The one
question that comes to mind: Any interesting ERROR messages in the
PostgreSQL logs?  I'm getting more and more suspicious that something
about the entire DB cluster has gotten unstable, and if that's the
case, Slony-I wouldn't do any better than the DB it is running on...
-- 
let name="cbbrowne" and tld="linuxdatabases.info" in String.concat "@" [name;tld];;
http://linuxfinances.info/info/lisp.html
"On the  other hand, O'Reilly's book  about running W95 has  a toad as
the cover animal.  Makes sense; both have lots of  warts and croak all
the time."  --- Michael Kagalenko,
From cbbrowne at ca.afilias.info  Wed Feb 20 15:38:47 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Feb 20 15:39:00 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BC9223.30402@serioustechnology.com> (Geoffrey's message of
	"Wed, 20 Feb 2008 15:48:35 -0500")
References: <47BB0046.10109@serioustechnology.com>
	<47BB3B54.7050306@serioustechnology.com>
	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>
	<47BB4528.6090505@serioustechnology.com>
	<20080219232427.GH6148@crankycanuck.ca>
	<47BB6CE0.7030502@serioustechnology.com>
	<20080220133111.GB12005@crankycanuck.ca>
	<47BC315B.3060708@serioustechnology.com>
	<20080220192344.GC13020@crankycanuck.ca>
	<47BC85B4.2000608@serioustechnology.com>
	<72dbd3150802201234n3e7e3ff0oa82a48f0c9a5951@mail.gmail.com>
	<47BC9223.30402@serioustechnology.com>
Message-ID: <60prurfbav.fsf@dba2.int.libertyrms.com>

Geoffrey <lists@serioustechnology.com> writes:
> David Rees wrote:
>> On Wed, Feb 20, 2008 at 11:55 AM, Geoffrey <lists@serioustechnology.com> wrote:
>>>  I simply don't understand how one table inparticular could get so far
>>>  out of sync.  We're talking 300 records.
>> Have you checked that replication is up to date (look at the
>> sl_status
>> view on the master)?
>
> I'm assuming I should be looking at st_last_event and
> st_last_received? They are within one on both machines. (9303, 9302
> and 7278, 7277).
>
> This would indicate they are keeping up pretty well.  Still, one table
> is out of sync by over 300 records.

That's all useful to know.  That tells us that the 300 records aren't
simply a timing difference due to the slon being behind.

It would be *VERY* interesting to track down what happened to those
300 records.  If their data were still in sl_log_[1/2], then we could
trace through the logs and get a more exact picture of what did/didn't
happen.  

I suspect that the sl_log_* data is long gone, by now :-(.

>> What do the slon logs say? Post them up using pastbin or something for review.
>
> They are huge, so I'm not sure what I should be looking for.  I don't
> see any warnings or errors, only CONFIG, DEBUG1, DEBUG2 references.

That doesn't suggest any conumdrums, not up front, anyways.
-- 
"cbbrowne","@","linuxfinances.info"
http://www3.sympatico.ca/cbbrowne/finances.html
"It  is easier to move a  problem around  (for  example, by moving the
problem to a different part  of the overall network architecture) than
it is to solve it."  -- RFC 1925
From lists at serioustechnology.com  Wed Feb 20 16:13:37 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb 20 16:13:54 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <60tzk3fbkc.fsf@dba2.int.libertyrms.com>
References: <47BB0046.10109@serioustechnology.com>	<47BB02AA.2060902@serioustechnology.com>	<62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>	<47BB3B54.7050306@serioustechnology.com>	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>	<47BB4528.6090505@serioustechnology.com>	<20080219232427.GH6148@crankycanuck.ca>	<47BB6CE0.7030502@serioustechnology.com>	<20080220133111.GB12005@crankycanuck.ca>	<47BC315B.3060708@serioustechnology.com>	<20080220192344.GC13020@crankycanuck.ca>	<47BC85B4.2000608@serioustechnology.com>
	<60tzk3fbkc.fsf@dba2.int.libertyrms.com>
Message-ID: <47BCC231.3080506@serioustechnology.com>

Christopher, I appreciate your efforts as well as those of everyone else 
on the list.  I'm glad to see you folks haven't give up on me yet. :)

Christopher Browne wrote:
> Geoffrey <lists@serioustechnology.com> writes:
>> Andrew Sullivan wrote:
>>> I am by no means willing to dismiss the suggestion that there are bugs in
>>> Slony; but this still looks to me very much like there's something we don't
>>> know about what happened, that explains the errors you're seeing.
>> I would so love to figure out this issue.  I appreciate your efforts.
>>
>> I simply don't understand how one table inparticular could get so far
>> out of sync.  We're talking 300 records.
>>
>> I can't imagine that slony is that fragile.  There's got to be
>> something going on that we don't see.
> 
> I agree.  From what I have heard, it doesn't sound like you have
> experienced anything that should be scratching any of the edge points
> of Slony-I.
> 
> 300 records don't just disappear.
> 
> When I put this all together, I'm increasingly suspicious that you may
> have experienced hardware problems or some such thing that might cause
> data loss that Slony-I would have no way to address.

Understand, I'm not saying that I'm losing data, just that there are 
inconsistencies between the replication server and the primary.  I don't 
believe we are losing data on the primary at all.  What I see is the 
number of records in tables don't match, thus the replication process is 
not working as expected.  The weird thing is, not every table is 
affected, just a handful.  We're talking 88 tables and 84 sequences, but 
only 4 tables have problems.  Here's a comparison of record counts:

< count for adest 54055
---
 > count for adest 54056
65c65
< count for mcarr 22560
---
 > count for mcarr 22572
67c67
< count for mcust 63757
---
 > count for mcust 63774
94c94
< count for tract 75380
---
 > count for tract 75420

This hardware has been rock solid since it was installed.  If we were 
losing data on the primary, we would definitely hear about it.  One 
thing I didn't mention is the actual configuration.  Two boxes connected 
to a single data silo.  It's a hot/hot configuration. Separate 
postmaster for each database.  Half the postmasters run on one server, 
the other half on the other.  If/when one fails, the other picks up the 
postmaster processes.  Each database has it's own IP, so I reference the 
host by multiple host names.  Connect to database mwr via host mwr.  In 
the event of failure, mwr IP is moved to the other machine.

<snip>

> You've grown suspicious about *every* component, which, on the one
> hand, is unsurprising, but on the other, not much useful.  I haven't
> heard you mention anything that would cause me to expect Slony-I to
> have eaten data, or to have even "started to look hungrily at the
> data."

The only reason I keep looking at slony is because the system is rock 
solid.  We don't lose data and these boxes are up 24/7.  Folks are 
hitting them constantly.  Slony is the only new part of the equation.

> The notices you have mentioned are all benign things.  The one
> question that comes to mind: Any interesting ERROR messages in the
> PostgreSQL logs?  I'm getting more and more suspicious that something
> about the entire DB cluster has gotten unstable, and if that's the
> case, Slony-I wouldn't do any better than the DB it is running on...

There are no postgresql errors to speak of on the primary.

I do see the following in the postgresql log on the slave:

2008-02-19 19:30:59 [3216] NOTICE:  type "_mwr_cluster.xxid" is not yet 
defined
DETAIL:  Creating a shell type definition.
2008-02-19 19:30:59 [3216] NOTICE:  argument type _mwr_cluster.xxid is 
only a shell
2008-02-19 19:30:59 [3216] NOTICE:  type "_mwr_cluster.xxid_snapshot" is 
not yet defined
DETAIL:  Creating a shell type definition.
2008-02-19 19:30:59 [3216] NOTICE:  argument type 
_mwr_cluster.xxid_snapshot is only a shell

Since these are NOTICEs, I assume this is normal.

During the initial replication, I do see a number of:

2008-02-19 19:32:28 [2463] LOG:  checkpoints are occurring too 
frequently (6 seconds apart)

But our problem doesn't seem to start until after the initial replication.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From drees76 at gmail.com  Wed Feb 20 16:15:33 2008
From: drees76 at gmail.com (David Rees)
Date: Wed Feb 20 16:15:50 2008
Subject: [Slony1-general] Slony Docs For Multiple Databases
In-Reply-To: <Pine.LNX.4.64.0802201503190.18630@.>
References: <Pine.LNX.4.64.0802201503190.18630@.>
Message-ID: <72dbd3150802201615i614abae5gb955c2acb9058851@mail.gmail.com>

On Wed, Feb 20, 2008 at 2:10 PM, John Evans <admin@kilnar.com> wrote:
>  All of the documentation that I've found is for replicating a single
>  database to another postmaster instance (locally or remotely). I have
>  been unable to find anything that allows me to extrapolate how to do 20
>  databases from a postmaster to 20 databases on another postmaster.
>
>  If anyone has any pointers on the best way to do this, I would
>  appreciate it.

Replicating 20 database is the same as replicating 1, you just need 20
times as many configuration files or scripts and 20 slon daemons
running per database.

>  Also, from my reading, I have been lead to believe that a single instance
>  of 'slon' will be running for each database/node. Is this correct? Does
>  this mean that I'll have 20 instances of slon running on each server? If
>  this is the case, is the memory footprint of the process known? I can
>  find out on my own, but if someone knows off the top of the head, I'd
>  appreciate the information.

Yes, you need 1 instance of slon per database node, so if you have 20
databases to replicate from one node to another, you will have 40 slon
daemons running.

Also note that each slon appears to keep 4 connections open to the
database it's monitoring and 1 connection open to the other node, so
with 20 databases and 2 nodes, you'll have about 100 slony related
database connection open all the time.

On my Fedora 8 32bit test system, ps reports a single slon instance as
using about 70MB VSZ and 4-5MB RSS. The amount of memory used seems to
be more on some CentOS 64bit systems I've checked.

-Dave
From lists at serioustechnology.com  Wed Feb 20 16:17:08 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb 20 16:17:23 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <60prurfbav.fsf@dba2.int.libertyrms.com>
References: <47BB0046.10109@serioustechnology.com>	<47BB3B54.7050306@serioustechnology.com>	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>	<47BB4528.6090505@serioustechnology.com>	<20080219232427.GH6148@crankycanuck.ca>	<47BB6CE0.7030502@serioustechnology.com>	<20080220133111.GB12005@crankycanuck.ca>	<47BC315B.3060708@serioustechnology.com>	<20080220192344.GC13020@crankycanuck.ca>	<47BC85B4.2000608@serioustechnology.com>	<72dbd3150802201234n3e7e3ff0oa82a48f0c9a5951@mail.gmail.com>	<47BC9223.30402@serioustechnology.com>
	<60prurfbav.fsf@dba2.int.libertyrms.com>
Message-ID: <47BCC304.3010304@serioustechnology.com>

Christopher Browne wrote:
> Geoffrey <lists@serioustechnology.com> writes:
>> David Rees wrote:
>>> On Wed, Feb 20, 2008 at 11:55 AM, Geoffrey <lists@serioustechnology.com> wrote:
>>>>  I simply don't understand how one table inparticular could get so far
>>>>  out of sync.  We're talking 300 records.
>>> Have you checked that replication is up to date (look at the
>>> sl_status
>>> view on the master)?
>> I'm assuming I should be looking at st_last_event and
>> st_last_received? They are within one on both machines. (9303, 9302
>> and 7278, 7277).
>>
>> This would indicate they are keeping up pretty well.  Still, one table
>> is out of sync by over 300 records.
> 
> That's all useful to know.  That tells us that the 300 records aren't
> simply a timing difference due to the slon being behind.
> 
> It would be *VERY* interesting to track down what happened to those
> 300 records.  If their data were still in sl_log_[1/2], then we could
> trace through the logs and get a more exact picture of what did/didn't
> happen.
> 
> I suspect that the sl_log_* data is long gone, by now :-(.

Well, I'll be honest with you, I would be glad to test replication on 
another of our databases and do what ever is necessary to retain that 
data so we could look into this issue.  Is there a way to do this?

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From drees76 at gmail.com  Wed Feb 20 16:20:25 2008
From: drees76 at gmail.com (David Rees)
Date: Wed Feb 20 16:20:46 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BCC231.3080506@serioustechnology.com>
References: <47BB0046.10109@serioustechnology.com>
	<47BB4528.6090505@serioustechnology.com>
	<20080219232427.GH6148@crankycanuck.ca>
	<47BB6CE0.7030502@serioustechnology.com>
	<20080220133111.GB12005@crankycanuck.ca>
	<47BC315B.3060708@serioustechnology.com>
	<20080220192344.GC13020@crankycanuck.ca>
	<47BC85B4.2000608@serioustechnology.com>
	<60tzk3fbkc.fsf@dba2.int.libertyrms.com>
	<47BCC231.3080506@serioustechnology.com>
Message-ID: <72dbd3150802201620m4996c878j1c01bd9ce6d38456@mail.gmail.com>

On Wed, Feb 20, 2008 at 4:13 PM, Geoffrey <lists@serioustechnology.com> wrote:
>  During the initial replication, I do see a number of:
>
>  2008-02-19 19:32:28 [2463] LOG:  checkpoints are occurring too
>  frequently (6 seconds apart)

Those are normal.

Did you ever mention what version of PostgreSQL and Slony you are using?

-Dave
From lists at serioustechnology.com  Wed Feb 20 16:24:58 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Wed Feb 20 16:25:29 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <72dbd3150802201620m4996c878j1c01bd9ce6d38456@mail.gmail.com>
References: <47BB0046.10109@serioustechnology.com>	
	<47BB4528.6090505@serioustechnology.com>	
	<20080219232427.GH6148@crankycanuck.ca>	
	<47BB6CE0.7030502@serioustechnology.com>	
	<20080220133111.GB12005@crankycanuck.ca>	
	<47BC315B.3060708@serioustechnology.com>	
	<20080220192344.GC13020@crankycanuck.ca>	
	<47BC85B4.2000608@serioustechnology.com>	
	<60tzk3fbkc.fsf@dba2.int.libertyrms.com>	
	<47BCC231.3080506@serioustechnology.com>
	<72dbd3150802201620m4996c878j1c01bd9ce6d38456@mail.gmail.com>
Message-ID: <47BCC4DA.5080807@serioustechnology.com>

David Rees wrote:
> On Wed, Feb 20, 2008 at 4:13 PM, Geoffrey <lists@serioustechnology.com> wrote:
>>  During the initial replication, I do see a number of:
>>
>>  2008-02-19 19:32:28 [2463] LOG:  checkpoints are occurring too
>>  frequently (6 seconds apart)
> 
> Those are normal.
> 
> Did you ever mention what version of PostgreSQL and Slony you are using?

Postgresql on both machines is 7.4.19, slony on both machines is 1.2.12

Both machines are running Red Hat Enterprise Advance Server 4.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From JanWieck at Yahoo.com  Wed Feb 20 15:12:33 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu Feb 21 00:07:48 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47B9FCCC.5030004@serioustechnology.com>
References: <47B9E86F.2040907@serioustechnology.com>	<60odaegdxc.fsf@dba2.int.libertyrms.com>
	<47B9FCCC.5030004@serioustechnology.com>
Message-ID: <47BCB3E1.1010001@Yahoo.com>

On 2/18/2008 4:46 PM, Geoffrey wrote:
> Christopher Browne wrote:
>> Geoffrey <lists@serioustechnology.com> writes:
>>> I want to make sure I have a good handle on this issue.  We currently
>>> have a master/slave configuration.  In the event the slave must be
>>> rebooted, what are the proper steps to insure that slony picks up from
>>> where it left off.
>>>
>>> What we are currently doing is simply restart the slon daemons for
>>> each database.
>> 
>> That seems apropos.
>> 
>>> For the most part this appears to work, but what concerns me is that
>>> I have one table on one database where the number of records on the
>>> master node has not increased in a while and the slave does not
>>> appear to be 'trying to catch up.'  That is to say, the slave has
>>> fewer records in that table then the master and the slave table is
>>> not growing.
>> 
>> Well, then "get thee to the slon logs..."
>> 
>> -> Do they indicate, for the subscriber, that errors are being experienced?
> 
> Not that I can tell.  That is the first placed I looked.
> 
>> -> Do they indicate that SYNCs are being processed, and data applied?
> 
> SYNCs processed, but says nothing to process:
> 
> 2008-02-18 16:39:27 EST DEBUG2 localListenThread: Received event 1,10345 
> SYNC
> 2008-02-18 16:39:32 EST DEBUG2 remoteListenThread_2: queue event 2,7343 SYNC
> 2008-02-18 16:39:32 EST DEBUG2 remoteWorkerThread_2: Received event 
> 2,7343 SYNC
> 2008-02-18 16:39:32 EST DEBUG2 remoteWorkerThread_2: SYNC 7343 processing
> 2008-02-18 16:39:32 EST DEBUG2 remoteWorkerThread_2: no sets need 
> syncing for this event

This is the slon log of node 1. Unless node 1 is the lagging subscriber 
(which I actually don't expect), this isn't telling us much.

The question is if the slon for the subscriber is running and if that 
ones log shows any errors.


Jan


> 
>> -> Is the subscriber in question behind (according to the view
>>    sl_status) by an increasing amount of time?
> 
> I'm not sure what I'm looking for here.  From the slave:
> 
> master=# select * from  _master_cluster.sl_status;
>   st_origin | st_received | st_last_event |      st_last_event_ts      | 
> st_last_received |    st_last_received_ts     | 
> st_last_received_event_ts  | st_lag_num_events | st_lag_time
> -----------+-------------+---------------+----------------------------+------------------+----------------------------+----------------------------+-------------------+-------------
>           2 |           1 |          7377 | 02/18/2008 16:45:11.298502 | 
>      7377 | 02/18/2008 16:45:11.728581 | 02/18/2008 16:45:11.298502 | 
>     0 | @ 4.40 secs
> 
> 
>> Error messages in the slon logs should give some idea of what is going on.
> 
> 


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin
From lists at serioustechnology.com  Thu Feb 21 05:01:48 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Thu Feb 21 05:02:12 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BCB3E1.1010001@Yahoo.com>
References: <47B9E86F.2040907@serioustechnology.com>	<60odaegdxc.fsf@dba2.int.libertyrms.com>
	<47B9FCCC.5030004@serioustechnology.com>
	<47BCB3E1.1010001@Yahoo.com>
Message-ID: <47BD763C.9000602@serioustechnology.com>

Jan Wieck wrote:

> This is the slon log of node 1. Unless node 1 is the lagging subscriber 
> (which I actually don't expect), this isn't telling us much.
> 
> The question is if the slon for the subscriber is running and if that 
> ones log shows any errors.

I don't see anything that slaps me in the head here either.  Here's the 
tail end of the node 2 (slave) log:

2008-02-21 08:00:49 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 13125
2008-02-21 08:00:50 EST DEBUG2 localListenThread: Received event 2,13125 
SYNC
2008-02-21 08:00:58 EST DEBUG2 remoteListenThread_1: queue event 1,15647 
SYNC
2008-02-21 08:00:58 EST DEBUG2 remoteWorkerThread_1: Received event 
1,15647 SYNC2008-02-21 08:00:58 EST DEBUG2 remoteWorkerThread_1: SYNC 
15647 processing
2008-02-21 08:00:58 EST DEBUG2 remoteWorkerThread_1: syncing set 2 with 
75 table(s) from provider 1

2008-02-21 07:58:55 EST DEBUG2  ssy_action_list length: 0
2008-02-21 07:58:55 EST DEBUG2 remoteWorkerThread_1: syncing set 1 with 
20 table(s) from provider 1
2008-02-21 07:58:55 EST DEBUG2  ssy_action_list length: 0
2008-02-21 07:58:55 EST DEBUG2 remoteWorkerThread_1: current local 
log_status is 0
2008-02-21 07:58:55 EST DEBUG2 remoteWorkerThread_1_1: current remote 
log_status = 0
2008-02-21 07:58:55 EST DEBUG2 remoteHelperThread_1_1: 0.006 seconds 
delay for first row
2008-02-21 07:58:55 EST DEBUG2 remoteHelperThread_1_1: 0.007 seconds 
until close cursor
2008-02-21 07:58:55 EST DEBUG2 remoteHelperThread_1_1: inserts=0 
updates=0 deletes=0
2008-02-21 07:58:56 EST DEBUG2 remoteWorkerThread_1: new sl_rowid_seq 
value: 1000000000000000
2008-02-21 07:58:56 EST DEBUG2 remoteWorkerThread_1: SYNC 15632 done in 
0.099 seconds
2008-02-21 07:58:56 EST DEBUG2 remoteWorkerThread_1: forward confirm 
2,13113 received by 1
2008-02-21 07:58:59 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 13114
2008-02-21 07:59:00 EST DEBUG2 localListenThread: Received event 2,13114 
SYNC

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From lists at serioustechnology.com  Thu Feb 21 05:15:19 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Thu Feb 21 05:15:25 2008
Subject: [Slony1-general] logs and/or schema info
Message-ID: <47BD7967.4030500@serioustechnology.com>

If anyone is interested, I'm willing to provide our slony logs and 
portions of our database schema if they think it might help tracking 
down our slony problem.

Since yesterday, the disparity between the number of records in one 
table has grown substantially.  The interesting thing is that out of the 
88 tables and 84 sequences, there's just a handful of tables exhibiting 
problems and it's always the same tables.  Note the difference in record 
counts for the 'tract' table, yesterday it was a difference of 40 
records, now it's 275.

< count for adest 54055
---
 > count for adest 54056
65c65
< count for mcarr 22560
---
 > count for mcarr 22572
67c67
< count for mcust 63758
---
 > count for mcust 63775
94c94
< count for tract 75381
---
 > count for tract 75106

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From ajs at crankycanuck.ca  Thu Feb 21 05:38:20 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Feb 21 05:38:31 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BC85B4.2000608@serioustechnology.com>
References: <62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>
	<47BB3B54.7050306@serioustechnology.com>
	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>
	<47BB4528.6090505@serioustechnology.com>
	<20080219232427.GH6148@crankycanuck.ca>
	<47BB6CE0.7030502@serioustechnology.com>
	<20080220133111.GB12005@crankycanuck.ca>
	<47BC315B.3060708@serioustechnology.com>
	<20080220192344.GC13020@crankycanuck.ca>
	<47BC85B4.2000608@serioustechnology.com>
Message-ID: <20080221133820.GA17293@crankycanuck.ca>

On Wed, Feb 20, 2008 at 02:55:32PM -0500, Geoffrey wrote:
> >1.  Somewhere, your application or some person got in and removed (or maybe
> >renamed and re-created) a table that was referenced by _something_ that was
> >still open.
> 
> The only tables that could possibly be removed would be temp tables.  I 
> assure you, none of the tables that are being replicated are being 
> removed by anyone.  The application is not designed that way.

Just because an application isn't designed to do something doesn't mean it
never does ;-)  Temp tables could indeed cause the message in question, but
_only if_ something was looking for that temp table.  (A temp table created
by a stored procedure without execute would fall into this case, for
instance, because the plans are cached.)

> >2.  Slony was dropped from the node without some set of your connections
> >having disconnected, and they're still expecting the triggers they can 
> >still
> >see to be able to write into that table.
> 
> Can you define 'dropped from the node?'

Somehow, that node stopped being a Slony replica, and so the Slony schema
was removed.  Someone attempted to insert something into a replicated table
(or delete something, or update something), and the trigger fired without
the underlying table into which to insert being there.  If someone had
superuser permission on the database, and was fooling with the underlying
Slony tables, for instance, all bets are off.  I have seen bigger messes
created by fat fingers.

> I simply don't understand how one table inparticular could get so far 
> out of sync.  We're talking 300 records.

Yes.  Note, however, that 300 records could be just a couple of SYNCs, if
the failure happened at just the right moment.
 
> I can't imagine that slony is that fragile.  There's got to be something 
> going on that we don't see.

I agree.

A
From ajs at crankycanuck.ca  Thu Feb 21 05:43:27 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Feb 21 05:43:34 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <47BCC231.3080506@serioustechnology.com>
References: <5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>
	<47BB4528.6090505@serioustechnology.com>
	<20080219232427.GH6148@crankycanuck.ca>
	<47BB6CE0.7030502@serioustechnology.com>
	<20080220133111.GB12005@crankycanuck.ca>
	<47BC315B.3060708@serioustechnology.com>
	<20080220192344.GC13020@crankycanuck.ca>
	<47BC85B4.2000608@serioustechnology.com>
	<60tzk3fbkc.fsf@dba2.int.libertyrms.com>
	<47BCC231.3080506@serioustechnology.com>
Message-ID: <20080221134327.GB17293@crankycanuck.ca>

On Wed, Feb 20, 2008 at 07:13:37PM -0500, Geoffrey wrote:

> thing I didn't mention is the actual configuration.  Two boxes connected 
> to a single data silo.  It's a hot/hot configuration. Separate 
> postmaster for each database.  Half the postmasters run on one server, 
> the other half on the other.  If/when one fails, the other picks up the 
> postmaster processes. 

How do you guarantee that the first is actually dead before the other "picks
up the postmaster process"?  I'm assuming what you mean is something like
this:

server1 <-------> disk <-------> server2

Server1 and server2 are both attached to the disk at the same time.  When
server1 "goes away", server2 fires up a postgres instance on the same data
area server1 was using, goes into recovery mode, and takes over the hostname
and IP of server1.  

In order for this to work, you have to be absolutely certain that server1 is
dead and disconnected from the shared disk before server2 starts the
postgres process on the same data area.  Without that, you are sure to have
database corruption of some kind.  That is, the data from server1 MUST BE
FLUSHED and on the platters before server2 starts using the same data area. 
So it might not be enough to be sure server1 is dead.  You have to be sure
the disk's cache is flushed too, or you could have a mess.

A
From lists at serioustechnology.com  Thu Feb 21 05:56:13 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Thu Feb 21 05:56:17 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <20080221133820.GA17293@crankycanuck.ca>
References: <62751.65.38.38.5.1203448225.squirrel@look.libertyrms.com>	<47BB3B54.7050306@serioustechnology.com>	<5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>	<47BB4528.6090505@serioustechnology.com>	<20080219232427.GH6148@crankycanuck.ca>	<47BB6CE0.7030502@serioustechnology.com>	<20080220133111.GB12005@crankycanuck.ca>	<47BC315B.3060708@serioustechnology.com>	<20080220192344.GC13020@crankycanuck.ca>	<47BC85B4.2000608@serioustechnology.com>
	<20080221133820.GA17293@crankycanuck.ca>
Message-ID: <47BD82FD.3020804@serioustechnology.com>

Andrew Sullivan wrote:
> On Wed, Feb 20, 2008 at 02:55:32PM -0500, Geoffrey wrote:
>>> 1.  Somewhere, your application or some person got in and removed (or maybe
>>> renamed and re-created) a table that was referenced by _something_ that was
>>> still open.
>> The only tables that could possibly be removed would be temp tables.  I 
>> assure you, none of the tables that are being replicated are being 
>> removed by anyone.  The application is not designed that way.
> 
> Just because an application isn't designed to do something doesn't mean it
> never does ;-)

Agreed, but the way this data is used, if a table was ever dropped, it 
would be immediately apparent.  I know this is not happening.  Our 
application is constantly touching virtually every table in the 
application, if one was ever dropped, the data integrity issue would be 
immediately apparent.

> Temp tables could indeed cause the message in question, but
> _only if_ something was looking for that temp table.  (A temp table created
> by a stored procedure without execute would fall into this case, for
> instance, because the plans are cached.)

This could be the case, but I don't know.  I've inquired of others on 
the team to verify.  Question is, is this error related to our slony 
problems?  We've only seen this error following the initial replication 
of data via slony, so I suspect it's a slony issue, but I don't know if 
it's related to our data problem.

>>> 2.  Slony was dropped from the node without some set of your connections
>>> having disconnected, and they're still expecting the triggers they can 
>>> still
>>> see to be able to write into that table.
>> Can you define 'dropped from the node?'
> 
> Somehow, that node stopped being a Slony replica, and so the Slony schema
> was removed.

I would expect to see this blatantly noted in the log files.

> Someone attempted to insert something into a replicated table
> (or delete something, or update something), and the trigger fired without
> the underlying table into which to insert being there.  If someone had
> superuser permission on the database, and was fooling with the underlying
> Slony tables, for instance, all bets are off.  I have seen bigger messes
> created by fat fingers.

I'm the only person who has access to the replication server, so I don't 
believe this is the issue.  I didn't even start looking at any of the 
slony schema data until the problem appeared.

>> I simply don't understand how one table inparticular could get so far 
>> out of sync.  We're talking 300 records.
> 
> Yes.  Note, however, that 300 records could be just a couple of SYNCs, if
> the failure happened at just the right moment.

Understood, and the difference of 300 dropped to around 40 last night, 
but this morning is back up over 250.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From lists at serioustechnology.com  Thu Feb 21 06:08:35 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Thu Feb 21 06:08:41 2008
Subject: [Slony1-general] proper procedure for re-starting slony after
	replication slave reboots
In-Reply-To: <20080221134327.GB17293@crankycanuck.ca>
References: <5C26108C-6632-48FD-B52D-CF185631ECF2@khera.org>	<47BB4528.6090505@serioustechnology.com>	<20080219232427.GH6148@crankycanuck.ca>	<47BB6CE0.7030502@serioustechnology.com>	<20080220133111.GB12005@crankycanuck.ca>	<47BC315B.3060708@serioustechnology.com>	<20080220192344.GC13020@crankycanuck.ca>	<47BC85B4.2000608@serioustechnology.com>	<60tzk3fbkc.fsf@dba2.int.libertyrms.com>	<47BCC231.3080506@serioustechnology.com>
	<20080221134327.GB17293@crankycanuck.ca>
Message-ID: <47BD85E3.60600@serioustechnology.com>

Andrew Sullivan wrote:
> On Wed, Feb 20, 2008 at 07:13:37PM -0500, Geoffrey wrote:
> 
>> thing I didn't mention is the actual configuration.  Two boxes connected 
>> to a single data silo.  It's a hot/hot configuration. Separate 
>> postmaster for each database.  Half the postmasters run on one server, 
>> the other half on the other.  If/when one fails, the other picks up the 
>> postmaster processes. 
> 
> How do you guarantee that the first is actually dead before the other "picks
> up the postmaster process"?  I'm assuming what you mean is something like
> this:
> 
> server1 <-------> disk <-------> server2

Correct.  It's all handled by the Red Hat cluster software.

> Server1 and server2 are both attached to the disk at the same time.  When
> server1 "goes away", server2 fires up a postgres instance on the same data
> area server1 was using, goes into recovery mode, and takes over the hostname
> and IP of server1.  

Correct.

> In order for this to work, you have to be absolutely certain that server1 is
> dead and disconnected from the shared disk before server2 starts the
> postgres process on the same data area.  Without that, you are sure to have
> database corruption of some kind.  That is, the data from server1 MUST BE
> FLUSHED and on the platters before server2 starts using the same data area. 
> So it might not be enough to be sure server1 is dead.  You have to be sure
> the disk's cache is flushed too, or you could have a mess.

Agreed. We have tested this extensively.  It's a moot point with regard 
to our slony implementation as we've not had a failover since trying to 
get slony implemented.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From lists at serioustechnology.com  Thu Feb 21 06:41:12 2008
From: lists at serioustechnology.com (Geoffrey)
Date: Thu Feb 21 06:41:19 2008
Subject: [Slony1-general] We believe we have figured out our problem
Message-ID: <47BD8D88.70809@serioustechnology.com>

It is not a slony problem, but you all already knew that.

Here's the deal.  All our databases communicate via a network process. 
For convenience, when a record is placed into a certain set of tables, 
that record is then populated across all the databases.  The process 
that does this DISABLES TRIGGERS in order for this to not cause other 
issues.  Therefore, what is happening is, the slony trigger on the 
master is being disabled at times in order to complete this process.

I want to thank all you folks for sticking with us.  The responsible 
developer is looking into addressing this issue and we hope to be 
retesting our slony solution before the end of the week.

Your persistence has helped us to stay on top of this issue and insures 
that our months of work in slony implementation have not been wasted.

If I ever run into any of you in person, I will gladly purchase you a 
beverage of your choice.

-- 
Until later, Geoffrey

Those who would give up essential Liberty, to purchase a little
temporary Safety, deserve neither Liberty nor Safety.
  - Benjamin Franklin
From ajs at crankycanuck.ca  Thu Feb 21 06:47:55 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Feb 21 06:48:03 2008
Subject: [Slony1-general] runaway vacuum
In-Reply-To: <47BB74B8.2030808@emolecules.com>
References: <47B463D5.90605@emolecules.com> <47BB74B8.2030808@emolecules.com>
Message-ID: <20080221144755.GC17293@crankycanuck.ca>

On Tue, Feb 19, 2008 at 04:30:48PM -0800, Craig James wrote:
> table ..." on one of the replicated tables.  Since TRUNCATE can't be 
> handled by Slony, this lead to a duplicate-key violation in the slave 
> database, 

Ugh.  Note that you could do TRUNCATE via EXECUTE SCRIPT, I think.  Also, I
posted here an evil hack some time ago that allowed bulk loading across a
Slony node.  ISTR it had some bugs, but it might work as a starting point
for this sort of thing.

> Is there any way at all to block, trap, or otherwise prevent TRUNCATE of 
> certain tables?  

No, no more than there's some way to prevent some clever person from
performing ALTER TABLE.  I regard this as one of the most severe limitations
in Slony: it will break if you do certain things, but it can't prevent you
from doing them.

A

From cedric.villemain at dalibo.com  Thu Feb 21 06:53:48 2008
From: cedric.villemain at dalibo.com (=?ISO-8859-1?Q?C=E9dric_Villemain?=)
Date: Thu Feb 21 06:53:58 2008
Subject: [Slony1-general] runaway vacuum
In-Reply-To: <20080221144755.GC17293@crankycanuck.ca>
References: <47B463D5.90605@emolecules.com> <47BB74B8.2030808@emolecules.com>
	<20080221144755.GC17293@crankycanuck.ca>
Message-ID: <47BD907C.2050603@dalibo.com>

Andrew Sullivan a ?crit :
> On Tue, Feb 19, 2008 at 04:30:48PM -0800, Craig James wrote:
>   
>> table ..." on one of the replicated tables.  Since TRUNCATE can't be 
>> handled by Slony, this lead to a duplicate-key violation in the slave 
>> database, 
>>     
>
> Ugh.  Note that you could do TRUNCATE via EXECUTE SCRIPT, I think.  Also, I
> posted here an evil hack some time ago that allowed bulk loading across a
> Slony node.  ISTR it had some bugs, but it might work as a starting point
> for this sort of thing.
>
>   
>> Is there any way at all to block, trap, or otherwise prevent TRUNCATE of 
>> certain tables?  
>>     
>
> No, no more than there's some way to prevent some clever person from
> performing ALTER TABLE.  I regard this as one of the most severe limitations
> in Slony: it will break if you do certain things, but it can't prevent you
> from doing them.
>
>   

Simon had provide some work (and patches) for 'trigger on truncate', 
check pg-hacker ML. But I am not aware of the current state of that.


> A
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>   


-- 
C?dric Villemain
Administrateur de Base de Donn?es
Cel: +33 (0)6 74 15 56 53
http://dalibo.com - http://dalibo.org

From ajs at crankycanuck.ca  Thu Feb 21 07:23:27 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Feb 21 07:23:36 2008
Subject: [Slony1-general] other replication solutions
In-Reply-To: <47BC7FCC.3090704@serioustechnology.com>
References: <47BC347D.7090709@serioustechnology.com>
	<3AED03A3-12F5-4877-9F3E-F24E539F2739@khera.org>
	<47BC4D94.90007@serioustechnology.com>
	<47BC7FCC.3090704@serioustechnology.com>
Message-ID: <20080221152327.GH17293@crankycanuck.ca>

On Wed, Feb 20, 2008 at 02:30:20PM -0500, Geoffrey wrote:
> As we apparently can not determine what the problem is with our slony 
> implementation and therefore will not be using slony, I'm hoping there 
> might be other replication solutions out there.  Anyone know of any?

Command Prompt (my employer) will sell you one.  Many of the other systems
(but not CMD's) work the same way as Slony, and if you're unable to figure
out what's wrong here, I suspect you won't have any luck with them either. 
You could use something like Sequoia to commit into two back ends at the
same time.

But I still suspect there's a problem you haven't uncovered.  And given that
Slony is just using SQL to move the data around, if you're having problems
with this, either you have a problem of which you're unaware, or else you've
failed to tell us something critical.

A
From ajs at crankycanuck.ca  Thu Feb 21 07:04:58 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Feb 21 07:24:26 2008
Subject: [Slony1-general] Slony-I build cant find postgresql include folder
In-Reply-To: <15586012.post@talk.nabble.com>
References: <15586012.post@talk.nabble.com>
Message-ID: <20080221150458.GE17293@crankycanuck.ca>

On Wed, Feb 20, 2008 at 02:17:33AM -0800, ajcity wrote:
>   I wanna setup Slony-I on a remote RedHat Linux machine which uses the
> postgresql that comes distributed with the Redhat and the Slony doc doesn't
> cover that kind of thing. 

You probably need the postgresql-devel (or whatever they're called under Red
Hat) RPM sets.

A

From ajs at crankycanuck.ca  Thu Feb 21 07:35:57 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Feb 21 07:36:10 2008
Subject: [Slony1-general] still straw grasping
In-Reply-To: <47BC8FA1.2040903@serioustechnology.com>
References: <47BC8FA1.2040903@serioustechnology.com>
Message-ID: <20080221153557.GK17293@crankycanuck.ca>

On Wed, Feb 20, 2008 at 03:37:53PM -0500, Geoffrey wrote:
> I want to make sure that I'm not missing something in my configuration. 
> A quick summary of my config and my startup procedures.
> 
> My nodes are defined as follows:

[nodes]

I don't think there's anything in this that is wrong.  I also don't think
you're chasing down the right path, because you seem to be getting
replication, but inconsistently (that is, from your description so far, no
table is just not getting replicated; instead, in the cases where you have
problems, some tables don't seem to get all the changes).

You have not so far posted a single error message from the logs of any slon
daemon, AFAIR.  

This suggests to me that there's something that is preventing certain
tables' changes from being captured by the logtrigger, or else that
something is up on the replica that isn't on the origin.

I suspect it would help if you posted the _complete_ schema, including any
rules and triggers, for the table that's giving you pain.

A

From ajs at crankycanuck.ca  Thu Feb 21 07:13:27 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Feb 21 07:41:07 2008
Subject: [Slony1-general] should initial replication be done when no one
	has access to the database?
In-Reply-To: <47BC30DC.9090705@serioustechnology.com>
References: <47BC30DC.9090705@serioustechnology.com>
Message-ID: <20080221151327.GF17293@crankycanuck.ca>

On Wed, Feb 20, 2008 at 08:53:32AM -0500, Geoffrey wrote:
> Subject says it all.  We are grasping at straws to resolve this issue. 
> We again had problems with replication over night.  We have a single 
> table that does not appear to be properly replicating.

Define "not properly replicating"?  Also, if you have query logs from the
two nodes with every mention of that table, it'd be real handy.

A

From ajs at crankycanuck.ca  Thu Feb 21 07:14:14 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Feb 21 07:41:09 2008
Subject: [Slony1-general] narrowing down this problem
In-Reply-To: <47BC347D.7090709@serioustechnology.com>
References: <47BC347D.7090709@serioustechnology.com>
Message-ID: <20080221151414.GG17293@crankycanuck.ca>

On Wed, Feb 20, 2008 at 09:09:01AM -0500, Geoffrey wrote:
> Okay, so we have one table that appears to be having a problem in 
> replication.  The unique thing about this table is that we have a field 
> that often times is null, yet it has in index as follows:
> 
> "tract_order_num_key" unique, btree (order_num)

Is that the supposed-to-be-primary-key on the table?  It won't work.

A

From craig_james at emolecules.com  Thu Feb 21 08:09:08 2008
From: craig_james at emolecules.com (Craig James)
Date: Thu Feb 21 08:09:14 2008
Subject: [Slony1-general] runaway vacuum
In-Reply-To: <20080221144755.GC17293@crankycanuck.ca>
References: <47B463D5.90605@emolecules.com> <47BB74B8.2030808@emolecules.com>
	<20080221144755.GC17293@crankycanuck.ca>
Message-ID: <47BDA224.4070008@emolecules.com>

Andrew Sullivan wrote:
> On Tue, Feb 19, 2008 at 04:30:48PM -0800, Craig James wrote:
>> table ..." on one of the replicated tables.  Since TRUNCATE can't be 
>> handled by Slony, this lead to a duplicate-key violation in the slave 
>> database, 
> 
> Ugh.  Note that you could do TRUNCATE via EXECUTE SCRIPT, I think.  Also, I
> posted here an evil hack some time ago that allowed bulk loading across a
> Slony node.  ISTR it had some bugs, but it might work as a starting point
> for this sort of thing.
> 
>> Is there any way at all to block, trap, or otherwise prevent TRUNCATE of 
>> certain tables?  
> 
> No, no more than there's some way to prevent some clever person from
> performing ALTER TABLE.  I regard this as one of the most severe limitations
> in Slony: it will break if you do certain things, but it can't prevent you
> from doing them.

I don't mind that Slony can't handle truncate -- the documentation is quite clear -- but the real problem is that the daemon just kept trying to repeat an operation that could never succeed.  To all apearances, Slony was just fine.  It was only when I notice that a backup web site was producing different answers than the primary web site that I realized there was a problem.

In a situation like this, some sort of ACTIVE response from Slony would be nice.  Here's an idea.  When the Slony daemon detects an unrecoverable error, it should STOP, and send an email to a configurable administration email address.  Something like this:

  UNRECOVERABLE ERROR: duplicate key violates unique constraint "your_constraint_name".
  Slony is shutting down.  You must correct this error before you restart Slony.

Some errors, like network problems, are recoverable and Slony should keep going.  But others, like a foreign-key constraint violation, won't get better no matter how many times the daemon retries the operation.

Craig


From ajs at crankycanuck.ca  Thu Feb 21 09:13:59 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Feb 21 09:14:18 2008
Subject: [Slony1-general] runaway vacuum
In-Reply-To: <47BD907C.2050603@dalibo.com>
References: <47B463D5.90605@emolecules.com> <47BB74B8.2030808@emolecules.com>
	<20080221144755.GC17293@crankycanuck.ca>
	<47BD907C.2050603@dalibo.com>
Message-ID: <20080221171359.GA18657@crankycanuck.ca>

On Thu, Feb 21, 2008 at 03:53:48PM +0100, C?dric Villemain wrote:
> Simon had provide some work (and patches) for 'trigger on truncate', 
> check pg-hacker ML. But I am not aware of the current state of that.

Yeah, this might be available once 8.4 shows up.  But we're a long way from
that today :)

A
From ajs at crankycanuck.ca  Thu Feb 21 09:15:36 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Feb 21 09:15:50 2008
Subject: [Slony1-general] runaway vacuum
In-Reply-To: <47BDA224.4070008@emolecules.com>
References: <47B463D5.90605@emolecules.com> <47BB74B8.2030808@emolecules.com>
	<20080221144755.GC17293@crankycanuck.ca>
	<47BDA224.4070008@emolecules.com>
Message-ID: <20080221171536.GB18657@crankycanuck.ca>

On Thu, Feb 21, 2008 at 08:09:08AM -0800, Craig James wrote:

> fine.  It was only when I notice that a backup web site was producing 
> different answers than the primary web site that I realized there was a 
> problem.
> 
> In a situation like this, some sort of ACTIVE response from Slony would be 
> nice.  Here's an idea.  When the Slony daemon detects an unrecoverable 
> error, it should STOP, and send an email to a configurable administration 
> email address.  Something like this:

No, no, that should not go in the daemon.  That should go in your monitoring
system.  I believe there are Nagios plugins floating about.  They could be
smarter, though, particularly about this sort of recoverable/non-recoverable
distinction you're mentioning.

A

From Ow.Mun.Heng at wdc.com  Thu Feb 21 10:20:56 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Thu Feb 21 10:21:41 2008
Subject: [Slony1-general] replication Lag, sync grouping not happening
Message-ID: <1203618056.18865.4.camel@neuromancer.home.net>

I'm not sure what is happening, usually when the slave lags behind, I
will stop the slon process and then add in the -o10 -g500 options to the
master process an I usually will see that the syncs on the subscriber
will be grouped together.

AS of right now, I'm not seeing this happening and it's just processing
the syncs 1 by 1 and it's taking a long time for this to happen.

I also tried the -o10 -g500 on both the master and the slave and still
it goes 1 by 1.
2008-02-22 01:58:51 MYT DEBUG2 remoteHelperThread_1_1: inserts=0 updates=0 deletes=0
2008-02-22 01:58:51 MYT DEBUG2 remoteWorkerThread_1: SYNC 483711 done in 88.818 seconds
2008-02-22 02:00:07 MYT DEBUG2 remoteHelperThread_1_1: inserts=1984 updates=0 deletes=0
2008-02-22 02:00:10 MYT DEBUG2 remoteWorkerThread_1: SYNC 483712 done in 78.634 seconds
2008-02-22 02:01:34 MYT DEBUG2 remoteHelperThread_1_1: inserts=529 updates=0 deletes=56
2008-02-22 02:01:36 MYT DEBUG2 remoteWorkerThread_1: SYNC 483713 done in 85.745 seconds
2008-02-22 02:03:25 MYT DEBUG2 remoteHelperThread_1_1: inserts=1532 updates=0 deletes=0
2008-02-22 02:03:28 MYT DEBUG2 remoteWorkerThread_1: SYNC 483714 done in 112.476 seconds
2008-02-22 02:05:47 MYT DEBUG2 remoteHelperThread_1_1: inserts=1557 updates=0 deletes=0
2008-02-22 02:05:49 MYT DEBUG2 remoteWorkerThread_1: SYNC 483715 done in 140.691 seconds
2008-02-22 02:08:26 MYT DEBUG2 remoteHelperThread_1_1: inserts=2600 updates=0 deletes=225
2008-02-22 02:08:27 MYT DEBUG2 remoteWorkerThread_1: SYNC 483716 done in 157.839 seconds


From cbbrowne at ca.afilias.info  Thu Feb 21 10:47:40 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Feb 21 10:47:48 2008
Subject: [Slony1-general] replication Lag, sync grouping not happening
In-Reply-To: <1203618056.18865.4.camel@neuromancer.home.net> (Ow Mun Heng's
	message of "Fri, 22 Feb 2008 02:20:56 +0800")
References: <1203618056.18865.4.camel@neuromancer.home.net>
Message-ID: <60fxvmf8oj.fsf@dba2.int.libertyrms.com>

Ow Mun Heng <Ow.Mun.Heng@wdc.com> writes:
> I'm not sure what is happening, usually when the slave lags behind, I
> will stop the slon process and then add in the -o10 -g500 options to the
> master process an I usually will see that the syncs on the subscriber
> will be grouped together.
>
> AS of right now, I'm not seeing this happening and it's just processing
> the syncs 1 by 1 and it's taking a long time for this to happen.
>
> I also tried the -o10 -g500 on both the master and the slave and still
> it goes 1 by 1.
> 2008-02-22 01:58:51 MYT DEBUG2 remoteHelperThread_1_1: inserts=0 updates=0 deletes=0
> 2008-02-22 01:58:51 MYT DEBUG2 remoteWorkerThread_1: SYNC 483711 done in 88.818 seconds
> 2008-02-22 02:00:07 MYT DEBUG2 remoteHelperThread_1_1: inserts=1984 updates=0 deletes=0
> 2008-02-22 02:00:10 MYT DEBUG2 remoteWorkerThread_1: SYNC 483712 done in 78.634 seconds
> 2008-02-22 02:01:34 MYT DEBUG2 remoteHelperThread_1_1: inserts=529 updates=0 deletes=56
> 2008-02-22 02:01:36 MYT DEBUG2 remoteWorkerThread_1: SYNC 483713 done in 85.745 seconds
> 2008-02-22 02:03:25 MYT DEBUG2 remoteHelperThread_1_1: inserts=1532 updates=0 deletes=0
> 2008-02-22 02:03:28 MYT DEBUG2 remoteWorkerThread_1: SYNC 483714 done in 112.476 seconds
> 2008-02-22 02:05:47 MYT DEBUG2 remoteHelperThread_1_1: inserts=1557 updates=0 deletes=0
> 2008-02-22 02:05:49 MYT DEBUG2 remoteWorkerThread_1: SYNC 483715 done in 140.691 seconds
> 2008-02-22 02:08:26 MYT DEBUG2 remoteHelperThread_1_1: inserts=2600 updates=0 deletes=225
> 2008-02-22 02:08:27 MYT DEBUG2 remoteWorkerThread_1: SYNC 483716 done in 157.839 seconds

I believe that -o10 causes Slony-I to try to track having SYNC
processing time take an estimated time of 10ms per group; the value is
measured in milliseconds, not seconds.

That being the case, if the last *single* SYNC took "lots more than
10ms," then the slon will not be considering processing several SYNCs
at once.  (And note that since the times were also >>> 10s, the
principle would still hold if -o was measuring in seconds.)

Based on the timings you indicate, the only way that you'll see SYNC
grouping is if you set the value to something more like 200000.
-- 
select 'cbbrowne' || '@' || 'linuxfinances.info';
http://linuxfinances.info/info/multiplexor.html
What happens  when your "Windows Digital Nervous  System" crashes?  Do
we call that a "Microsoft Nervous Breakdown?"
From Ow.Mun.Heng at wdc.com  Thu Feb 21 11:37:06 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Thu Feb 21 11:37:28 2008
Subject: [Solved] Re: [Slony1-general] replication Lag, sync grouping not
	happening
In-Reply-To: <60fxvmf8oj.fsf@dba2.int.libertyrms.com>
References: <1203618056.18865.4.camel@neuromancer.home.net>
	<60fxvmf8oj.fsf@dba2.int.libertyrms.com>
Message-ID: <1203622626.18865.15.camel@neuromancer.home.net>


On Thu, 2008-02-21 at 18:47 +0000, Christopher Browne wrote:
> Ow Mun Heng <Ow.Mun.Heng@wdc.com> writes:
> > I'm not sure what is happening, usually when the slave lags behind, I
> > will stop the slon process and then add in the -o10 -g500 options to the
> > master process an I usually will see that the syncs on the subscriber
> > will be grouped together.
> >
> > AS of right now, I'm not seeing this happening and it's just processing
> > the syncs 1 by 1 and it's taking a long time for this to happen.
> >
> > I also tried the -o10 -g500 on both the master and the slave and still
> > it goes 1 by 1.
> > 2008-02-22 01:58:51 MYT DEBUG2 remoteHelperThread_1_1: inserts=0 updates=0 deletes=0
> > 2008-02-22 01:58:51 MYT DEBUG2 remoteWorkerThread_1: SYNC 483711 done in 88.818 seconds
> > 2008-02-22 02:00:07 MYT DEBUG2 remoteHelperThread_1_1: inserts=1984 updates=0 deletes=0
> > 2008-02-22 02:00:10 MYT DEBUG2 remoteWorkerThread_1: SYNC 483712 done in 78.634 seconds
> > 2008-02-22 02:01:34 MYT DEBUG2 remoteHelperThread_1_1: inserts=529 updates=0 deletes=56
> > 2008-02-22 02:01:36 MYT DEBUG2 remoteWorkerThread_1: SYNC 483713 done in 85.745 seconds
> > 2008-02-22 02:03:25 MYT DEBUG2 remoteHelperThread_1_1: inserts=1532 updates=0 deletes=0
> > 2008-02-22 02:03:28 MYT DEBUG2 remoteWorkerThread_1: SYNC 483714 done in 112.476 seconds
> > 2008-02-22 02:05:47 MYT DEBUG2 remoteHelperThread_1_1: inserts=1557 updates=0 deletes=0
> > 2008-02-22 02:05:49 MYT DEBUG2 remoteWorkerThread_1: SYNC 483715 done in 140.691 seconds
> > 2008-02-22 02:08:26 MYT DEBUG2 remoteHelperThread_1_1: inserts=2600 updates=0 deletes=225
> > 2008-02-22 02:08:27 MYT DEBUG2 remoteWorkerThread_1: SYNC 483716 done in 157.839 seconds
> 
> I believe that -o10 causes Slony-I to try to track having SYNC
> processing time take an estimated time of 10ms per group; the value is
> measured in milliseconds, not seconds.
> 
> That being the case, if the last *single* SYNC took "lots more than
> 10ms," then the slon will not be considering processing several SYNCs
> at once.  (And note that since the times were also >>> 10s, the
> principle would still hold if -o was measuring in seconds.)
> 
> Based on the timings you indicate, the only way that you'll see SYNC
> grouping is if you set the value to something more like 200000.

Master : slon -d4 -c2 -g500 -s60000 -o200000 -f slon_master.conf
Slave :  slon -d2     -g500         -o200000 -f slon_slave1.conf | egrep -i 'done in|inserts='

2008-02-22 03:11:58 MYT DEBUG2 remoteHelperThread_1_1: inserts=807 updates=0 deletes=130
2008-02-22 03:12:04 MYT DEBUG2 remoteWorkerThread_1: SYNC 483756 done in 62.840 seconds
2008-02-22 03:13:19 MYT DEBUG2 remoteHelperThread_1_1: inserts=4626 updates=0 deletes=8
2008-02-22 03:13:19 MYT DEBUG2 remoteWorkerThread_1: SYNC 483759 done in 75.382 seconds
2008-02-22 03:14:49 MYT DEBUG2 remoteHelperThread_1_1: inserts=8824 updates=0 deletes=418
2008-02-22 03:14:50 MYT DEBUG2 remoteWorkerThread_1: SYNC 483766 done in 90.575 second
2008-02-22 03:17:06 MYT DEBUG2 remoteHelperThread_1_1: inserts=19587 updates=0 deletes=566
2008-02-22 03:17:07 MYT DEBUG2 remoteWorkerThread_1: SYNC 483781 done in 136.992 seconds
2008-02-22 03:20:12 MYT DEBUG2 remoteHelperThread_1_1: inserts=24451 updates=1138 deletes=484
2008-02-22 03:20:14 MYT DEBUG2 remoteWorkerThread_1: SYNC 483802 done in 187.493 seconds

Seems like this is starting back to go in groups.

To be frank, the -o -s options really befuddles me. I've read the docs
but I guess I don't really understand them enough to know whether these
options work on the master or the slave. (hence as above, I just put it
on both master ans slave)

On another front, I tend to believe that one of the reason for the lag
is because my disks are slow. (1x 500GB IDE 7200 rpm and they're bogged
down, atop shows 90% usage nearly 80% of the time) To add to that, I
noticed that it will start to slow even more when sl_log_1/2 becomes
large ~2GB and no amount of vacuum/reindex/recreate index will get it
back up to speed. (fetch 100 from log becomes real slow >500secs )

Chris(you) already shown me how to manually force a logswitch, and thus,
now I'm considering making a job to manually force the switch like every
6 hours just to get the size under control. Is this a good Idea?
From drees76 at gmail.com  Thu Feb 21 12:00:10 2008
From: drees76 at gmail.com (David Rees)
Date: Thu Feb 21 12:00:20 2008
Subject: [Slony1-general] runaway vacuum
In-Reply-To: <20080221171536.GB18657@crankycanuck.ca>
References: <47B463D5.90605@emolecules.com> <47BB74B8.2030808@emolecules.com>
	<20080221144755.GC17293@crankycanuck.ca>
	<47BDA224.4070008@emolecules.com>
	<20080221171536.GB18657@crankycanuck.ca>
Message-ID: <72dbd3150802211200v793776f0ucf48d56a3786794@mail.gmail.com>

On Thu, Feb 21, 2008 at 9:15 AM, Andrew Sullivan <ajs@crankycanuck.ca> wrote:
> On Thu, Feb 21, 2008 at 08:09:08AM -0800, Craig James wrote:
>  > In a situation like this, some sort of ACTIVE response from Slony would be
>  > nice.  Here's an idea.  When the Slony daemon detects an unrecoverable
>  > error, it should STOP, and send an email to a configurable administration
>  > email address.  Something like this:
>
>  No, no, that should not go in the daemon.  That should go in your monitoring
>  system.  I believe there are Nagios plugins floating about.  They could be
>  smarter, though, particularly about this sort of recoverable/non-recoverable
>  distinction you're mentioning.

Yep, we use Nagios to monitor replication status, it works quite well.
When things get out of sync (has actually never happened in production
yet!) we simply go through the slon/pg logs to figure out what went
wrong.

-Dave
From damien at dalibo.info  Thu Feb 21 14:14:50 2008
From: damien at dalibo.info (damien clochard)
Date: Thu Feb 21 14:15:04 2008
Subject: [Slony1-general] French translation of Slony documentation
Message-ID: <200802212314.50648.damien@dalibo.info>

hi !

The last fews weeks, the PostgresSQL french spreaking community 
(http://www.postgresqlfr.org) has launched a new i18n project. We're going to 
translate Slony's documentation.

We'll start in the forthcoming days. For more info on our workgroup , check 
out our wiki :

http://svn.postgresqlfr.org/wiki/Slony

We're open to any advice or suggestion. 
If you want to get involved in this project, please join our mailing-list :

http://listes.postgresql.fr/cgi-bin/mailman/listinfo/slony-traduction

I really can't tell when we will release the first translation, but i believe 
this could go pretty fast. 

? bient?t ! :-)

-- 
damien clochard
http://dalibo.org | http://dalibo.com
From cbbrowne at ca.afilias.info  Thu Feb 21 14:24:55 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Feb 21 14:25:15 2008
Subject: [Slony1-general] French translation of Slony documentation
In-Reply-To: <200802212314.50648.damien@dalibo.info> (damien clochard's
	message of "Thu, 21 Feb 2008 23:14:50 +0100")
References: <200802212314.50648.damien@dalibo.info>
Message-ID: <607igyeymg.fsf@dba2.int.libertyrms.com>

damien clochard <damien@dalibo.info> writes:
> hi !
>
> The last fews weeks, the PostgresSQL french spreaking community 
> (http://www.postgresqlfr.org) has launched a new i18n project. We're going to 
> translate Slony's documentation.
>
> We'll start in the forthcoming days. For more info on our workgroup , check 
> out our wiki :
>
> http://svn.postgresqlfr.org/wiki/Slony
>
> We're open to any advice or suggestion. 
> If you want to get involved in this project, please join our mailing-list :
>
> http://listes.postgresql.fr/cgi-bin/mailman/listinfo/slony-traduction
>
> I really can't tell when we will release the first translation, but i believe 
> this could go pretty fast. 
>
> ?? bient??t ! :-)

May I suggest that you follow CVS HEAD on this...

There are quite a number of things in HEAD that are not in the 1.2
branch (naturally, as there are new features in HEAD).  Features are
frequently identified with version information; where they aren't, and
should be, that is likely a bug :-).

At any rate, I frequently find myself adding docs to HEAD and not
bothering with adding it to older versions, even though a particular
change may be relevant to both branches...
-- 
output = ("cbbrowne" "@" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From cbbrowne at ca.afilias.info  Thu Feb 21 14:47:44 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Feb 21 14:47:57 2008
Subject: [Solved] Re: [Slony1-general] replication Lag,
	sync grouping not happening
In-Reply-To: <1203622626.18865.15.camel@neuromancer.home.net> (Ow Mun Heng's
	message of "Fri, 22 Feb 2008 03:37:06 +0800")
References: <1203618056.18865.4.camel@neuromancer.home.net>
	<60fxvmf8oj.fsf@dba2.int.libertyrms.com>
	<1203622626.18865.15.camel@neuromancer.home.net>
Message-ID: <603armexkf.fsf@dba2.int.libertyrms.com>

Ow Mun Heng <Ow.Mun.Heng@wdc.com> writes:
> On Thu, 2008-02-21 at 18:47 +0000, Christopher Browne wrote:
>> Ow Mun Heng <Ow.Mun.Heng@wdc.com> writes:
>> > I'm not sure what is happening, usually when the slave lags behind, I
>> > will stop the slon process and then add in the -o10 -g500 options to the
>> > master process an I usually will see that the syncs on the subscriber
>> > will be grouped together.
>> >
>> > AS of right now, I'm not seeing this happening and it's just processing
>> > the syncs 1 by 1 and it's taking a long time for this to happen.
>> >
>> > I also tried the -o10 -g500 on both the master and the slave and still
>> > it goes 1 by 1.
>> > 2008-02-22 01:58:51 MYT DEBUG2 remoteHelperThread_1_1: inserts=0 updates=0 deletes=0
>> > 2008-02-22 01:58:51 MYT DEBUG2 remoteWorkerThread_1: SYNC 483711 done in 88.818 seconds
>> > 2008-02-22 02:00:07 MYT DEBUG2 remoteHelperThread_1_1: inserts=1984 updates=0 deletes=0
>> > 2008-02-22 02:00:10 MYT DEBUG2 remoteWorkerThread_1: SYNC 483712 done in 78.634 seconds
>> > 2008-02-22 02:01:34 MYT DEBUG2 remoteHelperThread_1_1: inserts=529 updates=0 deletes=56
>> > 2008-02-22 02:01:36 MYT DEBUG2 remoteWorkerThread_1: SYNC 483713 done in 85.745 seconds
>> > 2008-02-22 02:03:25 MYT DEBUG2 remoteHelperThread_1_1: inserts=1532 updates=0 deletes=0
>> > 2008-02-22 02:03:28 MYT DEBUG2 remoteWorkerThread_1: SYNC 483714 done in 112.476 seconds
>> > 2008-02-22 02:05:47 MYT DEBUG2 remoteHelperThread_1_1: inserts=1557 updates=0 deletes=0
>> > 2008-02-22 02:05:49 MYT DEBUG2 remoteWorkerThread_1: SYNC 483715 done in 140.691 seconds
>> > 2008-02-22 02:08:26 MYT DEBUG2 remoteHelperThread_1_1: inserts=2600 updates=0 deletes=225
>> > 2008-02-22 02:08:27 MYT DEBUG2 remoteWorkerThread_1: SYNC 483716 done in 157.839 seconds
>> 
>> I believe that -o10 causes Slony-I to try to track having SYNC
>> processing time take an estimated time of 10ms per group; the value is
>> measured in milliseconds, not seconds.
>> 
>> That being the case, if the last *single* SYNC took "lots more than
>> 10ms," then the slon will not be considering processing several SYNCs
>> at once.  (And note that since the times were also >>> 10s, the
>> principle would still hold if -o was measuring in seconds.)
>> 
>> Based on the timings you indicate, the only way that you'll see SYNC
>> grouping is if you set the value to something more like 200000.
>
> Master : slon -d4 -c2 -g500 -s60000 -o200000 -f slon_master.conf
> Slave :  slon -d2     -g500         -o200000 -f slon_slave1.conf | egrep -i 'done in|inserts='
>
> 2008-02-22 03:11:58 MYT DEBUG2 remoteHelperThread_1_1: inserts=807 updates=0 deletes=130
> 2008-02-22 03:12:04 MYT DEBUG2 remoteWorkerThread_1: SYNC 483756 done in 62.840 seconds
> 2008-02-22 03:13:19 MYT DEBUG2 remoteHelperThread_1_1: inserts=4626 updates=0 deletes=8
> 2008-02-22 03:13:19 MYT DEBUG2 remoteWorkerThread_1: SYNC 483759 done in 75.382 seconds
> 2008-02-22 03:14:49 MYT DEBUG2 remoteHelperThread_1_1: inserts=8824 updates=0 deletes=418
> 2008-02-22 03:14:50 MYT DEBUG2 remoteWorkerThread_1: SYNC 483766 done in 90.575 second
> 2008-02-22 03:17:06 MYT DEBUG2 remoteHelperThread_1_1: inserts=19587 updates=0 deletes=566
> 2008-02-22 03:17:07 MYT DEBUG2 remoteWorkerThread_1: SYNC 483781 done in 136.992 seconds
> 2008-02-22 03:20:12 MYT DEBUG2 remoteHelperThread_1_1: inserts=24451 updates=1138 deletes=484
> 2008-02-22 03:20:14 MYT DEBUG2 remoteWorkerThread_1: SYNC 483802 done in 187.493 seconds
>
> Seems like this is starting back to go in groups.
>
> To be frank, the -o -s options really befuddles me. I've read the docs
> but I guess I don't really understand them enough to know whether these
> options work on the master or the slave. (hence as above, I just put it
> on both master ans slave)

Yes, the docs aren't clear enough.

-o and -g are relevant to subscriber behaviour; they do not affect the
origin in any way.

-s and -t are relevant to origin behaviour, primarily.  (Events are,
every so often, generated on ALL nodes, so it's not solely an "origin
thing.")

Clearly this warrants elaborating on the docs...

> On another front, I tend to believe that one of the reason for the lag
> is because my disks are slow. (1x 500GB IDE 7200 rpm and they're bogged
> down, atop shows 90% usage nearly 80% of the time) To add to that, I
> noticed that it will start to slow even more when sl_log_1/2 becomes
> large ~2GB and no amount of vacuum/reindex/recreate index will get it
> back up to speed. (fetch 100 from log becomes real slow >500secs )
>
> Chris(you) already shown me how to manually force a logswitch, and thus,
> now I'm considering making a job to manually force the switch like every
> 6 hours just to get the size under control. Is this a good Idea?

You can automate that by, um, ...  Hey, that's not well enough
documented :-).

The default, as controlled by data in sl_registry, is to switch logs
once per week.

Actually, looking at the code, the way it accesses sl_registry,
there's not a straightforward way to change that to either daily or
multiple times per day.  I think for CVS HEAD, I'm inclined to clean
that stuff out because there is a cleaner way, using the new parameter,
cleanup_interval.

For now, I think that running a script that starts the log switch
every 6 hours is probably about the right idea.
-- 
select 'cbbrowne' || '@' || 'linuxdatabases.info';
http://www3.sympatico.ca/cbbrowne/nonrdbms.html
"...make -k all to compile  everything in the core distribution.  This
will take anywhere from 15 minutes  (on a Cray Y-MP) to 12 hours."  
-- X Window System Release Notes
From Ow.Mun.Heng at wdc.com  Thu Feb 21 14:57:56 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Thu Feb 21 14:58:20 2008
Subject: [Solved] Re: [Slony1-general] replication Lag, sync grouping
	not happening
In-Reply-To: <603armexkf.fsf@dba2.int.libertyrms.com>
References: <1203618056.18865.4.camel@neuromancer.home.net>
	<60fxvmf8oj.fsf@dba2.int.libertyrms.com>
	<1203622626.18865.15.camel@neuromancer.home.net>
	<603armexkf.fsf@dba2.int.libertyrms.com>
Message-ID: <1203634676.18865.25.camel@neuromancer.home.net>


On Thu, 2008-02-21 at 22:47 +0000, Christopher Browne wrote:
> Ow Mun Heng <Ow.Mun.Heng@wdc.com> writes:
> > On Thu, 2008-02-21 at 18:47 +0000, Christopher Browne wrote:

> > Chris(you) already shown me how to manually force a logswitch, and thus,
> > now I'm considering making a job to manually force the switch like every
> > 6 hours just to get the size under control. Is this a good Idea?
> 
> You can automate that by, um, ...  Hey, that's not well enough
> documented :-).

No wonder I'm confused ;-)

> 
> The default, as controlled by data in sl_registry, is to switch logs
> once per week.
> 
> Actually, looking at the code, the way it accesses sl_registry,
> there's not a straightforward way to change that to either daily or
> multiple times per day.  I think for CVS HEAD, I'm inclined to clean
> that stuff out because there is a cleaner way, using the new parameter,
> cleanup_interval.
> 
> For now, I think that running a script that starts the log switch
> every 6 hours is probably about the right idea.

Okie. Thanks for the pointer. I will do that via pgagent
From troy at troywolf.com  Fri Feb 22 07:18:59 2008
From: troy at troywolf.com (Troy Wolf)
Date: Fri Feb 22 07:19:06 2008
Subject: [Slony1-general] Slony locks tables that are not even in
	replication sets?
Message-ID: <e0d7c3f50802220718x3c19e3e0y8e253dc053bb8ca0@mail.gmail.com>

Slony-I 1.2.12
Postgres 8.2.1
2 Nodes: 1 origin, 1 subscriber, 1 replication set

This discussion is regarding locking--a popular and sometimes confusing
topic for a lot of us I know. I've read the documentation several times now
and searched the mailing list (via Nabble.com) for related locking issues.

I continue to see posts that mention spreading tables across multiple
replication sets in order to reduce locking issues. The documentation has
more than one place that suggests this is a good strategy. However, on this
page (http://slony.info/documentation/ddlchanges.html), there is this
important note:

--- BEGIN QUOTE ----------------------------------------------
You may be able to define replication sets that consist of smaller sets of
tables so that fewer locks need to be taken in order for the DDL script to
make it into place.

If a particular DDL script only affects one table, it should be unnecessary
to lock all application tables.

Note: Actually, as of version 1.1.5 and later, this is NOT TRUE. The danger
of someone making DDL changes that crosses replication sets seems
sufficiently palpable that slon has been changed to lock ALL replicated
tables, whether they are in the specified replication set or not.
--- END QUOTE ----------------------------------------------

Therefore, running execute_script will LOCK ALL TABLES IN ALL REPLICATION
SETS regardless. Correct?

Now onto my issue at hand: Twice now, we've seen a pattern that looks like
Slony is waiting on a large index creation to complete, and then an
application gets stuck waiting behind Slony's locks. Here is why this is
such a mystery to me. The index is being created by a non-Slony process on a
table that is not in any replication set and that has no foreign key
relationships to any other table. It makes no sense to me that Slony would
be waiting on this index to finish.

On top of that, the application commit that is waiting is simply trying to
insert rows into a table that is again not contained in any replication set
nor does it have any foreign key relationships to any other table.

On top of THAT, why would Slony need to lock anything outside of it's own
_slony schema objects when simply logging SYNC events? From everything I can
tell, Slony had nothing to actually replicate during this time--no need to
require any locks on any of my tables whether in replication sets or not.

At this point in time, I'm not 100% sure the application commit was blocked
by Slony except we never had this problem before Slony.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080222/=
67507079/attachment.htm
From ajs at crankycanuck.ca  Fri Feb 22 07:33:14 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri Feb 22 07:33:27 2008
Subject: [Slony1-general] Slony locks tables that are not even in
	replication sets?
In-Reply-To: <e0d7c3f50802220718x3c19e3e0y8e253dc053bb8ca0@mail.gmail.com>
References: <e0d7c3f50802220718x3c19e3e0y8e253dc053bb8ca0@mail.gmail.com>
Message-ID: <20080222153314.GE24680@crankycanuck.ca>

On Fri, Feb 22, 2008 at 09:18:59AM -0600, Troy Wolf wrote:
> Therefore, running execute_script will LOCK ALL TABLES IN ALL REPLICATION
> SETS regardless. Correct?

Yes.  Not all of us agree with that decision, by the way, but that's how
it's implemented now.
 
> Now onto my issue at hand: Twice now, we've seen a pattern that looks like
> Slony is waiting on a large index creation to complete, and then an
> application gets stuck waiting behind Slony's locks. 
[. . .]
> At this point in time, I'm not 100% sure the application commit was blocked
> by Slony except we never had this problem before Slony.

I'm not dismissing your description, but output from pg_locks would be
helpful here.  Without a specific case, it's hard to say much more.

A

From Ow.Mun.Heng at wdc.com  Fri Feb 22 10:23:20 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Fri Feb 22 10:23:33 2008
Subject: [Slony1-general] Slave died, no access till next week,
	master logs filling up. What should I do
Message-ID: <1203704600.21659.23.camel@neuromancer.home.net>

I came to work today and seems like the slave server died. (power trip?
No it was not connected to a UPS :-()

I've not been able to locate/determine if the slave is really dead or
otherwise and it's the weekend in Asia and there's no one in the office
till Next week.

As of now, the master is still trying to contact the slave (slon is
still running on the master) and log_1 and log_2 is filling up.

And yesterday, I just created a job to manually force the log_switch to
occur. So, right now, I'm at a loss as to what i can do.


From ajs at crankycanuck.ca  Fri Feb 22 10:32:42 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri Feb 22 10:33:01 2008
Subject: [Slony1-general] Slave died, no access till next week,
	master logs filling up. What should I do
In-Reply-To: <1203704600.21659.23.camel@neuromancer.home.net>
References: <1203704600.21659.23.camel@neuromancer.home.net>
Message-ID: <20080222183242.GF24680@crankycanuck.ca>

On Sat, Feb 23, 2008 at 02:23:20AM +0800, Ow Mun Heng wrote:
> 
> And yesterday, I just created a job to manually force the log_switch to
> occur. So, right now, I'm at a loss as to what i can do.

Add more disk?  What is the harm in the log filling up?

A
From craig_james at emolecules.com  Fri Feb 22 10:37:00 2008
From: craig_james at emolecules.com (Craig James)
Date: Fri Feb 22 10:37:01 2008
Subject: [Slony1-general] Slave died, no access till next week,	master
	logs filling up. What should I do
In-Reply-To: <1203704600.21659.23.camel@neuromancer.home.net>
References: <1203704600.21659.23.camel@neuromancer.home.net>
Message-ID: <47BF164C.40003@emolecules.com>

Ow Mun Heng wrote:
> I came to work today and seems like the slave server died. (power trip?
> No it was not connected to a UPS :-()
> 
> I've not been able to locate/determine if the slave is really dead or
> otherwise and it's the weekend in Asia and there's no one in the office
> till Next week.
> 
> As of now, the master is still trying to contact the slave (slon is
> still running on the master) and log_1 and log_2 is filling up.
> 
> And yesterday, I just created a job to manually force the log_switch to
> occur. So, right now, I'm at a loss as to what i can do.

Just kill all of the Slony daemons.  Next week when the other server is back, start them again.  It will figure out what it missed, and will catch up with no problems.

Craig
From Ow.Mun.Heng at wdc.com  Fri Feb 22 10:37:23 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Fri Feb 22 10:37:38 2008
Subject: [Slony1-general] Slave died, no access till next week, master
	logs filling up. What should I do
In-Reply-To: <20080222183242.GF24680@crankycanuck.ca>
References: <1203704600.21659.23.camel@neuromancer.home.net>
	<20080222183242.GF24680@crankycanuck.ca>
Message-ID: <1203705443.21659.26.camel@neuromancer.home.net>


On Fri, 2008-02-22 at 13:32 -0500, Andrew Sullivan wrote:
> On Sat, Feb 23, 2008 at 02:23:20AM +0800, Ow Mun Heng wrote:
> > 
> > And yesterday, I just created a job to manually force the log_switch to
> > occur. So, right now, I'm at a loss as to what i can do.
> Add more disk?  What is the harm in the log filling up?

I wish I could. The so-called server is just a celeron box w/ 4 IDE
disks (on master) 1x80G OS, 2x500G Raid1, 1x500G slony logs

When the logs start to fill up >2G, I will see the dreaded fetch 100
from log and it takes upwards to 500++ seconds for it to get the 1st row
and thus replication will fall behind _way_way_ much.




From Ow.Mun.Heng at wdc.com  Fri Feb 22 10:41:58 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Fri Feb 22 10:42:11 2008
Subject: [Slony1-general] Slave died, no access till next week,	master
	logs filling up. What should I do
In-Reply-To: <47BF164C.40003@emolecules.com>
References: <1203704600.21659.23.camel@neuromancer.home.net>
	<47BF164C.40003@emolecules.com>
Message-ID: <1203705718.21659.31.camel@neuromancer.home.net>


On Fri, 2008-02-22 at 10:37 -0800, Craig James wrote:
> Ow Mun Heng wrote:
> > I came to work today and seems like the slave server died. (power trip?
> > No it was not connected to a UPS :-()
> > 
> > I've not been able to locate/determine if the slave is really dead or
> > otherwise and it's the weekend in Asia and there's no one in the office
> > till Next week.
> > 
> > As of now, the master is still trying to contact the slave (slon is
> > still running on the master) and log_1 and log_2 is filling up.
> > 
> > And yesterday, I just created a job to manually force the log_switch to
> > occur. So, right now, I'm at a loss as to what i can do.
> 
> Just kill all of the Slony daemons.  Next week when the other server is back, start them again.  It will figure out what it missed, and will catch up with no problems.

Huh? YOu sure about that? I think I read somewhere on the slony docs
that I should at least keep the master slon process online so that I
don't generate 1 big sync or something along those lines.

Doesn't the slony daemon work on triggers or it only implements the
triggers and creates the syncs. The logs will fill up no matter what
correct?

So, if what you're saying is correct, by killing the Master's slon
process, I am just stopping any syncs from being generated, so that I
don't fall behind in terms of the syncs (no syncs catchup later on) but
since the changes are still being logged, I'm still OK?

Is this understanding correct?

From ajs at crankycanuck.ca  Fri Feb 22 11:02:53 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri Feb 22 11:03:08 2008
Subject: [Slony1-general] Slave died, no access till next week,
	master logs filling up. What should I do
In-Reply-To: <1203705443.21659.26.camel@neuromancer.home.net>
References: <1203704600.21659.23.camel@neuromancer.home.net>
	<20080222183242.GF24680@crankycanuck.ca>
	<1203705443.21659.26.camel@neuromancer.home.net>
Message-ID: <20080222190253.GG24680@crankycanuck.ca>

On Sat, Feb 23, 2008 at 02:37:23AM +0800, Ow Mun Heng wrote:
> When the logs start to fill up >2G, I will see the dreaded fetch 100
> from log and it takes upwards to 500++ seconds for it to get the 1st row
> and thus replication will fall behind _way_way_ much.

If you don't have a replica into which to send your data, what's the
difference how long the operation takes?

I'm supposing that in the end you're going to want to drop the failed node
and start again, because the rate you seem to be talking about sounds like
it's going to overwhelm you.  I just have a nagging suspicion that your DROP
NODE message isn't going to have any effect until you get the log switch
completed.  I may be wrong, though.

A
From wmoran at collaborativefusion.com  Fri Feb 22 11:07:02 2008
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Fri Feb 22 11:07:10 2008
Subject: [Slony1-general] Slave died, no access till next week, master
	logs filling up. What should I do
In-Reply-To: <1203705718.21659.31.camel@neuromancer.home.net>
References: <1203704600.21659.23.camel@neuromancer.home.net>
	<47BF164C.40003@emolecules.com>
	<1203705718.21659.31.camel@neuromancer.home.net>
Message-ID: <20080222140702.2a350026.wmoran@collaborativefusion.com>

In response to Ow Mun Heng <Ow.Mun.Heng@wdc.com>:

> 
> On Fri, 2008-02-22 at 10:37 -0800, Craig James wrote:
> > Ow Mun Heng wrote:
> > > I came to work today and seems like the slave server died. (power trip?
> > > No it was not connected to a UPS :-()
> > > 
> > > I've not been able to locate/determine if the slave is really dead or
> > > otherwise and it's the weekend in Asia and there's no one in the office
> > > till Next week.
> > > 
> > > As of now, the master is still trying to contact the slave (slon is
> > > still running on the master) and log_1 and log_2 is filling up.
> > > 
> > > And yesterday, I just created a job to manually force the log_switch to
> > > occur. So, right now, I'm at a loss as to what i can do.
> > 
> > Just kill all of the Slony daemons.  Next week when the other server is back, start them again.  It will figure out what it missed, and will catch up with no problems.
> 
> Huh? YOu sure about that? I think I read somewhere on the slony docs
> that I should at least keep the master slon process online so that I
> don't generate 1 big sync or something along those lines.
> 
> Doesn't the slony daemon work on triggers or it only implements the
> triggers and creates the syncs. The logs will fill up no matter what
> correct?
> 
> So, if what you're saying is correct, by killing the Master's slon
> process, I am just stopping any syncs from being generated, so that I
> don't fall behind in terms of the syncs (no syncs catchup later on) but
> since the changes are still being logged, I'm still OK?
> 
> Is this understanding correct?

Basically yes.  The only way this could bite you is if you generate so
much data change over the weekend that your slon logs fill up.  If
that's the case, the only thing I can think to recommend is to deinstall
slony from that database and rebuild the cluster once everything's
running again.

-- 
Bill Moran
Collaborative Fusion Inc.
http://people.collaborativefusion.com/~wmoran/

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023
From cbbrowne at ca.afilias.info  Fri Feb 22 13:31:49 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Feb 22 13:31:58 2008
Subject: [Slony1-general] Slave died, no access till next week,
	master logs filling up. What should I do
In-Reply-To: <47BF164C.40003@emolecules.com> (Craig James's message of "Fri,
	22 Feb 2008 10:37:00 -0800")
References: <1203704600.21659.23.camel@neuromancer.home.net>
	<47BF164C.40003@emolecules.com>
Message-ID: <60oda8ekze.fsf@dba2.int.libertyrms.com>

Craig James <craig_james@emolecules.com> writes:
> Ow Mun Heng wrote:
>> I came to work today and seems like the slave server died. (power trip?
>> No it was not connected to a UPS :-()
>> I've not been able to locate/determine if the slave is really dead or
>> otherwise and it's the weekend in Asia and there's no one in the office
>> till Next week.
>> As of now, the master is still trying to contact the slave (slon is
>> still running on the master) and log_1 and log_2 is filling up.
>> And yesterday, I just created a job to manually force the log_switch
>> to
>> occur. So, right now, I'm at a loss as to what i can do.
>
> Just kill all of the Slony daemons.  Next week when the other server
> is back, start them again.  It will figure out what it missed, and
> will catch up with no problems.

That's not quite accurate...

If you kill ALL the daemons, and don't have *something* maintaining
the creation of SYNCs (e.g. - a script running the "generate_sync()"
stored function), then there will be one really gigantic SYNC covering
the interval of [time slon for origin died] until [time slon for
origin restarted].

It might well be a good answer to set up an every-minute or
every-few-minutes script that runs against the origin and runs the
"generate_sync()" stored function.  There's not much value in lower
intervals than that.  And that's about all the maintenance that is of
value between now and next week.

So I would suggest one of two choices:

a) Set up generate_sync() cron job, and kill all slons.

b) Increase the various sync parms for the slon for the origin node;
-s 60000 and -t 120000 will mean you SYNC once per minute, when things
are busy.

That reduces the work level a bit., either way.
-- 
let name="cbbrowne" and tld="linuxfinances.info" in name ^ "@" ^ tld;;
http://www3.sympatico.ca/cbbrowne/sgml.html
If you eat a live  frog in the morning, nothing  worse will happen  to
either of you for the rest of the day.
From Ow.Mun.Heng at wdc.com  Fri Feb 22 14:06:48 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Fri Feb 22 14:07:08 2008
Subject: [Slony1-general] Slave died, no access till next week,	master
	logs filling up. What should I do
In-Reply-To: <60oda8ekze.fsf@dba2.int.libertyrms.com>
References: <1203704600.21659.23.camel@neuromancer.home.net>
	<47BF164C.40003@emolecules.com>
	<60oda8ekze.fsf@dba2.int.libertyrms.com>
Message-ID: <1203718008.21659.45.camel@neuromancer.home.net>


On Fri, 2008-02-22 at 21:31 +0000, Christopher Browne wrote:
> Craig James <craig_james@emolecules.com> writes:
> > Ow Mun Heng wrote:
> >> I came to work today and seems like the slave server died. (power trip?
> >> No it was not connected to a UPS :-()
> >> I've not been able to locate/determine if the slave is really dead or
> >> otherwise and it's the weekend in Asia and there's no one in the office
> >> till Next week.
> >> As of now, the master is still trying to contact the slave (slon is
> >> still running on the master) and log_1 and log_2 is filling up.
> >> And yesterday, I just created a job to manually force the log_switch
> >> to
> >> occur. So, right now, I'm at a loss as to what i can do.
> >
> > Just kill all of the Slony daemons.  Next week when the other server
> > is back, start them again.  It will figure out what it missed, and
> > will catch up with no problems.
> 
> That's not quite accurate...
> 
> If you kill ALL the daemons, and don't have *something* maintaining
> the creation of SYNCs (e.g. - a script running the "generate_sync()"
> stored function), then there will be one really gigantic SYNC covering
> the interval of [time slon for origin died] until [time slon for
> origin restarted].

I remember reading that in the docs, but I took the advise anyway and
killed the slon daemon a few hours ago.


> a) Set up generate_sync() cron job, and kill all slons.

Where is this generate_sync() anyway? I only saw a
generate_sync_event(interval) stored function in the cluster DB.

> b) Increase the various sync parms for the slon for the origin node;
> -s 60000 and -t 120000 will mean you SYNC once per minute, when things
> are busy.
> 
> That reduces the work level a bit., either way.

Thanks. I've already restarted the process on the origin with 
slon -c2 -d2 -s60000 -t 120000 (actually I was already using -s60000)

I guess my concern now is that the slon logs are filling up and going
past the 2GB threshold (for both log_1 and 2 which would mean that
there's not going to be much help in gettting things back to speed when
next week comes)

Thanks guys. Appreciate the comments/advise.


From craig_james at emolecules.com  Fri Feb 22 19:18:11 2008
From: craig_james at emolecules.com (Craig James)
Date: Fri Feb 22 19:18:19 2008
Subject: [Slony1-general] STILL can't migrate a node.
In-Reply-To: <20080221171359.GA18657@crankycanuck.ca>
References: <47B463D5.90605@emolecules.com>
	<47BB74B8.2030808@emolecules.com>	<20080221144755.GC17293@crankycanuck.ca>	<47BD907C.2050603@dalibo.com>
	<20080221171359.GA18657@crankycanuck.ca>
Message-ID: <47BF9073.7060708@emolecules.com>

I'm trying to migrate a node for the second time, and no luck.  Last time I tried it, it just got stuck, and due to lack of time, I didn't investigate.

This time I watched -- it got stuck again, doing some sort of huge SELECT statement.  I was under the impression that migrating a node was a fairly simple operation that should happen in a short time (less than a minute?) even for large databases.

I waited 10 minutes, during which the entire system was completely locked up (no other process could access the database), and our web site was offline.  I finally had to kill all of the slon daemons and kill Postgres to get our site back on the air, then run the node-unlock command to get Slony back in shape.

This system appears to otherwise be working well.  I can insert, update and delete records, and they're copied to the slave node immediately.

What's up?  Am I just too impatient?

Thanks,
Craig
From cbbrowne at ca.afilias.info  Fri Feb 22 19:40:36 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Feb 22 19:40:50 2008
Subject: [Slony1-general] Slave died, no access till next week,
	master logs filling up. What should I do
In-Reply-To: <1203718008.21659.45.camel@neuromancer.home.net> (Ow Mun Heng's
	message of "Sat, 23 Feb 2008 06:06:48 +0800")
References: <1203704600.21659.23.camel@neuromancer.home.net>
	<47BF164C.40003@emolecules.com>
	<60oda8ekze.fsf@dba2.int.libertyrms.com>
	<1203718008.21659.45.camel@neuromancer.home.net>
Message-ID: <60k5kwe3wr.fsf@dba2.int.libertyrms.com>

Ow Mun Heng <Ow.Mun.Heng@wdc.com> writes:
> On Fri, 2008-02-22 at 21:31 +0000, Christopher Browne wrote:
>> Craig James <craig_james@emolecules.com> writes:
>> > Ow Mun Heng wrote:
>> >> I came to work today and seems like the slave server died. (power trip?
>> >> No it was not connected to a UPS :-()
>> >> I've not been able to locate/determine if the slave is really dead or
>> >> otherwise and it's the weekend in Asia and there's no one in the office
>> >> till Next week.
>> >> As of now, the master is still trying to contact the slave (slon is
>> >> still running on the master) and log_1 and log_2 is filling up.
>> >> And yesterday, I just created a job to manually force the log_switch
>> >> to
>> >> occur. So, right now, I'm at a loss as to what i can do.
>> >
>> > Just kill all of the Slony daemons.  Next week when the other server
>> > is back, start them again.  It will figure out what it missed, and
>> > will catch up with no problems.
>> 
>> That's not quite accurate...
>> 
>> If you kill ALL the daemons, and don't have *something* maintaining
>> the creation of SYNCs (e.g. - a script running the "generate_sync()"
>> stored function), then there will be one really gigantic SYNC covering
>> the interval of [time slon for origin died] until [time slon for
>> origin restarted].
>
> I remember reading that in the docs, but I took the advise anyway and
> killed the slon daemon a few hours ago.

You'll have a Pretty Big SYNC at some point, then.  No reason for it
not to work, but it'll be big...

>> a) Set up generate_sync() cron job, and kill all slons.
>
> Where is this generate_sync() anyway? I only saw a
> generate_sync_event(interval) stored function in the cluster DB.

Ah.  Working from memory on that.  generate_sync_event() would be the
one.

>> b) Increase the various sync parms for the slon for the origin node;
>> -s 60000 and -t 120000 will mean you SYNC once per minute, when things
>> are busy.
>> 
>> That reduces the work level a bit., either way.
>
> Thanks. I've already restarted the process on the origin with 
> slon -c2 -d2 -s60000 -t 120000 (actually I was already using -s60000)
>
> I guess my concern now is that the slon logs are filling up and going
> past the 2GB threshold (for both log_1 and 2 which would mean that
> there's not going to be much help in gettting things back to speed when
> next week comes)

Yeah, it'll be a lot of catching up.

You'll get a feel for whether or not you have enough hardware to be
able to catch up at all, which is a useful thing to know.
-- 
output = ("cbbrowne" "@" "acm.org")
http://cbbrowne.com/info/finances.html
"Administering  a Linux  server  is no  more  difficult than  properly
running Windows NT."  -- Infoworld, November 24, 1997
From craig_james at emolecules.com  Fri Feb 22 21:20:14 2008
From: craig_james at emolecules.com (Craig James)
Date: Fri Feb 22 21:20:23 2008
Subject: [Slony1-general] STILL can't migrate a node.
In-Reply-To: <47BF9073.7060708@emolecules.com>
References: <47B463D5.90605@emolecules.com>	<47BB74B8.2030808@emolecules.com>	<20080221144755.GC17293@crankycanuck.ca>	<47BD907C.2050603@dalibo.com>	<20080221171359.GA18657@crankycanuck.ca>
	<47BF9073.7060708@emolecules.com>
Message-ID: <47BFAD0E.2000902@emolecules.com>

A little more info on this problem...

Craig James wrote:
> I'm trying to migrate a node for the second time, and no luck.  Last 
> time I tried it, it just got stuck, and due to lack of time, I didn't 
> investigate.
> 
> This time I watched -- it got stuck again, doing some sort of huge 
> SELECT statement.  I was under the impression that migrating a node was 
> a fairly simple operation that should happen in a short time (less than 
> a minute?) even for large databases.
> 
> I waited 10 minutes, during which the entire system was completely 
> locked up (no other process could access the database), and our web site 
> was offline.  I finally had to kill all of the slon daemons and kill 
> Postgres to get our site back on the air, then run the node-unlock 
> command to get Slony back in shape.
> 
> This system appears to otherwise be working well.  I can insert, update 
> and delete records, and they're copied to the slave node immediately.
> 
> What's up?  Am I just too impatient?

I tried it again, after vacuuming the slony tables that are subject to bloat.  This time I shut everything off, started the migration of the master to node 2, and waited for 35 minutes, but the SELECT never finished.  vmstat showed massive I/O and CPU activity the whole time.

Again, after I killed postgres, restarted, and unlocked the node, Slony went back to performing perfectly.

Any help would be appreciated ... I have to do this before about noon Saturday in order to complete the rest of the weekend's tasks by Sunday evening.

Here is the script I'm using:

slonik <<_EOF_

cluster name = db_cluster;
node 1 admin conninfo = 'dbname=db host=server1 user=postgres';
node 2 admin conninfo = 'dbname=db host=server2 user=postgres';

lock set (id = 1, origin = 1);
wait for event (origin = 1, confirmed = 2);
move set (id = 1, old origin = 1, new origin = 2);

_EOF_



Thanks,
Craig
From devrim at CommandPrompt.com  Fri Feb 22 21:42:37 2008
From: devrim at CommandPrompt.com (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Fri Feb 22 21:43:17 2008
Subject: [Slony1-general] Slony-I build cant find postgresql include folder
In-Reply-To: <15586012.post@talk.nabble.com>
References: <15586012.post@talk.nabble.com>
Message-ID: <1203745357.30267.71.camel@localhost.localdomain>

SGksCgpPbiBXZWQsIDIwMDgtMDItMjAgYXQgMDI6MTcgLTA4MDAsIGFqY2l0eSB3cm90ZToKPiBJ
IHdhbm5hIHNldHVwIFNsb255LUkgb24gYSByZW1vdGUgUmVkSGF0IExpbnV4IG1hY2hpbmUgd2hp
Y2ggdXNlcyB0aGUKPiBwb3N0Z3Jlc3FsIHRoYXQgY29tZXMgZGlzdHJpYnV0ZWQgd2l0aCB0aGUg
UmVkaGF0IGFuZCB0aGUgU2xvbnkgZG9jCj4gZG9lc24ndCBjb3ZlciB0aGF0IGtpbmQgb2YgdGhp
bmcuIAoKWW91IGNhbiBhbHNvIHVzZSB0aGUgUlBNIHJlcG9zaXRvcnkgYXQgaHR0cDovL3l1bS5w
Z3NxbHJwbXMub3JnIHRvCmluc3RhbGwgU2xvbnkgb24geW91ciBSZWQgSGF0IHNlcnZlci4gRGV0
YWlscyBhcmUgYXQgdGhlIHdlYnNpdGUuCgpSZWdhcmRzLAotLSAKRGV2cmltIEfDnE5Ew5xaICwg
UkhDRQpQb3N0Z3JlU1FMIFJlcGxpY2F0aW9uLCBDb25zdWx0aW5nLCBDdXN0b20gRGV2ZWxvcG1l
bnQsIDI0eDcgc3VwcG9ydApNYW5hZ2VkIFNlcnZpY2VzLCBTaGFyZWQgYW5kIERlZGljYXRlZCBI
b3N0aW5nCkNvLUF1dGhvcnM6IHBsUEhQLCBPREJDbmcgLSBodHRwOi8vd3d3LmNvbW1hbmRwcm9t
cHQuY29tLwotLS0tLS0tLS0tLS0tLSBuZXh0IHBhcnQgLS0tLS0tLS0tLS0tLS0KQSBub24tdGV4
dCBhdHRhY2htZW50IHdhcyBzY3J1YmJlZC4uLgpOYW1lOiBub3QgYXZhaWxhYmxlClR5cGU6IGFw
cGxpY2F0aW9uL3BncC1zaWduYXR1cmUKU2l6ZTogMTg5IGJ5dGVzCkRlc2M6IFRoaXMgaXMgYSBk
aWdpdGFsbHkgc2lnbmVkIG1lc3NhZ2UgcGFydApVcmwgOiBodHRwOi8vbGlzdHMuc2xvbnkuaW5m
by9waXBlcm1haWwvc2xvbnkxLWdlbmVyYWwvYXR0YWNobWVudHMvMjAwODAyMjIvZWU5ZjZmZTEv
YXR0YWNobWVudC5wZ3AK
From victor.aluko at gmail.com  Sat Feb 23 04:43:42 2008
From: victor.aluko at gmail.com (ajcity)
Date: Sat Feb 23 04:44:04 2008
Subject: [Slony1-general] Slony-I build cant find postgresql include folder
In-Reply-To: <15586012.post@talk.nabble.com>
References: <15586012.post@talk.nabble.com>
Message-ID: <15651661.post@talk.nabble.com>


I've downloaded the rpm and tried to install it but it keeps giving an error
that it requires some lib files and the rpm recommends postgresql 7.4 rpm
package while there is a postgresql 8.1.5 running on the machine.
 What makes it worse is that I MUST NOT tamper with the server bcos it must
always be running without any glitches else the company will lose a lot of
money.
  What postgresql rpm carries all the libs without requiring the machine to
restart?

-- 
View this message in context: http://www.nabble.com/Slony-I-build-cant-find-postgresql-include-folder-tp15586012p15651661.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From lavalamp at spiritual-machines.org  Sat Feb 23 07:23:44 2008
From: lavalamp at spiritual-machines.org (Brian A. Seklecki)
Date: Sat Feb 23 07:25:10 2008
Subject: [Slony1-general] syslog output?
In-Reply-To: <E0C08BB6-70F3-4B5C-94C3-A31F6E2215DF@richyen.com>
References: <E0C08BB6-70F3-4B5C-94C3-A31F6E2215DF@richyen.com>
Message-ID: <1203780225.3352.13.camel@new-host>


On Mon, 2008-01-28 at 13:33 -0800, Richard Yen wrote:
> Hi All,
> 
> Wondering if maybe I'm doing something wrong here.  I've set my syslog  
> param in slon.conf to 2, which should log to syslog.  The docs say  

Yea -- it's def a bug.

It might be time to modify the RC script to do:

# slon [args]  2>&1 3>&1  > /dev/null

~BAS


> "some messages will still go to the standard output," so I'm wondering  
> if these "some messages" are redundant since everything already goes  
> to syslog.  Can I simply pipe standard output to /dev/null?
> 
> Thanks!
> --Richard
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 
> 
> 
> 
> 

From lavalamp at spiritual-machines.org  Sat Feb 23 07:25:33 2008
From: lavalamp at spiritual-machines.org (Brian A. Seklecki)
Date: Sat Feb 23 07:25:58 2008
Subject: [Slony1-general] Building Slony 1.2.13 on MacOSX 10.4 (PPC)
In-Reply-To: <EBE586B4-972E-418F-8856-33C646A781DF@logicunited.com>
References: <EBE586B4-972E-418F-8856-33C646A781DF@logicunited.com>
Message-ID: <1203780333.3352.15.camel@new-host>

On Mon, 2008-02-11 at 10:20 +0100, Rudolf van der Leeden wrote:
> Running ranlib as suggested:    ranlib /fink/lib/libpgport.a      

Try Pkgsrc instead...CC me on any bug reports.

~BAS

> solves the problem.

From craig_james at emolecules.com  Sat Feb 23 10:28:57 2008
From: craig_james at emolecules.com (Craig James)
Date: Sat Feb 23 10:29:11 2008
Subject: [Slony1-general] STILL can't migrate a node.
In-Reply-To: <47C04A24.8010209@Yahoo.com>
References: <47B463D5.90605@emolecules.com>	<47BB74B8.2030808@emolecules.com>	<20080221144755.GC17293@crankycanuck.ca>	<47BD907C.2050603@dalibo.com>	<20080221171359.GA18657@crankycanuck.ca>	<47BF9073.7060708@emolecules.com>
	<47BFAD0E.2000902@emolecules.com> <47C04A24.8010209@Yahoo.com>
Message-ID: <47C065E9.7070408@emolecules.com>

Jan Wieck wrote:
> On 2/23/2008 12:20 AM, Craig James wrote:
>> A little more info on this problem...
>>
>> Craig James wrote:
>>> I'm trying to migrate a node for the second time, and no luck.  Last 
>>> time I tried it, it just got stuck, and due to lack of time, I didn't 
>>> investigate.
>>>
>>> This time I watched -- it got stuck again, doing some sort of huge 
>>> SELECT statement.  I was under the impression that migrating a node 
>>> was a fairly simple operation that should happen in a short time 
>>> (less than a minute?) even for large databases.
>>>
>>> I waited 10 minutes, during which the entire system was completely 
>>> locked up (no other process could access the database), and our web 
>>> site was offline.  I finally had to kill all of the slon daemons and 
>>> kill Postgres to get our site back on the air, then run the 
>>> node-unlock command to get Slony back in shape.
>>>
>>> This system appears to otherwise be working well.  I can insert, 
>>> update and delete records, and they're copied to the slave node 
>>> immediately.
>>>
>>> What's up?  Am I just too impatient?
>>
>> I tried it again, after vacuuming the slony tables that are subject to 
>> bloat.  This time I shut everything off, started the migration of the 
>> master to node 2, and waited for 35 minutes, but the SELECT never 
>> finished.  vmstat showed massive I/O and CPU activity the whole time.
> 
> What SELECT are you referring to? I don't see where in the MOVE SET you 
> have to perform any SELECT.

You tell me?  It is the slon daemon that is executing this select.  There were no other connections to the database the second time I tried this.


>> Again, after I killed postgres, restarted, and unlocked the node, 
>> Slony went back to performing perfectly.
> 
> Killing postgres is a bad idea. Stop that habit right now, before you 
> physically corrupt any of your databases.

Thanks for the advice, but I don't think it's a problem.  That's one of the features of a robust relational database with a write-ahead log -- it can withstand being killed without corrupting data.  Besides, I had no choice, my web site went offline because slon apparently took an exclusive lock on the tables, blocking all other activity.  And I killed a SELECT, not an INSERT or UPDATE.

But that's a topic for a separate discussion ... I have to fix this Slony problem first.

> Anyhow, apparently the LOCK SET part of the process succeeds. So what I 
> now assume is that the WAIT FOR EVENT never finishes. First, you don't 
> need a WAIT FOR EVENT between LOCK SET and MOVE SET. Both events are 
> executed on the origin, so by the time the LOCK SET finishes, everything 
> is ready for the MOVE.

I don't think it got as far as this, but I don't know the internals.  When I execute the script, the SELECT starts, and that's where everything comes to a sudden halt.

> But what this indicates is that node 2 never confirms the LOCK SET. Can 
> it be that you actually have a problem with the connection from node 2 
> to node 1? What is the content of the view sl_status on both nodes?

Both nodes seem to be normal -- the st_last_event_ts is just a few seconds prior to the query, st_lag_time is 00:00:11.465251 (node 1) and 00:00:07.50164.

> If you want to speed up this communication in order to meet your Sat. 
> noon deadline, I'll be available on IRC, channel #slony on freenode.

Thanks, but I don't use an IRC client, hope you get this.

Craig
From JanWieck at Yahoo.com  Sat Feb 23 10:46:58 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Sat Feb 23 10:47:13 2008
Subject: [Slony1-general] STILL can't migrate a node.
In-Reply-To: <47C065E9.7070408@emolecules.com>
References: <47B463D5.90605@emolecules.com>	<47BB74B8.2030808@emolecules.com>	<20080221144755.GC17293@crankycanuck.ca>	<47BD907C.2050603@dalibo.com>	<20080221171359.GA18657@crankycanuck.ca>	<47BF9073.7060708@emolecules.com>
	<47BFAD0E.2000902@emolecules.com> <47C04A24.8010209@Yahoo.com>
	<47C065E9.7070408@emolecules.com>
Message-ID: <47C06A22.5050301@Yahoo.com>

On 2/23/2008 1:28 PM, Craig James wrote:
> Jan Wieck wrote:
>> On 2/23/2008 12:20 AM, Craig James wrote:
>>> A little more info on this problem...
>>>
>>> Craig James wrote:
>>>> I'm trying to migrate a node for the second time, and no luck.  Last 
>>>> time I tried it, it just got stuck, and due to lack of time, I didn't 
>>>> investigate.
>>>>
>>>> This time I watched -- it got stuck again, doing some sort of huge 
>>>> SELECT statement.  I was under the impression that migrating a node 
>>>> was a fairly simple operation that should happen in a short time 
>>>> (less than a minute?) even for large databases.
>>>>
>>>> I waited 10 minutes, during which the entire system was completely 
>>>> locked up (no other process could access the database), and our web 
>>>> site was offline.  I finally had to kill all of the slon daemons and 
>>>> kill Postgres to get our site back on the air, then run the 
>>>> node-unlock command to get Slony back in shape.
>>>>
>>>> This system appears to otherwise be working well.  I can insert, 
>>>> update and delete records, and they're copied to the slave node 
>>>> immediately.
>>>>
>>>> What's up?  Am I just too impatient?
>>>
>>> I tried it again, after vacuuming the slony tables that are subject to 
>>> bloat.  This time I shut everything off, started the migration of the 
>>> master to node 2, and waited for 35 minutes, but the SELECT never 
>>> finished.  vmstat showed massive I/O and CPU activity the whole time.
>> 
>> What SELECT are you referring to? I don't see where in the MOVE SET you 
>> have to perform any SELECT.
> 
> You tell me?  It is the slon daemon that is executing this select.  There were no other connections to the database the second time I tried this.
> 
> 
>>> Again, after I killed postgres, restarted, and unlocked the node, 
>>> Slony went back to performing perfectly.
>> 
>> Killing postgres is a bad idea. Stop that habit right now, before you 
>> physically corrupt any of your databases.
> 
> Thanks for the advice, but I don't think it's a problem.  That's one of the features of a robust relational database with a write-ahead log -- it can withstand being killed without corrupting data.  Besides, I had no choice, my web site went offline because slon apparently took an exclusive lock on the tables, blocking all other activity.  And I killed a SELECT, not an INSERT or UPDATE.
> 
> But that's a topic for a separate discussion ... I have to fix this Slony problem first.

I don't think there is anything to discuss about killing the postmaster.

If you can't do it with pg_ctl, then don't do it.


> 
>> Anyhow, apparently the LOCK SET part of the process succeeds. So what I 
>> now assume is that the WAIT FOR EVENT never finishes. First, you don't 
>> need a WAIT FOR EVENT between LOCK SET and MOVE SET. Both events are 
>> executed on the origin, so by the time the LOCK SET finishes, everything 
>> is ready for the MOVE.
> 
> I don't think it got as far as this, but I don't know the internals.  When I execute the script, the SELECT starts, and that's where everything comes to a sudden halt.

I do know a little bit about the internals.

The lockSet() stored procedure, called by slonik to perform the LOCK SET 
command, tries to add a trigger to all the tables in the set. That 
requires an access exclusive lock on those tables. If nothing else is 
connected to the database, then there is no reason why that would block.

So either your slonik script gets past that point, or your assumption 
nothing is holding any kind of lock on any table in the set is wrong.

> 
>> But what this indicates is that node 2 never confirms the LOCK SET. Can 
>> it be that you actually have a problem with the connection from node 2 
>> to node 1? What is the content of the view sl_status on both nodes?
> 
> Both nodes seem to be normal -- the st_last_event_ts is just a few seconds prior to the query, st_lag_time is 00:00:11.465251 (node 1) and 00:00:07.50164.

That tells us at least that your sl_path settings are correct.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From ajs at crankycanuck.ca  Sat Feb 23 12:12:14 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Sat Feb 23 12:12:31 2008
Subject: [Slony1-general] STILL can't migrate a node.
In-Reply-To: <47C065E9.7070408@emolecules.com>
References: <47B463D5.90605@emolecules.com> <47BB74B8.2030808@emolecules.com>
	<20080221144755.GC17293@crankycanuck.ca>
	<47BD907C.2050603@dalibo.com>
	<20080221171359.GA18657@crankycanuck.ca>
	<47BF9073.7060708@emolecules.com>
	<47BFAD0E.2000902@emolecules.com> <47C04A24.8010209@Yahoo.com>
	<47C065E9.7070408@emolecules.com>
Message-ID: <20080223201214.GA30904@crankycanuck.ca>

On Sat, Feb 23, 2008 at 10:28:57AM -0800, Craig James wrote:
> 
> You tell me?  It is the slon daemon that is executing this select.  There 
> were no other connections to the database the second time I tried this.

[. . .]

> withstand being killed without corrupting data.  Besides, I had no choice, 
> my web site went offline because slon apparently took an exclusive lock on 
> the tables, blocking all other activity.  And I killed a SELECT, not an 
> INSERT or UPDATE.

So your website is offline the second time you tried this?

Slony will indeed take an exclusive lock blocking all other activity, and if
something is in line ahead of it, you're going to have a problem.  I think I
already suggested that your problem would be much easier (read "possible")
to diagnose with some data from pg_locks.

A

From JanWieck at Yahoo.com  Sat Feb 23 08:30:28 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Sat Feb 23 15:29:21 2008
Subject: [Slony1-general] STILL can't migrate a node.
In-Reply-To: <47BFAD0E.2000902@emolecules.com>
References: <47B463D5.90605@emolecules.com>	<47BB74B8.2030808@emolecules.com>	<20080221144755.GC17293@crankycanuck.ca>	<47BD907C.2050603@dalibo.com>	<20080221171359.GA18657@crankycanuck.ca>	<47BF9073.7060708@emolecules.com>
	<47BFAD0E.2000902@emolecules.com>
Message-ID: <47C04A24.8010209@Yahoo.com>

On 2/23/2008 12:20 AM, Craig James wrote:
> A little more info on this problem...
> 
> Craig James wrote:
>> I'm trying to migrate a node for the second time, and no luck.  Last 
>> time I tried it, it just got stuck, and due to lack of time, I didn't 
>> investigate.
>> 
>> This time I watched -- it got stuck again, doing some sort of huge 
>> SELECT statement.  I was under the impression that migrating a node was 
>> a fairly simple operation that should happen in a short time (less than 
>> a minute?) even for large databases.
>> 
>> I waited 10 minutes, during which the entire system was completely 
>> locked up (no other process could access the database), and our web site 
>> was offline.  I finally had to kill all of the slon daemons and kill 
>> Postgres to get our site back on the air, then run the node-unlock 
>> command to get Slony back in shape.
>> 
>> This system appears to otherwise be working well.  I can insert, update 
>> and delete records, and they're copied to the slave node immediately.
>> 
>> What's up?  Am I just too impatient?
> 
> I tried it again, after vacuuming the slony tables that are subject to bloat.  This time I shut everything off, started the migration of the master to node 2, and waited for 35 minutes, but the SELECT never finished.  vmstat showed massive I/O and CPU activity the whole time.

What SELECT are you referring to? I don't see where in the MOVE SET you 
have to perform any SELECT.

> Again, after I killed postgres, restarted, and unlocked the node, Slony went back to performing perfectly.

Killing postgres is a bad idea. Stop that habit right now, before you 
physically corrupt any of your databases.

Anyhow, apparently the LOCK SET part of the process succeeds. So what I 
now assume is that the WAIT FOR EVENT never finishes. First, you don't 
need a WAIT FOR EVENT between LOCK SET and MOVE SET. Both events are 
executed on the origin, so by the time the LOCK SET finishes, everything 
is ready for the MOVE.

But what this indicates is that node 2 never confirms the LOCK SET. Can 
it be that you actually have a problem with the connection from node 2 
to node 1? What is the content of the view sl_status on both nodes?

If you want to speed up this communication in order to meet your Sat. 
noon deadline, I'll be available on IRC, channel #slony on freenode.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From postgresql.org at msqr.us  Sun Feb 24 17:19:51 2008
From: postgresql.org at msqr.us (Matt Magoffin)
Date: Sun Feb 24 17:20:11 2008
Subject: [Slony1-general] possible to replicate from text column in 8.1 into
 an xml column in 8.3?
Message-ID: <49584.192.168.1.108.1203902391.squirrel@msqr.us>

Hello,

I'm trying to plan out a migration from Postgres 8.1 to Postgres 8.3. We
currently use Slony to replicate our database, and I understand we should
be able to replicate from 8.1 into 8.3 as long as we have the same version
of Slony installed in both databases.

In 8.1 I have a table that uses a text column to store XML data, and in
8.3 we plan to turn that into an xml type column. I can migrate that data
using pg_dump and pg_restore with the --data-only option.

I was wondering if I could just set up replication between the 8.1
database and 8.3 database, and have the data replicated directly from the
text column in 8.1 into the xml column in 8.3.

-- m@
From cjames at emolecules.com  Mon Feb 25 06:59:29 2008
From: cjames at emolecules.com (Craig A. James)
Date: Mon Feb 25 06:59:35 2008
Subject: [Slony1-general] STILL can't migrate a node.
In-Reply-To: <47C07385.5080602@Yahoo.com>
References: <47B463D5.90605@emolecules.com>	<47BB74B8.2030808@emolecules.com>	<20080221144755.GC17293@crankycanuck.ca>	<47BD907C.2050603@dalibo.com>	<20080221171359.GA18657@crankycanuck.ca>	<47BF9073.7060708@emolecules.com>
	<47BFAD0E.2000902@emolecules.com> <47C04A24.8010209@Yahoo.com>
	<47C065E9.7070408@emolecules.com> <47C06A22.5050301@Yahoo.com>
	<47C06BC1.8060805@emolecules.com> <47C07385.5080602@Yahoo.com>
Message-ID: <47C2D7D1.7060206@emolecules.com>

I sent the attached email ony to Jan by mistake, here it is, I hope Jan or somebody has an idea what's going on.

A quick summary:
  1. Shut of Apache
  2. Restart Postgres and Slony
  3. lock set (id = 1, origin = 1);
     wait for event (origin = 1, confirmed = 2);
     move set (id = 1, old origin = 1, new origin = 2);

When I do this, Slony starts a SELECT that never finishes ("never" means I waited 10 minutes, 35 minutes, and 10 minutes in three different tests, with no sign of progress).

Per Jan's request, I've attached the output of two SQL queries:

    select relname, granted, pid, mode from pg_locks L, pg_class C
        where C.oid = L.relation and locktype = 'relation';

    select * from pg_stat_activity;

In addition, I've attached the output of vmstat, a few snapshots taken over a 30-minute peroid.  It's looks to me like postgres is doing some sort of huge join or sort, based on the changing amounts of CPU and I/O during the 30 minutes I monitored it.  No other processes were active; this was entirely due to the backend Postgres process being used by Slony.

>From these, you can see that a Slon process is doing a "SELECT moveSet(...)".  ps(1) confirms that this process is the one using all the CPU time shown by vmstat.

I sure could use some help on this one, it's become a critical roadblock to our operation.

Thanks,
Craig

------------------------------------

Jan,

Thanks for all your help with this, I really appreciate it.  Your additional instructions regarding pg_stat_activity made more sense. I also had to enable the stats_command_string in the postgresql.conf file.

I'm also appending some output from vmstat to show that the system was busy doing something -- I let it run for about ten minutes, and the vmstat results are for various snapshots during that 10 minutes.

Craig



emol_warehouse_1=# select datid, datname, procpid, usename, current_query, query_start, backend_start from pg_stat_activity;
 datid   |     datname      | procpid | usename  |                   current_query                    |          query_start          |         backend_start         
----------+------------------+---------+----------+----------------------------------------------------+-------------------------------+-------------------------------
  164853 | emol_warehouse_1 |   24747 | postgres | <IDLE>                                             | 2008-02-23 18:44:09.995787-08 | 2008-02-23 18:41:55.334255-08
   18368 | global           |   24741 | postgres | <IDLE>                                             | 2008-02-23 18:47:30.099887-08 | 2008-02-23 18:41:38.458081-08
20209283 | accounting       |   24740 | postgres | <IDLE>                                             | 2008-02-23 18:47:29.540243-08 | 2008-02-23 18:41:38.421729-08
  164853 | emol_warehouse_1 |   24739 | postgres | <IDLE>                                             | 2008-02-23 18:47:29.195255-08 | 2008-02-23 18:41:38.386496-08
  164853 | emol_warehouse_1 |   24738 | postgres | select "_emol_warehouse_1_cluster".moveSet(1, 2);  | 2008-02-23 18:41:35.215066-08 | 2008-02-23 18:41:35.154836-08
   18368 | global           |   24731 | postgres | <IDLE>                                             | 2008-02-23 18:41:26.398179-08 | 2008-02-23 18:41:26.394739-08
   18368 | global           |   24729 | postgres | <IDLE>                                             | 2008-02-23 18:47:36.747641-08 | 2008-02-23 18:41:26.394256-08
   18368 | global           |   24730 | postgres | <IDLE>                                             | 2008-02-23 18:47:35.972451-08 | 2008-02-23 18:41:26.392295-08
20209283 | accounting       |   24724 | postgres | <IDLE>                                             | 2008-02-23 18:41:26.388876-08 | 2008-02-23 18:41:26.385995-08
20209283 | accounting       |   24723 | postgres | <IDLE>                                             | 2008-02-23 18:47:37.797022-08 | 2008-02-23 18:41:26.380974-08
20209283 | accounting       |   24722 | postgres | <IDLE>                                             | 2008-02-23 18:47:37.420454-08 | 2008-02-23 18:41:26.378364-08
  164853 | emol_warehouse_1 |   24717 | postgres | <IDLE>                                             | 2008-02-23 18:47:37.080398-08 | 2008-02-23 18:41:26.356934-08
  164853 | emol_warehouse_1 |   24716 | postgres | <IDLE>                                             | 2008-02-23 18:41:26.360746-08 | 2008-02-23 18:41:26.358003-08
  164853 | emol_warehouse_1 |   24715 | postgres | <IDLE>                                             | 2008-02-23 18:47:36.216511-08 | 2008-02-23 18:41:26.352855-08
20209283 | accounting       |   24710 | postgres | <IDLE>                                             | 2008-02-23 18:47:34.69257-08  | 2008-02-23 18:41:26.319118-08
  164853 | emol_warehouse_1 |   24706 | postgres | <IDLE>                                             | 2008-02-23 18:47:34.692909-08 | 2008-02-23 18:41:26.301898-08
   18368 | global           |   24709 | postgres | <IDLE>                                             | 2008-02-23 18:47:34.728589-08 | 2008-02-23 18:41:26.316133-08
20209283 | accounting       |   24695 | postgres | <IDLE>                                             | 2008-02-23 18:47:37.6937-08   | 2008-02-23 18:41:26.24967-08
  164853 | emol_warehouse_1 |   24694 | postgres | <IDLE>                                             | 2008-02-23 18:47:37.38218-08  | 2008-02-23 18:41:26.24526-08
   18368 | global           |   24693 | postgres | <IDLE>                                             | 2008-02-23 18:47:32.19262-08  | 2008-02-23 18:41:26.241983-08
(20 rows)


emol_warehouse_1=# select relname, granted, pid, mode from pg_locks L, pg_class C
emol_warehouse_1-#         where C.oid = L.relation and locktype = 'relation'; 
                    relname                      | granted |  pid  |        mode         
--------------------------------------------------+---------+-------+---------------------
pg_index                                         | t       | 24738 | AccessShareLock
pg_index                                         | t       | 24738 | RowShareLock
PartInd_emol_warehouse_1_cluster_sl_log_2-node-1 | t       | 24738 | AccessExclusiveLock
db_tables                                        | t       | 24738 | AccessExclusiveLock
db_columns                                       | t       | 24738 | AccessExclusiveLock
sl_table                                         | t       | 24738 | AccessShareLock
sl_table                                         | t       | 24738 | RowShareLock
sl_table                                         | t       | 24738 | RowExclusiveLock
pg_locks                                         | t       | 24747 | AccessShareLock
sl_set                                           | t       | 24738 | AccessShareLock
sl_set                                           | t       | 24738 | RowShareLock
sl_set                                           | t       | 24738 | RowExclusiveLock
logp                                             | t       | 24738 | AccessExclusiveLock
catalogue_issue                                  | t       | 24738 | AccessExclusiveLock
supplier_priority                                | t       | 24738 | AccessExclusiveLock
sl_setsync                                       | t       | 24738 | RowExclusiveLock
pg_indexes                                       | t       | 24738 | AccessShareLock
sl_node                                          | t       | 24738 | AccessShareLock
sl_node                                          | t       | 24738 | RowShareLock
supplier_detail                                  | t       | 24738 | AccessExclusiveLock
chmoogle_thesaurus                               | t       | 24738 | AccessExclusiveLock
parent_properties                                | t       | 24738 | AccessExclusiveLock
db_names                                         | t       | 24738 | AccessExclusiveLock
sl_listen                                        | t       | 24738 | AccessShareLock
sl_listen                                        | t       | 24738 | RowExclusiveLock
pg_class                                         | t       | 24738 | AccessShareLock
pg_class                                         | t       | 24738 | RowShareLock
version_properties                               | t       | 24738 | AccessExclusiveLock
parent                                           | t       | 24738 | AccessExclusiveLock
str_conntab                                      | t       | 24738 | AccessExclusiveLock
sl_log_status                                    | t       | 24738 | AccessShareLock
pg_class                                         | t       | 24747 | AccessShareLock
pg_tablespace                                    | t       | 24738 | AccessShareLock
sl_path                                          | t       | 24738 | AccessShareLock
sl_path                                          | t       | 24738 | RowShareLock
chmoogle_thesaurus_types                         | t       | 24738 | AccessExclusiveLock
catalogue_info                                   | t       | 24738 | AccessExclusiveLock
chmoogle_molkeys                                 | t       | 24738 | AccessExclusiveLock
pg_class_oid_index                               | t       | 24747 | AccessShareLock
version                                          | t       | 24738 | AccessExclusiveLock
sl_event                                         | t       | 24738 | AccessShareLock
sl_log_2                                         | t       | 24738 | AccessShareLock
sl_log_2                                         | t       | 24738 | ShareLock
sl_log_2                                         | t       | 24738 | AccessExclusiveLock
chmoogle_statistics                              | t       | 24738 | AccessExclusiveLock
sl_subscribe                                     | t       | 24738 | AccessShareLock
sl_subscribe                                     | t       | 24738 | RowExclusiveLock
chmoogle_cansmiles                               | t       | 24738 | AccessExclusiveLock
pg_namespace                                     | t       | 24738 | AccessShareLock
pg_namespace                                     | t       | 24738 | RowShareLock
sample                                           | t       | 24738 | AccessExclusiveLock
sl_config_lock                                   | t       | 24738 | AccessExclusiveLock
(52 rows)



$ vmstat 2
procs -----------memory---------- ---swap-- -----io---- --system-- ----cpu----
r  b   swpd   free   buff  cache   si   so    bi    bo   in    cs us sy id wa
1  0 138112 138244  20156 3577564    0    1     0     0    0     0  5  1 92  2
1  0 138112 139988  20084 3575816    0    0  1968    12  413   179 24  1 75  0
1  0 138112 140964  19984 3575136    0    0  2232    20  440   362 24  1 75  0
1  0 138112 141756  19888 3574972    0    0  2340    75  527   497 25  0 75  0
1  0 138112 138216  19888 3578612    0    0  1774     4  420   174 24  0 75  0
1  0 138112 141464  19808 3575572    0    0  1436    42  397   321 24  0 75  0
0  1 138112 137388  19808 3579732    0    0  2078     8  455   277 21  1 74  5
1  0 138112 140700  19808 3576612    0    0  1312    20  392   377 20  1 74  5
1  0 138112 137048  19808 3580512    0    0  1980    57  392   539 23  0 72  5
[snip]
2  0 138112 141432  19248 3578472    0    0     0    61  322   533 25  1 74  0
1  0 138112 141448  19248 3578472    0    0     0    27  274   181 25  0 75  0
1  0 138112 141448  19248 3578472    0    0     0    25  259   111 25  0 75  0
1  0 138112 141456  19248 3578472    0    0     0     8  263   290 25  0 75  0
1  0 138112 141456  19248 3578472    0    0     0    20  282   304 25  0 75  0
1  0 138112 141580  19256 3578464    0    0     0    64  318   443 25  0 75  0
1  0 138112 141580  19256 3578464    0    0     0     4  268   296 25  0 75  0
1  0 138112 141584  19256 3578464    0    0     0    15  259   174 25  0 75  0
1  0 138112 141584  19256 3578464    0    0     0     8  263   141 25  0 75  0
1  0 138112 141600  19256 3578464    0    0     0    20  284   317 25  0 75  0
1  0 138112 141600  19256 3578464    0    0     0    85  321   441 25  0 75  0
1  0 138112 141600  19256 3578464    0    0     0    10  269   143 25  0 75  0
1  0 138112 141600  19256 3578464    0    0     0    27  259   220 25  0 75  0
1  0 138112 141476  19256 3578464    0    0     0     8  261   185 25  0 75  0
1  0 138112 141476  19256 3578464    0    0     0    20  285   210 25  0 75  0
1  0 138112 141476  19256 3578464    0    0     0    59  318   440 25  0 75  0
1  0 138112 141476  19256 3578464    0    0     0     4  267   141 25  0 75  0
1  0 138112 141476  19256 3578464    0    0     0    31  260   103 25  0 75  0
1  0 138112 141476  19256 3578464    0    0     0     8  263   237 25  0 75  0
1  0 138112 141476  19256 3578464    0    0     0    33  285   282 25  0 75  0
[snip]
1  0 138112 140856  19304 3578416    0    0     0    27  259   250 25  0 75  0
1  0 138112 140856  19304 3578416    0    0     0     8  262   200 25  0 75  0
1  0 138112 140856  19304 3578416    0    0     0    20  283   236 25  0 75  0
1  0 138112 140856  19304 3578416    0    0     0    63  318   508 25  0 75  0
1  0 138112 140856  19304 3578416    0    0     0    10  265   149 25  0 75  0
1  0 138112 140732  19304 3578416    0    0     0    25  259   108 25  0 75  0
1  0 138112 140732  19304 3578416    0    0     0     8  262   269 25  0 75  0
0  1 138112 137440  18736 3582364    0    0  4184    26  575   815 17  1 75  7
0  2 138112 138184  17360 3584000    0    0  5216    89  755   969  0  1 70 28
0  1 138112 139956  16176 3584144    0    0  4564    12  775   902  1  1 73 25
1  0 138112 138400  15164 3586976    0    0  6368   115  764   642  1  1 75 23
0  1 138112 141136  14620 3585180    0    0  6936     8  731   331  1  1 74 24
0  2 138112 140068  14564 3587056    0    0  6168    32  804   448  1  1 74 24
1  2 138112 139960  14240 3587640    0    0  5364    61  770   741  1  1 73 25
0  1 138112 136492  14120 3591660    0    0  7036    20  872  1511  1  1 73 24
0  1 138112 140772  13412 3588728    0    0  8544    24 1043  1951  1  2 75 23
0  1 138112 136460  13104 3593716    0    0 10052     8  933  1615  1  2 73 24
0  1 138112 139356  12840 3591380    0    0  9156    16  891  1511  1  1 72 26
0  2 138112 137084  12704 3594116    0    0  8928    47  929  1718  1  2 72 26
0  1 138112 140276  12688 3591272    0    0  8836    20  965  1757  1  1 72 26
0  1 138112 138052  12688 3593092    0    0  8800    19  999  1679  1  1 75 23
[snip]
0  2 138112 139288  12056 3595024    0    0  7804    32  722  1207  1  1 73 25
0  1 138112 137544  11840 3597060    0    0  7804    55  662  1096  1  1 73 25
0  1 138112 138468  11784 3596596    0    0  6972    32  625  1031  1  1 72 26
0  1 138112 137520  11476 3597944    0    0  8564    28  767  1291  1  1 75 23
0  1 138112 141640  11120 3594920    0    0  9220    14  655   933  1  1 74 24
0  1 138112 138464  11092 3598328    0    0 29424    12 1340  1471  2  4 75 19
0  2 138112 139628  10572 3597808    0    0 23832    49 1540  2858  2  3 73 21
0  1 138112 138560  10436 3599244    0    0 13844    32 1172  2170  1  2 71 25
0  1 138112 139348  10428 3598732    0    0 12774    35 1010  1781  2  2 75 22
0  1 138112 137676  10276 3600444    0    0 13680    68 1089  2035  1  2 74 23
procs -----------memory---------- ---swap-- -----io---- --system-- ----cpu----
r  b   swpd   free   buff  cache   si   so    bi    bo   in    cs us sy id wa
0  2 138112 141024   9952 3597908    0    0 13844    12 1371  2562  1  3 75 21
0  1 138112 139704   9884 3599276    0    0 13460    55 1099  2014  1  2 71 26
1  1 138112 139540   9824 3599872    0    0 13596    32 1185  2430  1  3 73 23
0  1 138112 138348   9696 3601040    0    0 11212   118 1139  2031  1  2 75 22
0  1 138112 137564   9572 3602464    0    0 25148    69  595   949  2  2 73 23
1  1 138112 139960   9496 3600460    0    0 15644    12  660  1117  1  2 73 24
1  1 138112 140912   9460 3599716    0    0  7292   156 1017  1782  1  2 73 24
0  1 138112 139664   9380 3601356    0    0 23516    28 1518  2840  3  3 72 22
0  1 138112 139108   8708 3603068    0    0 40538    16 1205  2320  5  4 75 17
0  1 138112 141668   7420 3602016    0    0 30164    23 1312  2418  3  4 74 19
0  1 138112 138516   6212 3606604    0    0 20012    18 1075  1855  2  3 74 20
0  2 138112 140756   6192 3604804    0    0  4176    49  501   704  0  0 72 27
0  1 138112 140940   6164 3604572    0    0  5300    32  613  1006  1  1 72 26
0  1 138112 141064   6056 3604940    0    0  5136     0  629   913  0  1 75 24
0  1 138112 141972   6044 3604172    0    0  4788    49  705  1216  1  1 73 25
0  1 138112 138004   5780 3608076    0    0  6972    12  633   986  0  1 74 24
0  2 138112 141496   5512 3604964    0    0  6360    39  786   414  1  1 72 25
0  1 138112 141016   5468 3605788    0    0  8316    32  874   878  1  1 71 26
0  1 138112 141832   5332 3605664    0    0 37456     0  553  1051  4  2 75 19
0  1 138112 140940   5332 3606452    0    0 19016    41  985  1674  2  3 74 21
0  1 138112 141720   5332 3605672    0    0 15964    12  619   740  2  2 73 24
0  2 138112 137296   5292 3610132    0    0 29388    53 1069   877  3  3 74 19
0  1 138112 138600   4556 3617372    0    0 29104    32 1214  1237  3  5 70 22

From cbbrowne at ca.afilias.info  Mon Feb 25 07:18:19 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Feb 25 07:19:16 2008
Subject: [Slony1-general] possible to replicate from text column in 8.1
	into an xml column in 8.3?
In-Reply-To: <49584.192.168.1.108.1203902391.squirrel@msqr.us> (Matt
	Magoffin's message of "Mon, 25 Feb 2008 14:19:51 +1300 (NZDT)")
References: <49584.192.168.1.108.1203902391.squirrel@msqr.us>
Message-ID: <60wsotm5dw.fsf@dba2.int.libertyrms.com>

"Matt Magoffin" <postgresql.org@msqr.us> writes:
> I'm trying to plan out a migration from Postgres 8.1 to Postgres 8.3. We
> currently use Slony to replicate our database, and I understand we should
> be able to replicate from 8.1 into 8.3 as long as we have the same version
> of Slony installed in both databases.
>
> In 8.1 I have a table that uses a text column to store XML data, and in
> 8.3 we plan to turn that into an xml type column. I can migrate that data
> using pg_dump and pg_restore with the --data-only option.
>
> I was wondering if I could just set up replication between the 8.1
> database and 8.3 database, and have the data replicated directly from the
> text column in 8.1 into the xml column in 8.3.

I'd expect that to work OK, but with one pretty big "foot gun" in
play, namely the fact that the XML type does validation.  That
provides the risk that if you ever put invalid XML into the column in
the 8.1 database, the attempt to INSERT/UPDATE it on the 8.3 instance
will fail, and replication will fail (until/unless you delete the
offending tuples from sl_log_[12] by hand).  That would be a nice way
to really muss up the cluster.
-- 
output = reverse("gro.mca" "@" "enworbbc")
http://linuxdatabases.info/info/wp.html
"In the  free software world, a  rising tide DOES lift  all boats, and
once the  user has tasted  Unix it's easy  for them to  switch between
Unices."
-- david parsons
From stephane.schildknecht at postgresqlfr.org  Mon Feb 25 08:04:22 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Mon Feb 25 08:04:29 2008
Subject: [Slony1-general] STILL can't migrate a node.
In-Reply-To: <47C2D7D1.7060206@emolecules.com>
References: <47B463D5.90605@emolecules.com>	<47BB74B8.2030808@emolecules.com>	<20080221144755.GC17293@crankycanuck.ca>	<47BD907C.2050603@dalibo.com>	<20080221171359.GA18657@crankycanuck.ca>	<47BF9073.7060708@emolecules.com>	<47BFAD0E.2000902@emolecules.com>
	<47C04A24.8010209@Yahoo.com>	<47C065E9.7070408@emolecules.com>
	<47C06A22.5050301@Yahoo.com>	<47C06BC1.8060805@emolecules.com>
	<47C07385.5080602@Yahoo.com> <47C2D7D1.7060206@emolecules.com>
Message-ID: <47C2E706.9060805@postgresqlfr.org>

Craig A. James a ?crit :
> I sent the attached email ony to Jan by mistake, here it is, I hope Jan
> or somebody has an idea what's going on.
> 
> A quick summary:
>  1. Shut of Apache
>  2. Restart Postgres and Slony
>  3. lock set (id = 1, origin = 1);
>     wait for event (origin = 1, confirmed = 2);
>     move set (id = 1, old origin = 1, new origin = 2);
> 
> When I do this, Slony starts a SELECT that never finishes ("never" means
> I waited 10 minutes, 35 minutes, and 10 minutes in three different
> tests, with no sign of progress).
> 
> Per Jan's request, I've attached the output of two SQL queries:
> 
>    select relname, granted, pid, mode from pg_locks L, pg_class C
>        where C.oid = L.relation and locktype = 'relation';
> 
>    select * from pg_stat_activity;
> 
> In addition, I've attached the output of vmstat, a few snapshots taken
> over a 30-minute peroid.  It's looks to me like postgres is doing some
> sort of huge join or sort, based on the changing amounts of CPU and I/O
> during the 30 minutes I monitored it.  No other processes were active;
> this was entirely due to the backend Postgres process being used by Slony.
> 
>> From these, you can see that a Slon process is doing a "SELECT
>> moveSet(...)".  ps(1) confirms that this process is the one using all
>> the CPU time shown by vmstat.
> 
> I sure could use some help on this one, it's become a critical roadblock
> to our operation.
> 
> Thanks,
> Craig
> 

Hi, Not sure I got the full story as this thread began with vacuum and then
came to move set...

But, concerning move set hanging, I remember some experienced the problem. So
had I. I then stoppped the shell having initiated the move set, and restartzed
it (I have to test if set is locked...).

Second time, move set was ok.

I surely don't know if that could be of any help.

Cheers,

-- 
St?phane SCHILDKNECHT
Pr?sident de PostgreSQLFr
T?l. 09 53 69 97 12
http://www.postgresqlfr.org
From troy at troywolf.com  Mon Feb 25 08:26:58 2008
From: troy at troywolf.com (Troy Wolf)
Date: Mon Feb 25 08:27:04 2008
Subject: [Slony1-general] Test?
Message-ID: <e0d7c3f50802250826r5397df4evb831a6be5acc1027@mail.gmail.com>

Sorry for this junk, but I subscribed and posted a message at about 8:50 CST
on Fri, Feb 22nd. I have yet to see my message hit the list. So this is a
test to see if anything I post makes it to the list. I have my subscription
configured to send me my own posts. I have received my daily digest every
day, but my message, so far, is not in it.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080225/=
9094844c/attachment.htm
From vivek at khera.org  Mon Feb 25 08:37:17 2008
From: vivek at khera.org (Vivek Khera)
Date: Mon Feb 25 08:37:23 2008
Subject: [Slony1-general] Building Slony 1.2.13 on MacOSX 10.4 (PPC)
In-Reply-To: <1203780333.3352.15.camel@new-host>
References: <EBE586B4-972E-418F-8856-33C646A781DF@logicunited.com>
	<1203780333.3352.15.camel@new-host>
Message-ID: <50647843-A079-40A4-8EB5-0865C48E964D@khera.org>


On Feb 23, 2008, at 10:25 AM, Brian A. Seklecki wrote:

> Try Pkgsrc instead...CC me on any bug reports.

Yeah... i use netbsd's pkgsrc on my mac.  It generally works well,  
except when it comes to upgrading stuff :-(

I used bootcamp to re-partition the drive and reformat a small  
partition as UFS.

From vivek at khera.org  Mon Feb 25 08:40:42 2008
From: vivek at khera.org (Vivek Khera)
Date: Mon Feb 25 08:40:48 2008
Subject: [Slony1-general] STILL can't migrate a node.
In-Reply-To: <47BF9073.7060708@emolecules.com>
References: <47B463D5.90605@emolecules.com>
	<47BB74B8.2030808@emolecules.com>	<20080221144755.GC17293@crankycanuck.ca>	<47BD907C.2050603@dalibo.com>
	<20080221171359.GA18657@crankycanuck.ca>
	<47BF9073.7060708@emolecules.com>
Message-ID: <FB24F819-9779-4D9C-B274-E7CB92FF5281@khera.org>


On Feb 22, 2008, at 10:18 PM, Craig James wrote:

> This time I watched -- it got stuck again, doing some sort of huge  
> SELECT statement.  I was under the impression that migrating a node  
> was a fairly simple operation that should happen in a short time  
> (less than a minute?) even for large databases.

Pre-conditions: no active transactions and replication is "caught up"  
-- ie you're not tens or hundreds of events behind on any node.

The move origin takes time approaching zero for me.

From darcyb at commandprompt.com  Mon Feb 25 08:44:07 2008
From: darcyb at commandprompt.com (Darcy Buskermolen)
Date: Mon Feb 25 08:45:22 2008
Subject: [Slony1-general] Test?
In-Reply-To: <e0d7c3f50802250826r5397df4evb831a6be5acc1027@mail.gmail.com>
References: <e0d7c3f50802250826r5397df4evb831a6be5acc1027@mail.gmail.com>
Message-ID: <200802250844.08300.darcyb@commandprompt.com>

On Monday 25 February 2008 08:26:58 Troy Wolf wrote:
> Sorry for this junk, but I subscribed and posted a message at about 8:50
> CST on Fri, Feb 22nd. I have yet to see my message hit the list. So this is
> a test to see if anything I post makes it to the list. I have my
> subscription configured to send me my own posts. I have received my daily
> digest every day, but my message, so far, is not in it.

Your message did make the list

-- 
Darcy Buskermolen
Command Prompt, Inc.
+1.503.667.4564 X 102
http://www.commandprompt.com/
PostgreSQL solutions since 1997
From postgresql.org at msqr.us  Mon Feb 25 11:41:09 2008
From: postgresql.org at msqr.us (Matt Magoffin)
Date: Mon Feb 25 11:41:25 2008
Subject: [Slony1-general] possible to replicate from text column in 8.1 
	into an xml column in 8.3?
In-Reply-To: <60wsotm5dw.fsf@dba2.int.libertyrms.com>
References: <49584.192.168.1.108.1203902391.squirrel@msqr.us>
	<60wsotm5dw.fsf@dba2.int.libertyrms.com>
Message-ID: <49323.192.168.1.108.1203968469.squirrel@msqr.us>

> I'd expect that to work OK, but with one pretty big "foot gun" in
> play, namely the fact that the XML type does validation.  That
> provides the risk that if you ever put invalid XML into the column in
> the 8.1 database, the attempt to INSERT/UPDATE it on the 8.3 instance
> will fail, and replication will fail (until/unless you delete the
> offending tuples from sl_log_[12] by hand).  That would be a nice way
> to really muss up the cluster.

That's great! I know every XML document is well-formed, and I have loaded
all the data into an xml type column using pg_restore without errors. So
I'll move forward with validating this approach (replicating using Slony).

-- m@
From cbbrowne at ca.afilias.info  Mon Feb 25 11:48:13 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Feb 25 11:48:22 2008
Subject: [Slony1-general] possible to replicate from text column in 8.1
	into an xml column in 8.3?
In-Reply-To: <49323.192.168.1.108.1203968469.squirrel@msqr.us> (Matt
	Magoffin's message of "Tue, 26 Feb 2008 08:41:09 +1300 (NZDT)")
References: <49584.192.168.1.108.1203902391.squirrel@msqr.us>
	<60wsotm5dw.fsf@dba2.int.libertyrms.com>
	<49323.192.168.1.108.1203968469.squirrel@msqr.us>
Message-ID: <608x18u8aq.fsf@dba2.int.libertyrms.com>

"Matt Magoffin" <postgresql.org@msqr.us> writes:
>> I'd expect that to work OK, but with one pretty big "foot gun" in
>> play, namely the fact that the XML type does validation.  That
>> provides the risk that if you ever put invalid XML into the column in
>> the 8.1 database, the attempt to INSERT/UPDATE it on the 8.3 instance
>> will fail, and replication will fail (until/unless you delete the
>> offending tuples from sl_log_[12] by hand).  That would be a nice way
>> to really muss up the cluster.
>
> That's great! I know every XML document is well-formed, and I have loaded
> all the data into an xml type column using pg_restore without errors. So
> I'll move forward with validating this approach (replicating using Slony).

Yes, I'd strongly recommend trying this out in a test environment
before unleashing it on production :-).
-- 
let name="cbbrowne" and tld="linuxdatabases.info" in name ^ "@" ^ tld;;
http://linuxdatabases.info/info/advocacy.html
"We should start referring to processes which run in the background by
their correct technical name:... paenguins."  -- Kevin M Bealer
From troy at troywolf.com  Mon Feb 25 12:38:43 2008
From: troy at troywolf.com (Troy Wolf)
Date: Mon Feb 25 12:38:54 2008
Subject: [Slony1-general] Slony locks tables that are not even in
	replication sets?
Message-ID: <e0d7c3f50802251238i5bfe8d9t2a803e9f3295f3fd@mail.gmail.com>

Troy Wolf:
>>Now onto my issue at hand: Twice now, we've seen a pattern that looks like
>>Slony is waiting on a large index creation to complete, and then an
>>application gets stuck waiting behind Slony's locks.

Andrew Sullivan:
>I'm not dismissing your description, but output from pg_locks would be
>helpful here.  Without a specific case, it's hard to say much more.

I did query pg_locks at the time as well as pg_stat_activity. I know
now to save that output for discussion purposes here. If anyone knows
how, with postgres, to see not only the locks but exactly WHO is
blocking who, please share. With pg_locks, and based on the time of
the processes, I can make an educated guess.

pg_locks showed 2 exclusive locks held by a nightly batch process and
slony waiting for a lock and another application process waiting on a
lock. This process does some data copying into some large history
tables and rebuilds some indexes. These tables are not part of any
replication set nor do they have any relationships to other tables.
One of the locks was for an index being created. Slony showed waiting
on a lock, and had been for about an hour. The moment we killed the
index creation postgres process, Slony did it's thing. It was
definitely waiting on that lock.

I guess what I was looking for from this group was a confirmation of
"Yes. Certain Slony tasks require locking ALL tables in the
database--not just the replication set". Or "No, that is not possible.
You need to figure out what really happened."
From y-mori at sraoss.co.jp  Tue Feb 26 06:07:34 2008
From: y-mori at sraoss.co.jp (Yoshiharu Mori)
Date: Tue Feb 26 06:07:45 2008
Subject: [Slony1-general] failover problems with 3 nodes
Message-ID: <20080226230734.de0f09d5.y-mori@sraoss.co.jp>

Hello.

I am using 3 nodes (2 slaves subscribed to one master).
Replication is fine, but FAILOVER command don't work correctly.

I do FAILOVER from "node 1 (master)" to "node 3" as following.

    failover ( id = 1, backup node = 3);

I could confirm that node 3 was master and INSERT tupples 
updated to the slave (node 2) 

However I couldn't FAILOVER from "node 1 (master)" to "node 2".

    failover ( id = 1, backup node = 2);

INSERT tupples didn't updated to the slave ( node 3).


I use postgresql-8.2.6 and slony-I 1.2.12 (Because of xxid.*.sql problems, I couldn't setup the slony-I-1.2.13--;).

Any suggestion?
 
* Here is failed-logs for failover from "node 1" to "node 2";

- sl_event table -

-[ RECORD 52 ]----------------------------------------------------------
ev_origin    | 2
ev_seqno     | 17
ev_timestamp | 2008-02-26 22:25:19.358396
ev_minxid    | 0
ev_maxxid    | 0
ev_xip       |
ev_type      | ACCEPT_SET
ev_data1     | 1
ev_data2     | 1
ev_data3     | 2
ev_data4     | 31
..
-[ RECORD 53 ]----------------------------------------------------------
ev_origin    | 1
ev_seqno     | 31
ev_timestamp | 2008-02-26 22:25:19.358396
ev_minxid    | 620469
ev_maxxid    | 620471
ev_xip       | '620469'
ev_type      | FAILOVER_SET
ev_data1     | 1
ev_data2     | 2
ev_data3     | 1
..
---

/* FAILED CASE; node2-logs. It was expected as master */

- FAILOVER_SET and ACCEPT SET were not received. -

2008-02-26 22:25:19 JST DEBUG2 setNodeLastEvent: no_id=1 event_seq=31
2008-02-26 22:25:19 JST CONFIG storeNode: no_id=3 no_comment='Slave node2'
2008-02-26 22:25:19 JST DEBUG2 setNodeLastEvent: no_id=3 event_seq=16
2008-02-26 22:25:19 JST CONFIG storePath: pa_server=1 pa_client=2 pa_conninfo="host=osspc7.sra.co.jp dbname=testdb user=mori port=5555" pa_connretry=10
2008-02-26 22:25:19 JST CONFIG storePath: pa_server=3 pa_client=2 pa_conninfo="host=osspc9.sra.co.jp dbname=testdb user=mori port=5555" pa_connretry=10
2008-02-26 22:25:19 JST CONFIG storeListen: li_origin=1 li_receiver=2 li_provider=1
2008-02-26 22:25:19 JST CONFIG storeListen: li_origin=3 li_receiver=2 li_provider=3
2008-02-26 22:25:19 JST CONFIG storeListen: li_origin=1 li_receiver=2 li_provider=3
2008-02-26 22:25:19 JST CONFIG storeListen: li_origin=3 li_receiver=2 li_provider=1
2008-02-26 22:25:19 JST CONFIG storeSet: set_id=1 set_origin=2 set_comment='first replication set'
2008-02-26 22:25:19 JST DEBUG2 sched_wakeup_node(): no_id=2 (0 threads + worker signaled)
2008-02-26 22:25:19 JST DEBUG2 main: last local event sequence = 17
2008-02-26 22:25:19 JST CONFIG main: configuration complete - starting threads
2008-02-26 22:25:19 JST DEBUG1 localListenThread: thread starts
2008-02-26 22:25:19 JST DEBUG4 version for "host=osspc8.sra.co.jp dbname=testdb user=mori port=5555" is 80206
2008-02-26 22:25:19 JST CONFIG enableNode: no_id=1
2008-02-26 22:25:19 JST CONFIG enableNode: no_id=3
2008-02-26 22:25:19 JST DEBUG1 main: running scheduler mainloop
2008-02-26 22:25:19 JST DEBUG1 remoteWorkerThread_1: thread starts
2008-02-26 22:25:19 JST DEBUG1 remoteWorkerThread_3: thread starts
2008-02-26 22:25:19 JST DEBUG1 remoteListenThread_1: thread starts
2008-02-26 22:25:19 JST DEBUG2 remoteListenThread_1: start listening for event origin 1
2008-02-26 22:25:19 JST DEBUG2 remoteListenThread_1: start listening for event origin 3
2008-02-26 22:25:19 JST DEBUG4 version for "host=osspc8.sra.co.jp dbname=testdb user=mori port=5555" is 80206
2008-02-26 22:25:19 JST DEBUG1 remoteListenThread_3: thread starts
2008-02-26 22:25:19 JST DEBUG2 remoteListenThread_3: start listening for event origin 3
2008-02-26 22:25:19 JST DEBUG2 remoteListenThread_3: start listening for event origin 1
2008-02-26 22:25:19 JST DEBUG1 cleanupThread: thread starts
2008-02-26 22:25:19 JST DEBUG4 cleanupThread: bias = 35383
2008-02-26 22:25:19 JST DEBUG1 syncThread: thread starts
2008-02-26 22:25:19 JST DEBUG4 version for "host=osspc8.sra.co.jp dbname=testdb user=mori port=5555" is 80206
2008-02-26 22:25:19 JST DEBUG4 remoteWorkerThread_3: update provider configuration
2008-02-26 22:25:19 JST DEBUG4 version for "host=osspc8.sra.co.jp dbname=testdb user=mori port=5555" is 80206
2008-02-26 22:25:19 JST DEBUG4 remoteWorkerThread_1: update provider configuration
2008-02-26 22:25:19 JST DEBUG4 version for "host=osspc8.sra.co.jp dbname=testdb user=mori port=5555" is 80206
2008-02-26 22:25:19 JST DEBUG4 version for "host=osspc7.sra.co.jp dbname=testdb user=mori port=5555" is 80206
2008-02-26 22:25:19 JST DEBUG4 version for "host=osspc9.sra.co.jp dbname=testdb user=mori port=5555" is 80206
2008-02-26 22:25:19 JST DEBUG1 remoteListenThread_1: connected to 'host=osspc7.sra.co.jp dbname=testdb user=mori port=5555'
2008-02-26 22:25:19 JST DEBUG1 remoteListenThread_3: connected to 'host=osspc9.sra.co.jp dbname=testdb user=mori port=5555'
2008-02-26 22:25:19 JST DEBUG2 remoteWorkerThread_1: forward confirm 1,30 received by 3
2008-02-26 22:25:19 JST DEBUG2 remoteWorkerThread_3: forward confirm 2,16 received by 1
2008-02-26 22:25:19 JST DEBUG2 remoteWorkerThread_1: forward confirm 3,16 received by 1
2008-02-26 22:25:19 JST DEBUG2 remoteWorkerThread_1: forward confirm 2,16 received by 3
2008-02-26 22:25:21 JST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 18
2008-02-26 22:25:21 JST DEBUG2 remoteListenThread_1: queue event 3,17 SYNC
2008-02-26 22:25:21 JST DEBUG2 remoteWorkerThread_3: Received event 3,17 SYNC
2008-02-26 22:25:21 JST DEBUG2 remoteWorkerThread_3: SYNC 17 processing
2008-02-26 22:25:21 JST DEBUG2 remoteWorkerThread_3: no sets need syncing for this e

/* FAILED CASE: node 3-log. It was expected as slave */

2008-02-26 22:25:20 JST DEBUG2 remoteListenThread_1: queue event 2,17 ACCEPT_SET
2008-02-26 22:25:20 JST DEBUG2 remoteListenThread_2: queue event 1,31 FAILOVER_SET
2008-02-26 22:25:20 JST DEBUG2 remoteListenThread_2: queue event 2,17 ACCEPT_SET
2008-02-26 22:25:20 JST DEBUG2 remoteWorker_event: event 2,17 ignored - duplicate
2008-02-26 22:25:20 JST DEBUG2 remoteWorkerThread_2: Received event 2,17 ACCEPT_SET
2008-02-26 22:25:20 JST DEBUG2 remoteWorkerThread_1: Received event 1,31 FAILOVER_SET
2008-02-26 22:25:20 JST DEBUG2 start processing ACCEPT_SET
2008-02-26 22:25:20 JST DEBUG2 ACCEPT: set=1
2008-02-26 22:25:20 JST DEBUG2 ACCEPT: old origin=1
2008-02-26 22:25:20 JST DEBUG2 ACCEPT: new origin=2
2008-02-26 22:25:20 JST DEBUG2 ACCEPT: move set seq=31
2008-02-26 22:25:20 JST DEBUG2 got parms ACCEPT_SET
2008-02-26 22:25:20 JST DEBUG2 ACCEPT_SET - node not origin
2008-02-26 22:25:20 JST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep
2008-02-26 22:25:24 JST DEBUG2 localListenThread: Received event 3,17 SYNC
2008-02-26 22:25:26 JST DEBUG2 remoteListenThread_1: queue event 2,18 SYNC
2008-02-26 22:25:26 JST DEBUG2 remoteListenThread_2: queue event 2,18 SYNC
2008-02-26 22:25:26 JST DEBUG2 remoteWorker_event: event 2,18 ignored - duplicate
2008-02-26 22:25:30 JST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 18
2008-02-26 22:25:30 JST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep <-- this loop has occured !!
2008-02-26 22:25:32 JST DEBUG2 remoteListenThread_1: queue event 2,19 SYNC
2008-02-26 22:25:32 JST DEBUG2 remoteListenThread_2: queue event 2,19 SYNC
2008-02-26 22:25:32 JST DEBUG2 remoteWorker_event: event 2,19 ignored - duplicate
2008-02-26 22:25:36 JST DEBUG2 localListenThread: Received event 3,18 SYNC
2008-02-26 22:25:38 JST DEBUG2 remoteListenThread_1: LISTEN
2008-02-26 22:25:38 JST DEBUG2 remoteListenThread_2: LISTEN

---

*Here is success-logs for failover from "node 1" to "node 3";

---

/* SUCCESS CASE: node 2-logs. It was expected as slave ***/

2008-02-26 18:43:19 JST DEBUG2 remoteWorkerThread_3: Received event 3,8 ACCEPT_SET
2008-02-26 18:43:19 JST DEBUG2 start processing ACCEPT_SET
2008-02-26 18:43:19 JST DEBUG2 ACCEPT: set=1
2008-02-26 18:43:19 JST DEBUG2 ACCEPT: old origin=1
2008-02-26 18:43:19 JST DEBUG2 ACCEPT: new origin=3
2008-02-26 18:43:19 JST DEBUG2 ACCEPT: move set seq=21
2008-02-26 18:43:19 JST DEBUG2 got parms ACCEPT_SET
2008-02-26 18:43:19 JST DEBUG2 ACCEPT_SET - node not origin
2008-02-26 18:43:19 JST DEBUG2 ACCEPT_SET - MOVE_SET or FAILOVER_SET exists - adjusting setsync status
2008-02-26 18:43:19 JST DEBUG2 ACCEPT_SET - done
2008-02-26 18:43:19 JST DEBUG2 slon_retry() from pid=15333
2008-02-26 18:43:19 JST DEBUG1 slon: retry requested
2008-02-26 18:43:19 JST DEBUG2 slon: notify worker process to shutdown
2008-02-26 18:43:19 JST DEBUG1 syncThread: thread done
2008-02-26 18:43:19 JST DEBUG1 localListenThread: thread done
2008-02-26 18:43:19 JST INFO   remoteListenThread_1: disconnecting from 'host=osspc7.sra.co.jp dbname=testdb user=mori port=5555'
2008-02-26 18:43:19 JST DEBUG1 remoteListenThread_1: thread done
2008-02-26 18:43:19 JST DEBUG1 cleanupThread: thread done
2008-02-26 18:43:19 JST INFO   remoteListenThread_3: disconnecting from 'host=osspc9.sra.co.jp dbname=testdb user=mori port=5555'
2008-02-26 18:43:19 JST DEBUG1 remoteListenThread_3: thread done
2008-02-26 18:43:19 JST DEBUG1 main: scheduler mainloop returned
2008-02-26 18:43:19 JST DEBUG2 main: wait for remote threads
2008-02-26 18:43:19 JST DEBUG4 remoteWorkerThread_1: update provider configuration
2008-02-26 18:43:19 JST DEBUG1 remoteWorkerThread_1: thread done
2008-02-26 18:43:19 JST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + worker signaled)
2008-02-26 18:43:19 JST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads + worker signaled)
2008-02-26 18:43:19 JST DEBUG1 main: done
2008-02-26 18:43:19 JST DEBUG2 slon: child terminated status: 0; pid: 15333, current worker pid: 15333
2008-02-26 18:43:19 JST DEBUG1 slon: restart of worker
2008-02-26 18:43:19 JST CONFIG main: slon version 1.2.12 starting up
2008-02-26 18:43:19 JST DEBUG2 slon: watchdog process started
2008-02-26 18:43:19 JST DEBUG2 slon: watchdog ready - pid = 15245
2008-02-26 18:43:19 JST DEBUG2 slon: worker process created - pid = 15356
2008-02-26 18:43:19 JST CONFIG main: local node id = 2
2008-02-26 18:43:19 JST DEBUG2 main: main process started
2008-02-26 18:43:19 JST CONFIG main: launching sched_start_mainloop
2008-02-26 18:43:19 JST CONFIG main: loading current cluster configuration
2008-02-26 18:43:19 JST CONFIG storeNode: no_id=1 no_comment='Master Node'
2008-02-26 18:43:19 JST DEBUG2 setNodeLastEvent: no_id=1 event_seq=21
2008-02-26 18:43:19 JST CONFIG storeNode: no_id=3 no_comment='Slave node2'
2008-02-26 18:43:19 JST DEBUG2 setNodeLastEvent: no_id=3 event_seq=8
2008-02-26 18:43:19 JST CONFIG storePath: pa_server=1 pa_client=2 pa_conninfo="host=osspc7.sra.co.jp dbname=testdb user=mori port=5555" pa_connretry=10
2008-02-26 18:43:19 JST CONFIG storePath: pa_server=3 pa_client=2 pa_conninfo="host=osspc9.sra.co.jp dbname=testdb user=mori port=5555" pa_connretry=10
2008-02-26 18:43:19 JST CONFIG storeListen: li_origin=1 li_receiver=2 li_provider=1
2008-02-26 18:43:19 JST CONFIG storeListen: li_origin=1 li_receiver=2 li_provider=3
2008-02-26 18:43:19 JST CONFIG storeListen: li_origin=3 li_receiver=2 li_provider=3
2008-02-26 18:43:19 JST CONFIG storeSet: set_id=1 set_origin=3 set_comment='first replication set'
2008-02-26 18:43:19 JST WARN   remoteWorker_wakeup: node 3 - no worker thread
2008-02-26 18:43:19 JST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads + worker signaled)
2008-02-26 18:43:19 JST CONFIG storeSubscribe: sub_set=1 sub_provider=3 sub_forward='t'
2008-02-26 18:43:19 JST WARN   remoteWorker_wakeup: node 3 - no worker thread
2008-02-26 18:43:19 JST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads + worker signaled)
2008-02-26 18:43:19 JST CONFIG enableSubscription: sub_set=1
2008-02-26 18:43:19 JST WARN   remoteWorker_wakeup: node 3 - no worker thread
2008-02-26 18:43:19 JST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads + worker signaled)
2008-02-26 18:43:19 JST DEBUG2 main: last local event sequence = 7
2008-02-26 18:43:19 JST CONFIG main: configuration complete - starting threads
2008-02-26 18:43:19 JST DEBUG1 localListenThread: thread starts
2008-02-26 18:43:19 JST DEBUG4 version for "host=osspc8.sra.co.jp dbname=testdb user=mori port=5555" is 80206
2008-02-26 18:43:20 JST CONFIG enableNode: no_id=1
2008-02-26 18:43:20 JST CONFIG enableNode: no_id=3
2008-02-26 18:43:20 JST DEBUG1 main: running scheduler mainloop
2008-02-26 18:43:20 JST DEBUG1 remoteWorkerThread_1: thread starts
2008-02-26 18:43:20 JST DEBUG1 remoteListenThread_3: thread starts
2008-02-26 18:43:20 JST DEBUG2 remoteListenThread_3: start listening for event origin 1
2008-02-26 18:43:20 JST DEBUG2 remoteListenThread_3: start listening for event origin 3
2008-02-26 18:43:20 JST DEBUG1 remoteWorkerThread_3: thread starts
2008-02-26 18:43:20 JST DEBUG1 remoteListenThread_1: thread starts
2008-02-26 18:43:20 JST DEBUG2 remoteListenThread_1: start listening for event origin 1
2008-02-26 18:43:20 JST DEBUG4 version for "host=osspc8.sra.co.jp dbname=testdb user=mori port=5555" is 80206
2008-02-26 18:43:20 JST DEBUG4 remoteWorkerThread_1: update provider configuration
2008-02-26 18:43:20 JST DEBUG1 cleanupThread: thread starts
2008-02-26 18:43:20 JST DEBUG4 cleanupThread: bias = 35383
2008-02-26 18:43:20 JST DEBUG1 syncThread: thread starts
2008-02-26 18:43:20 JST DEBUG4 version for "host=osspc8.sra.co.jp dbname=testdb user=mori port=5555" is 80206
2008-02-26 18:43:20 JST DEBUG4 remoteWorkerThread_3: update provider configuration
2008-02-26 18:43:20 JST DEBUG1 remoteWorkerThread_3: helper thread for provider 3 created
2008-02-26 18:43:20 JST DEBUG4 remoteWorkerThread_3: added active set 1 to provider 3
2008-02-26 18:43:20 JST DEBUG4 remoteHelperThread_3_3: waiting for work
2008-02-26 18:43:20 JST DEBUG4 version for "host=osspc8.sra.co.jp dbname=testdb user=mori port=5555" is 80206
2008-02-26 18:43:20 JST DEBUG4 version for "host=osspc8.sra.co.jp dbname=testdb user=mori port=5555" is 80206
2008-02-26 18:43:20 JST DEBUG4 version for "host=osspc9.sra.co.jp dbname=testdb user=mori port=5555" is 80206
2008-02-26 18:43:20 JST DEBUG4 version for "host=osspc7.sra.co.jp dbname=testdb user=mori port=5555" is 80206
2008-02-26 18:43:20 JST DEBUG1 remoteListenThread_3: connected to 'host=osspc9.sra.co.jp dbname=testdb user=mori port=5555'
2008-02-26 18:43:20 JST DEBUG2 remoteWorkerThread_3: forward confirm 1,21 received by 3
2008-02-26 18:43:20 JST DEBUG2 remoteWorkerThread_3: forward confirm 2,7 received by 1
2008-02-26 18:43:20 JST DEBUG2 remoteWorkerThread_3: forward confirm 3,8 received by 1
2008-02-26 18:43:20 JST DEBUG1 remoteListenThread_1: connected to 'host=osspc7.sra.co.jp dbname=testdb user=mori port=5555'
2008-02-26 18:43:20 JST DEBUG2 remoteWorkerThread_3: forward confirm 2,7 received by 3
2008-02-26 18:43:22 JST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 8
2008-02-26 18:43:22 JST DEBUG2 remoteListenThread_3: LISTEN
2008-02-26 18:43:24 JST DEBUG2 remoteWorkerThread_3: forward confirm 2,8 received by 3
2008-02-26 18:43:24 JST DEBUG2 remoteListenThread_3: queue event 3,9 SYNC
2008-02-26 18:43:24 JST DEBUG2 remoteListenThread_3: UNLISTEN
2008-02-26 18:43:24 JST DEBUG2 remoteWorkerThread_3: Received event 3,9 SYNC

/* SUCESS CASE; node 3-logs. It was expected as master ***/

2008-02-26 18:43:14 JST DEBUG2 remoteListenThread_2: queue event 1,21 FAILOVER_SET
2008-02-26 18:43:14 JST DEBUG2 remoteWorkerThread_1: Received event 1,21 FAILOVER_SET
2008-02-26 18:43:14 JST CONFIG storeSet: set_id=1 set_origin=3 set_comment='<unchanged>' - update set
2008-02-26 18:43:14 JST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + worker signaled)
2008-02-26 18:43:14 JST DEBUG2 sched_wakeup_node(): no_id=3 (0 threads + worker signaled)
2008-02-26 18:43:14 JST DEBUG2 remoteWorkerThread_2: forward confirm 1,21 received by 2
2008-02-26 18:43:14 JST CONFIG storeListen: li_origin=1 li_receiver=3 li_provider=1
2008-02-26 18:43:14 JST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + worker signaled)
2008-02-26 18:43:14 JST CONFIG storeListen: li_origin=2 li_receiver=3 li_provider=2
2008-02-26 18:43:14 JST DEBUG2 sched_wakeup_node(): no_id=2 (0 threads + worker signaled)
2008-02-26 18:43:14 JST DEBUG4 remoteWorkerThread_2: update provider configuration
2008-02-26 18:43:14 JST CONFIG storeListen: li_origin=1 li_receiver=3 li_provider=2
2008-02-26 18:43:14 JST DEBUG2 sched_wakeup_node(): no_id=2 (0 threads + worker signaled)
2008-02-26 18:43:14 JST DEBUG4 remoteWorkerThread_2: update provider configuration
2008-02-26 18:43:14 JST CONFIG storeListen: li_origin=2 li_receiver=3 li_provider=1
2008-02-26 18:43:14 JST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + worker signaled)
2008-02-26 18:43:14 JST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + worker signaled)
2008-02-26 18:43:14 JST DEBUG2 sched_wakeup_node(): no_id=2 (0 threads + worker signaled)
2008-02-26 18:43:14 JST DEBUG4 remoteWorkerThread_2: update provider configuration
2008-02-26 18:43:14 JST DEBUG4 remoteWorkerThread_1: update provider configuration
2008-02-26 18:43:14 JST DEBUG1 remoteWorkerThread_1: helper thread for provider 2 terminated
2008-02-26 18:43:14 JST DEBUG1 remoteWorkerThread_1: disconnecting from data provider 2
2008-02-26 18:43:16 JST DEBUG2 remoteListenThread_2: queue event 2,7 SYNC
2008-02-26 18:43:16 JST DEBUG2 remoteWorkerThread_2: Received event 2,7 SYNC
2008-02-26 18:43:16 JST DEBUG3 calc sync size - last time: 1 last length: 4010 ideal: 14 proposed size: 3
2008-02-26 18:43:16 JST DEBUG2 remoteWorkerThread_2: SYNC 7 processing
2008-02-26 18:43:16 JST DEBUG2 remoteWorkerThread_2: no sets need syncing for this event
2008-02-26 18:43:16 JST DEBUG2 remoteWorkerThread_2: forward confirm 3,8 received by 1
2008-02-26 18:43:18 JST DEBUG2 localListenThread: Received event 3,7 SYNC
2008-02-26 18:43:18 JST DEBUG2 localListenThread: Received event 3,8 ACCEPT_SET
2008-02-26 18:43:18 JST DEBUG2 localListenThread: ACCEPT_SET
2008-02-26 18:43:18 JST CONFIG storeListen: li_origin=1 li_receiver=3 li_provider=1
2008-02-26 18:43:18 JST DEBUG4 remoteWorkerThread_1: update provider configuration
2008-02-26 18:43:18 JST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + worker signaled)
2008-02-26 18:43:18 JST CONFIG storeListen: li_origin=2 li_receiver=3 li_provider=2
2008-02-26 18:43:18 JST DEBUG2 sched_wakeup_node(): no_id=2 (0 threads + worker signaled)
2008-02-26 18:43:18 JST DEBUG4 remoteWorkerThread_2: update provider configuration
2008-02-26 18:43:18 JST CONFIG storeListen: li_origin=1 li_receiver=3 li_provider=2
2008-02-26 18:43:18 JST DEBUG2 sched_wakeup_node(): no_id=2 (0 threads + worker signaled)
2008-02-26 18:43:18 JST DEBUG4 remoteWorkerThread_2: update provider configuration
2008-02-26 18:43:18 JST CONFIG storeListen: li_origin=2 li_receiver=3 li_provider=1
2008-02-26 18:43:18 JST DEBUG4 remoteWorkerThread_1: update provider configuration

-- 
SRA OSS.Inc.Japan
Yoshiharu Mori <y-mori@sraoss.co.jp>
From troy at troywolf.com  Tue Feb 26 13:12:44 2008
From: troy at troywolf.com (Troy Wolf)
Date: Tue Feb 26 13:12:57 2008
Subject: [Slony1-general] Slony locks tables that are not even in
	replication sets?
Message-ID: <e0d7c3f50802261312s64de041ardc83ae3e804698b6@mail.gmail.com>

I have some more data that may shed some light for you gurus.

To recap, we had an episode where slony replication stopped--that is,
no SYNC events were being processed. At that time we ran a couple
queries to try to find out why. We did a select on pg_locks, and
unfortunately, as previously explained, I don't have that data now,
but will not make that mistake the next time. We also ran this query:

select * from pg_stat_activity where current_query != '<IDLE>';

I do have the results from that, and it shows at least the time
sequence of the activity. Based on this, it appears that Slony is
waiting for an index creation to complete. The rows below have been
cleaned to protect the innocent.

Process ID  | User      | Waiting   | Query Start           | Query
-------------------------------------------------------------------------------------------------------------------------
407         | postgres  | t         | (null)                | ANALYZE
myapp.a_large_app_table
27479       | appusr    | f         | 2008-02-21 19:43:02   | create
index a_large_app_table_ak1 on myapp.a_large_app_table(col1,col2)
tablespace my_table_space
7061        | slony     | t         | 2008-02-21 19:43:16   | listen
"_slony_Event";
7073        | slony     | t         | 2008-02-21 19:43:18   | commit
transaction;
7071        | slony     | t         | 2008-02-21 19:43:25   | notify
"_slony_Event"; notify "_slony_Confirm"; insert into
"_slony".sl_event(ev_origin,...
28045       | appusr    | t         | 2008-02-21 19:43:41   | commit
7072        | slony     | t         | 2008-02-21 19:47:45   | select
"_slony".cleanupEvent();

Important things to note -- I only have a single replication set, and
"a_large_app_table" is not in it. This large table does not have any
foreign key relationships to any other tables. Therefore, I do not
expect an index being created on this table--even if it takes days to
generate--would block slony. On top of that, Slony did not have any
data to replicate, so it was only doing it's bare minimum SYNC
logging, and apparently, a cleanupEvent(). Our attention was drawn to
this for two reasons. First, we have a "health monitoring" batch
script that simply checks the sl_event table for recent SYNCs and
alerts us if they do not exist. Second, process_id 28045 was blocked
waiting to commit some inserts, and that app has a health check that
alerted us. The table the app is trying to insert into is also not in
the replication set nor does it have any relationships. So we can't
imagine why it would be blocked by Slony or the index creation. That
app has never been blocked by anything before Slony, so the
off-the-top assumption is that it was blocked by Slony, but why would
Slony be blocked by an index creation on an unrelated table?

I don't know what else the data could suggest. This has happened
twice. The next time, I'll be better prepared to collect data.
From y-mori at sraoss.co.jp  Wed Feb 27 00:03:49 2008
From: y-mori at sraoss.co.jp (Yoshiharu Mori)
Date: Wed Feb 27 00:04:11 2008
Subject: [Slony1-general] failover problems with 3 nodes
In-Reply-To: <20080226230734.de0f09d5.y-mori@sraoss.co.jp>
References: <20080226230734.de0f09d5.y-mori@sraoss.co.jp>
Message-ID: <20080227170349.c9c17a42.y-mori@sraoss.co.jp>

> Hello.
> 
> I am using 3 nodes (2 slaves subscribed to one master).
> Replication is fine, but FAILOVER command don't work correctly.
> 
> I do FAILOVER from "node 1 (master)" to "node 3" as following.
> 
>     failover ( id = 1, backup node = 3);
> 
> I could confirm that node 3 was master and INSERT tupples 
> updated to the slave (node 2) 
> 
> However I couldn't FAILOVER from "node 1 (master)" to "node 2".
> 
>     failover ( id = 1, backup node = 2);
> 
> INSERT tupples didn't updated to the slave ( node 3).
> 
> 
> I use postgresql-8.2.6 and slony-I 1.2.12 (Because of xxid.*.sql problems, I couldn't setup the slony-I-1.2.13--;).
> 
> Any suggestion?

I had checked the version diffrences.

---
SUCCESS FAILOVER from node 1 to node 2
---
slony-I 1.2.11

---
FAIL  FAILOVER from node 1 to node 2
---
slony-I 1.2.12
slony-I 1.2.13
slony-I 1.2.13 CVS-HEAD
-- 
SRA OSS,Inc.Japan
Yoshiharu Mori <y-mori@sraoss.co.jp>
http://www.sraoss.co.jp/

From liobod.slony at gmail.com  Wed Feb 27 00:57:17 2008
From: liobod.slony at gmail.com (lio bod)
Date: Wed Feb 27 00:57:40 2008
Subject: [Slony1-general] slonik_uninstall_nodes unsafe ?
Message-ID: <d4f444290802270057pb3baf8blcad0050167c2a952@mail.gmail.com>

Hello world,

I'm trying some procedures to modify a cluster : my need is to change the
slave host (same base, same name, same model...) for a given master base
master host.

I had a look on slonik_uninstall_nodes command and i wonder why the doc (
http://www.slony.info/documentation/adminscripts.html) tells it is 'a
*VERY*unsafe script'

I'd like to know why it's supposed to be so unsafe?

 Any better practice?
Any best practice?

Thx,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080227/=
a23f9bdf/attachment.htm
From stephane.schildknecht at postgresqlfr.org  Wed Feb 27 01:25:29 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Wed Feb 27 01:25:54 2008
Subject: [Slony1-general] failover problems with 3 nodes
In-Reply-To: <20080227170349.c9c17a42.y-mori@sraoss.co.jp>
References: <20080226230734.de0f09d5.y-mori@sraoss.co.jp>
	<20080227170349.c9c17a42.y-mori@sraoss.co.jp>
Message-ID: <47C52C89.7080307@postgresqlfr.org>

Yoshiharu Mori a ?crit :
>> Hello.
>>
>> I am using 3 nodes (2 slaves subscribed to one master).
>> Replication is fine, but FAILOVER command don't work correctly.
>>
>> I do FAILOVER from "node 1 (master)" to "node 3" as following.
>>
>>     failover ( id = 1, backup node = 3);
>>
>> I could confirm that node 3 was master and INSERT tupples 
>> updated to the slave (node 2) 
>>
>> However I couldn't FAILOVER from "node 1 (master)" to "node 2".
>>
>>     failover ( id = 1, backup node = 2);
>>
>> INSERT tupples didn't updated to the slave ( node 3).
>>
>>
>> I use postgresql-8.2.6 and slony-I 1.2.12 (Because of xxid.*.sql problems, I couldn't setup the slony-I-1.2.13--;).
>>
>> Any suggestion?
> 
> I had checked the version diffrences.
> 
> ---
> SUCCESS FAILOVER from node 1 to node 2
> ---
> slony-I 1.2.11
> 
> ---
> FAIL  FAILOVER from node 1 to node 2
> ---
> slony-I 1.2.12
> slony-I 1.2.13
> slony-I 1.2.13 CVS-HEAD

How could it ever work ? AFAIK, you can't have different slony versions on
nodes involved in the same replication network.

Regards,

-- 
St?phane SCHILDKNECHT
Pr?sident de PostgreSQLFr
T?l. 09 53 69 97 12
http://www.postgresqlfr.org

From stephane.schildknecht at postgresqlfr.org  Wed Feb 27 01:32:59 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Wed Feb 27 01:33:22 2008
Subject: [Slony1-general] slonik_uninstall_nodes unsafe ?
In-Reply-To: <d4f444290802270057pb3baf8blcad0050167c2a952@mail.gmail.com>
References: <d4f444290802270057pb3baf8blcad0050167c2a952@mail.gmail.com>
Message-ID: <47C52E4B.1010401@postgresqlfr.org>

lio bod a ?crit :
> Hello world,
>  
> I'm trying some procedures to modify a cluster : my need is to change
> the slave host (same base, same name, same model...) for a given master
> base master host.
>  
> I had a look on slonik_uninstall_nodes command and i wonder why the doc
> (http://www.slony.info/documentation/adminscripts.html) tells it is 'a
> /VERY/ unsafe script'
>  
> I'd like to know why it's supposed to be so unsafe?
>  
> Any better practice?
> Any best practice?
>  
> Thx,

It is unsafe because it drops all replication information from all nodes
involved. You then lose your replication network in a single operation. Be sure
it won't break your databases, but you will lose your replication.

If your need is to add a new slave and drop the old one, you'd better have a
look at bare-metal functions.

something like "select _INSTANCE.uninstallnode()" on the old slave should do
the work. You could also execute some drop node procedure on a "specific" node,
which is not, what the perl script you invoked does.

You could have a look at slonik_drop_node also. You just should verify twice
you will execute that script against the right node :-)

Regards,

-- 
St?phane SCHILDKNECHT
Pr?sident de PostgreSQLFr
T?l. 09 53 69 97 12
http://www.postgresqlfr.org
From sebastien at lardiere.net  Wed Feb 27 01:53:11 2008
From: sebastien at lardiere.net (Sebastien Lardiere)
Date: Wed Feb 27 01:53:34 2008
Subject: [Slony1-general] Failover problem
Message-ID: <59857555e731dd4ffc215a665a0646f9@80.247.230.89>


Hello 

I'm testing a setup with Pg 8.3.0 and Slony 1.2.13 on RHEL4.

I've got setup 3 box, and run Slony. Everything is ok (replication, move set, drop node, store node, ...) , until i try a failover. 

The script is : 

#!/bin/sh
slonik <<_EOF_
cluster name = foo_repl;
node 1 admin conninfo = 'dbname=bar host=10.99.29.38 user=postgres';
node 2 admin conninfo = 'dbname=bar host=10.99.29.49 user=postgres';
node 3 admin conninfo = 'dbname=bar host=10.99.29.120 user=postgres';

#failover (id = 2, backup node = 1);

_EOF_

And the output is : 

[root@foo01 ~]# ./slonik_failover.sh
<stdin>:7: NOTICE:  failedNode: set 1 has other direct receivers - change providers only
<stdin>:7: PGRES_FATAL_ERROR select "_qsr_repl".failedNode(2, 1);  - ERROR:  null value in column "li_provider" violates not-null constraint
CONTEXT:  SQL statement "INSERT INTO "_qsr_repl".sl_listen (li_origin, li_provider, li_receiver) select distinct set_origin, sub_provider,  $1  from "_qsr_repl".sl_set, "_qsr_repl".sl_subscribe where set_origin =  $2  and sub_set = set_id and sub_receiver =  $3  and sub_active"
PL/pgSQL function "rebuildlistenentries" line 75 at SQL statement
SQL statement "SELECT  "_qsr_repl".RebuildListenEntries()"
PL/pgSQL function "failednode" line 155 at PERFORM

I debug this pl and see that slony try to use node 2 ( old master in failover ), and i think it's not correct. Nothing is done, and, if i re-activate the old master, replication work. 

I try to switch over with move set, it's ok, but a new failover with the new master failed with the same error. 

I unable to reproduce this bug on Debian boxes, with the same version of slony and PostgreSQL : it works !

WTF ?

--
S?bastien 

From sebastien at lardiere.net  Wed Feb 27 02:00:16 2008
From: sebastien at lardiere.net (Sebastien Lardiere)
Date: Wed Feb 27 02:00:40 2008
Subject: [Slony1-general] Failover problem
In-Reply-To: <59857555e731dd4ffc215a665a0646f9@80.247.230.89>
References: <59857555e731dd4ffc215a665a0646f9@80.247.230.89>
Message-ID: <fe4748ae4cc6f7bc697af4007216a819@80.247.230.89>


> 
> #failover (id = 2, backup node = 1);

It's a mistake in the mail, this line is uncomment in the script ;)

--
S?bastien

From stephane.schildknecht at postgresqlfr.org  Wed Feb 27 02:06:01 2008
From: stephane.schildknecht at postgresqlfr.org (=?UTF-8?B?IlN0w6lwaGFuZSBBLiBTY2hpbGRrbmVjaHQi?=)
Date: Wed Feb 27 02:06:26 2008
Subject: [Slony1-general] Failover problem
In-Reply-To: <59857555e731dd4ffc215a665a0646f9@80.247.230.89>
References: <59857555e731dd4ffc215a665a0646f9@80.247.230.89>
Message-ID: <47C53609.60903@postgresqlfr.org>

Sebastien Lardiere a ?crit :
> Hello 
> 
> I'm testing a setup with Pg 8.3.0 and Slony 1.2.13 on RHEL4.
> 
(...)

Hello,

Could you give us the scheme of your replication network ?

Which node is master, is replication in L or in Y ?

Regards,

-- 
St?phane SCHILDKNECHT
Pr?sident de PostgreSQLFr
T?l. 09 53 69 97 12
http://www.postgresqlfr.org

From sebastien at lardiere.net  Wed Feb 27 02:19:10 2008
From: sebastien at lardiere.net (Sebastien Lardiere)
Date: Wed Feb 27 02:19:35 2008
Subject: [Slony1-general] Failover problem
In-Reply-To: <47C53609.60903@postgresqlfr.org>
References: <47C53609.60903@postgresqlfr.org>
Message-ID: <d98ae9a64c6097017c7c502fc2e32bba@80.247.230.89>


On Wed, 27 Feb 2008 11:06:01 +0100, "St?phane A. Schildknecht"
<stephane.schildknecht@postgresqlfr.org> wrote:
> Sebastien Lardiere a ?crit :
>> Hello
>>
>> I'm testing a setup with Pg 8.3.0 and Slony 1.2.13 on RHEL4.
>>
> (...)
> 
> Hello,
> 
> Could you give us the scheme of your replication network ?
> 
> Which node is master, is replication in L or in Y ?
> 

1 is master, 2 and 3 are slave, so in Y. I test "move set", with 2 in master and 3 and 1 in slave, also in Y, and it's the same.

But I "drop node" the third, and failover work, with only 2 node ! 


--
S?bastien

From liobod.slony at gmail.com  Wed Feb 27 02:20:27 2008
From: liobod.slony at gmail.com (lio bod)
Date: Wed Feb 27 02:20:51 2008
Subject: [Slony1-general] slonik_uninstall_nodes unsafe ?
In-Reply-To: <47C52E4B.1010401@postgresqlfr.org>
References: <d4f444290802270057pb3baf8blcad0050167c2a952@mail.gmail.com>
	<47C52E4B.1010401@postgresqlfr.org>
Message-ID: <d4f444290802270220y62c35737w9b62e9620920a0ca@mail.gmail.com>

You guess right : my purpose is precisly to add a new slave and drop the old
one.
Of course, i stop replication process before performing so.
What are the kind of 'all replication information ' i can loose? So far, i
can't see what it implies...

My first step would to retrieve my primary dabase just as before it was
instrumented by slony. So i can have a 'virgin dump' and restore it in any
not replicated architecture.
My second step would be to restart on entirely new replication with a new
configuration by creating a new cluster and so on.

Btw, on my opinion, bare-metal functions are not really industrial. I have
to supply an integrated solution to a system operator.
I would rather think of using perl commands just by switching through
differents slon_tool.conf.
Any comments on this appoach?

thanks.


2008/2/27, "St=E9phane A. Schildknecht" <
stephane.schildknecht@postgresqlfr.org>:
>
> lio bod a =E9crit :
> > Hello world,
> >
> > I'm trying some procedures to modify a cluster : my need is to change
> > the slave host (same base, same name, same model...) for a given master
> > base master host.
> >
> > I had a look on slonik_uninstall_nodes command and i wonder why the doc
> > (http://www.slony.info/documentation/adminscripts.html) tells it is 'a
> > /VERY/ unsafe script'
> >
> > I'd like to know why it's supposed to be so unsafe?
> >
> > Any better practice?
> > Any best practice?
> >
> > Thx,
>
> It is unsafe because it drops all replication information from all nodes
> involved. You then lose your replication network in a single operation. Be
> sure
> it won't break your databases, but you will lose your replication.
>
> If your need is to add a new slave and drop the old one, you'd better have
> a
> look at bare-metal functions.
>
> something like "select _INSTANCE.uninstallnode()" on the old slave should
> do
> the work. You could also execute some drop node procedure on a "specific"
> node,
> which is not, what the perl script you invoked does.
>
> You could have a look at slonik_drop_node also. You just should verify
> twice
> you will execute that script against the right node :-)
>
> Regards,
>
> --
> St=E9phane SCHILDKNECHT
> Pr=E9sident de PostgreSQLFr
> T=E9l. 09 53 69 97 12
> http://www.postgresqlfr.org
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080227/=
6cc1d7d5/attachment-0001.htm
From liobod.slony at gmail.com  Wed Feb 27 02:53:53 2008
From: liobod.slony at gmail.com (lio bod)
Date: Wed Feb 27 02:54:20 2008
Subject: [Slony1-general] gmake rpm vs tarball
In-Reply-To: <1203016714.24118.245.camel@localhost.localdomain>
References: <d4f444290802130422k2bb187d4xd29649be8ab9604a@mail.gmail.com>
	<1203016714.24118.245.camel@localhost.localdomain>
Message-ID: <d4f444290802270253g14ab2cf5yc18a6c2b73f53d35@mail.gmail.com>

That's a while you answered me and i noticed i forgot to thank you.
So please consider, that's done now.

Btw, i'm not an nor an linux neither an hardwhare expert and i have a doubt
in front of the 'i686' suffix of the rpm:
Could you confirm that the rpm you suggest me will fit my box RHEL-4/386.

I know i can make just an install test but i don't want to break (once
more) my already succesful installed env ;-)

Thx,



2008/2/14, Devrim G=DCND=DCZ <devrim@commandprompt.com>:
>
> Hi,
>
> On Wed, 2008-02-13 at 13:22 +0100, lio bod wrote:
> > I tried  rpm -ivh  postgresql-slony1-1.2.12.tar.bz2
> > Does not work.
>
> Use our yum repository to install Slony-I on your RHEL-4 box:
>
> http://yum.pgsqlrpms.org/
>
>
> http://yum.pgsqlrpms.org/7.4/redhat/rhel-4-i386/slony1-1.2.12-2.rhel4.i68=
6.rpm
>
> (or there is also an 64 bit version, if you need). This is built against
> PostgreSQL 7.4, so you should not have any problems.
>
> If you don't have yum installed on your server and you have only
> up2date, there is now way to use this repository (newer repo creation
> scripts don't work with  up2date). You can install packages
> individually, of course.
>
> Regards,
>
> --
> Devrim G=DCND=DCZ , RHCE
> PostgreSQL Replication, Consulting, Custom Development, 24x7 support
> Managed Services, Shared and Dedicated Hosting
> Co-Authors: plPHP, ODBCng - http://www.commandprompt.com/
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080227/=
b279569e/attachment.htm
From y-mori at sraoss.co.jp  Wed Feb 27 03:11:29 2008
From: y-mori at sraoss.co.jp (Yoshiharu Mori)
Date: Wed Feb 27 03:11:55 2008
Subject: [Slony1-general] failover problems with 3 nodes
In-Reply-To: <47C52C89.7080307@postgresqlfr.org>
References: <20080226230734.de0f09d5.y-mori@sraoss.co.jp>
	<20080227170349.c9c17a42.y-mori@sraoss.co.jp>
	<47C52C89.7080307@postgresqlfr.org>
Message-ID: <20080227201129.27ae8860.y-mori@sraoss.co.jp>

> >> Hello.
> >>
> >> I am using 3 nodes (2 slaves subscribed to one master).
> >> Replication is fine, but FAILOVER command don't work correctly.
> >>
> >> I do FAILOVER from "node 1 (master)" to "node 3" as following.
> >>
> >>     failover ( id = 1, backup node = 3);
> >>
> >> I could confirm that node 3 was master and INSERT tupples 
> >> updated to the slave (node 2) 
> >>
> >> However I couldn't FAILOVER from "node 1 (master)" to "node 2".
> >>
> >>     failover ( id = 1, backup node = 2);
> >>
> >> INSERT tupples didn't updated to the slave ( node 3).
> >>
> >>
> >> I use postgresql-8.2.6 and slony-I 1.2.12 (Because of xxid.*.sql problems, I couldn't setup the slony-I-1.2.13--;).
> >>
> >> Any suggestion?
> > 
> > I had checked the version diffrences.
> > 
> > ---
> > SUCCESS FAILOVER from node 1 to node 2
> > ---
> > slony-I 1.2.11
> > 
> > ---
> > FAIL  FAILOVER from node 1 to node 2
> > ---
> > slony-I 1.2.12
> > slony-I 1.2.13
> > slony-I 1.2.13 CVS-HEAD
> 
> How could it ever work ? AFAIK, you can't have different slony versions on
> nodes involved in the same replication network.

Of cource,I use the same versions on nodes. 
Only slony-I 1.2.11, I could confirm failover.

Regards,

-- 
SRA OSS, Inc. Japan
Yoshiharu Mori <y-mori@sraoss.co.jp>
http://www.sraoss.co.jp/
From stephane.schildknecht at postgresqlfr.org  Wed Feb 27 03:16:11 2008
From: stephane.schildknecht at postgresqlfr.org (=?UTF-8?B?IlN0w6lwaGFuZSBBLiBTY2hpbGRrbmVjaHQi?=)
Date: Wed Feb 27 03:16:37 2008
Subject: [Slony1-general] Failover problem
In-Reply-To: <d98ae9a64c6097017c7c502fc2e32bba@80.247.230.89>
References: <47C53609.60903@postgresqlfr.org>
	<d98ae9a64c6097017c7c502fc2e32bba@80.247.230.89>
Message-ID: <47C5467B.3050406@postgresqlfr.org>

Sebastien Lardiere a ?crit :
> On Wed, 27 Feb 2008 11:06:01 +0100, "St?phane A. Schildknecht"
> <stephane.schildknecht@postgresqlfr.org> wrote:
>> Sebastien Lardiere a ?crit :
>>> Hello
>>>
>>> I'm testing a setup with Pg 8.3.0 and Slony 1.2.13 on RHEL4.
>>>
>> (...)
>>
>> Hello,
>>
>> Could you give us the scheme of your replication network ?
>>
>> Which node is master, is replication in L or in Y ?
>>
> 
> 1 is master, 2 and 3 are slave, so in Y. I test "move set", with 2 in master and 3 and 1 in slave, also in Y, and it's the same.
> 
> But I "drop node" the third, and failover work, with only 2 node ! 

Regarding documentation and my experience, you should be able to failover in Y
also.

If you have
  1
 / \
2   3

and execute failover( id=1, backup node=2), you should get
 2
 |
 3

What do you have in sl_subscribe table before and after failover ?

SAS
From stephane.schildknecht at postgresqlfr.org  Wed Feb 27 03:18:22 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Wed Feb 27 03:18:48 2008
Subject: [Slony1-general] failover problems with 3 nodes
In-Reply-To: <20080227201129.27ae8860.y-mori@sraoss.co.jp>
References: <20080226230734.de0f09d5.y-mori@sraoss.co.jp>	<20080227170349.c9c17a42.y-mori@sraoss.co.jp>	<47C52C89.7080307@postgresqlfr.org>
	<20080227201129.27ae8860.y-mori@sraoss.co.jp>
Message-ID: <47C546FE.5090706@postgresqlfr.org>

Yoshiharu Mori a ?crit :
>>>> Hello.
>>>>
>>>> I am using 3 nodes (2 slaves subscribed to one master).
>>>> Replication is fine, but FAILOVER command don't work correctly.
>>>>
>>>> I do FAILOVER from "node 1 (master)" to "node 3" as following.
>>>>
>>>>     failover ( id = 1, backup node = 3);
>>>>
>>>> I could confirm that node 3 was master and INSERT tupples 
>>>> updated to the slave (node 2) 
>>>>
>>>> However I couldn't FAILOVER from "node 1 (master)" to "node 2".
>>>>
>>>>     failover ( id = 1, backup node = 2);
>>>>
>>>> INSERT tupples didn't updated to the slave ( node 3).
>>>>
>>>>
>>>> I use postgresql-8.2.6 and slony-I 1.2.12 (Because of xxid.*.sql problems, I couldn't setup the slony-I-1.2.13--;).
>>>>
>>>> Any suggestion?
>>> I had checked the version diffrences.
>>>
>>> ---
>>> SUCCESS FAILOVER from node 1 to node 2
>>> ---
>>> slony-I 1.2.11
>>>
>>> ---
>>> FAIL  FAILOVER from node 1 to node 2
>>> ---
>>> slony-I 1.2.12
>>> slony-I 1.2.13
>>> slony-I 1.2.13 CVS-HEAD
>> How could it ever work ? AFAIK, you can't have different slony versions on
>> nodes involved in the same replication network.
> 
> Of cource,I use the same versions on nodes. 
> Only slony-I 1.2.11, I could confirm failover.
> 
> Regards,
> 

Oh, sorry, I misunderstood... You told us 1.2.13 has a buggy failover procedure?

Which could be related to problems S?bastien reported with failover.

Hey, I should test failover before updating to 1.2.13...

Regards,
SAS
From stephane.schildknecht at postgresqlfr.org  Wed Feb 27 03:23:45 2008
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-1?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Wed Feb 27 03:24:10 2008
Subject: [Slony1-general] slonik_uninstall_nodes unsafe ?
In-Reply-To: <d4f444290802270220y62c35737w9b62e9620920a0ca@mail.gmail.com>
References: <d4f444290802270057pb3baf8blcad0050167c2a952@mail.gmail.com>	
	<47C52E4B.1010401@postgresqlfr.org>
	<d4f444290802270220y62c35737w9b62e9620920a0ca@mail.gmail.com>
Message-ID: <47C54841.8020403@postgresqlfr.org>

lio bod a ?crit :
>  
> You guess right : my purpose is precisly to add a new slave and drop the
> old one.
> Of course, i stop replication process before performing so.
> What are the kind of 'all replication information ' i can loose? So far,
> i can't see what it implies...
>  
> My first step would to retrieve my primary dabase just as before it was
> instrumented by slony. So i can have a 'virgin dump' and restore it in
> any not replicated architecture.
> My second step would be to restart on entirely new replication with a
> new configuration by creating a new cluster and so on.

Then, OK, it would be safe for you, as you really want to clean all nodes from
slony hacking.

>  
> Btw, on my opinion, bare-metal functions are not really industrial. I
> have to supply an integrated solution to a system operator.
> I would rather think of using perl commands just by switching through
> differents slon_tool.conf.
> Any comments on this appoach?

Yes... I never used perl comands because the first time I wanted to use them,
they were buggy, and I din't know perl well enough to be able to repair them.
What's more, these scriptts (as far as I understood them) didn't let me do all
I wanted to (cross-replication, multiple replication from different nodes to a
single spare one...). That's why I used slonik function embedded in shell
scripts I wrote. So now, I have 2 or 3 configuration files and some 10 shell
scripts that allow me to do almost everythng.

That was, in my opinion, the best industrial way of acting.

Regards,
-- 
St?phane SCHILDKNECHT
Pr?sident de PostgreSQLFr
T?l. 09 53 69 97 12
http://www.postgresqlfr.org
From mike at openbunker.org  Wed Feb 27 03:29:17 2008
From: mike at openbunker.org (Mikhail Kolesnik)
Date: Wed Feb 27 03:30:00 2008
Subject: [Slony1-general] failover problems with 3 nodes
In-Reply-To: <47C546FE.5090706@postgresqlfr.org>
References: <20080226230734.de0f09d5.y-mori@sraoss.co.jp>
	<20080227170349.c9c17a42.y-mori@sraoss.co.jp>
	<47C52C89.7080307@postgresqlfr.org>
	<20080227201129.27ae8860.y-mori@sraoss.co.jp>
	<47C546FE.5090706@postgresqlfr.org>
Message-ID: <20080227132917.4489327c@amilo.home>

Hello, St?phane.

On Wed, 27 Feb 2008 12:18:22 +0100
"St?phane A. Schildknecht" <stephane.schildknecht@postgresqlfr.org>
wrote:

> [...]
> Hey, I should test failover before updating to 1.2.13...

I have some strange periodic problems with 'ACCEPT_SET - MOVE_SET or
FAILOVER_SET not received yet - sleep' on 1.2.12 and 1.2.13. Looks
similar to this one.

I should try to downgrade to 1.2.11 and try if my 'move set' problems
will disappear. Here is the initial problem description:
http://lists.slony.info/pipermail/slony1-general/2008-February/007445.html

-- 
Mikhail Kolesnik
ICQ: 260259143
IRC: mike_k at freenode/#crux, rusnet/#yalta
From sebastien at lardiere.net  Wed Feb 27 03:42:53 2008
From: sebastien at lardiere.net (Sebastien Lardiere)
Date: Wed Feb 27 03:43:20 2008
Subject: [Slony1-general] Failover problem
In-Reply-To: <47C5467B.3050406@postgresqlfr.org>
References: <47C5467B.3050406@postgresqlfr.org>
Message-ID: <8f40a0235eef4167716790311804a7ff@80.247.230.89>


On Wed, 27 Feb 2008 12:16:11 +0100, "St?phane A. Schildknecht"
<stephane.schildknecht@postgresqlfr.org> wrote:
>>
>> 1 is master, 2 and 3 are slave, so in Y. I test "move set", with 2 in
> master and 3 and 1 in slave, also in Y, and it's the same.
>>
>> But I "drop node" the third, and failover work, with only 2 node !
> 
> Regarding documentation and my experience, you should be able to failover
> in Y
> also.
> 
> If you have
>   1
>  / \
> 2   3
> 
> and execute failover( id=1, backup node=2), you should get
>  2
>  |
>  3
> 
> What do you have in sl_subscribe table before and after failover ?

Yes, when it work, i agree, 

Before failover, i've got : 


bar=# select * from _qsr_repl.sl_subscribe ;
 sub_set | sub_provider | sub_receiver | sub_forward | sub_active
---------+--------------+--------------+-------------+------------
       1 |            1 |            2 | f           | t
       1 |            1 |            3 | f           | t
(2 rows)

And after too, because failover fail, and so nothing is done. 



--
S?bastien

From stephane.schildknecht at postgresqlfr.org  Wed Feb 27 03:53:35 2008
From: stephane.schildknecht at postgresqlfr.org (=?UTF-8?B?IlN0w6lwaGFuZSBBLiBTY2hpbGRrbmVjaHQi?=)
Date: Wed Feb 27 03:54:02 2008
Subject: [Slony1-general] Failover problem
In-Reply-To: <8f40a0235eef4167716790311804a7ff@80.247.230.89>
References: <47C5467B.3050406@postgresqlfr.org>
	<8f40a0235eef4167716790311804a7ff@80.247.230.89>
Message-ID: <47C54F3F.80601@postgresqlfr.org>

Sebastien Lardiere a ?crit :
> On Wed, 27 Feb 2008 12:16:11 +0100, "St?phane A. Schildknecht"
> <stephane.schildknecht@postgresqlfr.org> wrote:
>>> 1 is master, 2 and 3 are slave, so in Y. I test "move set", with 2 in
>> master and 3 and 1 in slave, also in Y, and it's the same.
>>> But I "drop node" the third, and failover work, with only 2 node !
>> Regarding documentation and my experience, you should be able to failover
>> in Y
>> also.
>>
>> If you have
>>   1
>>  / \
>> 2   3
>>
>> and execute failover( id=1, backup node=2), you should get
>>  2
>>  |
>>  3
>>
>> What do you have in sl_subscribe table before and after failover ?
> 
> Yes, when it work, i agree, 
> 
> Before failover, i've got : 
> 
> 
> bar=# select * from _qsr_repl.sl_subscribe ;
>  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
> ---------+--------------+--------------+-------------+------------
>        1 |            1 |            2 | f           | t
>        1 |            1 |            3 | f           | t
> (2 rows)
> 
> And after too, because failover fail, and so nothing is done. 


Shouldn't node 2 and/or 3 be forwarder if you want to use one of them as a master ?


-- 
St?phane SCHILDKNECHT
Pr?sident de PostgreSQLFr
T?l. 09 53 69 97 12
http://www.postgresqlfr.org
From sebastien at lardiere.net  Wed Feb 27 04:30:05 2008
From: sebastien at lardiere.net (Sebastien Lardiere)
Date: Wed Feb 27 04:30:33 2008
Subject: [Slony1-general] Failover problem
In-Reply-To: <47C54F3F.80601@postgresqlfr.org>
References: <47C54F3F.80601@postgresqlfr.org>
Message-ID: <84b844506fdff41d1f811a958e3695f4@80.247.230.89>


On Wed, 27 Feb 2008 12:53:35 +0100, "St?phane A. Schildknecht"
<stephane.schildknecht@postgresqlfr.org> wrote:
>>
>> Before failover, i've got :
>>
>>
>> bar=# select * from _qsr_repl.sl_subscribe ;
>>  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
>> ---------+--------------+--------------+-------------+------------
>>        1 |            1 |            2 | f           | t
>>        1 |            1 |            3 | f           | t
>> (2 rows)
>>
>> And after too, because failover fail, and so nothing is done.
> 
> 
> Shouldn't node 2 and/or 3 be forwarder if you want to use one of them as a
> master ?

You're right, i enable forward for 2 and 3, and then failover work. 

On 2, it's ok : 

infoctr=# select * from _qsr_repl.sl_subscribe ;
 sub_set | sub_provider | sub_receiver | sub_forward | sub_active
---------+--------------+--------------+-------------+------------
       1 |            2 |            3 | t           | t
(1 row)


But on 3 , I've got a problem : 

infoctr=# SELECT * from _qsr_repl.sl_subscribe ;
 sub_set | sub_provider | sub_receiver | sub_forward | sub_active
---------+--------------+--------------+-------------+------------
       1 |            3 |            2 | t           | t
       1 |            2 |            3 | t           | t

And replication doesn't work anymore


--
S?bastien

From sebastien at lardiere.net  Wed Feb 27 04:55:42 2008
From: sebastien at lardiere.net (Sebastien Lardiere)
Date: Wed Feb 27 04:56:09 2008
Subject: [Slony1-general] Failover problem
In-Reply-To: <84b844506fdff41d1f811a958e3695f4@80.247.230.89>
References: <84b844506fdff41d1f811a958e3695f4@80.247.230.89>
Message-ID: <528c40fff13169c0dc5222be998ba836@80.247.230.89>


On Wed, 27 Feb 2008 13:30:05 +0100, Sebastien Lardiere <sebastien@lardiere.net> wrote:
> 
> On Wed, 27 Feb 2008 12:53:35 +0100, "St?phane A. Schildknecht"
> <stephane.schildknecht@postgresqlfr.org> wrote:
>>>
>>> Before failover, i've got :
>>>
>>>
>>> bar=# select * from _qsr_repl.sl_subscribe ;
>>>  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
>>> ---------+--------------+--------------+-------------+------------
>>>        1 |            1 |            2 | f           | t
>>>        1 |            1 |            3 | f           | t
>>> (2 rows)
>>>
>>> And after too, because failover fail, and so nothing is done.
>>
>>
>> Shouldn't node 2 and/or 3 be forwarder if you want to use one of them as
> a
>> master ?
> 
> You're right, i enable forward for 2 and 3, and then failover work.
> 
> On 2, it's ok :
> 
> infoctr=# select * from _qsr_repl.sl_subscribe ;
>  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
> ---------+--------------+--------------+-------------+------------
>        1 |            2 |            3 | t           | t
> (1 row)
> 
> 
> But on 3 , I've got a problem :
> 
> infoctr=# SELECT * from _qsr_repl.sl_subscribe ;
>  sub_set | sub_provider | sub_receiver | sub_forward | sub_active
> ---------+--------------+--------------+-------------+------------
>        1 |            3 |            2 | t           | t
>        1 |            2 |            3 | t           | t
> 
> And replication doesn't work anymore
> 

Well, i think that node 3 doesn't apply "drop node" about node 1, so always know node1. 

Node 2, the new master, doesn't have any info about node 1 anymore ...


--
S?bastien 

From liobod.slony at gmail.com  Wed Feb 27 05:51:24 2008
From: liobod.slony at gmail.com (lio bod)
Date: Wed Feb 27 05:51:31 2008
Subject: [Slony1-general] magic slon_start 2
Message-ID: <d4f444290802270551x2b1556b8o3eb3fb9803ba6690@mail.gmail.com>

Hello world,

I got a metaphysical question.;-)
In fact, it's more an internal architeture slony question :

I replicate my first db following documenation ' 2. Replicating Your First
Database' (http://slony.info/documentation/firstdb.html)
This sample is not very precise on what command should be performed on
master or slave host.
I tried 'slon_start 2' on master host. I was quite sure it would failed.
Nope. I'm surprised : my replication seems to be ok. Magic!

What would be a good recommandation?
Run 'slon_start 2' on master or on salve host?

thx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080227/=
07c073a7/attachment.htm
From sebastien at lardiere.net  Wed Feb 27 07:49:25 2008
From: sebastien at lardiere.net (Sebastien Lardiere)
Date: Wed Feb 27 07:49:31 2008
Subject: [Slony1-general] Failover problem
In-Reply-To: <528c40fff13169c0dc5222be998ba836@80.247.230.89>
References: <528c40fff13169c0dc5222be998ba836@80.247.230.89>
Message-ID: <44739d4e169b1390728ced39b9e3561e@80.247.230.89>


On Wed, 27 Feb 2008 13:55:42 +0100, Sebastien Lardiere <sebastien@lardiere.net> wrote:
> 
>>> Shouldn't node 2 and/or 3 be forwarder if you want to use one of them
> as
>> a
>>> master ?
>>
>> You're right, i enable forward for 2 and 3, and then failover work.
>>

Ok, it work, and i understand why i've got some error : 

- I have to set all direct receiver with forward = yes

- When I do failover, i have to wait before send "drop node" for remover the old master.

Perhaps somewhere in the documentation, it could be write that in case of failover, future master and direct receiver have to be set with forward = yes ? 

--
S?bastien


From ajs at crankycanuck.ca  Wed Feb 27 07:51:44 2008
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Feb 27 07:51:57 2008
Subject: [Slony1-general] magic slon_start 2
In-Reply-To: <d4f444290802270551x2b1556b8o3eb3fb9803ba6690@mail.gmail.com>
References: <d4f444290802270551x2b1556b8o3eb3fb9803ba6690@mail.gmail.com>
Message-ID: <20080227155143.GB7920@crankycanuck.ca>

On Wed, Feb 27, 2008 at 02:51:24PM +0100, lio bod wrote:
> 
> What would be a good recommandation?
> Run 'slon_start 2' on master or on salve host?

The slon daemons can actually run on _any_ host, as long as they can talk to
all the nodes in the replication cluster.

A
From cedric.villemain at dalibo.com  Wed Feb 27 07:54:57 2008
From: cedric.villemain at dalibo.com (=?UTF-8?B?Q8OpZHJpYyBWaWxsZW1haW4=?=)
Date: Wed Feb 27 07:55:04 2008
Subject: [Slony1-general] Failover problem
In-Reply-To: <44739d4e169b1390728ced39b9e3561e@80.247.230.89>
References: <528c40fff13169c0dc5222be998ba836@80.247.230.89>
	<44739d4e169b1390728ced39b9e3561e@80.247.230.89>
Message-ID: <47C587D1.7040204@dalibo.com>

Sebastien Lardiere a ?crit :
> On Wed, 27 Feb 2008 13:55:42 +0100, Sebastien Lardiere <sebastien@lardiere.net> wrote:
>   
>>>> Shouldn't node 2 and/or 3 be forwarder if you want to use one of them
>>>>         
>> as
>>     
>>> a
>>>       
>>>> master ?
>>>>         
>>> You're right, i enable forward for 2 and 3, and then failover work.
>>>
>>>       
>
> Ok, it work, and i understand why i've got some error : 
>
> - I have to set all direct receiver with forward = yes
>
> - When I do failover, i have to wait before send "drop node" for remover the old master.
>
> Perhaps somewhere in the documentation, it could be write that in case of failover, future master and direct receiver have to be set with forward = yes ? 
>   

isn't it write somewhere ?

In the future, it is possible that every node will be forwarder by 
default (isn't it ? )

> --
> S?bastien
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>   


-- 
C?dric Villemain
Administrateur de Base de Donn?es
Cel: +33 (0)6 74 15 56 53
http://dalibo.com - http://dalibo.org

From stephane.schildknecht at postgresqlfr.org  Wed Feb 27 07:55:19 2008
From: stephane.schildknecht at postgresqlfr.org (=?UTF-8?B?IlN0w6lwaGFuZSBBLiBTY2hpbGRrbmVjaHQi?=)
Date: Wed Feb 27 07:55:24 2008
Subject: [Slony1-general] Failover problem
In-Reply-To: <44739d4e169b1390728ced39b9e3561e@80.247.230.89>
References: <528c40fff13169c0dc5222be998ba836@80.247.230.89>
	<44739d4e169b1390728ced39b9e3561e@80.247.230.89>
Message-ID: <47C587E7.80106@postgresqlfr.org>

Sebastien Lardiere a ?crit :
> On Wed, 27 Feb 2008 13:55:42 +0100, Sebastien Lardiere <sebastien@lardiere.net> wrote:
>>>> Shouldn't node 2 and/or 3 be forwarder if you want to use one of them
>> as
>>> a
>>>> master ?
>>> You're right, i enable forward for 2 and 3, and then failover work.
>>>
> 
> Ok, it work, and i understand why i've got some error : 
> 
> - I have to set all direct receiver with forward = yes
> 
> - When I do failover, i have to wait before send "drop node" for remover the old master.

Did you have to explicitly drop the node ? Documentation says it is done on
every subscriber by the failover command. (so does slonik source code).

> 
> Perhaps somewhere in the documentation, it could be write that in case of failover, future master and direct receiver have to be set with forward = yes ? 
> 

It's explained here :

http://slony.info/adminguide/slony1-1.2.6/doc/adminguide/stmtsubscribeset.html

In fact, it is said that you should turn it off if you are *certain* you will
*never* fail or switch against that node.

Best regards,

SAS
From sebastien at lardiere.net  Wed Feb 27 08:23:34 2008
From: sebastien at lardiere.net (Sebastien Lardiere)
Date: Wed Feb 27 08:23:40 2008
Subject: [Slony1-general] Failover problem
In-Reply-To: <47C587E7.80106@postgresqlfr.org>
References: <47C587E7.80106@postgresqlfr.org>
Message-ID: <a1a59b9f5561d51584fac29f4616a4a1@80.247.230.89>




On Wed, 27 Feb 2008 16:55:19 +0100, "St?phane A. Schildknecht"
<stephane.schildknecht@postgresqlfr.org> wrote:
> Sebastien Lardiere a ?crit :
>> On Wed, 27 Feb 2008 13:55:42 +0100, Sebastien Lardiere
> <sebastien@lardiere.net> wrote:
>>>>> Shouldn't node 2 and/or 3 be forwarder if you want to use one of them
>>> as
>>>> a
>>>>> master ?
>>>> You're right, i enable forward for 2 and 3, and then failover work.
>>>>
>>
>> Ok, it work, and i understand why i've got some error :
>>
>> - I have to set all direct receiver with forward = yes
>>
>> - When I do failover, i have to wait before send "drop node" for remover
> the old master.
> 
> Did you have to explicitly drop the node ? Documentation says it is done
> on
> every subscriber by the failover command. (so does slonik source code).
> 
No, in fact, the old master was not dropped after the failover. I wait and
verify that the new scheme work, then send "drop node" .

>
>>
>> Perhaps somewhere in the documentation, it could be write that in case
> of failover, future master and direct receiver have to be set with forward
> = yes ?
>>
>
> It's explained here :
>
>
http://slony.info/adminguide/slony1-1.2.6/doc/adminguide/stmtsubscribeset.html
>
> In fact, it is said that you should turn it off if you are *certain* you
> will
> *never* fail or switch against that node.

Right, now i remember why i always set forward = yes in the past ...

thanks a lot

--
S?bastien

From liobod.slony at gmail.com  Wed Feb 27 08:33:40 2008
From: liobod.slony at gmail.com (lio bod)
Date: Wed Feb 27 08:33:48 2008
Subject: [Slony1-general] magic slon_start 2
In-Reply-To: <20080227155143.GB7920@crankycanuck.ca>
References: <d4f444290802270551x2b1556b8o3eb3fb9803ba6690@mail.gmail.com>
	<20080227155143.GB7920@crankycanuck.ca>
Message-ID: <d4f444290802270833o7d3c920h9ffc9c1773673048@mail.gmail.com>

understood so.
magic!

rgds,


2008/2/27, Andrew Sullivan <ajs@crankycanuck.ca>:
>
> On Wed, Feb 27, 2008 at 02:51:24PM +0100, lio bod wrote:
> >
> > What would be a good recommandation?
> > Run 'slon_start 2' on master or on salve host?
>
> The slon daemons can actually run on _any_ host, as long as they can talk
> to
> all the nodes in the replication cluster.
>
> A
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080227/=
645b0646/attachment.htm
From cbbrowne at ca.afilias.info  Wed Feb 27 08:44:01 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Feb 27 08:44:08 2008
Subject: [Slony1-general] slonik_uninstall_nodes unsafe ?
In-Reply-To: <d4f444290802270057pb3baf8blcad0050167c2a952@mail.gmail.com> (lio
	bod's message of "Wed, 27 Feb 2008 09:57:17 +0100")
References: <d4f444290802270057pb3baf8blcad0050167c2a952@mail.gmail.com>
Message-ID: <60wsoqe4dq.fsf@dba2.int.libertyrms.com>

"lio bod" <liobod.slony@gmail.com> writes:
> Hello world,
>
> ?
>
> I'm trying some procedures to modify a cluster : my need is to change the slave host (same base, same name, same model...) for a given master base master host.
>
> ?
>
> I had a look on slonik_uninstall_nodes command and i wonder why the doc (http://www.slony.info/documentation/adminscripts.html)?tells it is?'a VERY unsafe script'
>
> ?
>
> I'd like to know why it's supposed to be so unsafe?
>
> Any?better practice?
>
> Any?best practice?

The point is that this script will remove replication from your entire
cluster in one fell swoop.

If you run this unintentionally, it could be a Rather Bad Thing.

I recall a case back in 2004 where I did something similar, and it
went badly wrong; I had two replication sets, set #1 (consisting of
most of the data for one of our registries), and set #2 (where I was
trying to add in some more tables).

I "fat-fingered" a script, and did:
  slonik_drop_set set1
rather than
  slonik_drop_set set2

which meant I had to reconstruct replication of most of the data
across the whole cluster.

Based on your (not quite specific enough) description, I don't think
you likely need (or want) to drop out replication and start over; I
would think that you should be able to accomplish what you're trying
to do by issuing some SUBSCRIBE SET requests to change the shape of
the "tree" of replication configuration so that the new slave points
to the master, and could then DROP NODE to get rid of a now-unused
node.

Doing things that way means that you never lose the protections that
replication is supposed to provide.
-- 
output = ("cbbrowne" "@" "cbbrowne.com")
http://linuxfinances.info/info/rdbms.html
"Recursion is the root of computation since it trades description for time."
-- Alan J. Perlis
From stephane.schildknecht at postgresqlfr.org  Wed Feb 27 08:48:20 2008
From: stephane.schildknecht at postgresqlfr.org (=?UTF-8?B?IlN0w6lwaGFuZSBBLiBTY2hpbGRrbmVjaHQi?=)
Date: Wed Feb 27 08:48:27 2008
Subject: [Slony1-general] Failover problem
In-Reply-To: <a1a59b9f5561d51584fac29f4616a4a1@80.247.230.89>
References: <47C587E7.80106@postgresqlfr.org>
	<a1a59b9f5561d51584fac29f4616a4a1@80.247.230.89>
Message-ID: <47C59454.2080100@postgresqlfr.org>

Sebastien Lardiere a ?crit :
> 
> 
> On Wed, 27 Feb 2008 16:55:19 +0100, "St?phane A. Schildknecht"
> <stephane.schildknecht@postgresqlfr.org> wrote:
>> Sebastien Lardiere a ?crit :
>>> On Wed, 27 Feb 2008 13:55:42 +0100, Sebastien Lardiere
>> <sebastien@lardiere.net> wrote:
>>>>>> Shouldn't node 2 and/or 3 be forwarder if you want to use one of them
>>>> as
>>>>> a
>>>>>> master ?
>>>>> You're right, i enable forward for 2 and 3, and then failover work.
>>>>>
>>> Ok, it work, and i understand why i've got some error :
>>>
>>> - I have to set all direct receiver with forward = yes
>>>
>>> - When I do failover, i have to wait before send "drop node" for remover
>> the old master.
>>
>> Did you have to explicitly drop the node ? Documentation says it is done
>> on
>> every subscriber by the failover command. (so does slonik source code).
>>
> No, in fact, the old master was not dropped after the failover. I wait and
> verify that the new scheme work, then send "drop node" .

"Ici, ?a marche" :-)

I tried with three nodes, 1, 2 and 3, 1 being master for 2 and 3.

A failover on node 1 leaves me with node 2 and 3, 2 being master for 3. Both 2
and 3 now ignore 1.

PG 8.2.6 and slony 1.2.13.

Regards,
SAS
From cbbrowne at ca.afilias.info  Wed Feb 27 09:03:16 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Feb 27 09:03:23 2008
Subject: [Slony1-general] magic slon_start 2
In-Reply-To: <d4f444290802270551x2b1556b8o3eb3fb9803ba6690@mail.gmail.com>
	(lio bod's message of "Wed, 27 Feb 2008 14:51:24 +0100")
References: <d4f444290802270551x2b1556b8o3eb3fb9803ba6690@mail.gmail.com>
Message-ID: <60skzee3hn.fsf@dba2.int.libertyrms.com>

"lio bod" <liobod.slony@gmail.com> writes:
> I got a metaphysical question.;-)
>
> In fact, it's more an internal architeture slony question :
>
> ?
>
> I replicate my first db following documenation ' 2. Replicating Your First Database'?(http://slony.info/documentation/firstdb.html)
>
> This sample is not very precise on what command should be performed on master or slave host.
>
> I tried 'slon_start 2' on master host.?I was quite sure it would failed.
>
> Nope. I'm surprised : my replication seems to be ok. Magic!
>
> ?
>
> What would be a good recommandation?
>
> Run 'slon_start 2' on master or on?salve host?

Well, I have two answers to that, and they may differ from what you expect :-).

1.  On our systems, at Afilias, we never run slon processes on
    database servers.  We instead run all the slons centrally on an
    application server.

2.  If the choice is between master and slave, I'd be somewhat
    inclined to run both slons on the "master" server, as...

    - Life tends to be simpler if they both run on one server, as
      you can more readily manage processes on one host, and as
      log files will naturally be on one server.

    - It is essential that a slon be kept running against the master
      node, as if it is not, for a period of time, then when it
      is restarted, that entire period of time will be treated as
      One Big Long SYNC, and having such a huge SYNC is likely to
      turn out badly :-(.
-- 
(reverse (concatenate 'string "ofni.sesabatadxunil" "@" "enworbbc"))
http://linuxdatabases.info/info/internet.html
"... The book [CLtL1] is about  400 pages of 8.5" by 11" Dover output.
Apparently  the publisher and  typesetter decided  that this  made the
lines too wide for  easy reading, so they will use a  6" by 9" format.
This  will make  the shape  of the  book approximately  cubical.  Now,
there are  26 chapters counting the  index, and a Rubik's  cube has 26
exterior cubies.  I'll let  you individually extrapolate and fantasize
from there."  -- GLS
From liobod.slony at gmail.com  Wed Feb 27 09:22:28 2008
From: liobod.slony at gmail.com (lio bod)
Date: Wed Feb 27 09:22:36 2008
Subject: [Slony1-general] magic slon_start 2
In-Reply-To: <60skzee3hn.fsf@dba2.int.libertyrms.com>
References: <d4f444290802270551x2b1556b8o3eb3fb9803ba6690@mail.gmail.com>
	<60skzee3hn.fsf@dba2.int.libertyrms.com>
Message-ID: <d4f444290802270922q2fc616e1if8024338d415cb0f@mail.gmail.com>

2008/2/27, Christopher Browne <cbbrowne@ca.afilias.info>:
>
> "lio bod" <liobod.slony@gmail.com> writes:
> > I got a metaphysical question.;-)
> >
> > In fact, it's more an internal architeture slony question :
> >
> >
> >
> > I replicate my first db following documenation ' 2. Replicating Your
> First Database' (http://slony.info/documentation/firstdb.html)
> >
> > This sample is not very precise on what command should be performed on
> master or slave host.
> >
> > I tried 'slon_start 2' on master host. I was quite sure it would failed.
> >
> > Nope. I'm surprised : my replication seems to be ok. Magic!
> >
> >
> >
> > What would be a good recommandation?
> >
> > Run 'slon_start 2' on master or on salve host?
>
> Well, I have two answers to that, and they may differ from what you expect
> :-).
>
> 1.  On our systems, at Afilias, we never run slon processes on
>    database servers.  We instead run all the slons centrally on an
>    application server.

2.  If the choice is between master and slave, I'd be somewhat
>    inclined to run both slons on the "master" server, as...
>
>    - Life tends to be simpler if they both run on one server, as
>      you can more readily manage processes on one host, and as
>      log files will naturally be on one server.


Agree. It's always obvious once it's written.

   - It is essential that a slon be kept running against the master
>      node, as if it is not, for a period of time, then when it
>      is restarted, that entire period of time will be treated as
>      One Big Long SYNC, and having such a huge SYNC is likely to
>      turn out badly :-(.


I don't catch the point
The big sync may also turn badly if the slon 2 start and restart after a
long time occures on the salve node.
?
Thx anyway

--
> (reverse (concatenate 'string "ofni.sesabatadxunil" "@" "enworbbc"))
> http://linuxdatabases.info/info/internet.html
> "... The book [CLtL1] is about  400 pages of 8.5" by 11" Dover output.
> Apparently  the publisher and  typesetter decided  that this  made the
> lines too wide for  easy reading, so they will use a  6" by 9" format.
> This  will make  the shape  of the  book approximately  cubical.  Now,
> there are  26 chapters counting the  index, and a Rubik's  cube has 26
> exterior cubies.  I'll let  you individually extrapolate and fantasize
> from there."  -- GLS
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080227/=
34b29985/attachment.htm
From cbbrowne at ca.afilias.info  Wed Feb 27 09:38:26 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Feb 27 09:38:34 2008
Subject: [Slony1-general] magic slon_start 2
In-Reply-To: <d4f444290802270922q2fc616e1if8024338d415cb0f@mail.gmail.com>
	(lio bod's message of "Wed, 27 Feb 2008 18:22:28 +0100")
References: <d4f444290802270551x2b1556b8o3eb3fb9803ba6690@mail.gmail.com>
	<60skzee3hn.fsf@dba2.int.libertyrms.com>
	<d4f444290802270922q2fc616e1if8024338d415cb0f@mail.gmail.com>
Message-ID: <60oda2e1v1.fsf@dba2.int.libertyrms.com>

"lio bod" <liobod.slony@gmail.com> writes:
> 2008/2/27, Christopher Browne <cbbrowne@ca.afilias.info>:
> Agree. It's always obvious once it's written.
>
>
>           ?? - It is essential that a slon be kept running against the master
>      ???? node, as if it is not, for a period of time, then when it
>      ???? is restarted, that entire period of time will be treated as
>      ???? One Big Long SYNC, and having such a huge SYNC is likely to
>      ???? turn out badly :-(.
>
>
> ?
>
> I don't catch the point
>
> The big sync?may also turn badly if the slon 2 start and restart after?a long time occures on the salve node.
>
> ?
>
> Thx anyway

No, the point is that if the slon for node #1 was running, throughout,
there won't be one single "very large SYNC."

If the slon for the master node *is* running, then there should be a
SYNC generated at least once every 2 minutes.  

Thus, if the slave DB is down for the weekend (e.g. -
Saturday/Sunday), then when you return on Monday, you'll find that the
slave node is behind by 2 days, and that the changes are divided up
across, um...  At least 1440 SYNCs (30 per hour, times 2x24).  If
there were times of heavy updates, there will more SYNCs than that.

In the catch-up process, the subscriber slon, on Monday, will be able
to gradually go through those changes, doing a COMMIT every so often,
so that the changes from the weekend will gradually flow in.

In contrast, if the slon for the "master" node was down for the
weekend, as well, then there will be just 1 SYNC, covering the time
from when the slon fell over until when it restarted.  There is no
ability to do the updates in more "bite-sized" pieces.
-- 
let name="cbbrowne" and tld="cbbrowne.com" in String.concat "@" [name;tld];;
http://linuxfinances.info/info/linuxdistributions.html
Q: What does the function NULL do?
A: The function NULL tests whether or not its argument is NIL or not.  If
   its argument is NIL the value of NULL is NIL.
-- Ken Tracton, Programmer's Guide to Lisp, page 73.
From cbbrowne at ca.afilias.info  Wed Feb 27 10:24:41 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Feb 27 10:24:50 2008
Subject: [Slony1-general] Failover problem
In-Reply-To: <44739d4e169b1390728ced39b9e3561e@80.247.230.89> (Sebastien
	Lardiere's message of "Wed, 27 Feb 2008 16:49:25 +0100")
References: <528c40fff13169c0dc5222be998ba836@80.247.230.89>
	<44739d4e169b1390728ced39b9e3561e@80.247.230.89>
Message-ID: <60fxvedzpy.fsf@dba2.int.libertyrms.com>

Sebastien Lardiere <sebastien@lardiere.net> writes:
> On Wed, 27 Feb 2008 13:55:42 +0100, Sebastien Lardiere <sebastien@lardiere.net> wrote:
>> 
>>>> Shouldn't node 2 and/or 3 be forwarder if you want to use one of them
>> as
>>> a
>>>> master ?
>>>
>>> You're right, i enable forward for 2 and 3, and then failover work.
>>>
>
> Ok, it work, and i understand why i've got some error : 
>
> - I have to set all direct receiver with forward = yes
>
> - When I do failover, i have to wait before send "drop node" for remover the old master.
>
> Perhaps somewhere in the documentation, it could be write that in case of failover, future master and direct receiver have to be set with forward = yes ? 

I'm adding this in more expressly in a couple of places (best
practices, slonik reference).
-- 
(format nil "~S@~S" "cbbrowne" "acm.org")
http://www3.sympatico.ca/cbbrowne/oses.html
Referring to undocumented  private communications allows one to  claim
virtually anything: "we discussed this idea in  our working group last
year, and concluded that it was totally brain-damaged".
-- from the Symbolics Guidelines for Sending Mail
From cbbrowne at ca.afilias.info  Wed Feb 27 10:28:03 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Feb 27 10:28:11 2008
Subject: [Slony1-general] Failover problem
In-Reply-To: <47C587D1.7040204@dalibo.com> (UTF's message of "Wed,
	27 Feb 2008 16:54:57 +0100")
References: <528c40fff13169c0dc5222be998ba836@80.247.230.89>
	<44739d4e169b1390728ced39b9e3561e@80.247.230.89>
	<47C587D1.7040204@dalibo.com>
Message-ID: <60bq62dzkc.fsf@dba2.int.libertyrms.com>

"=?UTF-8?B?Q8OpZHJpYyBWaWxsZW1haW4=?=" <cedric.villemain@dalibo.com> writes:
>> Perhaps somewhere in the documentation, it could be write that in
>> case of failover, future master and direct receiver have to be set
>> with forward = yes ?
>
> isn't it write somewhere ?

Yes, it's mentioned in a number of places, but possibly not
prominently enough, as confusion has clearly arisen.

> In the future, it is possible that every node will be forwarder by
> default (isn't it ? )

Some day, quite possibly.

I have given up, for now, on the idea of using COPY to handle the data
in sl_log_[1/2]; my proposal had some pretty fatal flaws in it.  But
the idea may eventually return, and if it does, it becomes really
necessary to copy over data into sl_log_[1/2], and that would turn
each node, potentially, into a forwarder.
-- 
(reverse (concatenate 'string "moc.enworbbc" "@" "enworbbc"))
http://www3.sympatico.ca/cbbrowne/finances.html
"Thou  shall  not kill.   Thou shall not   commit  adultery. Don't eat
pork. I'm  sorry, what was that  last one?? Don't   eat pork.  God has
spoken. Is that  the word of God or   is that pigs trying  to outsmart
everybody?" -- Jon Stewart
From cbbrowne at ca.afilias.info  Wed Feb 27 11:38:13 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Feb 27 11:38:22 2008
Subject: [Slony1-general] Failover problem
In-Reply-To: <60bq62dzkc.fsf@dba2.int.libertyrms.com> (Christopher Browne's
	message of "Wed, 27 Feb 2008 18:28:03 +0000")
References: <528c40fff13169c0dc5222be998ba836@80.247.230.89>
	<44739d4e169b1390728ced39b9e3561e@80.247.230.89>
	<47C587D1.7040204@dalibo.com> <60bq62dzkc.fsf@dba2.int.libertyrms.com>
Message-ID: <607igqdwbe.fsf@dba2.int.libertyrms.com>

Christopher Browne <cbbrowne@ca.afilias.info> writes:
> "=?UTF-8?B?Q8OpZHJpYyBWaWxsZW1haW4=?=" <cedric.villemain@dalibo.com> writes:
>>> Perhaps somewhere in the documentation, it could be write that in
>>> case of failover, future master and direct receiver have to be set
>>> with forward = yes ?
>>
>> isn't it write somewhere ?
>
> Yes, it's mentioned in a number of places, but possibly not
> prominently enough, as confusion has clearly arisen.

Rectified, in CVS HEAD...

http://lists.slony.info/pipermail/slony1-commit/2008-February/002189.html
-- 
"cbbrowne","@","ca.afilias.info"
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From cbbrowne at ca.afilias.info  Wed Feb 27 15:13:07 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Feb 27 15:13:20 2008
Subject: [Slony1-general] failover problems with 3 nodes
In-Reply-To: <20080227132917.4489327c@amilo.home> (Mikhail Kolesnik's message
	of "Wed, 27 Feb 2008 13:29:17 +0200")
References: <20080226230734.de0f09d5.y-mori@sraoss.co.jp>
	<20080227170349.c9c17a42.y-mori@sraoss.co.jp>
	<47C52C89.7080307@postgresqlfr.org>
	<20080227201129.27ae8860.y-mori@sraoss.co.jp>
	<47C546FE.5090706@postgresqlfr.org>
	<20080227132917.4489327c@amilo.home>
Message-ID: <60tzjuc7ss.fsf@dba2.int.libertyrms.com>

Mikhail Kolesnik <mike@openbunker.org> writes:
> Hello, St??phane.
>
> On Wed, 27 Feb 2008 12:18:22 +0100
> "St??phane A. Schildknecht" <stephane.schildknecht@postgresqlfr.org>
> wrote:
>
>> [...]
>> Hey, I should test failover before updating to 1.2.13...
>
> I have some strange periodic problems with 'ACCEPT_SET - MOVE_SET or
> FAILOVER_SET not received yet - sleep' on 1.2.12 and 1.2.13. Looks
> similar to this one.
>
> I should try to downgrade to 1.2.11 and try if my 'move set' problems
> will disappear. Here is the initial problem description:
> http://lists.slony.info/pipermail/slony1-general/2008-February/007445.html

There's something about this that isn't making sense...

I just did a CVS diff between 1.2.11 and REL_1_2_STABLE, and didn't
see anything that ought to have anything to do with this.

I haven't yet done any testing of this case, out of the samples
described; I intend to do so; but it's not making sense that changing
between 1.2.11 and 1.2.13 should make any difference in this...
-- 
output = ("cbbrowne" "@" "cbbrowne.com")
http://linuxfinances.info/info/rdbms.html
"I don't know why, but first C  programs tend to look a lot worse than
first programs  in any other  language (maybe except for  FORTRAN, but
then I suspect all FORTRAN programs look like `firsts')" -- Olaf Kirch
From Ow.Mun.Heng at wdc.com  Wed Feb 27 15:41:17 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Wed Feb 27 15:41:46 2008
Subject: [Slony1-general] Slave died, no access till next week,	master
	logs filling up. What should I do
In-Reply-To: <1203718008.21659.45.camel@neuromancer.home.net>
References: <1203704600.21659.23.camel@neuromancer.home.net>
	<47BF164C.40003@emolecules.com>
	<60oda8ekze.fsf@dba2.int.libertyrms.com>
	<1203718008.21659.45.camel@neuromancer.home.net>
Message-ID: <1204155677.25292.35.camel@neuromancer.home.net>


On Sat, 2008-02-23 at 06:06 +0800, Ow Mun Heng wrote:
> I guess my concern now is that the slon logs are filling up and going
> past the 2GB threshold (for both log_1 and 2 which would mean that
> there's not going to be much help in gettting things back to speed when


Seems like I managed to survive the storm. Many thanks again for the
advise. Much appreciated
From Ow.Mun.Heng at wdc.com  Wed Feb 27 18:28:03 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Wed Feb 27 18:28:37 2008
Subject: [Slony1-general] force log_switching to occur
In-Reply-To: <47B49A69.5020502@ca.afilias.info>
References: <1203016881.16694.1.camel@neuromancer.home.net>
	<47B49A69.5020502@ca.afilias.info>
Message-ID: <1204165683.25292.45.camel@neuromancer.home.net>


On Thu, 2008-02-14 at 19:45 +0000, Christopher Browne wrote:
> Ow Mun Heng wrote:
> > Is there a way to manually force slony to switch from log_1 to log_2?
> > The table is getting way big (~1+G) and I think that it's causing the
> > "fetch 100 from log" to become slow ~60-150secs.
> >   
> You can request this from SQL prompt via:
> 
> select _my_slony_schema.logswitch_start();
> 
> It may fail with the error:
> ERROR: Previous logswitch still in progress
> 
> It will not immediately lead to cleaning out sl_log_1; that is handled 
> as part of the cleanup thread, which normally runs every 10 minutes 
> (controlled by a parameter compiled into slon).

It's been > 10 min (actually couple of hours since the last Forced
log_switch and yet the log_1 is still not being cleaned up.

So.. how can I rectify this?

From liobod.slony at gmail.com  Thu Feb 28 00:34:33 2008
From: liobod.slony at gmail.com (lio bod)
Date: Thu Feb 28 00:34:56 2008
Subject: [Slony1-general] magic slon_start 2
In-Reply-To: <60oda2e1v1.fsf@dba2.int.libertyrms.com>
References: <d4f444290802270551x2b1556b8o3eb3fb9803ba6690@mail.gmail.com>
	<60skzee3hn.fsf@dba2.int.libertyrms.com>
	<d4f444290802270922q2fc616e1if8024338d415cb0f@mail.gmail.com>
	<60oda2e1v1.fsf@dba2.int.libertyrms.com>
Message-ID: <d4f444290802280034w2e48f477h59616e207d9bdd92@mail.gmail.com>

ok.I can see clearly now.
tfyt.




2008/2/27, Christopher Browne <cbbrowne@ca.afilias.info>:
>
> "lio bod" <liobod.slony@gmail.com> writes:
> > 2008/2/27, Christopher Browne <cbbrowne@ca.afilias.info>:
> > Agree. It's always obvious once it's written.
> >
> >
> >              - It is essential that a slon be kept running against the
> master
> >           node, as if it is not, for a period of time, then when it
> >           is restarted, that entire period of time will be treated as
> >           One Big Long SYNC, and having such a huge SYNC is likely to
> >           turn out badly :-(.
> >
> >
> >
> >
> > I don't catch the point
> >
> > The big sync may also turn badly if the slon 2 start and restart after a
> long time occures on the salve node.
> >
> > ?
> >
> > Thx anyway
>
> No, the point is that if the slon for node #1 was running, throughout,
> there won't be one single "very large SYNC."
>
> If the slon for the master node *is* running, then there should be a
> SYNC generated at least once every 2 minutes.
>
> Thus, if the slave DB is down for the weekend (e.g. -
> Saturday/Sunday), then when you return on Monday, you'll find that the
> slave node is behind by 2 days, and that the changes are divided up
> across, um...  At least 1440 SYNCs (30 per hour, times 2x24).  If
> there were times of heavy updates, there will more SYNCs than that.
>
> In the catch-up process, the subscriber slon, on Monday, will be able
> to gradually go through those changes, doing a COMMIT every so often,
> so that the changes from the weekend will gradually flow in.
>
> In contrast, if the slon for the "master" node was down for the
> weekend, as well, then there will be just 1 SYNC, covering the time
> from when the slon fell over until when it restarted.  There is no
> ability to do the updates in more "bite-sized" pieces.
> --
> let name=3D"cbbrowne" and tld=3D"cbbrowne.com" in String.concat "@"
> [name;tld];;
> http://linuxfinances.info/info/linuxdistributions.html
> Q: What does the function NULL do?
> A: The function NULL tests whether or not its argument is NIL or not.  If
>   its argument is NIL the value of NULL is NIL.
> -- Ken Tracton, Programmer's Guide to Lisp, page 73.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080228/=
2bae4368/attachment.htm
From aaytekin at gmail.com  Thu Feb 28 01:03:02 2008
From: aaytekin at gmail.com (Ahmet Aytekin)
Date: Thu Feb 28 01:03:44 2008
Subject: [Slony1-general] SLony installation guide
In-Reply-To: <1204165683.25292.45.camel@neuromancer.home.net>
References: <1203016881.16694.1.camel@neuromancer.home.net>
	<47B49A69.5020502@ca.afilias.info>
	<1204165683.25292.45.camel@neuromancer.home.net>
Message-ID: <1204189382.6202.131.camel@zaana>

Hi All,

I am new to Slony  and trying to install it on Postgresql 8.1.  Would
you suggest a step by step guide I need to install it asap.

Thanks for the help

Maruf

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080228/ecd647d4/attachment.htm
From y-mori at sraoss.co.jp  Thu Feb 28 01:19:21 2008
From: y-mori at sraoss.co.jp (Yoshiharu Mori)
Date: Thu Feb 28 01:19:47 2008
Subject: [Slony1-general] failover problems with 3 nodes
In-Reply-To: <20080227132917.4489327c@amilo.home>
References: <20080226230734.de0f09d5.y-mori@sraoss.co.jp>
	<20080227170349.c9c17a42.y-mori@sraoss.co.jp>
	<47C52C89.7080307@postgresqlfr.org>
	<20080227201129.27ae8860.y-mori@sraoss.co.jp>
	<47C546FE.5090706@postgresqlfr.org>
	<20080227132917.4489327c@amilo.home>
Message-ID: <20080228181921.629409b1.y-mori@sraoss.co.jp>

Hi 

> 
> > [...]
> > Hey, I should test failover before updating to 1.2.13...
> 
> I have some strange periodic problems with 'ACCEPT_SET - MOVE_SET or
> FAILOVER_SET not received yet - sleep' on 1.2.12 and 1.2.13. Looks
> similar to this one.

I tried 'move set' on 1.2.13 and 
'ACCEPT_SET - MOVE_SET or FAILOVER_SET not received yet - sleep' occurred on node 3.
I could update the tupples on node 2 and get new data on node 1,
but node 3 was never updated. 
I tried several times. The problem didn't always but sometimes happened.

execute command:
 lock set (id =1, origin =1);
 wait for event (origin = 1, confirmed = 2);
 move set (id=1, old origin = 1, new origin=2);

> 
> I should try to downgrade to 1.2.11 and try if my 'move set' problems
> will disappear. Here is the initial problem description:
> http://lists.slony.info/pipermail/slony1-general/2008-February/007445.html
> 
> -- 
> Mikhail Kolesnik
> ICQ: 260259143
> IRC: mike_k at freenode/#crux, rusnet/#yalta
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 


-- 
SRA OSS, Inc. ????
Yoshiharu Mori <y-mori@sraoss.co.jp>
http://www.sraoss.co.jp/
From plk.zuber at gmail.com  Thu Feb 28 05:16:24 2008
From: plk.zuber at gmail.com (=?UTF-8?Q?Filip_Rembia=C5=82kowski?=)
Date: Thu Feb 28 05:16:26 2008
Subject: [Slony1-general] SLony installation guide
In-Reply-To: <1204189382.6202.131.camel@zaana>
References: <1203016881.16694.1.camel@neuromancer.home.net>
	<47B49A69.5020502@ca.afilias.info>
	<1204165683.25292.45.camel@neuromancer.home.net>
	<1204189382.6202.131.camel@zaana>
Message-ID: <92869e660802280516i7f844f37na2e9eb9303b18f1f@mail.gmail.com>

shortest version:

make install on both hosts
initialize (use slonik)
start slon daemons
subscribe (use slonik)
wait for inital copy
monitor lag

more detailed guides found under search term "slony howto" or here:
http://slony.info/documentation/firstdb.html


2008/2/28, Ahmet Aytekin <aaytekin@gmail.com>:
>
>  Hi All,
>
>  I am new to Slony  and trying to install it on Postgresql 8.1.  Would you
> suggest a step by step guide I need to install it asap.
>
>  Thanks for the help
>
>  Maruf
>
>
> _______________________________________________
>  Slony1-general mailing list
>  Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>


-- 
Filip Rembia?kowski
From JanWieck at Yahoo.com  Thu Feb 28 05:33:47 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu Feb 28 05:36:00 2008
Subject: [Slony1-general] STILL can't migrate a node.
In-Reply-To: <47C2D7D1.7060206@emolecules.com>
References: <47B463D5.90605@emolecules.com>	<47BB74B8.2030808@emolecules.com>	<20080221144755.GC17293@crankycanuck.ca>	<47BD907C.2050603@dalibo.com>	<20080221171359.GA18657@crankycanuck.ca>	<47BF9073.7060708@emolecules.com>	<47BFAD0E.2000902@emolecules.com>
	<47C04A24.8010209@Yahoo.com>	<47C065E9.7070408@emolecules.com>
	<47C06A22.5050301@Yahoo.com>	<47C06BC1.8060805@emolecules.com>
	<47C07385.5080602@Yahoo.com> <47C2D7D1.7060206@emolecules.com>
Message-ID: <47C6B83B.9060208@Yahoo.com>

On 2/25/2008 9:59 AM, Craig A. James wrote:
> I sent the attached email ony to Jan by mistake, here it is, I hope Jan or somebody has an idea what's going on.

What I still don't understand is what that stored procedure moveSet() is 
waiting for. Are there any pg_locks entries with granted='f' at all?


Jan


> 
> A quick summary:
>   1. Shut of Apache
>   2. Restart Postgres and Slony
>   3. lock set (id = 1, origin = 1);
>      wait for event (origin = 1, confirmed = 2);
>      move set (id = 1, old origin = 1, new origin = 2);
> 
> When I do this, Slony starts a SELECT that never finishes ("never" means I waited 10 minutes, 35 minutes, and 10 minutes in three different tests, with no sign of progress).
> 
> Per Jan's request, I've attached the output of two SQL queries:
> 
>     select relname, granted, pid, mode from pg_locks L, pg_class C
>         where C.oid = L.relation and locktype = 'relation';
> 
>     select * from pg_stat_activity;
> 
> In addition, I've attached the output of vmstat, a few snapshots taken over a 30-minute peroid.  It's looks to me like postgres is doing some sort of huge join or sort, based on the changing amounts of CPU and I/O during the 30 minutes I monitored it.  No other processes were active; this was entirely due to the backend Postgres process being used by Slony.
> 
>>From these, you can see that a Slon process is doing a "SELECT moveSet(...)".  ps(1) confirms that this process is the one using all the CPU time shown by vmstat.
> 
> I sure could use some help on this one, it's become a critical roadblock to our operation.
> 
> Thanks,
> Craig
> 
> ------------------------------------
> 
> Jan,
> 
> Thanks for all your help with this, I really appreciate it.  Your additional instructions regarding pg_stat_activity made more sense. I also had to enable the stats_command_string in the postgresql.conf file.
> 
> I'm also appending some output from vmstat to show that the system was busy doing something -- I let it run for about ten minutes, and the vmstat results are for various snapshots during that 10 minutes.
> 
> Craig
> 
> 
> 
> emol_warehouse_1=# select datid, datname, procpid, usename, current_query, query_start, backend_start from pg_stat_activity;
>  datid   |     datname      | procpid | usename  |                   current_query                    |          query_start          |         backend_start         
> ----------+------------------+---------+----------+----------------------------------------------------+-------------------------------+-------------------------------
>   164853 | emol_warehouse_1 |   24747 | postgres | <IDLE>                                             | 2008-02-23 18:44:09.995787-08 | 2008-02-23 18:41:55.334255-08
>    18368 | global           |   24741 | postgres | <IDLE>                                             | 2008-02-23 18:47:30.099887-08 | 2008-02-23 18:41:38.458081-08
> 20209283 | accounting       |   24740 | postgres | <IDLE>                                             | 2008-02-23 18:47:29.540243-08 | 2008-02-23 18:41:38.421729-08
>   164853 | emol_warehouse_1 |   24739 | postgres | <IDLE>                                             | 2008-02-23 18:47:29.195255-08 | 2008-02-23 18:41:38.386496-08
>   164853 | emol_warehouse_1 |   24738 | postgres | select "_emol_warehouse_1_cluster".moveSet(1, 2);  | 2008-02-23 18:41:35.215066-08 | 2008-02-23 18:41:35.154836-08
>    18368 | global           |   24731 | postgres | <IDLE>                                             | 2008-02-23 18:41:26.398179-08 | 2008-02-23 18:41:26.394739-08
>    18368 | global           |   24729 | postgres | <IDLE>                                             | 2008-02-23 18:47:36.747641-08 | 2008-02-23 18:41:26.394256-08
>    18368 | global           |   24730 | postgres | <IDLE>                                             | 2008-02-23 18:47:35.972451-08 | 2008-02-23 18:41:26.392295-08
> 20209283 | accounting       |   24724 | postgres | <IDLE>                                             | 2008-02-23 18:41:26.388876-08 | 2008-02-23 18:41:26.385995-08
> 20209283 | accounting       |   24723 | postgres | <IDLE>                                             | 2008-02-23 18:47:37.797022-08 | 2008-02-23 18:41:26.380974-08
> 20209283 | accounting       |   24722 | postgres | <IDLE>                                             | 2008-02-23 18:47:37.420454-08 | 2008-02-23 18:41:26.378364-08
>   164853 | emol_warehouse_1 |   24717 | postgres | <IDLE>                                             | 2008-02-23 18:47:37.080398-08 | 2008-02-23 18:41:26.356934-08
>   164853 | emol_warehouse_1 |   24716 | postgres | <IDLE>                                             | 2008-02-23 18:41:26.360746-08 | 2008-02-23 18:41:26.358003-08
>   164853 | emol_warehouse_1 |   24715 | postgres | <IDLE>                                             | 2008-02-23 18:47:36.216511-08 | 2008-02-23 18:41:26.352855-08
> 20209283 | accounting       |   24710 | postgres | <IDLE>                                             | 2008-02-23 18:47:34.69257-08  | 2008-02-23 18:41:26.319118-08
>   164853 | emol_warehouse_1 |   24706 | postgres | <IDLE>                                             | 2008-02-23 18:47:34.692909-08 | 2008-02-23 18:41:26.301898-08
>    18368 | global           |   24709 | postgres | <IDLE>                                             | 2008-02-23 18:47:34.728589-08 | 2008-02-23 18:41:26.316133-08
> 20209283 | accounting       |   24695 | postgres | <IDLE>                                             | 2008-02-23 18:47:37.6937-08   | 2008-02-23 18:41:26.24967-08
>   164853 | emol_warehouse_1 |   24694 | postgres | <IDLE>                                             | 2008-02-23 18:47:37.38218-08  | 2008-02-23 18:41:26.24526-08
>    18368 | global           |   24693 | postgres | <IDLE>                                             | 2008-02-23 18:47:32.19262-08  | 2008-02-23 18:41:26.241983-08
> (20 rows)
> 
> 
> emol_warehouse_1=# select relname, granted, pid, mode from pg_locks L, pg_class C
> emol_warehouse_1-#         where C.oid = L.relation and locktype = 'relation'; 
>                     relname                      | granted |  pid  |        mode         
> --------------------------------------------------+---------+-------+---------------------
> pg_index                                         | t       | 24738 | AccessShareLock
> pg_index                                         | t       | 24738 | RowShareLock
> PartInd_emol_warehouse_1_cluster_sl_log_2-node-1 | t       | 24738 | AccessExclusiveLock
> db_tables                                        | t       | 24738 | AccessExclusiveLock
> db_columns                                       | t       | 24738 | AccessExclusiveLock
> sl_table                                         | t       | 24738 | AccessShareLock
> sl_table                                         | t       | 24738 | RowShareLock
> sl_table                                         | t       | 24738 | RowExclusiveLock
> pg_locks                                         | t       | 24747 | AccessShareLock
> sl_set                                           | t       | 24738 | AccessShareLock
> sl_set                                           | t       | 24738 | RowShareLock
> sl_set                                           | t       | 24738 | RowExclusiveLock
> logp                                             | t       | 24738 | AccessExclusiveLock
> catalogue_issue                                  | t       | 24738 | AccessExclusiveLock
> supplier_priority                                | t       | 24738 | AccessExclusiveLock
> sl_setsync                                       | t       | 24738 | RowExclusiveLock
> pg_indexes                                       | t       | 24738 | AccessShareLock
> sl_node                                          | t       | 24738 | AccessShareLock
> sl_node                                          | t       | 24738 | RowShareLock
> supplier_detail                                  | t       | 24738 | AccessExclusiveLock
> chmoogle_thesaurus                               | t       | 24738 | AccessExclusiveLock
> parent_properties                                | t       | 24738 | AccessExclusiveLock
> db_names                                         | t       | 24738 | AccessExclusiveLock
> sl_listen                                        | t       | 24738 | AccessShareLock
> sl_listen                                        | t       | 24738 | RowExclusiveLock
> pg_class                                         | t       | 24738 | AccessShareLock
> pg_class                                         | t       | 24738 | RowShareLock
> version_properties                               | t       | 24738 | AccessExclusiveLock
> parent                                           | t       | 24738 | AccessExclusiveLock
> str_conntab                                      | t       | 24738 | AccessExclusiveLock
> sl_log_status                                    | t       | 24738 | AccessShareLock
> pg_class                                         | t       | 24747 | AccessShareLock
> pg_tablespace                                    | t       | 24738 | AccessShareLock
> sl_path                                          | t       | 24738 | AccessShareLock
> sl_path                                          | t       | 24738 | RowShareLock
> chmoogle_thesaurus_types                         | t       | 24738 | AccessExclusiveLock
> catalogue_info                                   | t       | 24738 | AccessExclusiveLock
> chmoogle_molkeys                                 | t       | 24738 | AccessExclusiveLock
> pg_class_oid_index                               | t       | 24747 | AccessShareLock
> version                                          | t       | 24738 | AccessExclusiveLock
> sl_event                                         | t       | 24738 | AccessShareLock
> sl_log_2                                         | t       | 24738 | AccessShareLock
> sl_log_2                                         | t       | 24738 | ShareLock
> sl_log_2                                         | t       | 24738 | AccessExclusiveLock
> chmoogle_statistics                              | t       | 24738 | AccessExclusiveLock
> sl_subscribe                                     | t       | 24738 | AccessShareLock
> sl_subscribe                                     | t       | 24738 | RowExclusiveLock
> chmoogle_cansmiles                               | t       | 24738 | AccessExclusiveLock
> pg_namespace                                     | t       | 24738 | AccessShareLock
> pg_namespace                                     | t       | 24738 | RowShareLock
> sample                                           | t       | 24738 | AccessExclusiveLock
> sl_config_lock                                   | t       | 24738 | AccessExclusiveLock
> (52 rows)
> 
> 
> 
> $ vmstat 2
> procs -----------memory---------- ---swap-- -----io---- --system-- ----cpu----
> r  b   swpd   free   buff  cache   si   so    bi    bo   in    cs us sy id wa
> 1  0 138112 138244  20156 3577564    0    1     0     0    0     0  5  1 92  2
> 1  0 138112 139988  20084 3575816    0    0  1968    12  413   179 24  1 75  0
> 1  0 138112 140964  19984 3575136    0    0  2232    20  440   362 24  1 75  0
> 1  0 138112 141756  19888 3574972    0    0  2340    75  527   497 25  0 75  0
> 1  0 138112 138216  19888 3578612    0    0  1774     4  420   174 24  0 75  0
> 1  0 138112 141464  19808 3575572    0    0  1436    42  397   321 24  0 75  0
> 0  1 138112 137388  19808 3579732    0    0  2078     8  455   277 21  1 74  5
> 1  0 138112 140700  19808 3576612    0    0  1312    20  392   377 20  1 74  5
> 1  0 138112 137048  19808 3580512    0    0  1980    57  392   539 23  0 72  5
> [snip]
> 2  0 138112 141432  19248 3578472    0    0     0    61  322   533 25  1 74  0
> 1  0 138112 141448  19248 3578472    0    0     0    27  274   181 25  0 75  0
> 1  0 138112 141448  19248 3578472    0    0     0    25  259   111 25  0 75  0
> 1  0 138112 141456  19248 3578472    0    0     0     8  263   290 25  0 75  0
> 1  0 138112 141456  19248 3578472    0    0     0    20  282   304 25  0 75  0
> 1  0 138112 141580  19256 3578464    0    0     0    64  318   443 25  0 75  0
> 1  0 138112 141580  19256 3578464    0    0     0     4  268   296 25  0 75  0
> 1  0 138112 141584  19256 3578464    0    0     0    15  259   174 25  0 75  0
> 1  0 138112 141584  19256 3578464    0    0     0     8  263   141 25  0 75  0
> 1  0 138112 141600  19256 3578464    0    0     0    20  284   317 25  0 75  0
> 1  0 138112 141600  19256 3578464    0    0     0    85  321   441 25  0 75  0
> 1  0 138112 141600  19256 3578464    0    0     0    10  269   143 25  0 75  0
> 1  0 138112 141600  19256 3578464    0    0     0    27  259   220 25  0 75  0
> 1  0 138112 141476  19256 3578464    0    0     0     8  261   185 25  0 75  0
> 1  0 138112 141476  19256 3578464    0    0     0    20  285   210 25  0 75  0
> 1  0 138112 141476  19256 3578464    0    0     0    59  318   440 25  0 75  0
> 1  0 138112 141476  19256 3578464    0    0     0     4  267   141 25  0 75  0
> 1  0 138112 141476  19256 3578464    0    0     0    31  260   103 25  0 75  0
> 1  0 138112 141476  19256 3578464    0    0     0     8  263   237 25  0 75  0
> 1  0 138112 141476  19256 3578464    0    0     0    33  285   282 25  0 75  0
> [snip]
> 1  0 138112 140856  19304 3578416    0    0     0    27  259   250 25  0 75  0
> 1  0 138112 140856  19304 3578416    0    0     0     8  262   200 25  0 75  0
> 1  0 138112 140856  19304 3578416    0    0     0    20  283   236 25  0 75  0
> 1  0 138112 140856  19304 3578416    0    0     0    63  318   508 25  0 75  0
> 1  0 138112 140856  19304 3578416    0    0     0    10  265   149 25  0 75  0
> 1  0 138112 140732  19304 3578416    0    0     0    25  259   108 25  0 75  0
> 1  0 138112 140732  19304 3578416    0    0     0     8  262   269 25  0 75  0
> 0  1 138112 137440  18736 3582364    0    0  4184    26  575   815 17  1 75  7
> 0  2 138112 138184  17360 3584000    0    0  5216    89  755   969  0  1 70 28
> 0  1 138112 139956  16176 3584144    0    0  4564    12  775   902  1  1 73 25
> 1  0 138112 138400  15164 3586976    0    0  6368   115  764   642  1  1 75 23
> 0  1 138112 141136  14620 3585180    0    0  6936     8  731   331  1  1 74 24
> 0  2 138112 140068  14564 3587056    0    0  6168    32  804   448  1  1 74 24
> 1  2 138112 139960  14240 3587640    0    0  5364    61  770   741  1  1 73 25
> 0  1 138112 136492  14120 3591660    0    0  7036    20  872  1511  1  1 73 24
> 0  1 138112 140772  13412 3588728    0    0  8544    24 1043  1951  1  2 75 23
> 0  1 138112 136460  13104 3593716    0    0 10052     8  933  1615  1  2 73 24
> 0  1 138112 139356  12840 3591380    0    0  9156    16  891  1511  1  1 72 26
> 0  2 138112 137084  12704 3594116    0    0  8928    47  929  1718  1  2 72 26
> 0  1 138112 140276  12688 3591272    0    0  8836    20  965  1757  1  1 72 26
> 0  1 138112 138052  12688 3593092    0    0  8800    19  999  1679  1  1 75 23
> [snip]
> 0  2 138112 139288  12056 3595024    0    0  7804    32  722  1207  1  1 73 25
> 0  1 138112 137544  11840 3597060    0    0  7804    55  662  1096  1  1 73 25
> 0  1 138112 138468  11784 3596596    0    0  6972    32  625  1031  1  1 72 26
> 0  1 138112 137520  11476 3597944    0    0  8564    28  767  1291  1  1 75 23
> 0  1 138112 141640  11120 3594920    0    0  9220    14  655   933  1  1 74 24
> 0  1 138112 138464  11092 3598328    0    0 29424    12 1340  1471  2  4 75 19
> 0  2 138112 139628  10572 3597808    0    0 23832    49 1540  2858  2  3 73 21
> 0  1 138112 138560  10436 3599244    0    0 13844    32 1172  2170  1  2 71 25
> 0  1 138112 139348  10428 3598732    0    0 12774    35 1010  1781  2  2 75 22
> 0  1 138112 137676  10276 3600444    0    0 13680    68 1089  2035  1  2 74 23
> procs -----------memory---------- ---swap-- -----io---- --system-- ----cpu----
> r  b   swpd   free   buff  cache   si   so    bi    bo   in    cs us sy id wa
> 0  2 138112 141024   9952 3597908    0    0 13844    12 1371  2562  1  3 75 21
> 0  1 138112 139704   9884 3599276    0    0 13460    55 1099  2014  1  2 71 26
> 1  1 138112 139540   9824 3599872    0    0 13596    32 1185  2430  1  3 73 23
> 0  1 138112 138348   9696 3601040    0    0 11212   118 1139  2031  1  2 75 22
> 0  1 138112 137564   9572 3602464    0    0 25148    69  595   949  2  2 73 23
> 1  1 138112 139960   9496 3600460    0    0 15644    12  660  1117  1  2 73 24
> 1  1 138112 140912   9460 3599716    0    0  7292   156 1017  1782  1  2 73 24
> 0  1 138112 139664   9380 3601356    0    0 23516    28 1518  2840  3  3 72 22
> 0  1 138112 139108   8708 3603068    0    0 40538    16 1205  2320  5  4 75 17
> 0  1 138112 141668   7420 3602016    0    0 30164    23 1312  2418  3  4 74 19
> 0  1 138112 138516   6212 3606604    0    0 20012    18 1075  1855  2  3 74 20
> 0  2 138112 140756   6192 3604804    0    0  4176    49  501   704  0  0 72 27
> 0  1 138112 140940   6164 3604572    0    0  5300    32  613  1006  1  1 72 26
> 0  1 138112 141064   6056 3604940    0    0  5136     0  629   913  0  1 75 24
> 0  1 138112 141972   6044 3604172    0    0  4788    49  705  1216  1  1 73 25
> 0  1 138112 138004   5780 3608076    0    0  6972    12  633   986  0  1 74 24
> 0  2 138112 141496   5512 3604964    0    0  6360    39  786   414  1  1 72 25
> 0  1 138112 141016   5468 3605788    0    0  8316    32  874   878  1  1 71 26
> 0  1 138112 141832   5332 3605664    0    0 37456     0  553  1051  4  2 75 19
> 0  1 138112 140940   5332 3606452    0    0 19016    41  985  1674  2  3 74 21
> 0  1 138112 141720   5332 3605672    0    0 15964    12  619   740  2  2 73 24
> 0  2 138112 137296   5292 3610132    0    0 29388    53 1069   877  3  3 74 19
> 0  1 138112 138600   4556 3617372    0    0 29104    32 1214  1237  3  5 70 22
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From vivek at khera.org  Thu Feb 28 07:10:05 2008
From: vivek at khera.org (Vivek Khera)
Date: Thu Feb 28 07:10:10 2008
Subject: [Slony1-general] inconsistency in packaging
Message-ID: <60E5FE5E-6A34-47BA-A67D-CC06D35CE01E@khera.org>

I'm prepping the update for the slony port to FreeBSD's package  
system.  The 1.2.13 docs tarball uses a different root than the  
1.2.12.  That is, the 1.2.12 tar extracted into slony1-1.2.12/doc/ but  
the 1.2.13 extracts into doc/ only, so my scripts don't find the  
generated HTML.

Can this be added to the checklist to keep things consistent?  I have  
to figure out how to deal with this now, since they extract into  
different locations...

Thanks!

From vivek at khera.org  Thu Feb 28 07:15:14 2008
From: vivek at khera.org (Vivek Khera)
Date: Thu Feb 28 07:15:18 2008
Subject: [Slony1-general] inconsistency in packaging
In-Reply-To: <60E5FE5E-6A34-47BA-A67D-CC06D35CE01E@khera.org>
References: <60E5FE5E-6A34-47BA-A67D-CC06D35CE01E@khera.org>
Message-ID: <DD517CEF-00F9-49F8-99A3-37E70D40A92F@khera.org>


On Feb 28, 2008, at 10:10 AM, Vivek Khera wrote:

> I'm prepping the update for the slony port to FreeBSD's package  
> system.  The 1.2.13 docs tarball uses a different root than the  
> 1.2.12.  That is, the 1.2.12 tar extracted into slony1-1.2.12/doc/  
> but the 1.2.13 extracts into doc/ only, so my scripts don't find the  
> generated HTML.

Also, the man pages are missing from the doc tarball.

From vivek at khera.org  Thu Feb 28 07:19:39 2008
From: vivek at khera.org (Vivek Khera)
Date: Thu Feb 28 07:19:43 2008
Subject: [Slony1-general] inconsistency in packaging
In-Reply-To: <60E5FE5E-6A34-47BA-A67D-CC06D35CE01E@khera.org>
References: <60E5FE5E-6A34-47BA-A67D-CC06D35CE01E@khera.org>
Message-ID: <CA700DD1-FE3B-4482-88F8-44107425A032@khera.org>


On Feb 28, 2008, at 10:10 AM, Vivek Khera wrote:

> I'm prepping the update for the slony port to FreeBSD's package  
> system.  The 1.2.13 docs tarball uses a different root than the  
> 1.2.12.  That is, the 1.2.12 tar extracted into slony1-1.2.12/doc/  
> but the 1.2.13 extracts into doc/ only, so my scripts don't find the  
> generated HTML.

The complexenv.png file is missing from the doc tarball, too, leaving  
a dangling reference to an image from the plainpaths.html file.

From devrim at CommandPrompt.com  Thu Feb 28 09:27:43 2008
From: devrim at CommandPrompt.com (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Thu Feb 28 09:28:04 2008
Subject: [Slony1-general] gmake rpm vs tarball
In-Reply-To: <d4f444290802270253g14ab2cf5yc18a6c2b73f53d35@mail.gmail.com>
References: <d4f444290802130422k2bb187d4xd29649be8ab9604a@mail.gmail.com>
	<1203016714.24118.245.camel@localhost.localdomain>
	<d4f444290802270253g14ab2cf5yc18a6c2b73f53d35@mail.gmail.com>
Message-ID: <1204219663.3327.132.camel@localhost.localdomain>

SGksCgpPbiBXZWQsIDIwMDgtMDItMjcgYXQgMTE6NTMgKzAxMDAsIGxpbyBib2Qgd3JvdGU6Cj4g
VGhhdCdzIGEgd2hpbGUgeW91IGFuc3dlcmVkIG1lIGFuZCBpIG5vdGljZWQgaSBmb3Jnb3QgdG8g
dGhhbmsgeW91LiAKPiBTbyBwbGVhc2UgY29uc2lkZXIsIHRoYXQncyBkb25lIG5vdy4KCllvdSBh
cmUgd2VsY29tZS4KIAo+IEJ0dywgaSdtIG5vdCBhbiBub3IgYW4gbGludXggbmVpdGhlciBhbiBo
YXJkd2hhcmUgZXhwZXJ0IGFuZCBpIGhhdmUgYQo+IGRvdWJ0IGluIGZyb250IG9mIHRoZSAnaTY4
Nicgc3VmZml4IG9mIHRoZSBycG06Cj4gQ291bGQgeW91IGNvbmZpcm0gdGhhdCB0aGUgcnBtIHlv
dSBzdWdnZXN0IG1lIHdpbGwgZml0IG15IGJveAo+IFJIRUwtNC8zODYuCgpZZWFoLCBpdCB3aWxs
IGZpdC4gaTY4NiBpcyBmb3IgcHJvY2Vzc29ycyBQLTIgYW5kIGFib3ZlLgoKUmVnYXJkcywKCi0t
IApEZXZyaW0gR8OcTkTDnFogLCBSSENFClBvc3RncmVTUUwgUmVwbGljYXRpb24sIENvbnN1bHRp
bmcsIEN1c3RvbSBEZXZlbG9wbWVudCwgMjR4NyBzdXBwb3J0Ck1hbmFnZWQgU2VydmljZXMsIFNo
YXJlZCBhbmQgRGVkaWNhdGVkIEhvc3RpbmcKQ28tQXV0aG9yczogcGxQSFAsIE9EQkNuZyAtIGh0
dHA6Ly93d3cuY29tbWFuZHByb21wdC5jb20vCi0tLS0tLS0tLS0tLS0tIG5leHQgcGFydCAtLS0t
LS0tLS0tLS0tLQpBIG5vbi10ZXh0IGF0dGFjaG1lbnQgd2FzIHNjcnViYmVkLi4uCk5hbWU6IG5v
dCBhdmFpbGFibGUKVHlwZTogYXBwbGljYXRpb24vcGdwLXNpZ25hdHVyZQpTaXplOiAxODkgYnl0
ZXMKRGVzYzogVGhpcyBpcyBhIGRpZ2l0YWxseSBzaWduZWQgbWVzc2FnZSBwYXJ0ClVybCA6IGh0
dHA6Ly9saXN0cy5zbG9ueS5pbmZvL3BpcGVybWFpbC9zbG9ueTEtZ2VuZXJhbC9hdHRhY2htZW50
cy8yMDA4MDIyOC8yMWRkMDhkMi9hdHRhY2htZW50LnBncAo=
From Ow.Mun.Heng at wdc.com  Thu Feb 28 10:14:24 2008
From: Ow.Mun.Heng at wdc.com (Ow Mun Heng)
Date: Thu Feb 28 10:16:37 2008
Subject: [Slony1-general] Force cleanup of log_1/log_2
Message-ID: <1204222464.13933.2.camel@neuromancer.home.net>

I've done the forced log_switching and I've waited >12 hours and still
the cleanup on the logs has still not occurred.

How can I force the cleanup to happen?

I'm already doing -c2 and now trying -c1 options

From cbbrowne at ca.afilias.info  Thu Feb 28 13:01:51 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Feb 28 13:02:04 2008
Subject: [Slony1-general] Force cleanup of log_1/log_2
In-Reply-To: <1204222464.13933.2.camel@neuromancer.home.net> (Ow Mun Heng's
	message of "Fri, 29 Feb 2008 02:14:24 +0800")
References: <1204222464.13933.2.camel@neuromancer.home.net>
Message-ID: <60fxvcdccg.fsf@dba2.int.libertyrms.com>

Ow Mun Heng <Ow.Mun.Heng@wdc.com> writes:
> I've done the forced log_switching and I've waited >12 hours and still
> the cleanup on the logs has still not occurred.
>
> How can I force the cleanup to happen?
>
> I'm already doing -c2 and now trying -c1 options

Several things can prevent the log switch from completing, and running
TRUNCATE against sl_log_1.  (I assume, here, the switch from sl_log_1
to sl_log_2; the same applies, without loss of generality, to the
opposite direction...)

Basically, if there are any tuples left in sl_log_1, it won't TRUNCATE
sl_log_1, and complete the switch.

That could happen due to several different apparent causes that can
manifest, but at base, it goes back to the issue of whether:

- Replication is keeping up to date, that is, the data in sl_log_1 has
  all been replicated to all the subscribes, and hence isn't needed
  anymore;

- Confirmations are getting communicated back and forth successfully;

- OLD confirmations are therefore able to be purged.

The script "test_slony_state.pl" (or DBI-based brother,
test_slony_state-dbi.pl) monitors this sort of information, and
generates warnings when it sees some known-problematic states.  

Have you been running that script?

If you are seeing any sort of "mysterious" behaviour, the FIRST thing
to do is to run that script, and see if it has any guidance for you.

Indeed, when I was in operations, I would set the policy of running
test_slony_state.pl against *EVERY* cluster, once an hour, as a
standard health check.  It notices quite a number of conditions that
will eventually adversely affect a cluster.  

I am certain that the output from it would be useful in diagnosing why
you are seeing bloating of some of the Slony-I tables.
-- 
(reverse (concatenate 'string "gro.mca" "@" "enworbbc"))
http://www3.sympatico.ca/cbbrowne/spiritual.html
Is A.I. Possible?
Some ask "Can humans create intelligent machines?" In fact, humans do
it all the time. The question needs to be "Since it's possible in the
bedroom, why shouldn't it be possible in the laboratory?"
-- Mark Miller
From cjames at emolecules.com  Thu Feb 28 18:17:29 2008
From: cjames at emolecules.com (Craig A. James)
Date: Thu Feb 28 18:17:41 2008
Subject: [Slony1-general] STILL can't migrate a node.
In-Reply-To: <47C6B83B.9060208@Yahoo.com>
References: <47B463D5.90605@emolecules.com>	<47BB74B8.2030808@emolecules.com>	<20080221144755.GC17293@crankycanuck.ca>	<47BD907C.2050603@dalibo.com>	<20080221171359.GA18657@crankycanuck.ca>	<47BF9073.7060708@emolecules.com>	<47BFAD0E.2000902@emolecules.com>
	<47C04A24.8010209@Yahoo.com>	<47C065E9.7070408@emolecules.com>
	<47C06A22.5050301@Yahoo.com>	<47C06BC1.8060805@emolecules.com>
	<47C07385.5080602@Yahoo.com> <47C2D7D1.7060206@emolecules.com>
	<47C6B83B.9060208@Yahoo.com>
Message-ID: <47C76B39.4010400@emolecules.com>

Jan Wieck wrote:
> On 2/25/2008 9:59 AM, Craig A. James wrote:
>> I sent the attached email ony to Jan by mistake, here it is, I hope 
>> Jan or somebody has an idea what's going on.
> 
> What I still don't understand is what that stored procedure moveSet() is 
> waiting for. Are there any pg_locks entries with granted='f' at all?

Forget the whole question, the mystery is solved.

A rogue process with a bug in it had created five million tables (that's right, five MILLION tables) in the database.  The process was creating about 50,000 tables per day, and had been running for almost three months.

Now we can't even do a single "pg_dump --table" because getting an exclusive lock seems to take forever and a day.  So we can't dump it, and we can't use Slony to move the node.  Fortunately, we already had the most important tables replicated via Slony, which was still working well, so we were able to just drop Slony replication and transfer the whole load to the backup database.  We'll simply discard the corrupted database and recreate it with Slony.

What's really amazing is that Postgres works at all with five million tables.  Performance had degraded just a bit, but not enough to trigger any alarms, we didn't even notice that anything was wrong for three months, until we tried to do some sysadmin (move a Slony master node) and couldn't.

Thanks for all your help.

Craig
From henry at zen.co.za  Thu Feb 28 22:04:12 2008
From: henry at zen.co.za (Henry)
Date: Thu Feb 28 22:47:21 2008
Subject: [Slony1-general] STILL can't migrate a node.
In-Reply-To: <47C76B39.4010400@emolecules.com>
References: <47B463D5.90605@emolecules.com> <47BB74B8.2030808@emolecules.com>
	<20080221144755.GC17293@crankycanuck.ca>
	<47BD907C.2050603@dalibo.com>
	<20080221171359.GA18657@crankycanuck.ca>
	<47BF9073.7060708@emolecules.com> <47BFAD0E.2000902@emolecules.com>
	<47C04A24.8010209@Yahoo.com> <47C065E9.7070408@emolecules.com>
	<47C06A22.5050301@Yahoo.com> <47C06BC1.8060805@emolecules.com>
	<47C07385.5080602@Yahoo.com> <47C2D7D1.7060206@emolecules.com>
	<47C6B83B.9060208@Yahoo.com> <47C76B39.4010400@emolecules.com>
Message-ID: <64617.196.23.181.69.1204265052.squirrel@zenmail.co.za>

On Fri, February 29, 2008 4:17 am, Craig A. James wrote:
> Forget the whole question, the mystery is solved.
>
> What's really amazing is that Postgres works at all with five million
> tables.  Performance had degraded just a bit, but not enough to trigger
> any alarms, we didn't even notice that anything was wrong for three
> months, until we tried to do some sysadmin (move a Slony master node) and
> couldn't.

We've been using PG since 6.something.  It's always nice having our choice
and confidence in PG confirmed by front-line anecdotal stories like
this...  5 million tables and still soldiering on.  yeah baby!

Hats off to the PG devs (sorry for the unsolicited CC Tom, but you *had*
to know about this).

Regards
Henry

From victor.aluko at gmail.com  Fri Feb 29 00:32:15 2008
From: victor.aluko at gmail.com (ajcity)
Date: Fri Feb 29 00:32:39 2008
Subject: [Slony1-general] Slony-I build cant find postgresql include folder
In-Reply-To: <20080221150458.GE17293@crankycanuck.ca>
References: <15586012.post@talk.nabble.com>
	<20080221150458.GE17293@crankycanuck.ca>
Message-ID: <15754148.post@talk.nabble.com>


Hi all,
  For those who might be interested, I built the slony from tarball and it
installed even with the "Missing PG include folder" (or something like that)
error. So far no problems.

-- 
View this message in context: http://www.nabble.com/Slony-I-build-cant-find-postgresql-include-folder-tp15586012p15754148.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From victor.aluko at gmail.com  Fri Feb 29 01:26:28 2008
From: victor.aluko at gmail.com (ajcity)
Date: Fri Feb 29 01:26:53 2008
Subject: [Slony1-general] Slony queueing sync events but not replicating
Message-ID: <15754921.post@talk.nabble.com>


Hi All,
  I setup Slony to replicate between a RHEL 4 (master) and OpenSuSE 10.3
(slave). After using slonik scripts, it replicated the first 5 tables that I
added. When I stop the slon daemon on both machines (using Ctrl-C) and
restart it, it echoes a "SETADDTABLE error: table id 1 is already set.
Cannot copy" 
  I dropped the cluster, and created another cluster (testrepcluster). I
subscribed other tables to testrepcluster and started the slon daemons
before subscribing the slave to the master then it gives these logs:

2008-02-28 16:03:55 EST DEBUG2 remoteWorkerThread_1: prepare to copy table
"public"."downloads_till_30092007"
2008-02-28 16:03:56 EST DEBUG3 remoteWorkerThread_1: table
"public"."downloads_till_30092007" does not require Slony-I serial key
2008-02-28 16:03:56 EST DEBUG2 remoteWorkerThread_1: prepare to copy table
"public"."gamescribers"
2008-02-28 16:03:56 EST DEBUG3 remoteWorkerThread_1: table
"public"."gamescribers" does not require Slony-I serial key
2008-02-28 16:03:56 EST DEBUG2 remoteWorkerThread_1: prepare to copy table
"public"."glowapcontent"
2008-02-28 16:03:57 EST DEBUG3 remoteWorkerThread_1: table
"public"."glowapcontent" does not require Slony-I serial key
2008-02-28 16:03:57 EST DEBUG2 remoteWorkerThread_1: prepare to copy table
"public"."horoscopecontent"
2008-02-28 16:03:58 EST DEBUG3 remoteWorkerThread_1: table
"public"."horoscopecontent" does not require Slony-I serial key
2008-02-28 16:03:58 EST DEBUG2 remoteWorkerThread_1: prepare to copy table
"public"."lgpintable"
2008-02-28 16:03:58 EST DEBUG3 remoteWorkerThread_1: table
"public"."lgpintable" does not require Slony-I serial key
2008-02-28 16:03:58 EST DEBUG2 remoteWorkerThread_1: prepare to copy table
"public"."mobilexpression"
2008-02-28 16:03:59 EST DEBUG3 remoteWorkerThread_1: table
"public"."mobilexpression" does not require Slony-I serial key
2008-02-28 16:03:59 EST DEBUG2 remoteWorkerThread_1: prepare to copy table
"public"."religioususers"
2008-02-28 16:03:59 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 19
2008-02-28 16:04:00 EST DEBUG3 remoteWorkerThread_1: table
"public"."religioususers" does not require Slony-I serial key
2008-02-28 16:04:00 EST DEBUG2 remoteWorkerThread_1: all tables for set 1
found on subscriber
2008-02-28 16:04:01 EST DEBUG2 remoteWorkerThread_1: copy table
"public"."downloads_till_30092007"
2008-02-28 16:04:02 EST DEBUG3 remoteWorkerThread_1: table
"public"."downloads_till_30092007" does not require Slony-I serial key
2008-02-28 16:04:03 EST DEBUG2 remoteWorkerThread_1: Begin COPY of table
"public"."downloads_till_30092007"
2008-02-28 16:04:03 EST DEBUG2 remoteWorkerThread_1:  nodeon73 is 0
NOTICE:  truncate of "public"."downloads_till_30092007" succeeded
2008-02-28 16:04:04 EST DEBUG2 remoteListenThread_1: queue event 1,29 SYNC
2008-02-28 16:04:05 EST DEBUG2 localListenThread: Received event 2,19 SYNC
2008-02-28 16:04:09 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 20
2008-02-28 16:04:11 EST DEBUG2 localListenThread: Received event 2,20 SYNC
2008-02-28 16:04:17 EST DEBUG2 remoteListenThread_1: queue event 1,30 SYNC
2008-02-28 16:04:19 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 21
2008-02-28 16:04:22 EST DEBUG2 remoteListenThread_1: queue event 1,31 SYNC
2008-02-28 16:04:23 EST DEBUG2 localListenThread: Received event 2,21 SYNC
2008-02-28 16:04:29 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 22
2008-02-28 16:04:35 EST DEBUG2 remoteListenThread_1: queue event 1,32 SYNC
2008-02-28 16:04:35 EST DEBUG2 localListenThread: Received event 2,22 SYNC
2008-02-28 16:04:40 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 23
2008-02-28 16:04:41 EST DEBUG2 localListenThread: Received event 2,23 SYNC
2008-02-28 16:04:49 EST DEBUG2 remoteListenThread_1: queue event 1,33 SYNC
2008-02-28 16:04:50 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 24
2008-02-28 16:04:53 EST DEBUG2 localListenThread: Received event 2,24 SYNC
2008-02-28 16:04:54 EST DEBUG2 remoteListenThread_1: queue event 1,34 SYNC
2008-02-28 16:05:00 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 25
2008-02-28 16:05:05 EST DEBUG2 localListenThread: Received event 2,25 SYNC
2008-02-28 16:05:06 EST DEBUG2 remoteListenThread_1: queue event 1,35 SYNC
2008-02-28 16:05:10 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 26
2008-02-28 16:05:11 EST DEBUG2 localListenThread: Received event 2,26 SYNC
2008-02-28 16:05:18 EST DEBUG2 remoteListenThread_1: queue event 1,36 SYNC
2008-02-28 16:05:20 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 27
2008-02-28 16:05:23 EST DEBUG2 localListenThread: Received event 2,27 SYNC
2008-02-28 16:05:25 EST DEBUG2 remoteListenThread_1: queue event 1,37 SYNC

The logs continue like that and there is no data in any of the tables.
I stopped the daemons after leaving it throughout the night, dropped the
clusters on both the master and slave.
I then created another cluster, added tables different from the previous
ones and then subscribed again but the same thing happens with the logs and
no data in any of the tables.
Does anybody know what the problem could be?

   Victor

-- 
View this message in context: http://www.nabble.com/Slony-queueing-sync-events-but-not-replicating-tp15754921p15754921.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080229/885a46f5/attachment.htm
From victor.aluko at gmail.com  Fri Feb 29 01:28:27 2008
From: victor.aluko at gmail.com (ajcity)
Date: Fri Feb 29 01:28:52 2008
Subject: [Slony1-general] Slony queueing sync events but not replicating
In-Reply-To: <15754921.post@talk.nabble.com>
References: <15754921.post@talk.nabble.com>
Message-ID: <15754948.post@talk.nabble.com>




Hi All,
  I setup Slony to replicate between a RHEL 4 (master) and OpenSuSE 10.3
(slave). After using slonik scripts, it replicated the first 5 tables that I
added. When I stop the slon daemon on both machines (using Ctrl-C) and
restart it, it echoes a "SETADDTABLE error: table id 1 is already set.
Cannot copy" 
  I dropped the cluster, and created another cluster (testrepcluster). I
subscribed other tables to testrepcluster and started the slon daemons
before subscribing the slave to the master then it gives these logs:

2008-02-28 16:03:55 EST DEBUG2 remoteWorkerThread_1: prepare to copy table
"public"."downloads_till_30092007"
2008-02-28 16:03:56 EST DEBUG3 remoteWorkerThread_1: table
"public"."downloads_till_30092007" does not require Slony-I serial key
2008-02-28 16:03:56 EST DEBUG2 remoteWorkerThread_1: prepare to copy table
"public"."gamescribers"
2008-02-28 16:03:56 EST DEBUG3 remoteWorkerThread_1: table
"public"."gamescribers" does not require Slony-I serial key
2008-02-28 16:03:56 EST DEBUG2 remoteWorkerThread_1: prepare to copy table
"public"."glowapcontent"
2008-02-28 16:03:57 EST DEBUG3 remoteWorkerThread_1: table
"public"."glowapcontent" does not require Slony-I serial key
2008-02-28 16:03:57 EST DEBUG2 remoteWorkerThread_1: prepare to copy table
"public"."horoscopecontent"
2008-02-28 16:03:58 EST DEBUG3 remoteWorkerThread_1: table
"public"."horoscopecontent" does not require Slony-I serial key
2008-02-28 16:03:58 EST DEBUG2 remoteWorkerThread_1: prepare to copy table
"public"."lgpintable"
2008-02-28 16:03:58 EST DEBUG3 remoteWorkerThread_1: table
"public"."lgpintable" does not require Slony-I serial key
2008-02-28 16:03:58 EST DEBUG2 remoteWorkerThread_1: prepare to copy table
"public"."mobilexpression"
2008-02-28 16:03:59 EST DEBUG3 remoteWorkerThread_1: table
"public"."mobilexpression" does not require Slony-I serial key
2008-02-28 16:03:59 EST DEBUG2 remoteWorkerThread_1: prepare to copy table
"public"."religioususers"
2008-02-28 16:03:59 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 19
2008-02-28 16:04:00 EST DEBUG3 remoteWorkerThread_1: table
"public"."religioususers" does not require Slony-I serial key
2008-02-28 16:04:00 EST DEBUG2 remoteWorkerThread_1: all tables for set 1
found on subscriber
2008-02-28 16:04:01 EST DEBUG2 remoteWorkerThread_1: copy table
"public"."downloads_till_30092007"
2008-02-28 16:04:02 EST DEBUG3 remoteWorkerThread_1: table
"public"."downloads_till_30092007" does not require Slony-I serial key
2008-02-28 16:04:03 EST DEBUG2 remoteWorkerThread_1: Begin COPY of table
"public"."downloads_till_30092007"
2008-02-28 16:04:03 EST DEBUG2 remoteWorkerThread_1:  nodeon73 is 0
NOTICE:  truncate of "public"."downloads_till_30092007" succeeded
2008-02-28 16:04:04 EST DEBUG2 remoteListenThread_1: queue event 1,29 SYNC
2008-02-28 16:04:05 EST DEBUG2 localListenThread: Received event 2,19 SYNC
2008-02-28 16:04:09 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 20
2008-02-28 16:04:11 EST DEBUG2 localListenThread: Received event 2,20 SYNC
2008-02-28 16:04:17 EST DEBUG2 remoteListenThread_1: queue event 1,30 SYNC
2008-02-28 16:04:19 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 21
2008-02-28 16:04:22 EST DEBUG2 remoteListenThread_1: queue event 1,31 SYNC
2008-02-28 16:04:23 EST DEBUG2 localListenThread: Received event 2,21 SYNC
2008-02-28 16:04:29 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 22
2008-02-28 16:04:35 EST DEBUG2 remoteListenThread_1: queue event 1,32 SYNC
2008-02-28 16:04:35 EST DEBUG2 localListenThread: Received event 2,22 SYNC
2008-02-28 16:04:40 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 23
2008-02-28 16:04:41 EST DEBUG2 localListenThread: Received event 2,23 SYNC
2008-02-28 16:04:49 EST DEBUG2 remoteListenThread_1: queue event 1,33 SYNC
2008-02-28 16:04:50 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 24
2008-02-28 16:04:53 EST DEBUG2 localListenThread: Received event 2,24 SYNC
2008-02-28 16:04:54 EST DEBUG2 remoteListenThread_1: queue event 1,34 SYNC
2008-02-28 16:05:00 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 25
2008-02-28 16:05:05 EST DEBUG2 localListenThread: Received event 2,25 SYNC
2008-02-28 16:05:06 EST DEBUG2 remoteListenThread_1: queue event 1,35 SYNC
2008-02-28 16:05:10 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 26
2008-02-28 16:05:11 EST DEBUG2 localListenThread: Received event 2,26 SYNC
2008-02-28 16:05:18 EST DEBUG2 remoteListenThread_1: queue event 1,36 SYNC
2008-02-28 16:05:20 EST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 27
2008-02-28 16:05:23 EST DEBUG2 localListenThread: Received event 2,27 SYNC
2008-02-28 16:05:25 EST DEBUG2 remoteListenThread_1: queue event 1,37 SYNC

The logs continue like that and there is no data in any of the tables.
I stopped the daemons after leaving it throughout the night, dropped the
clusters on both the master and slave.
I then created another cluster, added tables different from the previous
ones and then subscribed again but the same thing happens with the logs and
no data in any of the tables.
Does anybody know what the problem could be?

   Victor


-- 
View this message in context: http://www.nabble.com/Slony-queueing-sync-events-but-not-replicating-tp15754921p15754948.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From liobod.slony at gmail.com  Fri Feb 29 06:28:16 2008
From: liobod.slony at gmail.com (lio bod)
Date: Fri Feb 29 06:28:21 2008
Subject: [Slony1-general] gmake rpm vs tarball
In-Reply-To: <1204219663.3327.132.camel@localhost.localdomain>
References: <d4f444290802130422k2bb187d4xd29649be8ab9604a@mail.gmail.com>
	<1203016714.24118.245.camel@localhost.localdomain>
	<d4f444290802270253g14ab2cf5yc18a6c2b73f53d35@mail.gmail.com>
	<1204219663.3327.132.camel@localhost.localdomain>
Message-ID: <d4f444290802290628w3dec3c7bpd312d7b1dca965f5@mail.gmail.com>

I confirm.
I took the time to reinstall postgres & slony from scratch with rpm it and,
so far, it works.
I'm glad to know i will not require a compilation env on prod ;-)

Soon, i'll have to do the same for a rhel-4-x86_64.
So stay tuned.

rgds

2008/2/28, Devrim G=DCND=DCZ <devrim@commandprompt.com>:
>
> Hi,
>
> On Wed, 2008-02-27 at 11:53 +0100, lio bod wrote:
> > That's a while you answered me and i noticed i forgot to thank you.
> > So please consider, that's done now.
>
> You are welcome.
>
> > Btw, i'm not an nor an linux neither an hardwhare expert and i have a
> > doubt in front of the 'i686' suffix of the rpm:
> > Could you confirm that the rpm you suggest me will fit my box
> > RHEL-4/386.
>
> Yeah, it will fit. i686 is for processors P-2 and above.
>
> Regards,
>
> --
> Devrim G=DCND=DCZ , RHCE
> PostgreSQL Replication, Consulting, Custom Development, 24x7 support
> Managed Services, Shared and Dedicated Hosting
> Co-Authors: plPHP, ODBCng - http://www.commandprompt.com/
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20080229/=
973ad47c/attachment.htm
From JanWieck at Yahoo.com  Fri Feb 29 06:06:33 2008
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Fri Feb 29 07:03:16 2008
Subject: [Slony1-general] STILL can't migrate a node.
In-Reply-To: <47C76B39.4010400@emolecules.com>
References: <47B463D5.90605@emolecules.com>	<47BB74B8.2030808@emolecules.com>	<20080221144755.GC17293@crankycanuck.ca>	<47BD907C.2050603@dalibo.com>	<20080221171359.GA18657@crankycanuck.ca>	<47BF9073.7060708@emolecules.com>	<47BFAD0E.2000902@emolecules.com>	<47C04A24.8010209@Yahoo.com>	<47C065E9.7070408@emolecules.com>	<47C06A22.5050301@Yahoo.com>	<47C06BC1.8060805@emolecules.com>	<47C07385.5080602@Yahoo.com>
	<47C2D7D1.7060206@emolecules.com>	<47C6B83B.9060208@Yahoo.com>
	<47C76B39.4010400@emolecules.com>
Message-ID: <47C81169.4060502@Yahoo.com>

On 2/28/2008 9:17 PM, Craig A. James wrote:
> Jan Wieck wrote:
>> On 2/25/2008 9:59 AM, Craig A. James wrote:
>>> I sent the attached email ony to Jan by mistake, here it is, I hope 
>>> Jan or somebody has an idea what's going on.
>> 
>> What I still don't understand is what that stored procedure moveSet() is 
>> waiting for. Are there any pg_locks entries with granted='f' at all?
> 
> Forget the whole question, the mystery is solved.
> 
> A rogue process with a bug in it had created five million tables (that's right, five MILLION tables) in the database.  The process was creating about 50,000 tables per day, and had been running for almost three months.

Thanks for sharing this. Good to know that it after all wasn't caused by 
Slony.

However, this should raise another question on your side. The time 
consuming part inside of either lockSet() or moveSet() is a loop over a 
join between Slony's sl_table and Postgres' pg_class. The join condition 
between the two is sl_table.tab_reloid -> pg_class.oid. Also involved in 
the query is pg_namespace, but I think we can ignore it in this case. 
Your pg_class had grown into a 5M row table, which suggests that the 
join should have led to an outer loop over sl_table performing index 
scans via pg_class_oid_index into pg_class. Is it possible that your 
vacuum strategy misses to vacuum the system catalog?


> 
> Now we can't even do a single "pg_dump --table" because getting an exclusive lock seems to take forever and a day.  So we can't dump it, and we can't use Slony to move the node.  Fortunately, we already had the most important tables replicated via Slony, which was still working well, so we were able to just drop Slony replication and transfer the whole load to the backup database.  We'll simply discard the corrupted database and recreate it with Slony.

An ANALYZE of pg_class might get you some relief on that.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From cbbrowne at ca.afilias.info  Fri Feb 29 09:06:04 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Feb 29 09:06:12 2008
Subject: [Slony1-general] STILL can't migrate a node.
In-Reply-To: <47C76B39.4010400@emolecules.com> (Craig A. James's message of
	"Thu, 28 Feb 2008 18:17:29 -0800")
References: <47B463D5.90605@emolecules.com> <47BB74B8.2030808@emolecules.com>
	<20080221144755.GC17293@crankycanuck.ca>
	<47BD907C.2050603@dalibo.com>
	<20080221171359.GA18657@crankycanuck.ca>
	<47BF9073.7060708@emolecules.com> <47BFAD0E.2000902@emolecules.com>
	<47C04A24.8010209@Yahoo.com> <47C065E9.7070408@emolecules.com>
	<47C06A22.5050301@Yahoo.com> <47C06BC1.8060805@emolecules.com>
	<47C07385.5080602@Yahoo.com> <47C2D7D1.7060206@emolecules.com>
	<47C6B83B.9060208@Yahoo.com> <47C76B39.4010400@emolecules.com>
Message-ID: <60wsonbslf.fsf@dba2.int.libertyrms.com>

"Craig A. James" <cjames@emolecules.com> writes:
> Jan Wieck wrote:
>> On 2/25/2008 9:59 AM, Craig A. James wrote:
>>> I sent the attached email ony to Jan by mistake, here it is, I hope
>>> Jan or somebody has an idea what's going on.
>> What I still don't understand is what that stored procedure
>> moveSet() is waiting for. Are there any pg_locks entries with
>> granted='f' at all?
>
> Forget the whole question, the mystery is solved.
>
> A rogue process with a bug in it had created five million tables (that's right, five MILLION tables) in the database.  The process was creating about 50,000 tables per day, and had been running for almost three months.
>
> Now we can't even do a single "pg_dump --table" because getting an exclusive lock seems to take forever and a day.  So we can't dump it, and we can't use Slony to move the node.  Fortunately, we already had the most important tables replicated via Slony, which was still working well, so we were able to just drop Slony replication and transfer the whole load to the backup database.  We'll simply discard the corrupted database and recreate it with Slony.
>
> What's really amazing is that Postgres works at all with five
> million tables.  Performance had degraded just a bit, but not enough
> to trigger any alarms, we didn't even notice that anything was wrong
> for three months, until we tried to do some sysadmin (move a Slony
> master node) and couldn't.
>
> Thanks for all your help.

I'm glad that the mystery eventually evaporated...

A while back, I created a database entitled "milliontablemarch", where
the point was to see how quickly I could create a rather correlated
;-) quantity of tables.

<http://archives.postgresql.org/pgsql-advocacy/2006-03/msg00095.php>

Apparently Certain Other Products aren't robust under such conditions;
the start of that message thread indicates:

-----------
I see in this post on planetmysql.org that this guy got MySQL to die
after creating 250k tables. Anyone want to see how far PostgreSQL can
go? :)

http://bobfield.blogspot.com/2006/03/million-tables.html

Also, can we beat his estimate of 27 hours for creation?
-----------

FYI, on the p570 cluster that I used, I was able to create 1m tables
in about 31 minutes, which did indeed beat 27h ;-).  Greg Sabino
Mullane had a faster time than I did, almost certainly on inferior
hardware than I had...
-- 
(reverse (concatenate 'string "ofni.sesabatadxunil" "@" "enworbbc"))
http://linuxdatabases.info/info/languages.html
cc hello.c, in Canada, results in:
  eh.oot
From cbbrowne at ca.afilias.info  Fri Feb 29 09:08:07 2008
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Feb 29 09:08:14 2008
Subject: [Slony1-general] Bug #35 - status???
Message-ID: <60skzbbsi0.fsf@dba2.int.libertyrms.com>

This doesn't seem to have progressed much this week, and it seems to
warrant release of 1.2.14 :-(.

http://www.slony.info/bugzilla/show_bug.cgi?id=35
-- 
select 'cbbrowne' || '@' || 'cbbrowne.com';
http://cbbrowne.com/info/
"It has every known bug fix to everything." -- KLH (out of context)
From dpage at pgadmin.org  Fri Feb 29 09:30:23 2008
From: dpage at pgadmin.org (Dave Page)
Date: Fri Feb 29 09:30:33 2008
Subject: [Slony1-general] Bug #35 - status???
In-Reply-To: <60skzbbsi0.fsf@dba2.int.libertyrms.com>
References: <60skzbbsi0.fsf@dba2.int.libertyrms.com>
Message-ID: <937d27e10802290930m58e60e94odc2ed3dfa9172f15@mail.gmail.com>

On Fri, Feb 29, 2008 at 5:08 PM, Christopher Browne
<cbbrowne@ca.afilias.info> wrote:
> This doesn't seem to have progressed much this week, and it seems to
> warrant release of 1.2.14 :-(.

Per the last few comments, I'm waiting for a yes or no answer from
Peter. Whilst his last comment seems valid, the amount of code that
would need to be stolen from Postgres seems quite excessive,
especially for the stable branch.

-- 
Dave Page
EnterpriseDB UK Ltd: http://www.enterprisedb.com
PostgreSQL UK 2008 Conference: http://www.postgresql.org.uk
From osess76 at hotmail.com  Tue Feb  5 02:25:36 2008
From: osess76 at hotmail.com (Jack B)
Date: Thu Sep 25 08:26:49 2008
Subject: [Slony1-general] Long timeout during slony fail over
Message-ID: <BLU126-W66B46C05DB85719657D9EB72C0@phx.gbl>



Hi

I am using the slony replication system for the postgresql database, which keeps database in sync between two machines: the active (master) and backup (slave) host. If, for some reason, the active host goes down, the backup host takes control and becomes the active host. During this switch-over process from backup to active, the failover script is executed on the backup machine. The script attempts to contact the peer host (which is now down), and gives up only after about 20 seconds. I would like to reduce this timeout in order to speed up the switch-over process.
 
I haven't succeeded in locating and configuring this 20 seconds timeout in slony, and was advised to decrease the number of TCP SYN retries of the operating system (net.ipv4.tcp_syn_retries, in /etc/sysctl.conf), instead. Reducing this value to 0, led to a timeout of ~5 seconds. A value of 1 led to a timeout of ~10 seconds etc. Thus it seems that every retry takes about 5 seconds. However, I feel that this parameter may have a broader range of implications on the system, and would like to know if there's a way of controlling the timeout within the slony.

Regards,

J 

_________________________________________________________________
Helping your favorite cause is as easy as instant messaging.?You IM, we give.
http://im.live.com/Messenger/IM/Home/?source=text_hotmail_join
From victor.aluko at gmail.com  Wed Feb  6 09:38:21 2008
From: victor.aluko at gmail.com (ajcity)
Date: Thu Sep 25 08:26:51 2008
Subject: [Slony1-general] Slony setup over a LAN but not replicating
Message-ID: <15308421.post@talk.nabble.com>


Hi all,
  I setup Slony over a LAN following all the stated procedures i.e. Create
similar schema,create service on both machines and add both engines, Create
the cluster, create set, add tables, add path & listens and subscribe
set......But with all these its not replicating even after I restarted the
slon services on both machines. What am I supposed to do next? 
  And there was something I wasnt sure of when I setup the '.conf' files on
both machines...Am I supposed to specify 'localhost' as the host on the
first machine (say NY) and specify the IP address of the second machine (say
TX) or should I specify their IP addresses?
  Much love.

                           Victor
-- 
View this message in context: http://www.nabble.com/Slony-setup-over-a-LAN-but-not-replicating-tp15308421p15308421.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From khera at kcilink.com  Mon Feb 11 11:38:39 2008
From: khera at kcilink.com (Vivek Khera)
Date: Thu Sep 25 08:26:53 2008
Subject: [Slony1-general] stop/restarting postmaster on replication machine
In-Reply-To: <47B0A016.604@serioustechnology.com>
References: <47B0A016.604@serioustechnology.com>
Message-ID: <0F8114F3-562C-4DE7-B0E7-72475940F452@kcilink.com>


On Feb 11, 2008, at 2:20 PM, Geoffrey wrote:

> If I need to restart the postmaster on my slony replication slave,  
> should I shut down the slony daemons first?

shouldn't matter.  i run slony using the daemontools monitoring  
programs, so i never bother to stop slony.  the daemontools monitor  
will automatically restart it for me once the postmaster is back up.   
if you don't have an auto-restart, then you will need to manually  
restart slony after the new postmaster is up and running.

From troy at troywolf.com  Thu Feb 21 19:58:42 2008
From: troy at troywolf.com (TroyWolf)
Date: Thu Sep 25 08:26:54 2008
Subject: [Slony1-general] Applying schema changes in slony
In-Reply-To: <516869C8-1F93-4F13-82A9-D8305A95DFF6@pandora.com>
References: <8d89ea1d0712191837s76f03d6bva36f61b3800c7afd@mail.gmail.com>
	<516869C8-1F93-4F13-82A9-D8305A95DFF6@pandora.com>
Message-ID: <15627246.post@talk.nabble.com>



Casey Duncan wrote:
> 
> - Use many small table sets instead of one big one. This means you  
> may need multiple execute script statements, but each one will lock  
> fewer tables at once.

See this note from the middle of this page of the Slony Documentation:
http://slony.info/documentation/ddlchanges.html

Slony Documentation wrote:
> 
> You may be able to define replication sets that consist of smaller sets of
> tables so that fewer locks need to be taken in order for the DDL script to
> make it into place.
> 
> If a particular DDL script only affects one table, it should be
> unnecessary to lock all application tables.
> 
>     Note: Actually, as of version 1.1.5 and later, this is NOT TRUE. The
> danger of someone making DDL changes that crosses replication sets seems
> sufficiently palpable that slon has been changed to lock ALL replicated
> tables, whether they are in the specified replication set or not. 
> 
If I'm reading the docs correctly, breaking tables into multiple table sets
no longer helps with locking issues. Correct?

-- 
View this message in context: http://www.nabble.com/Applying-schema-changes-in-slony-tp14429677p15627246.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From tgl at sss.pgh.pa.us  Thu Feb 28 23:06:07 2008
From: tgl at sss.pgh.pa.us (Tom Lane)
Date: Thu Sep 25 08:26:55 2008
Subject: [Slony1-general] STILL can't migrate a node. 
In-Reply-To: <64617.196.23.181.69.1204265052.squirrel@zenmail.co.za> 
References: <47B463D5.90605@emolecules.com> <47BB74B8.2030808@emolecules.com>
	<20080221144755.GC17293@crankycanuck.ca>
	<47BD907C.2050603@dalibo.com>
	<20080221171359.GA18657@crankycanuck.ca>
	<47BF9073.7060708@emolecules.com>
	<47BFAD0E.2000902@emolecules.com> <47C04A24.8010209@Yahoo.com>
	<47C065E9.7070408@emolecules.com> <47C06A22.5050301@Yahoo.com>
	<47C06BC1.8060805@emolecules.com> <47C07385.5080602@Yahoo.com>
	<47C2D7D1.7060206@emolecules.com> <47C6B83B.9060208@Yahoo.com>
	<47C76B39.4010400@emolecules.com>
	<64617.196.23.181.69.1204265052.squirrel@zenmail.co.za>
Message-ID: <23552.1204268745@sss.pgh.pa.us>

"Henry" <henry@zen.co.za> writes:
> On Fri, February 29, 2008 4:17 am, Craig A. James wrote:
>> What's really amazing is that Postgres works at all with five million
>> tables.

> Hats off to the PG devs (sorry for the unsolicited CC Tom, but you *had*
> to know about this).

ROTFL ... actually, it doesn't surprise me that PG can handle 5M tables;
we do have indexes on the system catalogs ya know.  What is a bit
surprising is that your filesystem didn't fall over with 5M files in the
same directory.

I have heard that some newer filesystems contain what are effectively
btree indexes on the contents of a directory.  That strikes me as a bit
stupid, since surely the design center for a filesystem is not zillions
of files per directory.  But as a database we're definitely supposed to
handle zillions of rows per table, and that applies to pg_class and
related catalogs just as much as any other table.

Interesting story in any case.  I guess pg_dump still needs work ;-)

			regards, tom lane
From khera at kcilink.com  Thu Feb 14 07:44:24 2008
From: khera at kcilink.com (Vivek Khera)
Date: Thu Sep 25 08:27:08 2008
Subject: [Slony1-general] Slony-1 stability?
In-Reply-To: <6ccff2720802132030v212ee4f7mc56e9aa4d446807@mail.gmail.com>
References: <6ccff2720802130006t4ccdd00fn5b6322a03445c52d@mail.gmail.com>
	<60wsp86ep0.fsf@dba2.int.libertyrms.com>
	<6ccff2720802132030v212ee4f7mc56e9aa4d446807@mail.gmail.com>
Message-ID: <07F3359D-D2DB-4137-84A1-0F354E0D3BD4@kcilink.com>


On Feb 13, 2008, at 11:30 PM, Satya wrote:

> Thanks for the reply. So if their nothing required to change to  
> support newer version of PostgreSQL, then how frequent are the  
> releases?
> I mean, how frequently slony-1 is released if it there are no  
> incompatibilities with  PostgreSQL?  Is it  like every  3months or  
> 6months or 12months??

when it becomes necessary due to some feature discovered to be  
functioning incorrectly.

check the release history of the 1.2 line to see how often that has  
been.

for the most part, slony 1.2 is a very stable, slow-moving target.

From HTomeh at facorelogic.com  Thu Feb 14 08:29:31 2008
From: HTomeh at facorelogic.com (Tomeh, Husam)
Date: Thu Sep 25 08:27:10 2008
Subject: [Slony1-general] adding index without telling Slony
In-Reply-To: <47B465DB.8070805@emolecules.com>
References: <47B465DB.8070805@emolecules.com>
Message-ID: <F1B0F9305B343E43A1C3EECE48B853D501A841F7@CITGSNA01SXCH02.ana.firstamdata.com>


Yes, you should be fine. Slony will not care about it unless you
explicitly do so.

Regards,
    Husam

-----Original Message-----
From: slony1-general-bounces@lists.slony.info
[mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Craig
James
Sent: Thursday, February 14, 2008 8:02 AM
To: slony1-general@lists.slony.info
Subject: [Slony1-general] adding index without telling Slony

On a replicated table, can I add an index without telling Slony?  Say if
I have a replicated table foo(a,b,c,...), could I do "create index ifoo
on foo(a)" on the master node without using Slony's EXECUTE SCRIPT?

The reason for this is to experiment with performance.  I'm not sure
which index might be best, and I don't want to propagate the index to
the slave node until I've tested several solutions.

Thanks,
Craig
_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general
**********************************************************************
This message contains confidential information intended only for the use of the addressee(s) named above and may contain information that is legally privileged.  If you are not the addressee, or the person responsible for delivering it to the addressee, you are hereby notified that reading, disseminating, distributing or copying this message is strictly prohibited.  If you have received this message by mistake, please immediately notify us by replying to the message and delete the original message immediately thereafter.

Thank you.

                                   FADLD Tag
**********************************************************************
From hannu at krosing.net  Wed Feb 20 07:51:29 2008
From: hannu at krosing.net (Hannu Krosing)
Date: Thu Sep 25 08:27:11 2008
Subject: [Slony1-general] logtrigger
In-Reply-To: <47BC4B56.8070205@serioustechnology.com>
References: <47BC4B56.8070205@serioustechnology.com>
Message-ID: <1203522683.6557.4.camel@huvostro>


On Wed, 2008-02-20 at 10:46 -0500, Geoffrey wrote:
> I see the following trigger that was added by slony:
> 
>      _mwr_cluster_logtrigger_1 AFTER INSERT OR DELETE OR UPDATE ON avlds 
> FOR EACH ROW EXECUTE PROCEDURE _mwr_cluster.logtrigger('_mwr_cluster', 
> '1', 'kvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv')

it tells the trigger what to log

k - key
v -value

> Is that correct?  That last field looks kind of like 'controlled junk.'
> 
> Still trying to track down our slony problems.
> 

