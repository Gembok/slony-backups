From golohvastov at gmail.com  Mon Nov  2 13:11:59 2009
From: golohvastov at gmail.com (golohvastov)
Date: Mon Nov  2 15:39:56 2009
Subject: [Slony1-general] beginner questtion Replication every 4 hours. is
	that possible
Message-ID: <26157833.post@talk.nabble.com>


Hi there 

I am new to Slony and I could not find anywhere how to make Slony
replication happening only every 4 hours.
Please help

Thank you.
-- 
View this message in context: http://old.nabble.com/beginner-questtion-Replication-every-4-hours.-is-that-possible-tp26157833p26157833.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From kevink at consistentstate.com  Mon Nov  2 15:50:17 2009
From: kevink at consistentstate.com (Kevin Kempter)
Date: Mon Nov  2 15:50:43 2009
Subject: [Slony1-general] beginner questtion Replication every 4 hours. is
	that possible
In-Reply-To: <26157833.post@talk.nabble.com>
References: <26157833.post@talk.nabble.com>
Message-ID: <200911021650.17678.kevink@consistentstate.com>

On Monday 02 November 2009 14:11:59 golohvastov wrote:
> Hi there
> 
> I am new to Slony and I could not find anywhere how to make Slony
> replication happening only every 4 hours.
> Please help
> 
> Thank you.
> 


You could use the slon -l flag for a lag time but this will only force a lag, 
as opposed to a sync that only occurs once every 4 hrs. If you're wanting to 
force a sync only once every 4 hours I'd suggest one of the following:


1) use slony log shipping and only run the ingestion of the logs once every 4 
hrs

2) setup a standard slony cluster and every 4 hours do the following:
  a) start the slon logs
  b) after the slave is fully sync'd kill the slon daemons. This could 
possibly be done via the -x flag or via a script that watches the slon logs and 
kill's the processes once it see's that the copy set(s) are complete


 
From nimesh.satam at gmail.com  Wed Nov  4 21:09:23 2009
From: nimesh.satam at gmail.com (Nimesh Satam)
Date: Wed Nov  4 21:09:52 2009
Subject: [Slony1-general] How to confirm if a certain insert has completed
	on the slave db?
Message-ID: <65f3e23d0911042109w6f666a22raa2ea8331b8e2f31@mail.gmail.com>

All,

Is there any way to confirm that a certain insert has been completed on the
slave database? Would the sl_confirm or sl_event table be useful for this?

For eg: If I have a batch of 100 Inserts, how do I make sure that all have
been done on slave?

Regards,
Nimesh.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20091105/=
3b43c3cc/attachment.html
From threshar at torgo.978.org  Thu Nov  5 06:19:16 2009
From: threshar at torgo.978.org (Jeff)
Date: Thu Nov  5 06:19:40 2009
Subject: [Slony1-general] How to confirm if a certain insert has completed
	on the slave db?
In-Reply-To: <65f3e23d0911042109w6f666a22raa2ea8331b8e2f31@mail.gmail.com>
References: <65f3e23d0911042109w6f666a22raa2ea8331b8e2f31@mail.gmail.com>
Message-ID: <B9C1AF04-C2CC-4E3A-B1B2-B5CF1E7E01E4@torgo.978.org>


On Nov 5, 2009, at 12:09 AM, Nimesh Satam wrote:

> All,
>
> Is there any way to confirm that a certain insert has been completed  
> on the slave database? Would the sl_confirm or sl_event table be  
> useful for this?
>
> For eg: If I have a batch of 100 Inserts, how do I make sure that  
> all have been done on slave?
>
> Regards,
> Nimesh.


I've got this code which seems to work well.  I added a function to my  
DB access layer called "syncexec" that executes and waits for the  
slaves (up to a predefined interval).

You'll need to do your work inside a transaction and get the txn's  
txnid for this to work (you can obtain it via _cluster.getCurrentXid()

-------------- next part --------------
A non-text attachment was scrubbed...
Name: slony_syncwait.sql
Type: application/octet-stream
Size: 1765 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20091105/1e2ffd7b/slony_syncwait.obj
-------------- next part --------------


--
Jeff Trout <jeff@jefftrout.com>
http://www.stuarthamm.net/
http://www.dellsmartexitin.com/



From DSasikumar at intelligent-addressing.co.uk  Fri Nov  6 07:07:57 2009
From: DSasikumar at intelligent-addressing.co.uk (Don Sasikumar)
Date: Fri Nov  6 07:08:44 2009
Subject: [Slony1-general] Setting up slonyi replication in windows Xp
Message-ID: <274BA771880F684D9CC7E88C5C29E9D9011AB6BE@hermod.intelligent-addressing.co.uk>

HI All,

 

Just thought it might be useful.

 

SLONY-I on Windows XP using Slony 1.2.16 and postgres 8.3

1.    Create 3 databases, master, slave1 and slave2 and ensure pl/pgsql
is setup in each.
 

2.    Create a schema with tables in the master database:
 

3.    Add a primary key to each table.
 

4.    Create a schema-only dump of the master database, and load it into
slave1 and slave2: 

[This step needs to be followed only if the slave does not exists
already.]

> pg_dump -s -U postgres master > schema.sql [creates an sql file with
commands necessary to create tables and insert data in it with which we
can build the slave databases]
> psql -U postgres slave1 < schema.sql [slave database gets contructed]
> psql -U postgres slave2 < schema.sql [slave database gets contructed]
 

5.    Create Slony config files for each slon engine (daemon on *nix).
The files should contain just the following two lines:



Contents of Master.conf

cluster_name='pgbench'
conn_info='host=ip of master port=5432 user=postgres dbname=master' 

Contents of Slave.conf

cluster_name='pgbench'
conn_info='host=ip slave port=5432 user=postgres dbname=master' 

[Create a file for each database, adjusting the dbname parameter as
required and adding any other connection options that may be needed. For
the IP, it's better to use domain names to ensure that all servers are
addressed uniformly in pgadmin and slonyi]
 

6.    (Windows only) Install the Slony-I service:

> slon -regservice Slony-I 

[Slony-II, Slony-II, Slony-A, etc could also be used]
 

7.    Register each of the engines (this is only necessary on Windows -
on *nix the slon daemons may be started individually and given the path
to the config file on the command line using the -f option):

> slon -addengine Slony-I C:\slony\master.conf
> slon -addengine Slony-I C:\slony\slave1.conf
> slon -addengine Slony-I C:\slony\slave2.conf 

 

8.    In pgAdmin under the Replication node in the master database,
create a new Slony-I cluster using the following options.

Create a cluster in the master database

Join existing cluster: Unchecked

Cluster name:          pgbench (any name)

Local node:            1        Master node

Admin node:            99       Admin node

    

9.    Under the Replication node, create a Slony-I cluster in each of
the slave databases using the following options:

Join existing cluster: Checked

Server:                <Select the server containing the master
database>

Database:              master

Cluster name:          pgbench

Local node:            10       Slave node 1

Admin node:            99 - Admin node

 

Join existing cluster: Checked

Server:                <Select the server containing the master
database>

Database:              master

Cluster name:          pgbench

Local node:            20       Slave node 2

Admin node:            99 - Admin node

    

10.  Create Paths on the master to both slaves, and on each slave back
to the master. Create the paths under each node on the master, using the
connection strings specified in the slon config files. Note that future
restructuring of the cluster may require additional paths to be defined.


Example 

Slave path in Master node =>host=ip/ dns port=5432 user=postgres
dbname=nlpg

Master path in slave node => host=ip/ dns port=5432 user=postgres
dbname=nlpg
 

11.  Create a Replication Set on the master using the following
settings: 

ID:                  1

Comment:             rep set1 (any name)

 

12.  Add the tables to the replication set using the following [existing
table names are available from the drop down]: 

Table:               public.accounts

ID:                  1

Index:               accounts_pkey

 

Table:               public.branches

ID:                  2

Index:               branches_pkey

 

Table:               public.history

ID:                  3

Index:               history_pkey

 

Table:               public.tellers

ID:                  4

Index:               tellers_pkey

 

[In this approach all the tables are being added to one replication set
in the master. Alternatively, we can add n replication set for each
table.]

    

13.  On the master node, create a new subscription for each slave using
the following options:

Origin:              1

Provider:            1 - Master node

Receiver:            10 - Slave node 1

 

Origin:              1

Provider:            1 - Master node

Receiver:            20 - Slave node 2

 

With 'Call forward' checked.

 

[Need not repeat this step for slave, since once the replication is up
and running, the replication sets are automatically created on the slave
clusters.]

    

14.  Start the slon service (or daemons on *nix): 

> net start Slony-I 

[Check if the slon service is up and running using the windows taskbar]
 

15.  Open 2 command prompt windows on the path postgres/8.3/bin where
slon is installed.

In the first window, type the command to bring the master database up.

slon cluster_name "host=ip/dns  dbname=nlpg user=postgres
password=abcdef"

In the second window, type the command to bring the slave database up
and listening to the master slon.

slon cluster_name "host= ip/dns  port=5432 dbname=nlpg user=postgres
password=abcd"

[Now, we have to wait until all the tables from the replication set(s)
in the master database are synchronised with those in the slave
database.Once the synchronisation is done, the replication is ready to
be tested.]

 

 

 

Regards,

Don

donsasikumar@gmail.com



Intelligent Addressing Ltd. Registered Office: 1 Adam Street, London WC2N 6DD Reg. in England No.3863861 VAT No.GB 802258946

This message has been scanned for viruses by BlackSpider MailControl - www.blackspider.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20091106/e41dc7ef/attachment.htm
From scott.marlowe at gmail.com  Fri Nov  6 13:07:51 2009
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Fri Nov  6 13:08:08 2009
Subject: [Slony1-general] very slow create set
Message-ID: <dcc563d10911061307o1f4881a1ge4c1b401536e1ed7@mail.gmail.com>

I have about 1500 objects in my main db, and running create set takes
about 5 seconds per table or sequence.  Is this typical?  Any changes
I can make in terms of cost of any slony functions or creating indexes
to make it faster?
From vivek at khera.org  Sat Nov  7 20:02:39 2009
From: vivek at khera.org (Vick Khera)
Date: Sat Nov  7 20:03:12 2009
Subject: [Slony1-general] very slow create set
In-Reply-To: <dcc563d10911061307o1f4881a1ge4c1b401536e1ed7@mail.gmail.com>
References: <dcc563d10911061307o1f4881a1ge4c1b401536e1ed7@mail.gmail.com>
Message-ID: <2968dfd60911072002l1790c6btc591ac4efaa81fb0@mail.gmail.com>

On Fri, Nov 6, 2009 at 4:07 PM, Scott Marlowe <scott.marlowe@gmail.com> wrote:
> I have about 1500 objects in my main db, and running create set takes
> about 5 seconds per table or sequence. ?Is this typical? ?Any changes
> I can make in terms of cost of any slony functions or creating indexes
> to make it faster?
>

The most complicated one I have has about 250 tables and about 50
sequences.  I don't recall it taking that long to create a set.
Perhaps the time is proportional to the number of objects?  Which
versions of everything are you using?  I run Pg 8.3 and slony1 1.2.x.
From scott.marlowe at gmail.com  Sat Nov  7 20:23:25 2009
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Sat Nov  7 20:23:58 2009
Subject: [Slony1-general] very slow create set
In-Reply-To: <2968dfd60911072002l1790c6btc591ac4efaa81fb0@mail.gmail.com>
References: <dcc563d10911061307o1f4881a1ge4c1b401536e1ed7@mail.gmail.com>
	<2968dfd60911072002l1790c6btc591ac4efaa81fb0@mail.gmail.com>
Message-ID: <dcc563d10911072023k3cc4997fp283caf72fca4fcfb@mail.gmail.com>

On Sat, Nov 7, 2009 at 9:02 PM, Vick Khera <vivek@khera.org> wrote:
> On Fri, Nov 6, 2009 at 4:07 PM, Scott Marlowe <scott.marlowe@gmail.com> wrote:
>> I have about 1500 objects in my main db, and running create set takes
>> about 5 seconds per table or sequence. ?Is this typical? ?Any changes
>> I can make in terms of cost of any slony functions or creating indexes
>> to make it faster?
>>
>
> The most complicated one I have has about 250 tables and about 50
> sequences. ?I don't recall it taking that long to create a set.
> Perhaps the time is proportional to the number of objects? ?Which
> versions of everything are you using? ?I run Pg 8.3 and slony1 1.2.x.

Yep, pg 8.3.7 and slony 1.2.14 at the moment.  If I do this on a
machine with JUST the objects I'm replicating, I can get 10 seconds
per table and 5 or so per sequence.

On the DB with 29000 other objects not replicated, it gets up to 30
seconds per table and 20 seconds per sequence.  Which means a 5 hour
time for create set.  And if autovac kicks in it blocks create set.
so I had to turn that off.
From melvin6925 at yahoo.com  Sat Nov  7 20:32:31 2009
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Sat Nov  7 20:33:05 2009
Subject: [Slony1-general] very slow create set
Message-ID: <302031.19923.qm@web53005.mail.re2.yahoo.com>

>Yep, pg 8.3.7 and slony 1.2.14 at the moment.=A0 If I do this on a
>machine with JUST the objects I'm replicating, I can get 10 seconds
>per table and 5 or so per sequence.

>On the DB with 29000 other objects not replicated, it gets up to 30
>seconds per table and 20 seconds per sequence.=A0 Which means a 5 hour
>time for create set.=A0 And if autovac kicks in it blocks create set.
>so I had to turn that off.

This sound more like O/S and hardware tuning than anything else. =

Just out of curiosity, what is the O/S and hardware configuration?

I would at least hope that the data and WAL files are on separate disks?

Melvin Davidson
 =






      =

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20091107/=
8a61f38a/attachment.htm
From scott.marlowe at gmail.com  Sat Nov  7 21:19:04 2009
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Sat Nov  7 21:19:38 2009
Subject: [Slony1-general] very slow create set
In-Reply-To: <302031.19923.qm@web53005.mail.re2.yahoo.com>
References: <302031.19923.qm@web53005.mail.re2.yahoo.com>
Message-ID: <dcc563d10911072119m6499dcb7xfa3b5f4864dfb217@mail.gmail.com>

On Sat, Nov 7, 2009 at 9:32 PM, Melvin Davidson <melvin6925@yahoo.com>wrote:

> >Yep, pg 8.3.7 and slony 1.2.14 at the moment.  If I do this on a
> >machine with JUST the objects I'm replicating, I can get 10 seconds
> >per table and 5 or so per sequence.
>
> >On the DB with 29000 other objects not replicated, it gets up to 30
> >seconds per table and 20 seconds per sequence.  Which means a 5 hour
> >time for create set.  And if autovac kicks in it blocks create set.
> >so I had to turn that off.
>
> This sound more like O/S and hardware tuning than anything else.
> Just out of curiosity, what is the O/S and hardware configuration?
>
> I would at least hope that the data and WAL files are on separate disks?
>

This an 8 core opteron 2.1GHz machine with 32G ram and a 12 disk RAID-10 for
pgdata and a two disk RAID-1 for pg_xlog.  The autovac when it kicks off,
since it's set to have a 20 ms cost delay, purposely takes a while, so as
NOT to get in the way.  So, on a 2 to 5Gig table file it can take 10 to 20
minutes.  While this is happening, setaddtable is blocked.  While it's
blocked, all access to the table by other queries are blocked, and nothing
gets though.  This is NOT a tuning issue on the OS / hardware.  This machine
can handle 10 to 20k transactions per minute under normal load.  But the
interaction between autovac and create set / setaddtable halts all access to
the table being vacuumed.  The pg_lock and pg_stat_activity show it pretty
clearly.

However, I can turn off autovac during the create set.  But the create set
takes wayyyy too long.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20091107/=
d48c37f9/attachment.htm
From scott.marlowe at gmail.com  Sat Nov  7 21:20:18 2009
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Sat Nov  7 21:20:52 2009
Subject: [Slony1-general] very slow create set
In-Reply-To: <302031.19923.qm@web53005.mail.re2.yahoo.com>
References: <302031.19923.qm@web53005.mail.re2.yahoo.com>
Message-ID: <dcc563d10911072120x21ccbc7bp1a8f7406c550e862@mail.gmail.com>

On Sat, Nov 7, 2009 at 9:32 PM, Melvin Davidson <melvin6925@yahoo.com>wrote:

> >Yep, pg 8.3.7 and slony 1.2.14 at the moment.  If I do this on a
> >machine with JUST the objects I'm replicating, I can get 10 seconds
> >per table and 5 or so per sequence.
>
> >On the DB with 29000 other objects not replicated, it gets up to 30
> >seconds per table and 20 seconds per sequence.  Which means a 5 hour
> >time for create set.  And if autovac kicks in it blocks create set.
> >so I had to turn that off.
>
> This sound more like O/S and hardware tuning than anything else.
> Just out of curiosity, what is the O/S and hardware configuration?
>
> I would at least hope that the data and WAL files are on separate disks?
>

By the by, the 12 disk RAID-10 can sustain reads of about 450Meg/second and
writes of about 380 to 400Megs/second.  However, autovac gets no where near
this, ON PURPOSE, to keep it out of the way of the rest of the db during the
day.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20091107/=
27f18969/attachment.htm
From scott.marlowe at gmail.com  Sat Nov  7 21:24:16 2009
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Sat Nov  7 21:24:49 2009
Subject: [Slony1-general] very slow create set
In-Reply-To: <dcc563d10911072119m6499dcb7xfa3b5f4864dfb217@mail.gmail.com>
References: <302031.19923.qm@web53005.mail.re2.yahoo.com>
	<dcc563d10911072119m6499dcb7xfa3b5f4864dfb217@mail.gmail.com>
Message-ID: <dcc563d10911072124x1dba8d77j7a59c95aa6712f25@mail.gmail.com>

On Sat, Nov 7, 2009 at 10:19 PM, Scott Marlowe <scott.marlowe@gmail.com>wro=
te:

>
> This an 8 core opteron 2.1GHz machine with 32G ram and a 12 disk RAID-10
> for pgdata and a two disk RAID-1 for pg_xlog.  The autovac when it kicks
> off, since it's set to have a 20 ms cost delay, purposely takes a while, =
so
> as NOT to get in the way.  So, on a 2 to 5Gig table file it can take 10 to
> 20 minutes.  While this is happening, setaddtable is blocked.  While it's
> blocked, all access to the table by other queries are blocked, and nothing
> gets though.  This is NOT a tuning issue on the OS / hardware.  This mach=
ine
> can handle 10 to 20k transactions per minute under normal load.  But the
> interaction between autovac and create set / setaddtable halts all access=
 to
> the table being vacuumed.  The pg_lock and pg_stat_activity show it pretty
> clearly.
>
> However, I can turn off autovac during the create set.  But the create set
> takes wayyyy too long.
>


Note that since it's CPU bound and I have 8 cores, I'm considering breaking
my cerate set up into approx 8 equal sets and create setting all 8 at once,
and each would hit a different core.  This would get my time down to 40
minutes to an hour, and I could just take the application offline if needs
be at 2am.  So it's still workable.  But if there was some simple "alter
function cost 10" type of fix that would rock.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20091107/=
9f0ee677/attachment.htm
From melvin6925 at yahoo.com  Sun Nov  8 06:40:32 2009
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Sun Nov  8 06:40:39 2009
Subject: [Slony1-general] very slow create set
In-Reply-To: <dcc563d10911072119m6499dcb7xfa3b5f4864dfb217@mail.gmail.com>
Message-ID: <444757.20972.qm@web53011.mail.re2.yahoo.com>

>This an 8 core opteron 2.1GHz machine with 32G ram and a 12 disk RAID-10 f=
or pgdata >and a two disk RAID-1 for pg_xlog.


OK, based on that information, I can only suggest the following tweaks. Try=
 them

1 at a time to see what difference they make. Note that shared_buffers & ma=
x_files_per_process require a restart. The others can be handled from psql =
with =


SELECT pg_reload_conf();



shared_buffers=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0 -->=
 increase to 48mb

work_mem=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=
=A0=A0=A0 --> increase to 8mb

maintenance_work_mem=A0=A0 --> increase to 32mb

max_files_per_process=A0=A0=A0=A0=A0 --> increase to 4000



Also, check the postgresql log to see if there are any warnings about
the need to increase other parameters such as max_fsm_pages or
checkpoint_segments.



Good luck,


Melvin Davidson =





      =

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20091108/=
50ada08c/attachment-0001.htm
From scott.marlowe at gmail.com  Sun Nov  8 18:13:58 2009
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Sun Nov  8 18:14:23 2009
Subject: [Slony1-general] very slow create set
In-Reply-To: <444757.20972.qm@web53011.mail.re2.yahoo.com>
References: <dcc563d10911072119m6499dcb7xfa3b5f4864dfb217@mail.gmail.com>
	<444757.20972.qm@web53011.mail.re2.yahoo.com>
Message-ID: <dcc563d10911081813w1820a6bx53b05ee298094b9c@mail.gmail.com>

(Note, please don't use html email on the list. Many folks' mail
readers can't read it and they just delete the message.)  (me
personally I could care less, gmail has a nice html -> text tool that
makes it easy)

On Sun, Nov 8, 2009 at 7:40 AM, Melvin Davidson <melvin6925@yahoo.com> wrote:
>
> >This an 8 core opteron 2.1GHz machine with 32G ram and a 12 disk RAID-10 for pgdata >and a two disk RAID-1 for pg_xlog.
>
> OK, based on that information, I can only suggest the following tweaks. Try them
> 1 at a time to see what difference they make. Note that shared_buffers & max_files_per_process require a restart. The others can be handled from psql with
> SELECT pg_reload_conf();
>
> shared_buffers??????????????????? --> increase to 48mb
> work_mem????????????????????????? --> increase to 8mb
> maintenance_work_mem?? --> increase to 32mb
> max_files_per_process????? --> increase to 4000

All those types of changes were made long ago, and these machines are
well tuned and run VERY fast, most of the time.  They started getting
slow at \d and tab completion in psql as we added ~2000 schemas and
Tom Lane gave us a very very good fix of setting some function with
"is visible" in it to a higher cost and voila, we went from 20 seconds
tab completions to sub second responses.  Same with \d, the speed up
was huge.

This setAddTable() function is, I'm assuming, running over the pg
catalogs for the most part, and they all easily fit in memory, and are
kept well vacuumed.  But they are good sized, with 30k various objects
in the db.  Some slowdown is expected, but this one seems extreme.

I'm guessing that some part of the setAddTable function is looking at
all schemas / objects when it should be only operating on the set in
question.   If it was it wouldn't matter if I had 1M schemas.

> Also, check the postgresql log to see if there are any warnings about the need to increase other parameters such as max_fsm_pages or checkpoint_segments.

You don't wanna be near one of these machines when we blow out the
FSM.  The second we have more objects to track than FSM can handle
things get out of control in a matter of days, if not hours.    Our
master write database has a setting of 10M for max_fsm_pages and 500k
max_fsm_relations.  Vacuum verbose shows us using 2M pages used in 37k
relations.

Regular db operations are fast, in the millisecond range for most
queries, and we're pushing only about 20M a second on our pg_xlog
drives that can handle about 60M to 70M sequential.

I have two very distinct problems to deal with.

The first is somewhat more bothersome than dangerous, and that's that
with a large catalog setAddTable is spinning a CPU for 30 seconds,
which in my past experience is a bad query plan nest loop joining 37k
records when it expected 40 or something like that.  I can live with
it, I have enough spare CPU to run two or three cores on a create set
and I don't have to run it more than once a year or so at most.

The second problem is MUCH MUCH worse, and that is that regular old
autovacuum is running it blocks setAddTable(), which blocks all access
to the referenced table. Note that once autovac was running on an FKed
table that caused this issue on a subscribing table for setAddTable().
 That problem can take out my database while trying to subscribe.  So
I have to remember to turn off autovacuum.  Or it gets in the way and
things go boom.

This is actually a classic example of priority inversion.  By lowering
the priority of auto-vacuum we slow down the one thing that is
blocking another thing, which is then blocking everything else.  The
more we slow down auto-vacuum the worse the problem becomes.  If
autovacuum ran at maximum speed, we'd be in trouble less during
subscription.  But I think disabling autovac while creating is the
safer bet.
From JanWieck at Yahoo.com  Mon Nov  9 06:52:25 2009
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Mon Nov  9 08:13:41 2009
Subject: [Slony1-general] very slow create set
In-Reply-To: <dcc563d10911072023k3cc4997fp283caf72fca4fcfb@mail.gmail.com>
References: <dcc563d10911061307o1f4881a1ge4c1b401536e1ed7@mail.gmail.com>	<2968dfd60911072002l1790c6btc591ac4efaa81fb0@mail.gmail.com>
	<dcc563d10911072023k3cc4997fp283caf72fca4fcfb@mail.gmail.com>
Message-ID: <4AF82CA9.6020005@Yahoo.com>

On 11/7/2009 11:23 PM, Scott Marlowe wrote:
> On Sat, Nov 7, 2009 at 9:02 PM, Vick Khera <vivek@khera.org> wrote:
>> On Fri, Nov 6, 2009 at 4:07 PM, Scott Marlowe <scott.marlowe@gmail.com> wrote:
>>> I have about 1500 objects in my main db, and running create set takes
>>> about 5 seconds per table or sequence.  Is this typical?  Any changes
>>> I can make in terms of cost of any slony functions or creating indexes
>>> to make it faster?
>>>
>>
>> The most complicated one I have has about 250 tables and about 50
>> sequences.  I don't recall it taking that long to create a set.
>> Perhaps the time is proportional to the number of objects?  Which
>> versions of everything are you using?  I run Pg 8.3 and slony1 1.2.x.
> 
> Yep, pg 8.3.7 and slony 1.2.14 at the moment.  If I do this on a
> machine with JUST the objects I'm replicating, I can get 10 seconds
> per table and 5 or so per sequence.
> 
> On the DB with 29000 other objects not replicated, it gets up to 30
> seconds per table and 20 seconds per sequence.  Which means a 5 hour
> time for create set.  And if autovac kicks in it blocks create set.
> so I had to turn that off.

This sounds like some of the system catalog operations done by Slony are 
using seq scans. I've never tested Slony with a database having that 
many objects.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin
From scott.marlowe at gmail.com  Mon Nov  9 08:20:53 2009
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Mon Nov  9 08:21:02 2009
Subject: [Slony1-general] very slow create set
In-Reply-To: <4AF82CA9.6020005@Yahoo.com>
References: <dcc563d10911061307o1f4881a1ge4c1b401536e1ed7@mail.gmail.com>
	<2968dfd60911072002l1790c6btc591ac4efaa81fb0@mail.gmail.com>
	<dcc563d10911072023k3cc4997fp283caf72fca4fcfb@mail.gmail.com>
	<4AF82CA9.6020005@Yahoo.com>
Message-ID: <dcc563d10911090820sbfd535k36f0644299930add@mail.gmail.com>

On Mon, Nov 9, 2009 at 7:52 AM, Jan Wieck <JanWieck@yahoo.com> wrote:
> On 11/7/2009 11:23 PM, Scott Marlowe wrote:
>>
>> On Sat, Nov 7, 2009 at 9:02 PM, Vick Khera <vivek@khera.org> wrote:
>>>
>>> On Fri, Nov 6, 2009 at 4:07 PM, Scott Marlowe <scott.marlowe@gmail.com>
>>> wrote:
>>>>
>>>> I have about 1500 objects in my main db, and running create set takes
>>>> about 5 seconds per table or sequence. ?Is this typical? ?Any changes
>>>> I can make in terms of cost of any slony functions or creating indexes
>>>> to make it faster?
>>>>
>>>
>>> The most complicated one I have has about 250 tables and about 50
>>> sequences. ?I don't recall it taking that long to create a set.
>>> Perhaps the time is proportional to the number of objects? ?Which
>>> versions of everything are you using? ?I run Pg 8.3 and slony1 1.2.x.
>>
>> Yep, pg 8.3.7 and slony 1.2.14 at the moment. ?If I do this on a
>> machine with JUST the objects I'm replicating, I can get 10 seconds
>> per table and 5 or so per sequence.
>>
>> On the DB with 29000 other objects not replicated, it gets up to 30
>> seconds per table and 20 seconds per sequence. ?Which means a 5 hour
>> time for create set. ?And if autovac kicks in it blocks create set.
>> so I had to turn that off.
>
> This sounds like some of the system catalog operations done by Slony are
> using seq scans. I've never tested Slony with a database having that many
> objects.

Could be.  But whatever it is it must be doing it over and over (i.e.
a seq scan feeding a nested loop ARG!!!)  because I can seq scan
things like pg_class in way under a second.  But if I had to do it 37k
times or so it would then be as slow as I'm seeing.  Got some good
troubleshooting tips I'm gonna look at.  Thanks to all who posted.
I'll keep you informed.
From michael at aers.ca  Mon Nov  9 09:37:59 2009
From: michael at aers.ca (michael@aers.ca)
Date: Mon Nov  9 09:38:13 2009
Subject: [Slony1-general] Order of steps for failover
Message-ID: <6B5AF6293A289F45826220B17ABE7937011A541B@BORON.aers.local>

We recently attempted a test of our failover procedure and it brought to
light a few issues with our slony configuration. The one thing I'm still
not certain about is the precise order of steps I should take when doing
the failover, and I'm beginning to wonder if it's because I have a
sub-optimal slony configuration.

 

We're basically working with 4 servers in two cities (2 per city) and I
want to plan for total failure in a single city.  Currently the
replication works in this manner:

 

Node 1 -> Node 2 -> Node 3 -> Node 4

 

Node 1 and Node 2 are in one city and 3 & 4 in another.

 

Having reviewed the Slony failover documentation
(http://www.slony.info/documentation/failover.html ) I get that if I had
everything subscribed to node 1, I would first subscribe node 4 to node
3, drop node 2, then failover node 1 to node 3. The setup I have in
place means I don't need to move the subscription, but I can't just drop
node 2. Should I then do the following?

 

failover (id = 2, backup node = 3);

failover (id = 1, backup node = 3);

 

drop node (id = 2, event node = 3);

drop node (id = 1, event node = 3);

 

I originally tried to just failover from 1 to 3 which seemed to cause
several issues, but I later realized that some of them were due to other
configuration choices I had made (ie. Not all nodes subscribed to
certain sets and such. Will be changing that.) Still, knowing how slony
seems to like to work from the end first, failing node 2 then node 1
seems logical.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20091109/58af0043/attachment.htm
From johnfrederickmoran at gmail.com  Wed Nov 11 04:02:07 2009
From: johnfrederickmoran at gmail.com (John Moran)
Date: Wed Nov 11 04:02:53 2009
Subject: [Slony1-general] Advisability of powering off slaves or master on
	Slony-I replication set
Message-ID: <a6e804a00911110402u22cdde57h1a96b0d5bd7cb732@mail.gmail.com>

Hello,

I maintain a relatively small slony-I replication set - tables will
only ever be hundreds of megabytes, and only then after a long time.

There seems to be a demand for me to turn off slaves or even the
master during periods of downtime, when our application isn't in use
at all (mostly night time, or when some slaves aren't needed). While I
appreciate that Slony-I's "normal mode of operation is that all nodes
are available", this hasn't been problematic so far. I suspect that
the volumes of data replicated by our application are small enough for
slaves to bring their event lag to zero within a minute or two of
becoming available again.

Am I asking for trouble by doing this?

Thanks
John Moran
From wmoran at potentialtech.com  Wed Nov 11 04:51:45 2009
From: wmoran at potentialtech.com (Bill Moran)
Date: Wed Nov 11 04:52:28 2009
Subject: [Slony1-general] Advisability of powering off slaves or master
	on Slony-I replication set
In-Reply-To: <a6e804a00911110402u22cdde57h1a96b0d5bd7cb732@mail.gmail.com>
References: <a6e804a00911110402u22cdde57h1a96b0d5bd7cb732@mail.gmail.com>
Message-ID: <20091111075145.03e36202.wmoran@potentialtech.com>

John Moran <johnfrederickmoran@gmail.com> wrote:
>
> Hello,
> 
> I maintain a relatively small slony-I replication set - tables will
> only ever be hundreds of megabytes, and only then after a long time.
> 
> There seems to be a demand for me to turn off slaves or even the
> master during periods of downtime, when our application isn't in use
> at all (mostly night time, or when some slaves aren't needed). While I
> appreciate that Slony-I's "normal mode of operation is that all nodes
> are available", this hasn't been problematic so far. I suspect that
> the volumes of data replicated by our application are small enough for
> slaves to bring their event lag to zero within a minute or two of
> becoming available again.
> 
> Am I asking for trouble by doing this?

Hey ... are we related?

Anyway, I don't know about asking for trouble.  Obviously, the queued
data for replication is going to cause the master database to grow
faster than would be expected while the slaves are off, and there's
going to be a spike in CPU/network/disk activity when you turn them
back on, but if all of those things are within your tolerance level,
you'll probably be OK.

Sounds to me like you're trying to solve the problem incorrectly, however.
I assume the concern is $$ saving by powering off the servers, and I'll
tell you from 1st hand experience that it's probably not that much money.
We're in the midst of management fighting us to save $$ whereever possible
right now, and I just yesterday did a test that demonstrates that going to
the trouble to turn off individual workstations saves us a mere $35 per
year per workstation.  Of course, servers tend to be more power hungry,
so you may need to double to quadruple that value, but (really) how much
money is that?

Perhaps considering some sort of server consolidation would be wiser, such
as vitalization or simply putting multiple services on each server.

Anyway, hope that helps.

-- 
Bill Moran
http://www.potentialtech.com
From johnfrederickmoran at gmail.com  Wed Nov 11 05:47:56 2009
From: johnfrederickmoran at gmail.com (John Moran)
Date: Wed Nov 11 05:48:01 2009
Subject: [Slony1-general] Advisability of powering off slaves or master on
	Slony-I replication set
In-Reply-To: <20091111075145.03e36202.wmoran@potentialtech.com>
References: <a6e804a00911110402u22cdde57h1a96b0d5bd7cb732@mail.gmail.com>
	<20091111075145.03e36202.wmoran@potentialtech.com>
Message-ID: <a6e804a00911110547u59abab76k807abeb2e8428fb8@mail.gmail.com>

Hi Bill,

> Hey ... are we related?

I live in the UK....perhaps we are long lost cousins? :-)

> Sounds to me like you're trying to solve the problem incorrectly, however.
> I assume the concern is $$ saving by powering off the servers, and I'll
> tell you from 1st hand experience that it's probably not that much money.
> We're in the midst of management fighting us to save $$ whereever possible
> right now, and I just yesterday did a test that demonstrates that going to
> the trouble to turn off individual workstations saves us a mere $35 per
> year per workstation.  Of course, servers tend to be more power hungry,
> so you may need to double to quadruple that value, but (really) how much
> money is that?
>
> Perhaps considering some sort of server consolidation would be wiser, such
> as vitalization or simply putting multiple services on each server.

I guess that it's more a matter of politics or perception; I'd like to
be able to boast that our application is very robust, and can easily
handle server downtime (which has been our experience with Slony thus
far), or turning off servers when not needed. I can see how it would
be a bad idea to widely recommend this to slony users, because some
have massive databases where, if a slave is left to fall behind for
long enough it will literally never be able to catch up with the
master. I myself do not have such a large database, and hope that my
users can attain acceptable performance while doing this sort of
thing. Not that this will be a "salesman on the road" type scenario,
but it's a valuable thing to be able to do

Thanks,
John Moran
From wmoran at potentialtech.com  Wed Nov 11 05:56:16 2009
From: wmoran at potentialtech.com (Bill Moran)
Date: Wed Nov 11 05:56:21 2009
Subject: [Slony1-general] Advisability of powering off slaves or master
	on  Slony-I replication set
In-Reply-To: <a6e804a00911110547u59abab76k807abeb2e8428fb8@mail.gmail.com>
References: <a6e804a00911110402u22cdde57h1a96b0d5bd7cb732@mail.gmail.com>
	<20091111075145.03e36202.wmoran@potentialtech.com>
	<a6e804a00911110547u59abab76k807abeb2e8428fb8@mail.gmail.com>
Message-ID: <20091111085616.c9d36ace.wmoran@potentialtech.com>

In response to John Moran <johnfrederickmoran@gmail.com>:

> Hi Bill,
> 
> > Hey ... are we related?
> 
> I live in the UK....perhaps we are long lost cousins? :-)

It's possible, that side of my family has a history of being rather ...
"friendly".

> I guess that it's more a matter of politics or perception; I'd like to
> be able to boast that our application is very robust, and can easily
> handle server downtime (which has been our experience with Slony thus
> far), or turning off servers when not needed. I can see how it would
> be a bad idea to widely recommend this to slony users, because some
> have massive databases where, if a slave is left to fall behind for
> long enough it will literally never be able to catch up with the
> master. I myself do not have such a large database, and hope that my
> users can attain acceptable performance while doing this sort of
> thing. Not that this will be a "salesman on the road" type scenario,
> but it's a valuable thing to be able to do

The big suggestion I would make here is to test aggressively.  Since
Slony is not really intended for this sort of use, it's not tested
against it.  As a result, there may be hidden problems that nobody has
yet come across.  Also, if you hit problem, you're going to have some
difficulty getting help since you're using the system in a way that
it's not really intended for.

-- 
Bill Moran
http://www.potentialtech.com
http://people.collaborativefusion.com/~wmoran/
From scott.marlowe at gmail.com  Wed Nov 11 06:23:04 2009
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Wed Nov 11 06:23:10 2009
Subject: [Slony1-general] Advisability of powering off slaves or master on
	Slony-I replication set
In-Reply-To: <20091111085616.c9d36ace.wmoran@potentialtech.com>
References: <a6e804a00911110402u22cdde57h1a96b0d5bd7cb732@mail.gmail.com>
	<20091111075145.03e36202.wmoran@potentialtech.com>
	<a6e804a00911110547u59abab76k807abeb2e8428fb8@mail.gmail.com>
	<20091111085616.c9d36ace.wmoran@potentialtech.com>
Message-ID: <dcc563d10911110623n37f6dbe6mef0d58b3d80fb6a3@mail.gmail.com>

On Wed, Nov 11, 2009 at 6:56 AM, Bill Moran <wmoran@potentialtech.com> wrote:
> In response to John Moran <johnfrederickmoran@gmail.com>:
>
>> Hi Bill,
>>
>> > Hey ... are we related?
>>
>> I live in the UK....perhaps we are long lost cousins? :-)
>
> It's possible, that side of my family has a history of being rather ...
> "friendly".
>
>> I guess that it's more a matter of politics or perception; I'd like to
>> be able to boast that our application is very robust, and can easily
>> handle server downtime (which has been our experience with Slony thus
>> far), or turning off servers when not needed. I can see how it would
>> be a bad idea to widely recommend this to slony users, because some
>> have massive databases where, if a slave is left to fall behind for
>> long enough it will literally never be able to catch up with the
>> master. I myself do not have such a large database, and hope that my
>> users can attain acceptable performance while doing this sort of
>> thing. Not that this will be a "salesman on the road" type scenario,
>> but it's a valuable thing to be able to do
>
> The big suggestion I would make here is to test aggressively. ?Since
> Slony is not really intended for this sort of use, it's not tested
> against it. ?As a result, there may be hidden problems that nobody has
> yet come across. ?Also, if you hit problem, you're going to have some
> difficulty getting help since you're using the system in a way that
> it's not really intended for.

But the good news is that his dbs are small, so they can likely just
resubscribe semi-automagically if needs be in a minute or two.
From cbbrowne at ca.afilias.info  Wed Nov 11 08:54:52 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Nov 11 08:55:04 2009
Subject: [Slony1-general] very slow create set
In-Reply-To: <dcc563d10911090820sbfd535k36f0644299930add@mail.gmail.com>
	(Scott Marlowe's message of "Mon, 9 Nov 2009 09:20:53 -0700")
References: <dcc563d10911061307o1f4881a1ge4c1b401536e1ed7@mail.gmail.com>
	<2968dfd60911072002l1790c6btc591ac4efaa81fb0@mail.gmail.com>
	<dcc563d10911072023k3cc4997fp283caf72fca4fcfb@mail.gmail.com>
	<4AF82CA9.6020005@Yahoo.com>
	<dcc563d10911090820sbfd535k36f0644299930add@mail.gmail.com>
Message-ID: <87skcldmsj.fsf@dba2.int.libertyrms.com>

Scott Marlowe <scott.marlowe@gmail.com> writes:
> On Mon, Nov 9, 2009 at 7:52 AM, Jan Wieck <JanWieck@yahoo.com> wrote:
>> On 11/7/2009 11:23 PM, Scott Marlowe wrote:
>>>
>>> On Sat, Nov 7, 2009 at 9:02 PM, Vick Khera <vivek@khera.org> wrote:
>>>>
>>>> On Fri, Nov 6, 2009 at 4:07 PM, Scott Marlowe <scott.marlowe@gmail.com>
>>>> wrote:
>>>>>
>>>>> I have about 1500 objects in my main db, and running create set takes
>>>>> about 5 seconds per table or sequence. ?Is this typical? ?Any changes
>>>>> I can make in terms of cost of any slony functions or creating indexes
>>>>> to make it faster?
>>>>>
>>>>
>>>> The most complicated one I have has about 250 tables and about 50
>>>> sequences. ?I don't recall it taking that long to create a set.
>>>> Perhaps the time is proportional to the number of objects? ?Which
>>>> versions of everything are you using? ?I run Pg 8.3 and slony1 1.2.x.
>>>
>>> Yep, pg 8.3.7 and slony 1.2.14 at the moment. ?If I do this on a
>>> machine with JUST the objects I'm replicating, I can get 10 seconds
>>> per table and 5 or so per sequence.
>>>
>>> On the DB with 29000 other objects not replicated, it gets up to 30
>>> seconds per table and 20 seconds per sequence. ?Which means a 5 hour
>>> time for create set. ?And if autovac kicks in it blocks create set.
>>> so I had to turn that off.
>>
>> This sounds like some of the system catalog operations done by Slony are
>> using seq scans. I've never tested Slony with a database having that many
>> objects.
>
> Could be.  But whatever it is it must be doing it over and over (i.e.
> a seq scan feeding a nested loop ARG!!!)  because I can seq scan
> things like pg_class in way under a second.  But if I had to do it 37k
> times or so it would then be as slow as I'm seeing.  Got some good
> troubleshooting tips I'm gonna look at.  Thanks to all who posted.
> I'll keep you informed.

A browse of the code in SetAddTable_int() doesn't show off anything that
*ought* to be notably slow.

There's a subquery that is a bit pointless:

   ... where attrelid = (select oid from pg_catalog.pg_class where oid = v_tab_reloid)

That subquery could be replaced by v_tab_reloid.  But it shouldn't
worsen things because there's an index on pg_class(oid).

The more major set of queries are in alterTableForReplication().  I'd be
suspicious of it.

You could exercise alterTableForReplication() by trying that over and
over again in conjunction with alterTableRestore()...

Pick a table, say the one with ID 5...

begin;
explain analyze select "_MyNameSpace".alterTableRestore(5);
explain analyze select "_MyNameSpace".alterTableForReplication(5);
commit;

Throwing some notify requests into alterTableForReplication(), and
logging to the millisecond, should help track down a query that is
slower than it ought to be.
-- 
let name="cbbrowne" and tld="ca.afilias.info" in name ^ "@" ^ tld;;
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Wed Nov 11 10:03:57 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Nov 11 10:04:11 2009
Subject: [Slony1-general] Advisability of powering off slaves or master on
	Slony-I replication set
In-Reply-To: <a6e804a00911110402u22cdde57h1a96b0d5bd7cb732@mail.gmail.com>
	(John Moran's message of "Wed, 11 Nov 2009 12:02:07 +0000")
References: <a6e804a00911110402u22cdde57h1a96b0d5bd7cb732@mail.gmail.com>
Message-ID: <87ocn9djle.fsf@dba2.int.libertyrms.com>

John Moran <johnfrederickmoran@gmail.com> writes:

> Hello,
>
> I maintain a relatively small slony-I replication set - tables will
> only ever be hundreds of megabytes, and only then after a long time.
>
> There seems to be a demand for me to turn off slaves or even the
> master during periods of downtime, when our application isn't in use
> at all (mostly night time, or when some slaves aren't needed). While I
> appreciate that Slony-I's "normal mode of operation is that all nodes
> are available", this hasn't been problematic so far. I suspect that
> the volumes of data replicated by our application are small enough for
> slaves to bring their event lag to zero within a minute or two of
> becoming available again.
>
> Am I asking for trouble by doing this?

What you *particularly* want to make sure of is that you're generating
SYNCs against the "master" any time it's up and running.

The case that turns out notably badly is where the "master" is accepting
changes while slon processes are down, and when slons finally return, it
generates one giant weekend-long SYNC that draws a barrel of changes in
as one big increment.

If:
 a) You keep the slon running against the master, or
 b) You run tools/generate_syncs.sh against the master via a cron job
then you're protected against that pathology.

So, if parts of the system are out for a few hours, it may take a number
of SYNCs to get up to date; things play more nicely if those SYNCs are
small, as opposed to being giant mudballs of changes that take a long
time to apply.
-- 
let name="cbbrowne" and tld="ca.afilias.info" in name ^ "@" ^ tld;;
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From scott.marlowe at gmail.com  Wed Nov 11 10:05:01 2009
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Wed Nov 11 10:05:14 2009
Subject: [Slony1-general] very slow create set
In-Reply-To: <87skcldmsj.fsf@dba2.int.libertyrms.com>
References: <dcc563d10911061307o1f4881a1ge4c1b401536e1ed7@mail.gmail.com>
	<2968dfd60911072002l1790c6btc591ac4efaa81fb0@mail.gmail.com>
	<dcc563d10911072023k3cc4997fp283caf72fca4fcfb@mail.gmail.com>
	<4AF82CA9.6020005@Yahoo.com>
	<dcc563d10911090820sbfd535k36f0644299930add@mail.gmail.com>
	<87skcldmsj.fsf@dba2.int.libertyrms.com>
Message-ID: <dcc563d10911111005s4be7880j31d81936dc5c13b6@mail.gmail.com>

On Wed, Nov 11, 2009 at 9:54 AM, Christopher Browne
<cbbrowne@ca.afilias.info> wrote:
> Scott Marlowe <scott.marlowe@gmail.com> writes:
>> On Mon, Nov 9, 2009 at 7:52 AM, Jan Wieck <JanWieck@yahoo.com> wrote:
>>> On 11/7/2009 11:23 PM, Scott Marlowe wrote:
>>>>
>>>> On Sat, Nov 7, 2009 at 9:02 PM, Vick Khera <vivek@khera.org> wrote:
>>>>>
>>>>> On Fri, Nov 6, 2009 at 4:07 PM, Scott Marlowe <scott.marlowe@gmail.com>
>>>>> wrote:
>>>>>>
>>>>>> I have about 1500 objects in my main db, and running create set takes
>>>>>> about 5 seconds per table or sequence. ?Is this typical? ?Any changes
>>>>>> I can make in terms of cost of any slony functions or creating indexes
>>>>>> to make it faster?
>>>>>>
>>>>>
>>>>> The most complicated one I have has about 250 tables and about 50
>>>>> sequences. ?I don't recall it taking that long to create a set.
>>>>> Perhaps the time is proportional to the number of objects? ?Which
>>>>> versions of everything are you using? ?I run Pg 8.3 and slony1 1.2.x.
>>>>
>>>> Yep, pg 8.3.7 and slony 1.2.14 at the moment. ?If I do this on a
>>>> machine with JUST the objects I'm replicating, I can get 10 seconds
>>>> per table and 5 or so per sequence.
>>>>
>>>> On the DB with 29000 other objects not replicated, it gets up to 30
>>>> seconds per table and 20 seconds per sequence. ?Which means a 5 hour
>>>> time for create set. ?And if autovac kicks in it blocks create set.
>>>> so I had to turn that off.
>>>
>>> This sounds like some of the system catalog operations done by Slony are
>>> using seq scans. I've never tested Slony with a database having that many
>>> objects.
>>
>> Could be. ?But whatever it is it must be doing it over and over (i.e.
>> a seq scan feeding a nested loop ARG!!!) ?because I can seq scan
>> things like pg_class in way under a second. ?But if I had to do it 37k
>> times or so it would then be as slow as I'm seeing. ?Got some good
>> troubleshooting tips I'm gonna look at. ?Thanks to all who posted.
>> I'll keep you informed.
>
> A browse of the code in SetAddTable_int() doesn't show off anything that
> *ought* to be notably slow.
>
> There's a subquery that is a bit pointless:
>
> ? ... where attrelid = (select oid from pg_catalog.pg_class where oid = v_tab_reloid)
>
> That subquery could be replaced by v_tab_reloid. ?But it shouldn't
> worsen things because there's an index on pg_class(oid).
>
> The more major set of queries are in alterTableForReplication(). ?I'd be
> suspicious of it.
>
> You could exercise alterTableForReplication() by trying that over and
> over again in conjunction with alterTableRestore()...
>
> Pick a table, say the one with ID 5...
>
> begin;
> explain analyze select "_MyNameSpace".alterTableRestore(5);
> explain analyze select "_MyNameSpace".alterTableForReplication(5);
> commit;
>
> Throwing some notify requests into alterTableForReplication(), and
> logging to the millisecond, should help track down a query that is
> slower than it ought to be.

Thanks I'll look into that.  I'm just getting a test environment setup
I can play with that's not production for this type of thing.  I'm
guessing that somewhere in there it's calling some pg system function
that gets super slow with large numbers of objects.
From johnfrederickmoran at gmail.com  Wed Nov 11 15:03:18 2009
From: johnfrederickmoran at gmail.com (John Moran)
Date: Wed Nov 11 15:03:42 2009
Subject: [Slony1-general] Advisability of powering off slaves or master on
	Slony-I replication set
In-Reply-To: <87ocn9djle.fsf@dba2.int.libertyrms.com>
References: <a6e804a00911110402u22cdde57h1a96b0d5bd7cb732@mail.gmail.com>
	<87ocn9djle.fsf@dba2.int.libertyrms.com>
Message-ID: <a6e804a00911111503p2b123c6bh6a6bff23a00c7ad3@mail.gmail.com>

Hi Chris,

> What you *particularly* want to make sure of is that you're generating
> SYNCs against the "master" any time it's up and running.
>
> The case that turns out notably badly is where the "master" is accepting
> changes while slon processes are down, and when slons finally return, it
> generates one giant weekend-long SYNC that draws a barrel of changes in
> as one big increment.

All slons are on the master (this is easier to manage generally). I
guess that's why I've never experienced this.

I've noticed that when connectivity is eventually restored (by which
time the event lag in sl_status has perhaps reached thousands or tens
of thousands of events), it tends to take 2 or 3 syncs to clear
through the backlog (each time, several thousand events are cleared).
The lag grows, even though there usually isn't any actual activity
that would have added to the backlog of events to be replicated (I
guess this is normal). Suppose little or no actual activity occurs -
does the fact that the event lag is quite big matter at all, or is the
expense of replicating those missed events the same, regardless of the
event lag?

> So, if parts of the system are out for a few hours, it may take a number
> of SYNCs to get up to date; things play more nicely if those SYNCs are
> small, as opposed to being giant mudballs of changes that take a long
> time to apply.

Care to comment on how sensible you think this is in general?

Thanks a lot,

John Moran
From nimesh.satam at gmail.com  Thu Nov 12 04:01:19 2009
From: nimesh.satam at gmail.com (Nimesh Satam)
Date: Thu Nov 12 04:02:08 2009
Subject: [Slony1-general] How to confirm if a certain insert has completed
	on the slave db?
In-Reply-To: <B9C1AF04-C2CC-4E3A-B1B2-B5CF1E7E01E4@torgo.978.org>
References: <65f3e23d0911042109w6f666a22raa2ea8331b8e2f31@mail.gmail.com>
	<B9C1AF04-C2CC-4E3A-B1B2-B5CF1E7E01E4@torgo.978.org>
Message-ID: <65f3e23d0911120401ja371bbdt4d9deff1dab06354@mail.gmail.com>

Jeff,

Thanks for the reply, can you let us know how we get the values for the
function inputs p_start , p_txid ?

Regards,
Nimesh.

On Thu, Nov 5, 2009 at 7:49 PM, Jeff <threshar@torgo.978.org> wrote:

>
> On Nov 5, 2009, at 12:09 AM, Nimesh Satam wrote:
>
>  All,
>>
>> Is there any way to confirm that a certain insert has been completed on
>> the slave database? Would the sl_confirm or sl_event table be useful for
>> this?
>>
>> For eg: If I have a batch of 100 Inserts, how do I make sure that all ha=
ve
>> been done on slave?
>>
>> Regards,
>> Nimesh.
>>
>
>
> I've got this code which seems to work well.  I added a function to my DB
> access layer called "syncexec" that executes and waits for the slaves (up=
 to
> a predefined interval).
>
> You'll need to do your work inside a transaction and get the txn's txnid
> for this to work (you can obtain it via _cluster.getCurrentXid()
>
>
>
>
> --
> Jeff Trout <jeff@jefftrout.com>
> http://www.stuarthamm.net/
> http://www.dellsmartexitin.com/
>
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20091112/=
f234131c/attachment.htm
From tamanna.madan at globallogic.com  Thu Nov 12 00:26:41 2009
From: tamanna.madan at globallogic.com (tamanna madaan)
Date: Thu Nov 12 07:52:54 2009
Subject: [Slony1-general] "duplicate key violates unique constraint" erro in
	slon.log
Message-ID: <68666423656E1444A011106C4E085F4DA88E68@ex3-del1.synapse.com>

Hi All

 

I have a cluster setup with one master and one slave . Replication from
master to slave is not taking place.

I am getting this error  "duplicate key violates unique constraint"  in
slon.log on slave . This error is thrown while 

slon is inserting a row in a table  on slave. This must be because of
the reason that duplicate rows

are being returned while querying sl_log_1 table. But I don't know from
where these duplicate rows have come

or whether there are any duplicate rows in sl_log_1 or not.

 

I am using postgres 8.1.2 and slony 1.1.5 .

 

Has somebody seen this problem earlier ??

Is  it because of some bug in slony or postgres ??

Which version of slony/postgres has fix for this problem ??

 

Any kind of suggestion is most welcome .

 

Thanks...

Tamanna

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20091112/8cdbbd2d/attachment.htm
From vivek at khera.org  Thu Nov 12 08:26:12 2009
From: vivek at khera.org (Vick Khera)
Date: Thu Nov 12 08:26:23 2009
Subject: [Slony1-general] "duplicate key violates unique constraint" erro 
	in slon.log
In-Reply-To: <68666423656E1444A011106C4E085F4DA88E68@ex3-del1.synapse.com>
References: <68666423656E1444A011106C4E085F4DA88E68@ex3-del1.synapse.com>
Message-ID: <2968dfd60911120826x4b6342a6s5b3efa282a425600@mail.gmail.com>

On Thu, Nov 12, 2009 at 3:26 AM, tamanna madaan
<tamanna.madan@globallogic.com> wrote:
> Has somebody seen this problem earlier ??
>
> Is? it because of some bug in slony or postgres ??
>
> Which version of slony/postgres has fix for this problem ??

Do you have triggers on the slave?  Did you alter *any* tables without
using the proper EXECUTE SCRIPT functionality?

I'll vote that you (or someone else) did.

Fix: drop the table in question from slony, and re-add it.  For good
measure, run EXECUTE SCRIPT on an empty script to ensure that
everything gets re-set properly, too.
From bnichols at ca.afilias.info  Thu Nov 12 08:55:48 2009
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Thu Nov 12 08:56:01 2009
Subject: [Slony1-general] "duplicate key violates unique constraint"
	erro in slon.log
In-Reply-To: <68666423656E1444A011106C4E085F4DA88E68@ex3-del1.synapse.com>
References: <68666423656E1444A011106C4E085F4DA88E68@ex3-del1.synapse.com>
Message-ID: <1258044948.9403.99.camel@bnicholson-desktop>

On Thu, 2009-11-12 at 13:56 +0530, tamanna madaan wrote:
> Hi All
> 
>  
> 
> I have a cluster setup with one master and one slave . Replication
> from master to slave is not taking place.
> 
> I am getting this error  ?duplicate key violates unique constraint?
>  in  slon.log on slave . This error is thrown while 
> 
> slon is inserting a row in a table  on slave. This must be because of
> the reason that duplicate rows
> 
> are being returned while querying sl_log_1 table. But I don?t know
> from where these duplicate rows have come
> 
> or whether there are any duplicate rows in sl_log_1 or not.
> 
>  
> 
> I am using postgres 8.1.2 and slony 1.1.5 .
> 
> Has somebody seen this problem earlier ??
> 
> Is  it because of some bug in slony or postgres ??
> 
> Which version of slony/postgres has fix for this problem ??

We've seen this problem before with 8.1.  While it is exceedingly hard
to trace exactly what is going on, all signs pointed to a bug in
Postgres.  We couldn't reproduce it though.

If I were you, I would upgrade to the latest minor version of your PG
release - you are 16 minor version behind.  They fix it for a reason :-)

You should really consider upgrading to Slony 1.2 as well, but that is
likely not the issue here.

-- 
Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.


From ajs at crankycanuck.ca  Thu Nov 12 09:59:05 2009
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Nov 12 09:59:22 2009
Subject: [Slony1-general] "duplicate key violates unique constraint"
	erro in slon.log
In-Reply-To: <1258044948.9403.99.camel@bnicholson-desktop>
References: <68666423656E1444A011106C4E085F4DA88E68@ex3-del1.synapse.com>
	<1258044948.9403.99.camel@bnicholson-desktop>
Message-ID: <20091112175905.GF4825@shinkuro.com>

On Thu, Nov 12, 2009 at 11:55:48AM -0500, Brad Nicholson wrote:

> We've seen this problem before with 8.1.  While it is exceedingly hard
> to trace exactly what is going on, all signs pointed to a bug in
> Postgres.  We couldn't reproduce it though.
> 
> If I were you, I would upgrade to the latest minor version of your PG
> release - you are 16 minor version behind.  They fix it for a reason :-)

What Brad said.  Also, note that the symptoms could be the result of
corrupt indexes.  If reindexing the databases at both ends makes this
problem magically disappear, you need to investigate what's happening
at the underlying hardware.  (I've seen this problem, too.)

A

-- 
Andrew Sullivan
ajs@crankycanuck.ca
From nagy at ecircle-ag.com  Fri Nov 13 02:01:33 2009
From: nagy at ecircle-ag.com (Csaba Nagy)
Date: Fri Nov 13 02:04:28 2009
Subject: [Slony1-general] "duplicate key violates unique constraint"
	erro 	in slon.log
In-Reply-To: <2968dfd60911120826x4b6342a6s5b3efa282a425600@mail.gmail.com>
References: <68666423656E1444A011106C4E085F4DA88E68@ex3-del1.synapse.com>
	<2968dfd60911120826x4b6342a6s5b3efa282a425600@mail.gmail.com>
Message-ID: <1258106493.2375.159.camel@pcd12478>

On Thu, 2009-11-12 at 17:26 +0100, Vick Khera wrote:
> Do you have triggers on the slave?  Did you alter *any* tables without
> using the proper EXECUTE SCRIPT functionality?
> 
> I'll vote that you (or someone else) did.
> 
> Fix: drop the table in question from slony, and re-add it.  For good
> measure, run EXECUTE SCRIPT on an empty script to ensure that
> everything gets re-set properly, too.

Just as additional input: I have seen this with postgres 8.2.4 and slony
1.2.16 too, immediately after the initial sync... there was only one
(fairly big, in the 10^10th rows and 100s of GB order of magnitude)
table, and it was most definitely not altered. After upgrading to slony
1.2.17 it did work - but maybe the upgrade is not the real problem, but
some other race condition in the slony/postgres code.

Cheers,
Csaba.


From scott.marlowe at gmail.com  Fri Nov 13 20:59:04 2009
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Fri Nov 13 20:59:38 2009
Subject: [Slony1-general] "duplicate key violates unique constraint" erro 
	in slon.log
In-Reply-To: <1258106493.2375.159.camel@pcd12478>
References: <68666423656E1444A011106C4E085F4DA88E68@ex3-del1.synapse.com>
	<2968dfd60911120826x4b6342a6s5b3efa282a425600@mail.gmail.com>
	<1258106493.2375.159.camel@pcd12478>
Message-ID: <dcc563d10911132059l728a4f1aq6fd3a64b3cad1247@mail.gmail.com>

On Fri, Nov 13, 2009 at 3:01 AM, Csaba Nagy <nagy@ecircle-ag.com> wrote:
> On Thu, 2009-11-12 at 17:26 +0100, Vick Khera wrote:
>> Do you have triggers on the slave? ?Did you alter *any* tables without
>> using the proper EXECUTE SCRIPT functionality?
>>
>> I'll vote that you (or someone else) did.
>>
>> Fix: drop the table in question from slony, and re-add it. ?For good
>> measure, run EXECUTE SCRIPT on an empty script to ensure that
>> everything gets re-set properly, too.
>
> Just as additional input: I have seen this with postgres 8.2.4 and slony
> 1.2.16 too, immediately after the initial sync... there was only one
> (fairly big, in the 10^10th rows and 100s of GB order of magnitude)
> table, and it was most definitely not altered. After upgrading to slony
> 1.2.17 it did work - but maybe the upgrade is not the real problem, but
> some other race condition in the slony/postgres code.

According to the Admin Guide to Slony,
(file:///home/smarlowe/work/slony1-2.0.2/doc/adminguide/maintenance.html)
:  "The  Duplicate Key Violation bug has helped track down a number of
rather obscure PostgreSQL race conditions, so that in modern versions
of Slony-I and PostgreSQL, there should be little to worry about."

So it's quite likely that a pg upgrade and / or slony upgrade fixed that.
From bnichols at ca.afilias.info  Mon Nov 16 10:29:51 2009
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Mon Nov 16 10:30:14 2009
Subject: [Slony1-general] "duplicate key violates unique constraint"
	erro  in slon.log
In-Reply-To: <dcc563d10911132059l728a4f1aq6fd3a64b3cad1247@mail.gmail.com>
References: <68666423656E1444A011106C4E085F4DA88E68@ex3-del1.synapse.com>
	<2968dfd60911120826x4b6342a6s5b3efa282a425600@mail.gmail.com>
	<1258106493.2375.159.camel@pcd12478>
	<dcc563d10911132059l728a4f1aq6fd3a64b3cad1247@mail.gmail.com>
Message-ID: <1258396191.9403.182.camel@bnicholson-desktop>

On Fri, 2009-11-13 at 21:59 -0700, Scott Marlowe wrote:
> On Fri, Nov 13, 2009 at 3:01 AM, Csaba Nagy <nagy@ecircle-ag.com> wrote:
> > On Thu, 2009-11-12 at 17:26 +0100, Vick Khera wrote:
> >> Do you have triggers on the slave?  Did you alter *any* tables without
> >> using the proper EXECUTE SCRIPT functionality?
> >>
> >> I'll vote that you (or someone else) did.
> >>
> >> Fix: drop the table in question from slony, and re-add it.  For good
> >> measure, run EXECUTE SCRIPT on an empty script to ensure that
> >> everything gets re-set properly, too.
> >
> > Just as additional input: I have seen this with postgres 8.2.4 and slony
> > 1.2.16 too, immediately after the initial sync... there was only one
> > (fairly big, in the 10^10th rows and 100s of GB order of magnitude)
> > table, and it was most definitely not altered. After upgrading to slony
> > 1.2.17 it did work - but maybe the upgrade is not the real problem, but
> > some other race condition in the slony/postgres code.
> 
> According to the Admin Guide to Slony,
> (file:///home/smarlowe/work/slony1-2.0.2/doc/adminguide/maintenance.html)
> :  "The  Duplicate Key Violation bug has helped track down a number of
> rather obscure PostgreSQL race conditions, so that in modern versions
> of Slony-I and PostgreSQL, there should be little to worry about."

That's only partially correct.  

There are strong indications of other manifestation(s) of this issue as
recently as 8.1.11, and we also have a mention of 8.2.4

The last bugs we isolated around this AFAIK where in 7.4 (around 7.4.7
or 7.4.8 if memory serves me) and were fixed. 

-- 
Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.


From dmk at mr-paradox.net  Tue Nov 17 16:15:29 2009
From: dmk at mr-paradox.net (David Kerr)
Date: Tue Nov 17 16:16:19 2009
Subject: [Slony1-general] Slony + PGPool for HA/Failover only
Message-ID: <20091118001529.GB81625@mr-paradox.net>

Howdy all,

I'm sure this probably comes up on the list frequently, but I couldn't find any
recent mentions of it so I thought it would be safer just to ask.  (If there's
a doc somewhere online that i missed in my googling please point me there!)

I need an HA solution for Postgres and I'm evaluating Slony / Slony + PGPool
along with some other non-replication options.

My understanding from googling is that since PGPool's replication is statement
based, which causes problems, many people pair it with Slony.

Slony handles the replication and PGPool handles the pooling and load 
balancing.

Geronimo will be handling my pooling.

I like the idea of load balancing, but how does that work, exactly, since Slony 
is asynchronous? 

Second for an HA solution, my real concern, if the load balancing isn't viable
(so to speak) can you configure PGPool to point to one node and then fail over
to another node (automatically) ? I understand that's a PGPool specific question
but since it seems like a common config with Slony I thought i'd chance it 
and ask here.

If that doesn't work, is there a reccomended / common solution in use by the 
community?

Thanks

Dave
From ajs at crankycanuck.ca  Wed Nov 18 05:40:14 2009
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Nov 18 05:40:29 2009
Subject: [Slony1-general] Slony + PGPool for HA/Failover only
In-Reply-To: <20091118001529.GB81625@mr-paradox.net>
References: <20091118001529.GB81625@mr-paradox.net>
Message-ID: <20091118134014.GA23083@shinkuro.com>

On Tue, Nov 17, 2009 at 04:15:29PM -0800, David Kerr wrote:
> I like the idea of load balancing, but how does that work, exactly, since Slony 
> is asynchronous? 

You handle read-only and read-write queries differently.  Read-only
queries get a consistent but not necessarily current view of the
data.  So if your pattern is "update database, read results" with web
pages, it just won't work -- you need the latest version of the data.
For environments where imperfect currency is ok (think most web pages,
for instance), it'll work.  But be careful: I never got pgpool to
work well for me.

> Second for an HA solution, my real concern, if the load balancing isn't viable
> (so to speak) can you configure PGPool to point to one node and then fail over
> to another node (automatically) ? I understand that's a PGPool specific question
> but since it seems like a common config with Slony I thought i'd chance it 
> and ask here.

The question here is how much data loss you're willing to take.
Alternatively, how much read-write downtime can you stand?

If you MUST have an up-to-date view of the data, but if the read-write
node fails you can just live with reads, then you're golden.  Set up
two pools.  Pool 1 is for read-write and read, and handles all the
data under normal circumstances.  Pool 2 is a standby, and if the
database on pool 1 is lost, it takes over.  Write transactions on this
pool always fail, but it will allow you to do read-only for as long as
you still have a good database system there.  The data is current as
of the last applied snapshot (which need not be the last write action
in the data origin, but it might be good enough).

If you can stand to lose some transactions, then you can do full
automatic failover.  But careful!  That data is lost more or less
forever, so you need to be prepared for that.

A

-- 
Andrew Sullivan
ajs@crankycanuck.ca
From cbbrowne at ca.afilias.info  Wed Nov 18 09:09:09 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Nov 18 09:09:29 2009
Subject: [Slony1-general] Advisability of powering off slaves or master on
	Slony-I replication set
In-Reply-To: <a6e804a00911111503p2b123c6bh6a6bff23a00c7ad3@mail.gmail.com>
	(John Moran's message of "Wed, 11 Nov 2009 23:03:18 +0000")
References: <a6e804a00911110402u22cdde57h1a96b0d5bd7cb732@mail.gmail.com>
	<87ocn9djle.fsf@dba2.int.libertyrms.com>
	<a6e804a00911111503p2b123c6bh6a6bff23a00c7ad3@mail.gmail.com>
Message-ID: <87hbsrbw0a.fsf@dba2.int.libertyrms.com>

John Moran <johnfrederickmoran@gmail.com> writes:
>> So, if parts of the system are out for a few hours, it may take a number
>> of SYNCs to get up to date; things play more nicely if those SYNCs are
>> small, as opposed to being giant mudballs of changes that take a long
>> time to apply.
>
> Care to comment on how sensible you think this is in general?

I hate the idea, but my opinion doesn't necessarily count on this :-).

I suppose what I'd prefer, generally speaking, would be for the
application to be somewhat aware of the possibility of outage, and to
use a mechanism that is a bit more "visible."

What feels more apropos would be for the application to visibly queue
changes ala pgq (which happens to be the basis for the Londiste
replication system).

But it's not as if I feel *really* strongly about this.

What you're trying to do surely *ought* to work OK with Slony-I.
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From dmk at mr-paradox.net  Wed Nov 18 16:06:34 2009
From: dmk at mr-paradox.net (David Kerr)
Date: Wed Nov 18 16:07:12 2009
Subject: [Slony1-general] Slony + PGPool for HA/Failover only
In-Reply-To: <20091118134014.GA23083@shinkuro.com>
References: <20091118001529.GB81625@mr-paradox.net>
	<20091118134014.GA23083@shinkuro.com>
Message-ID: <20091119000634.GE45500@mr-paradox.net>

On Wed, Nov 18, 2009 at 08:40:14AM -0500, Andrew Sullivan wrote:
- On Tue, Nov 17, 2009 at 04:15:29PM -0800, David Kerr wrote:
- > I like the idea of load balancing, but how does that work, exactly, since Slony 
- > is asynchronous? 
- 
- You handle read-only and read-write queries differently.  Read-only
- queries get a consistent but not necessarily current view of the
- data.  So if your pattern is "update database, read results" with web
- pages, it just won't work -- you need the latest version of the data.
- For environments where imperfect currency is ok (think most web pages,
- for instance), it'll work.  But be careful: I never got pgpool to
- work well for me.

ok thanks. that's how i thought it would be. that may still work for one 
section of the app, but not the part i'm working on now.

- > Second for an HA solution, my real concern, if the load balancing isn't viable
- > (so to speak) can you configure PGPool to point to one node and then fail over
- > to another node (automatically) ? I understand that's a PGPool specific question
- > but since it seems like a common config with Slony I thought i'd chance it 
- > and ask here.
- 
- The question here is how much data loss you're willing to take.
- Alternatively, how much read-write downtime can you stand?
The app is 90% writing, so that'd be a problem.

- If you can stand to lose some transactions, then you can do full
- automatic failover.  But careful!  That data is lost more or less
- forever, so you need to be prepared for that.

I just can't justify losing committed transactions w/o large explosions
being involved so I don't think that would be good for me. :)

That's for the input that's exactly what I was looking for!

Dave
From threshar at torgo.978.org  Thu Nov 19 07:20:12 2009
From: threshar at torgo.978.org (Jeff)
Date: Thu Nov 19 07:21:25 2009
Subject: [Slony1-general] How to confirm if a certain insert has completed
	on the slave db?
In-Reply-To: <65f3e23d0911120401ja371bbdt4d9deff1dab06354@mail.gmail.com>
References: <65f3e23d0911042109w6f666a22raa2ea8331b8e2f31@mail.gmail.com>
	<B9C1AF04-C2CC-4E3A-B1B2-B5CF1E7E01E4@torgo.978.org>
	<65f3e23d0911120401ja371bbdt4d9deff1dab06354@mail.gmail.com>
Message-ID: <F4BB44BC-F220-4B59-9D61-D37D730DAC91@torgo.978.org>


On Nov 12, 2009, at 7:01 AM, Nimesh Satam wrote:

> Jeff,
>
> Thanks for the reply, can you let us know how we get the values for  
> the function inputs p_start , p_txid ?
>
> Regards,
> Nimesh.

select now() as ctime, _replication.getCurrentXid() as txid")

and then you can feed those into syncwait

remember that you need to start a txn, do your insert, then call  
these.  If you do it outside the txn I don't know what txnid is  
related to the row inserted.

>
> On Thu, Nov 5, 2009 at 7:49 PM, Jeff <threshar@torgo.978.org> wrote:
>
> On Nov 5, 2009, at 12:09 AM, Nimesh Satam wrote:
>
> All,
>
> Is there any way to confirm that a certain insert has been completed  
> on the slave database? Would the sl_confirm or sl_event table be  
> useful for this?
>
> For eg: If I have a batch of 100 Inserts, how do I make sure that  
> all have been done on slave?
>
> Regards,
> Nimesh.
>
>
> I've got this code which seems to work well.  I added a function to  
> my DB access layer called "syncexec" that executes and waits for the  
> slaves (up to a predefined interval).
>
> You'll need to do your work inside a transaction and get the txn's  
> txnid for this to work (you can obtain it via _cluster.getCurrentXid()
>
>
>
>
> --
> Jeff Trout <jeff@jefftrout.com>
> http://www.stuarthamm.net/
> http://www.dellsmartexitin.com/
>
>
>
>
>

--
Jeff Trout <jeff@jefftrout.com>
http://www.stuarthamm.net/
http://www.dellsmartexitin.com/



From msquires at whitepages.com  Fri Nov 20 04:50:29 2009
From: msquires at whitepages.com (Michael Squires)
Date: Fri Nov 20 04:51:30 2009
Subject: [Slony1-general] Confused about failover sequencing
Message-ID: <C72BD095.6754%msquires@whitepages.com>

I'm testing various configuration changes so we can have a good set of
recipes for production use. I've got a configuration with master + 2 slaves,
and each slave is subscribed directly to the master. (So N1 -> N2, and N1 ->
N3)

I kill the postgres and slony processes on the master (N1).

I then issue this command to slonik:

   failover( id = 1, backup node = 2);

When this completes N2 is the new master and N3 is subscribed to N2, so the
failover appears to be working. But N1 is still represented in the sl_node
and the slon daemons on both N2 and N3 are trying to connect to it.

I can then do a 'drop node ( id = 1 , event node = 2);' to remove N1.

I'd like to do this in one script, rather than two of them. I've been unable
to come up with a way to wait for the failover to complete in slonik. I've
tried some obvious statements (and many variants), like:

 wait for event (  origin = all,  confirmed = all,  wait on = 2);


But so far have been unsuccessful. If I don't put a wait after the failover
to complete, the drop node fails because N3 is still subscribed to N1.

Is there a way to do this in a single slonik script?


Michael Lee Squires
 
WhitePages
Software Developer
msquires@whitepages.com
whitepages.com
P 206 973 5143  
C 206 388 7063


From kerdezixe at gmail.com  Sat Nov 21 08:27:42 2009
From: kerdezixe at gmail.com (Laurent Laborde)
Date: Sat Nov 21 08:27:56 2009
Subject: [Slony1-general] bloated pg_listener
Message-ID: <8a1bfe660911210827q418ce2aci38edb4196b7f8316@mail.gmail.com>

i had a slave that was lagging by 2~5mn.. slowly but surely, without
high load coming from the read request.
overall... the average load was good. (around ~4 on an octocore).

After many try i found that : vacuum analyze pg_listener; instantly
solved the problem.
(recovering from a 2mn lag in just a few second, and now synched with ~3s lag).

Any tought about why i could have a bloated pg_listener ?

postgresql 8.3.6
Slony 1.2.15
pgbouncer

-- 
Laurent "ker2x" Laborde
Sysadmin & DBA at http://www.over-blog.com/
From peter.geoghegan86 at gmail.com  Mon Nov 23 08:00:53 2009
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Mon Nov 23 08:00:58 2009
Subject: [Slony1-general] Getting benign AFTER triggers to fire on slaves
	only
Message-ID: <db471ace0911230800h33ed632cr7d3ced30fc4273bf@mail.gmail.com>

Hello,

I have a novel requirement; I'd like to fire AFTER triggers on
replicated tables, that don't modify the database, on slaves only.
These triggers just send notifications to client apps that connect to
the slave, which cause them to refresh their view to the table, so the
user doesn't have to refresh it themselves - that is all.

I've noticed that Slony-I disables my AFTER triggers on the slaves
(they actually don't even exist on the master).  While I understand
the rationale in having Slony do this, this is clearly something I can
safely override.

Does what I want seem reasonable? I see that I can ALTER TABLE
my_table ENABLE REPLICA TRIGGER my_trig and get the desired behaviour.
Should I just make a point of doing this after replication has been
setup on all slaves?

I'm using Slony 2.0.2, with Postgres 8.3.5 on the slave and 8.4.1 on the master.

Regards,
Peter Geoghegan
From cbbrowne at ca.afilias.info  Mon Nov 23 08:25:12 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Nov 23 08:25:17 2009
Subject: [Slony1-general] bloated pg_listener
In-Reply-To: <8a1bfe660911210827q418ce2aci38edb4196b7f8316@mail.gmail.com>
	(Laurent Laborde's message of "Sat, 21 Nov 2009 17:27:42 +0100")
References: <8a1bfe660911210827q418ce2aci38edb4196b7f8316@mail.gmail.com>
Message-ID: <87k4xh9pjr.fsf@dba2.int.libertyrms.com>

Laurent Laborde <kerdezixe@gmail.com> writes:
> i had a slave that was lagging by 2~5mn.. slowly but surely, without
> high load coming from the read request.
> overall... the average load was good. (around ~4 on an octocore).
>
> After many try i found that : vacuum analyze pg_listener; instantly
> solved the problem.
> (recovering from a 2mn lag in just a few second, and now synched with ~3s lag).
>
> Any tought about why i could have a bloated pg_listener ?
>
> postgresql 8.3.6
> Slony 1.2.15
> pgbouncer

Well, Slony-I does invalidate quite a lot of pg_listener tuples, so it's
not a surprise for this to happen if pg_listener isn't getting vacuumed.

The cleanup thread is supposed to vacuum pg_listener once in a while
(I'd expect about every 10 minutes, if slon runs with defaults), so I'd
wonder why that, or some other vacuum, is not happening.
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From bnichols at ca.afilias.info  Mon Nov 23 08:37:44 2009
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Mon Nov 23 08:37:48 2009
Subject: [Slony1-general] bloated pg_listener
In-Reply-To: <87k4xh9pjr.fsf@dba2.int.libertyrms.com>
References: <8a1bfe660911210827q418ce2aci38edb4196b7f8316@mail.gmail.com>
	<87k4xh9pjr.fsf@dba2.int.libertyrms.com>
Message-ID: <1258994264.9403.463.camel@bnicholson-desktop>

On Mon, 2009-11-23 at 11:25 -0500, Christopher Browne wrote:
> Laurent Laborde <kerdezixe@gmail.com> writes:
> > i had a slave that was lagging by 2~5mn.. slowly but surely, without
> > high load coming from the read request.
> > overall... the average load was good. (around ~4 on an octocore).
> >
> > After many try i found that : vacuum analyze pg_listener; instantly
> > solved the problem.
> > (recovering from a 2mn lag in just a few second, and now synched with ~3s lag).
> >
> > Any tought about why i could have a bloated pg_listener ?
> >
> > postgresql 8.3.6
> > Slony 1.2.15
> > pgbouncer
> 
> Well, Slony-I does invalidate quite a lot of pg_listener tuples, so it's
> not a surprise for this to happen if pg_listener isn't getting vacuumed.
> 
> The cleanup thread is supposed to vacuum pg_listener once in a while
> (I'd expect about every 10 minutes, if slon runs with defaults), so I'd
> wonder why that, or some other vacuum, is not happening.

If the vacuums are running, do you have a lot of long running
transactions against this database?

-- 
Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.


From melvin6925 at yahoo.com  Mon Nov 23 08:40:09 2009
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Mon Nov 23 08:40:13 2009
Subject: [Slony1-general] Getting benign AFTER triggers to fire on slaves
	only
In-Reply-To: <db471ace0911230800h33ed632cr7d3ced30fc4273bf@mail.gmail.com>
Message-ID: <410931.72682.qm@web53002.mail.re2.yahoo.com>

Perhaps this is what you want?

STORE TRIGGER=A0--=A0 Indicate that a trigger should not be disabled by
    Slony-I on a subscriber node
    =


http://slony.info/documentation/stmtstoretrigger.html

Melvin Davidson =



--- On Mon, 11/23/09, Peter Geoghegan <peter.geoghegan86@gmail.com> wrote:

From: Peter Geoghegan <peter.geoghegan86@gmail.com>
Subject: [Slony1-general] Getting benign AFTER triggers to fire on slaves o=
nly
To: slony1-general@lists.slony.info
Date: Monday, November 23, 2009, 10:00 AM

Hello,

I have a novel requirement; I'd like to fire AFTER triggers on
replicated tables, that don't modify the database, on slaves only.
These triggers just send notifications to client apps that connect to
the slave, which cause them to refresh their view to the table, so the
user doesn't have to refresh it themselves - that is all.

I've noticed that Slony-I disables my AFTER triggers on the slaves
(they actually don't even exist on the master).=A0 While I understand
the rationale in having Slony do this, this is clearly something I can
safely override.

Does what I want seem reasonable? I see that I can ALTER TABLE
my_table ENABLE REPLICA TRIGGER my_trig and get the desired behaviour.
Should I just make a point of doing this after replication has been
setup on all slaves?

I'm using Slony 2.0.2, with Postgres 8.3.5 on the slave and 8.4.1 on the ma=
ster.

Regards,
Peter Geoghegan
_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general



      =

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20091123/=
d5088f09/attachment.htm
From peter.geoghegan86 at gmail.com  Mon Nov 23 08:49:36 2009
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Mon Nov 23 08:49:41 2009
Subject: [Slony1-general] Getting benign AFTER triggers to fire on slaves 
	only
In-Reply-To: <410931.72682.qm@web53002.mail.re2.yahoo.com>
References: <db471ace0911230800h33ed632cr7d3ced30fc4273bf@mail.gmail.com>
	<410931.72682.qm@web53002.mail.re2.yahoo.com>
Message-ID: <db471ace0911230849x21cbe8f8m7f4d06dd9923981d@mail.gmail.com>

I see in the docs for SLONIK STORE TRIGGER that a "nifty trick is that you
can run STORE TRIGGER *before the trigger is installed;* that will not cause
any errors." That suggests to me that it doesn't matter that the trigger
doesn't actually exist on the master. Does it matter?

Regards,
Peter Geoghegan

2009/11/23 Melvin Davidson <melvin6925@yahoo.com>

> Perhaps this is what you want?
>
> STORE TRIGGER --  Indicate that a trigger should not be disabled by
> Slony-I on a subscriber node
>
> http://slony.info/documentation/stmtstoretrigger.html
>
> Melvin Davidson
>
>
> --- On *Mon, 11/23/09, Peter Geoghegan <peter.geoghegan86@gmail.com>*wrot=
e:
>
>
> From: Peter Geoghegan <peter.geoghegan86@gmail.com>
> Subject: [Slony1-general] Getting benign AFTER triggers to fire on slaves
> only
> To: slony1-general@lists.slony.info
> Date: Monday, November 23, 2009, 10:00 AM
>
> Hello,
>
> I have a novel requirement; I'd like to fire AFTER triggers on
> replicated tables, that don't modify the database, on slaves only.
> These triggers just send notifications to client apps that connect to
> the slave, which cause them to refresh their view to the table, so the
> user doesn't have to refresh it themselves - that is all.
>
> I've noticed that Slony-I disables my AFTER triggers on the slaves
> (they actually don't even exist on the master).  While I understand
> the rationale in having Slony do this, this is clearly something I can
> safely override.
>
> Does what I want seem reasonable? I see that I can ALTER TABLE
> my_table ENABLE REPLICA TRIGGER my_trig and get the desired behaviour.
> Should I just make a point of doing this after replication has been
> setup on all slaves?
>
> I'm using Slony 2.0.2, with Postgres 8.3.5 on the slave and 8.4.1 on the
> master.
>
> Regards,
> Peter Geoghegan
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info<http://mc/compose?to=3DSlony1-general@lis=
ts.slony.info>
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20091123/=
7eccd51a/attachment.htm
From melvin6925 at yahoo.com  Mon Nov 23 08:58:46 2009
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Mon Nov 23 08:58:51 2009
Subject: [Slony1-general] Debian Dapper package for Slony 1.2.15
In-Reply-To: <db471ace0911230849x21cbe8f8m7f4d06dd9923981d@mail.gmail.com>
Message-ID: <372124.83420.qm@web53012.mail.re2.yahoo.com>

We are looking to migrate our database from PostgreSQL 8.1 on
Description:=A0=A0=A0 Ubuntu 6.06.2 LTS =

Release:=A0=A0=A0=A0=A0=A0=A0 6.06
Codename:=A0=A0=A0=A0=A0=A0 dapper

However, there does not appear to be any deb packages for Slony on that Ubu=
ntu release,
and we are prohibited from doing a source compile due to company policy.

Does anyone have an alternative solution for installing Slony on Ubuntu 6.0=
6.2 LTS ?

Melvin Davidson =




      =

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20091123/=
e9588fa6/attachment.htm
From cbbrowne at ca.afilias.info  Mon Nov 23 09:27:31 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Nov 23 09:27:45 2009
Subject: [Slony1-general] Debian Dapper package for Slony 1.2.15
In-Reply-To: <372124.83420.qm@web53012.mail.re2.yahoo.com> (Melvin Davidson's
	message of "Mon, 23 Nov 2009 08:58:46 -0800 (PST)")
References: <372124.83420.qm@web53012.mail.re2.yahoo.com>
Message-ID: <87d4399mnw.fsf@dba2.int.libertyrms.com>

Melvin Davidson <melvin6925@yahoo.com> writes:
> We are looking to migrate our database from PostgreSQL 8.1 on
> Description:    Ubuntu 6.06.2 LTS
> Release:        6.06
> Codename:       dapper
>
> However, there does not appear to be any deb packages for Slony on that Ubuntu
> release,
> and we are prohibited from doing a source compile due to company policy.
>
> Does anyone have an alternative solution for installing Slony on Ubuntu 6.06.2
> LTS ?

I'd expect that nothing we might offer would represent anything other
than a deceptive circumlocution.

If you are only allowed to use packages that are part of the Ubuntu
6.06.2 release, and there is no Slony-I package, then I don't think
anyone is likely to be able to do anything that wouldn't amount to
deception.

If you're not allowed to do source compiles, I wouldn't think that it
would be any more permissible to have someone else do it for you.
-- 
let name="cbbrowne" and tld="ca.afilias.info" in name ^ "@" ^ tld;;
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From ajs at crankycanuck.ca  Mon Nov 23 11:03:41 2009
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon Nov 23 11:03:47 2009
Subject: [Slony1-general] Debian Dapper package for Slony 1.2.15
In-Reply-To: <87d4399mnw.fsf@dba2.int.libertyrms.com>
References: <372124.83420.qm@web53012.mail.re2.yahoo.com>
	<87d4399mnw.fsf@dba2.int.libertyrms.com>
Message-ID: <20091123190340.GO28309@shinkuro.com>

On Mon, Nov 23, 2009 at 12:27:31PM -0500, Christopher Browne wrote:
> If you're not allowed to do source compiles, I wouldn't think that it
> would be any more permissible to have someone else do it for you.

This is true, except that for reasons I've never understood companies
that adopt policies like, "You're not allowed to compile your own
software," also seem to think that random packages you get from the
backports directory are perfectly good.

My suggestion is twofold:

    1.  Teach your management that having an evironment in which you
    build and test your own critical packages is an _advantage_, not a
    burden, and that it will reduce costs in the long run.

    2.  Find out whether backports are ok.  This might have something
    to do with a support agreement.  If it does, find out how you can
    get to do what you need within the bounds of your support
    agreement.  Explain, slowly in small words if need be, to the
    sales person that you need to achieve certain goals, and if they
    won't support you you'll find someone who does.  Since Linux-based
    systems are a commodity, not providing this service is a good way
    to lose customers.

A

-- 
Andrew Sullivan
ajs@crankycanuck.ca
From scott.marlowe at gmail.com  Mon Nov 23 11:12:42 2009
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Mon Nov 23 11:12:46 2009
Subject: [Slony1-general] Debian Dapper package for Slony 1.2.15
In-Reply-To: <372124.83420.qm@web53012.mail.re2.yahoo.com>
References: <db471ace0911230849x21cbe8f8m7f4d06dd9923981d@mail.gmail.com>
	<372124.83420.qm@web53012.mail.re2.yahoo.com>
Message-ID: <dcc563d10911231112v7373cb7ne30a4404ea5fcbe9@mail.gmail.com>

On Mon, Nov 23, 2009 at 9:58 AM, Melvin Davidson <melvin6925@yahoo.com> wrote:
>
> We are looking to migrate our database from PostgreSQL 8.1 on
> Description:??? Ubuntu 6.06.2 LTS
> Release:??????? 6.06
> Codename:?????? dapper
>
> However, there does not appear to be any deb packages for Slony on that Ubuntu release,
> and we are prohibited from doing a source compile due to company policy.
>
> Does anyone have an alternative solution for installing Slony on Ubuntu 6.06.2 LTS ?

The solution is complex, but profitable.  In your spare time setup a
corporation to do nothing but provide slony packages you need for
ubuntu 6.06.2.  Have your company sign an exclusive contract with said
corporation to provide these packages for some small sum, like $20,000
a year.
From msquires at whitepages.com  Mon Nov 23 12:50:43 2009
From: msquires at whitepages.com (Michael Squires)
Date: Mon Nov 23 12:50:48 2009
Subject: [Slony1-general] Debian Dapper package for Slony 1.2.15
In-Reply-To: <dcc563d10911231112v7373cb7ne30a4404ea5fcbe9@mail.gmail.com>
Message-ID: <C73035A3.6867%msquires@whitepages.com>

Scott gets the "entrepreneur of the day" award! But if you don't do that, t=
hen I think that Andrew is right on the money in his suggestions.

At Whitepages we have our own repository so we can control exactly what ver=
sions of packages we make available in our different environments (e.g., pr=
oduction, development). Most of those are standard packages off the net tha=
t we don't modify at all. We also have a formal "build" system and a build =
(git) repository for packages that we build ourselves.

As Andrew points out, you can get more control by doing this when you need =
it. For example, we're deploying slony1-2.0.3 right now, although there isn=
't a formal package on the net for that yet. We decided that the feature se=
t was important enough to build it ourselves. In this case we're not really=
 modifying it locally (other than applying patches that haven't gone into t=
he 2.0.3 branch yet), and once a formal 2.0.3 package comes out we'll proba=
bly drop our own and switch to the standard one.

There are a number of reasons that a company might not want to build from s=
ource. If you can put a disciplined process in place (with source code cont=
rol, clear version numbers, etc) then I think you can reduce the uncertaint=
ies associated with building your own.

Good luck,
Michael


On 11/23/09 11:12 AM, "Scott Marlowe" <scott.marlowe@gmail.com> wrote:

On Mon, Nov 23, 2009 at 9:58 AM, Melvin Davidson <melvin6925@yahoo.com> wro=
te:
>
> We are looking to migrate our database from PostgreSQL 8.1 on
> Description:    Ubuntu 6.06.2 LTS
> Release:        6.06
> Codename:       dapper
>
> However, there does not appear to be any deb packages for Slony on that U=
buntu release,
> and we are prohibited from doing a source compile due to company policy.
>
> Does anyone have an alternative solution for installing Slony on Ubuntu 6=
.06.2 LTS ?

The solution is complex, but profitable.  In your spare time setup a
corporation to do nothing but provide slony packages you need for
ubuntu 6.06.2.  Have your company sign an exclusive contract with said
corporation to provide these packages for some small sum, like $20,000
a year.
_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


Michael Lee Squires

WhitePages
Software Developer
msquires@whitepages.com
whitepages.com
P 206 973 5143
C 206 388 7063

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20091123/=
69e82832/attachment.htm
From ajs at crankycanuck.ca  Mon Nov 23 15:46:25 2009
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon Nov 23 15:46:32 2009
Subject: [Slony1-general] Debian Dapper package for Slony 1.2.15
In-Reply-To: <C73035A3.6867%msquires@whitepages.com>
References: <dcc563d10911231112v7373cb7ne30a4404ea5fcbe9@mail.gmail.com>
	<C73035A3.6867%msquires@whitepages.com>
Message-ID: <20091123234625.GG28309@shinkuro.com>

On Mon, Nov 23, 2009 at 12:50:43PM -0800, Michael Squires wrote:
> 
> There are a number of reasons that a company might not want to build
> from source. If you can put a disciplined process in place (with
> source code control, clear version numbers, etc) then I think you
> can reduce the uncertainties associated with building your own.

By the way, despite my dismay at watching large companies re-invent
the problems one had with MSFT/IBM support in the dark days before
anyone could spell "cc" and "make", I think the comment about
disciplined processes is super important.  It is a foolish sysadmin
indeed who allows expedience to dictate performing compiles in the
production environment and so on.  And I say this as someone who,
against his better judgement, took those shortcuts on some occasions.

The plain fact is that you will be disciplined, whether you like it or
not.  The only question is whether you'll be disciplined in a planned
way, before deploying; whether you'll be disciplined later, in the
middle of the night, while trying to undo the effects of earlier poor
discipline; or whether you'll be disciplined by your bosses on Monday
morning, because everything went pear-shaped after a botched upgrade.
Your choice.

A

-- 
Andrew Sullivan
ajs@crankycanuck.ca
From kerdezixe at gmail.com  Tue Nov 24 03:15:26 2009
From: kerdezixe at gmail.com (Laurent Laborde)
Date: Tue Nov 24 03:15:31 2009
Subject: [Slony1-general] bloated pg_listener
In-Reply-To: <87k4xh9pjr.fsf@dba2.int.libertyrms.com>
References: <8a1bfe660911210827q418ce2aci38edb4196b7f8316@mail.gmail.com>
	<87k4xh9pjr.fsf@dba2.int.libertyrms.com>
Message-ID: <8a1bfe660911240315x4e1025edycc78af4b58b00d58@mail.gmail.com>

On Mon, Nov 23, 2009 at 5:25 PM, Christopher Browne
<cbbrowne@ca.afilias.info> wrote:
>
> Well, Slony-I does invalidate quite a lot of pg_listener tuples, so it's
> not a surprise for this to happen if pg_listener isn't getting vacuumed.
>
> The cleanup thread is supposed to vacuum pg_listener once in a while
> (I'd expect about every 10 minutes, if slon runs with defaults), so I'd
> wonder why that, or some other vacuum, is not happening.

Nope, i think there is a problem here.
here is a vacuum verbose analyze pg_listener on this specific slave
that lagged :

INFO:  vacuuming "pg_catalog.pg_listener"
INFO:  "pg_listener": removed 160 row versions in 7 pages
INFO:  "pg_listener": found 160 removable, 4 nonremovable row versions
in 20129 pages
DETAIL:  0 dead row versions cannot be removed yet.
There were 1609042 unused item pointers.
20129 pages contain useful free space.
0 pages are entirely empty.
CPU 0.00s/0.04u sec elapsed 0.04 sec.
INFO:  analyzing "pg_catalog.pg_listener"
INFO:  "pg_listener": scanned 20129 of 20129 pages, containing 4 live
rows and 0 dead rows; 4 rows in sample, 4 estimated total rows
VACUUM

1.6 millions unused item pointers.
20kpage * 8KB -> 160MB !!!!wtf!omg!bbq!!!!111oneone1!1oneoneeleven

I will check what you said.
Thx.

-- 
F4FQM
Kerunix Flan
Laurent Laborde
From stuart at stuartbishop.net  Tue Nov 24 04:21:45 2009
From: stuart at stuartbishop.net (Stuart Bishop)
Date: Tue Nov 24 04:21:52 2009
Subject: [Slony1-general] Debian Dapper package for Slony 1.2.15
In-Reply-To: <372124.83420.qm@web53012.mail.re2.yahoo.com>
Message-ID: <g2emz3amz89zgjp336UYAxe124vaj_firegpg@mail.gmail.com>

T24gTW9uLCBOb3YgMjMsIDIwMDkgYXQgMTE6NTggUE0sIE1lbHZpbiBEYXZpZHNvbiA8bWVsdmlu
NjkyNUB5YWhvby5jb20+IHdyb3RlOg0KPg0KPiBXZSBhcmUgbG9va2luZyB0byBtaWdyYXRlIG91
ciBkYXRhYmFzZSBmcm9tIFBvc3RncmVTUUwgOC4xIG9uDQo+IERlc2NyaXB0aW9uOsKgwqDCoCBV
YnVudHUgNi4wNi4yIExUUw0KPiBSZWxlYXNlOsKgwqDCoMKgwqDCoMKgIDYuMDYNCj4gQ29kZW5h
bWU6wqDCoMKgwqDCoMKgIGRhcHBlcg0KPg0KPiBIb3dldmVyLCB0aGVyZSBkb2VzIG5vdCBhcHBl
YXIgdG8gYmUgYW55IGRlYiBwYWNrYWdlcyBmb3IgU2xvbnkgb24gdGhhdCBVYnVudHUgcmVsZWFz
ZSwNCj4gYW5kIHdlIGFyZSBwcm9oaWJpdGVkIGZyb20gZG9pbmcgYSBzb3VyY2UgY29tcGlsZSBk
dWUgdG8gY29tcGFueSBwb2xpY3kuDQo+DQo+IERvZXMgYW55b25lIGhhdmUgYW4gYWx0ZXJuYXRp
dmUgc29sdXRpb24gZm9yIGluc3RhbGxpbmcgU2xvbnkgb24gVWJ1bnR1IDYuMDYuMiBMVFMgPw0K
Pg0KDQpJIGRvdWJ0IGFueSBwYWNrYWdlcyBleGlzdCBmb3IgRGFwcGVyLiBIb3dldmVyLCB0aGV5
IGRvIGV4aXN0IGZvciBsYXRlciB2ZXJzaW9ucyBvZiBVYnVudHUuDQoNCkkgdGhpbmsgeW91ciBv
cHRpb25zIGFyZToNCg0KIC0gVXBncmFkZSBVYnVudHUgdG8gOC4wNCBMVFMgKEhhcmR5KS4gVGhp
cyB3aWxsIGdldCB5b3UgU2xvbnktSSAxLjIuMTMgYW5kIGhhcHBlbnMgdG8gYmUgd2hhdCBJJ20g
Y3VycmVudGx5IHVzaW5nLg0KDQogLSBHZXQgYSBwYWNrYWdlIGJ1aWx0IGZvciA2LjAyIExUUyAo
RGFwcGVyKSwgcG9zc2libHkgdXNpbmcgYSBMYXVuY2hwYWQgUFBBIHRvIGhlbHAgd2l0aCB0aGlz
LiBUaGUgZXhpc3RpbmcgcGFja2FnZXMgbWlnaHQgaGVscCwgYW5kIG1heSBldmVuIGJ1aWxkIHVu
bW9kaWZpZWQuIEkgZXhwZWN0IHNvbWUgd29yayB3aWxsIGJlIG5lZWRlZCB0aG91Z2ggLSBDYW5v
bmljYWwgU3VwcG9ydCBvciB0aGUgRGViaWFuIG1haW50YWluZXIgKFBldGVyIEVpc2VudHJhdXQp
IG1pZ2h0IGJlIGFibGUgdG8gaGVscCBmb3IgYSBmZWUgaWYgeW91IGFyZSB1bmFibGUgb3IgdW53
aWxsaW5nIHRvIGRvIGl0IGluIGhvdXNlLg0KDQotLQ0KU3R1YXJ0IEJpc2hvcCA8c3R1YXJ0QHN0
dWFydGJpc2hvcC5uZXQ+DQpodHRwOi8vd3d3LnN0dWFydGJpc2hvcC5uZXQvDQotLS0tLS0tLS0t
LS0tLSBuZXh0IHBhcnQgLS0tLS0tLS0tLS0tLS0KQSBub24tdGV4dCBhdHRhY2htZW50IHdhcyBz
Y3J1YmJlZC4uLgpOYW1lOiBzaWduYXR1cmUuYXNjClR5cGU6IGFwcGxpY2F0aW9uL3BncC1zaWdu
YXR1cmUKU2l6ZTogMjYzIGJ5dGVzCkRlc2M6IE9wZW5QR1AgZGlnaXRhbCBzaWduYXR1cmUKVXJs
IDogaHR0cDovL2xpc3RzLnNsb255LmluZm8vcGlwZXJtYWlsL3Nsb255MS1nZW5lcmFsL2F0dGFj
aG1lbnRzLzIwMDkxMTI0LzdmZDdhMTc0L3NpZ25hdHVyZS5wZ3AK
From jlockyer at alpineclubofcanada.ca  Tue Nov 24 15:47:47 2009
From: jlockyer at alpineclubofcanada.ca (Jefferey Lockyer)
Date: Tue Nov 24 15:54:32 2009
Subject: [Slony1-general] Slony - Postgres 8.4 - Makefile issues !!!!!!1
Message-ID: <26505342.post@talk.nabble.com>


I am attempting to install Slony with an OSX postgreSQL source installation. 
./configure finished with no issues, but upon make, I receive the following
: 

./test-scanner < /dev/null > emptytestresult.log
cmp ./emptytestresult.log emptytestresult.expected
./test-scanner < ./test_sql.sql > test_sql.log
cmp ./test_sql.log ./test_sql.expected
./test-scanner < ./cstylecomments.sql > cstylecomments.log
cmp ./cstylecomments.log ./cstylecomments.expected
make[2]: Nothing to be done for `all'.
make[2]: Nothing to be done for `all'.
make[2]: Nothing to be done for `all'.
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../..
-DPGSHARE="\"/usr/local/pgsql/share/\""  -I/usr/local/pgsql/include/
-I/usr/local/pgsql/include/server/   -c -o parser.o parser.c
In file included from parser.y:1101:
scan.c:161: error: conflicting types for ?yyleng?
parser.y:25: error: previous declaration of ?yyleng? was here
scan.c:288: error: conflicting types for ?yyleng?
parser.y:25: error: previous declaration of ?yyleng? was here
make[2]: *** [parser.o] Error 1
make[1]: *** [all] Error 2
make: *** [all] Error 2


Anyone shed any light here as to what the 'yyleng' return type is coming
from ? Is there any place to staticly tell it return type of yyleng  ? 

Thanks in advance. 
Jeff.
-- 
View this message in context: http://old.nabble.com/Slony---Postgres-8.4---Makefile-issues-%21%21%21%21%21%211-tp26505342p26505342.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From scott.marlowe at gmail.com  Tue Nov 24 18:15:04 2009
From: scott.marlowe at gmail.com (Scott Marlowe)
Date: Tue Nov 24 18:15:13 2009
Subject: [Slony1-general] Slony - Postgres 8.4 - Makefile issues !!!!!!1
In-Reply-To: <26505342.post@talk.nabble.com>
References: <26505342.post@talk.nabble.com>
Message-ID: <dcc563d10911241815r79d78527qfeba21b93f85be7d@mail.gmail.com>

On Tue, Nov 24, 2009 at 4:47 PM, Jefferey Lockyer
<jlockyer@alpineclubofcanada.ca> wrote:
>
> I am attempting to install Slony with an OSX postgreSQL source installation.
> ./configure finished with no issues, but upon make, I receive the following
> :
>
> ./test-scanner < /dev/null > emptytestresult.log
> cmp ./emptytestresult.log emptytestresult.expected
> ./test-scanner < ./test_sql.sql > test_sql.log
> cmp ./test_sql.log ./test_sql.expected
> ./test-scanner < ./cstylecomments.sql > cstylecomments.log
> cmp ./cstylecomments.log ./cstylecomments.expected
> make[2]: Nothing to be done for `all'.
> make[2]: Nothing to be done for `all'.
> make[2]: Nothing to be done for `all'.
> gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../..
> -DPGSHARE="\"/usr/local/pgsql/share/\"" ?-I/usr/local/pgsql/include/
> -I/usr/local/pgsql/include/server/ ? -c -o parser.o parser.c
> In file included from parser.y:1101:
> scan.c:161: error: conflicting types for ?yyleng?
> parser.y:25: error: previous declaration of ?yyleng? was here
> scan.c:288: error: conflicting types for ?yyleng?
> parser.y:25: error: previous declaration of ?yyleng? was here

Just guessing but it looks like you need yacc / bison or flex
develpment packages installed (i.e. the .h files and stuff)
From kerdezixe at gmail.com  Wed Nov 25 00:33:27 2009
From: kerdezixe at gmail.com (Laurent Laborde)
Date: Wed Nov 25 00:33:32 2009
Subject: [Slony1-general] bloated pg_listener
In-Reply-To: <1258994264.9403.463.camel@bnicholson-desktop>
References: <8a1bfe660911210827q418ce2aci38edb4196b7f8316@mail.gmail.com>
	<87k4xh9pjr.fsf@dba2.int.libertyrms.com>
	<1258994264.9403.463.camel@bnicholson-desktop>
Message-ID: <8a1bfe660911250033h60f08d05u462babf1d638e4bb@mail.gmail.com>

On Mon, Nov 23, 2009 at 5:37 PM, Brad Nicholson
<bnichols@ca.afilias.info> wrote:
>
> If the vacuums are running, do you have a lot of long running
> transactions against this database?

nope :)


-- 
Laurent "ker2x" Laborde
Sysadmin & DBA at http://www.over-blog.com/
From bnichols at ca.afilias.info  Wed Nov 25 05:33:57 2009
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Wed Nov 25 05:34:01 2009
Subject: [Slony1-general] bloated pg_listener
In-Reply-To: <8a1bfe660911250033h60f08d05u462babf1d638e4bb@mail.gmail.com>
References: <8a1bfe660911210827q418ce2aci38edb4196b7f8316@mail.gmail.com>
	<87k4xh9pjr.fsf@dba2.int.libertyrms.com>
	<1258994264.9403.463.camel@bnicholson-desktop>
	<8a1bfe660911250033h60f08d05u462babf1d638e4bb@mail.gmail.com>
Message-ID: <1259156037.9403.590.camel@bnicholson-desktop>

On Wed, 2009-11-25 at 09:33 +0100, Laurent Laborde wrote:
> On Mon, Nov 23, 2009 at 5:37 PM, Brad Nicholson
> <bnichols@ca.afilias.info> wrote:
> >
> > If the vacuums are running, do you have a lot of long running
> > transactions against this database?
> 
> nope :)

-Did you disable vacuums in the slon?
-Do you have autovacuum enabled?  If so, post your autovac settings.
-If autovac is on, did you tell it to not process pg_listener?
-do you have appropriate logging set so the vacuums (either slon
triggers or autovac) will be logged?

-- 
Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.


From seklecki at noc.cfi.pgh.pa.us  Wed Nov 25 06:46:09 2009
From: seklecki at noc.cfi.pgh.pa.us (Brian A. Seklecki (CFI NOC))
Date: Wed Nov 25 08:52:29 2009
Subject: [Slony1-general] Slony - Postgres 8.4 - Makefile issues !!!!!!1
In-Reply-To: <dcc563d10911241815r79d78527qfeba21b93f85be7d@mail.gmail.com>
References: <26505342.post@talk.nabble.com>
	<dcc563d10911241815r79d78527qfeba21b93f85be7d@mail.gmail.com>
Message-ID: <4B0D4331.8000908@noc.cfi.pgh.pa.us>


>> parser.y:25: error: previous declaration of ?yyleng? was here
> 

Right; and it's not a makefile problem.  It's a build environment problem.

I recommend pkgsrc.org for getting 3rd party stuff to work on Apples 
these days.  Other projects are mostly in the cut.

~BAS

> Just guessing but it looks like you need yacc / bison or flex
> develpment packages installed (i.e. the .h files and stuff)
From kerdezixe at gmail.com  Thu Nov 26 01:26:33 2009
From: kerdezixe at gmail.com (Laurent Laborde)
Date: Thu Nov 26 01:26:36 2009
Subject: [Slony1-general] bloated pg_listener
In-Reply-To: <1259156037.9403.590.camel@bnicholson-desktop>
References: <8a1bfe660911210827q418ce2aci38edb4196b7f8316@mail.gmail.com>
	<87k4xh9pjr.fsf@dba2.int.libertyrms.com>
	<1258994264.9403.463.camel@bnicholson-desktop>
	<8a1bfe660911250033h60f08d05u462babf1d638e4bb@mail.gmail.com>
	<1259156037.9403.590.camel@bnicholson-desktop>
Message-ID: <8a1bfe660911260126u1abf9003w4f46e3561cc9e5a7@mail.gmail.com>

On Wed, Nov 25, 2009 at 2:33 PM, Brad Nicholson
<bnichols@ca.afilias.info> wrote:
> On Wed, 2009-11-25 at 09:33 +0100, Laurent Laborde wrote:
>> On Mon, Nov 23, 2009 at 5:37 PM, Brad Nicholson
>> <bnichols@ca.afilias.info> wrote:
>> >
>> > If the vacuums are running, do you have a lot of long running
>> > transactions against this database?
>>
>> nope :)
>
> -Did you disable vacuums in the slon?
> -Do you have autovacuum enabled? ?If so, post your autovac settings.
> -If autovac is on, did you tell it to not process pg_listener?
> -do you have appropriate logging set so the vacuums (either slon
> triggers or autovac) will be logged?

I can't find a correct documentation about *how* to activate "vacuum
from slony".
If i understand correctly, just disabling the autovacuum on thoses
tables should active the slony vacuum on them ?

-- 
Laurent "ker2x" Laborde
Sysadmin & DBA at http://www.over-blog.com/
From bnichols at ca.afilias.info  Thu Nov 26 12:15:05 2009
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Thu Nov 26 12:15:10 2009
Subject: [Slony1-general] bloated pg_listener
In-Reply-To: <8a1bfe660911260126u1abf9003w4f46e3561cc9e5a7@mail.gmail.com>
References: <8a1bfe660911210827q418ce2aci38edb4196b7f8316@mail.gmail.com>
	<87k4xh9pjr.fsf@dba2.int.libertyrms.com>
	<1258994264.9403.463.camel@bnicholson-desktop>
	<8a1bfe660911250033h60f08d05u462babf1d638e4bb@mail.gmail.com>
	<1259156037.9403.590.camel@bnicholson-desktop>
	<8a1bfe660911260126u1abf9003w4f46e3561cc9e5a7@mail.gmail.com>
Message-ID: <1259266505.9403.815.camel@bnicholson-desktop>

On Thu, 2009-11-26 at 10:26 +0100, Laurent Laborde wrote:
> On Wed, Nov 25, 2009 at 2:33 PM, Brad Nicholson
> <bnichols@ca.afilias.info> wrote:
> > On Wed, 2009-11-25 at 09:33 +0100, Laurent Laborde wrote:
> >> On Mon, Nov 23, 2009 at 5:37 PM, Brad Nicholson
> >> <bnichols@ca.afilias.info> wrote:
> >> >
> >> > If the vacuums are running, do you have a lot of long running
> >> > transactions against this database?
> >>
> >> nope :)
> >
> > -Did you disable vacuums in the slon?
> > -Do you have autovacuum enabled?  If so, post your autovac settings.
> > -If autovac is on, did you tell it to not process pg_listener?
> > -do you have appropriate logging set so the vacuums (either slon
> > triggers or autovac) will be logged?
> 
> I can't find a correct documentation about *how* to activate "vacuum
> from slony".

It is on by default - controlled by the slon.  Unless you turned it off,
it is running.

> If i understand correctly, just disabling the autovacuum on thoses
> tables should active the slony vacuum on them ?

No.  the Slon vacuum has nothing to do with autovacuum.

If you have slon vacuums on (which it sounds like), and you've disabled
autovacuum for these tables (which it also sounds like), and you have no
long running transactions against this node, I can only think of one
case.  Between the cleanup thread intervals where the vacuum runs (every
10 minutes by default), you have enough activity to bloat the table.

BTW, we have had good luck with turning off the slon triggered vacuums
and letting autovacuum handle things.
-- 
Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.


From cbbrowne at ca.afilias.info  Fri Nov 27 12:00:51 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Nov 27 12:00:54 2009
Subject: [Slony1-general] Change to keyword handling API in 8.5
Message-ID: <871vjj91qk.fsf@dba2.int.libertyrms.com>

I was just trying to do a build against 8.5, and found...

gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../.. -fpic -I/home/chris/dbs/postgresql-git-head/include/ -I/home/chris/dbs/postgresql-git-head/include/server/  -c -o slony1_funcs.o slony1_funcs.c
slony1_funcs.c: In function ?slon_quote_literal?:
slony1_funcs.c:1029: warning: pointer targets in passing argument 1 of ?pg_mblen? differ in signedness
slony1_funcs.c: In function ?slon_quote_identifier?:
slony1_funcs.c:1110: error: too few arguments to function ?ScanKeywordLookup?
make: *** [slony1_funcs.o] Error 1

There has apparently been two parameters added to ScanKeywordLookup()

   src/include/parser/keywords.h

I guess we need to detect this...
-- 
(reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Fri Nov 27 13:03:32 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Nov 27 13:03:36 2009
Subject: [Slony1-general] bloated pg_listener
In-Reply-To: <1259266505.9403.815.camel@bnicholson-desktop> (Brad Nicholson's
	message of "Thu, 26 Nov 2009 15:15:05 -0500")
References: <8a1bfe660911210827q418ce2aci38edb4196b7f8316@mail.gmail.com>
	<87k4xh9pjr.fsf@dba2.int.libertyrms.com>
	<1258994264.9403.463.camel@bnicholson-desktop>
	<8a1bfe660911250033h60f08d05u462babf1d638e4bb@mail.gmail.com>
	<1259156037.9403.590.camel@bnicholson-desktop>
	<8a1bfe660911260126u1abf9003w4f46e3561cc9e5a7@mail.gmail.com>
	<1259266505.9403.815.camel@bnicholson-desktop>
Message-ID: <87vdgv7k9n.fsf@dba2.int.libertyrms.com>

Brad Nicholson <bnichols@ca.afilias.info> writes:
> On Thu, 2009-11-26 at 10:26 +0100, Laurent Laborde wrote:
>> On Wed, Nov 25, 2009 at 2:33 PM, Brad Nicholson
>> <bnichols@ca.afilias.info> wrote:
>> > On Wed, 2009-11-25 at 09:33 +0100, Laurent Laborde wrote:
>> >> On Mon, Nov 23, 2009 at 5:37 PM, Brad Nicholson
>> >> <bnichols@ca.afilias.info> wrote:
>> >> >
>> >> > If the vacuums are running, do you have a lot of long running
>> >> > transactions against this database?
>> >>
>> >> nope :)
>> >
>> > -Did you disable vacuums in the slon?
>> > -Do you have autovacuum enabled?  If so, post your autovac settings.
>> > -If autovac is on, did you tell it to not process pg_listener?
>> > -do you have appropriate logging set so the vacuums (either slon
>> > triggers or autovac) will be logged?
>> 
>> I can't find a correct documentation about *how* to activate "vacuum
>> from slony".
>
> It is on by default - controlled by the slon.  Unless you turned it off,
> it is running.

Indeed.  (With a caveat...)

>> If i understand correctly, just disabling the autovacuum on thoses
>> tables should active the slony vacuum on them ?
>
> No.  the Slon vacuum has nothing to do with autovacuum.

"Nothing" is possibly a bit too strong a word, as there is an
interaction between the Slony vacuum regimen and that of autovacuum.

The slon vacuum thread *does* peek in on autovacuum data, and if
autovacuum is on, the vacuum thread will leave out those tables managed
by autovacuum.

That's true in both 1.2 and 2.0.  The logic is a fair bit cleaner and
smarter in 2.0.  (Actually, it's mostly *simpler* in 2.0!)

The logic, even in Slony-I 1.2, is smart enough to know if there are
tables you explicitly told autovac NOT to manage.

The overall result should be reasonably robust...

 - If you aren't running autovac, Slony-I will be vacuuming tables every
   ~10 minutes via the cleanup thread

 - If you *are* running autovac, then autovac "takes up the slack."

 - You *could* tell autovac to leave the tables for Slony-I, if you
   prefer.

Pre-8.3, I'd actually tend to recommend leaving vacuuming to Slony-I,
because the autovac logic was somewhat flawed.  (Nothing fundamentally
horrible - just that it was in its early days, and there were cases
where it wouldn't be robust enough for important systems.)

As of 8.3+, I'd commend turning on the autovac service (it became a
default behaviour at that point).  When you do that, Slony-I will
automatically step back and leave the vacuuming to autovac.  

No muss (the point of automatically vacuuming ;-)), no fuss.
-- 
"cbbrowne","@","ca.afilias.info"
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
