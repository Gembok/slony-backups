From andreas at kostyrka.org  Tue May  1 00:07:10 2007
From: andreas at kostyrka.org (Andreas Kostyrka)
Date: Tue May  1 00:06:58 2007
Subject: [Slony1-general] Implicit Sequences
In-Reply-To: <008401c78bb5$9d27a130$6501a8c0@bwlaptop>
References: <008401c78bb5$9d27a130$6501a8c0@bwlaptop>
Message-ID: <20070501070710.GE24074@andi-lap.la.revver.com>

* Bill Willits <bwillits@cox.net> [070501 08:28]:
>    Is it necessary to replicate sequences that are tied to table columns?
>    Or, does having the table as part of the replicated set sufficient to keep
>    the sequence-dependant data in sync.

Well, you need to add the sequences to the replication set explicitly.
You can do that any time, and you don't need it from the start
(meaning you can easily fix it).

What happens if you don't do? Slony doesn't update the sequence
values. Everything works fine, because on the slave nodes, slony
inserts complete rows, so test1_id_seq is never used. Now, when you
switch master nodes to one of the backups, you've got a problem => on
the first insert you'll get a "unique constraint error on test1_pkey".

Andreas
From dgagnon at tc2l.ca  Tue May  1 08:19:14 2007
From: dgagnon at tc2l.ca (Dominique Gagnon)
Date: Tue May  1 08:19:29 2007
Subject: [Slony1-general] problem with slonik_init_cluster
Message-ID: <1178032754.21684.20.camel@poste10-105.int.infoglobe.ca>

Hi,

The perl script slonik_init_cluster packaged in slony1-bin 1.2.1 (etch)
doesn't create everything necessary in order for replication to work
correctly when a slon_tools.conf based on the example included is used.

In particular, The paths and listens directives are not set and it seems
to be related to a perl array ($VIA) used in slonik_init_cluster which
is not defined anywhere.

My slon_tools.conf:  http://paste.uni.cc/15012


Dominique Gagnon

From andrew.george.hammond at gmail.com  Tue May  1 09:41:27 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Tue May  1 09:41:39 2007
Subject: [Slony1-general] problem with slonik_init_cluster
In-Reply-To: <1178032754.21684.20.camel@poste10-105.int.infoglobe.ca>
References: <1178032754.21684.20.camel@poste10-105.int.infoglobe.ca>
Message-ID: <5a0a9d6f0705010941xfcca727l32f7bac835f5b8e8@mail.gmail.com>

On 5/1/07, Dominique Gagnon <dgagnon@tc2l.ca> wrote:
>
> Hi,
>
> The perl script slonik_init_cluster packaged in slony1-bin 1.2.1 (etch)



Is there some reason you're running 1.2.1? Slony is currently at 1.2.9. and
quite a few bugs, some of them rather serious, have been fixed in the last 8
releases.


doesn't create everything necessary in order for replication to work
> correctly when a slon_tools.conf based on the example included is used.
>
> In particular, The paths and listens directives are not set and it seems
> to be related to a perl array ($VIA) used in slonik_init_cluster which
> is not defined anywhere.
>
> My slon_tools.conf:  http://paste.uni.cc/15012



I don't see anything obviously wrong, but then I don't use the
slon_tools.conf stuff.

Andrew
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070501/=
e3f12fa5/attachment.htm
From bwillits at cox.net  Tue May  1 11:27:41 2007
From: bwillits at cox.net (Bill Willits)
Date: Tue May  1 11:28:06 2007
Subject: [Slony1-general] Implicit Sequences
References: <008401c78bb5$9d27a130$6501a8c0@bwlaptop>
	<20070501070710.GE24074@andi-lap.la.revver.com>
Message-ID: <003c01c78c1e$67f78250$6501a8c0@bwlaptop>

>* Bill Willits <bwillits@cox.net> [070501 08:28]:
>>    Is it necessary to replicate sequences that are tied to table columns?
>>    Or, does having the table as part of the replicated set sufficient to 
>> keep
>>    the sequence-dependant data in sync.

From: "Andreas Kostyrka" ...
>
> Well, you need to add the sequences to the replication set explicitly.
> You can do that any time, and you don't need it from the start
> (meaning you can easily fix it).
>
> What happens if you don't do? Slony doesn't update the sequence
> values. Everything works fine, because on the slave nodes, slony
> inserts complete rows, so test1_id_seq is never used. Now, when you
> switch master nodes to one of the backups, you've got a problem => on
> the first insert you'll get a "unique constraint error on test1_pkey".
>
> Andreas

Thanks, Andreas.  Makes sense.

~Bill Willits 

From dgagnon at tc2l.ca  Tue May  1 13:08:19 2007
From: dgagnon at tc2l.ca (Dominique Gagnon)
Date: Tue May  1 13:08:43 2007
Subject: [Slony1-general] problem with slonik_init_cluster
In-Reply-To: <5a0a9d6f0705010941xfcca727l32f7bac835f5b8e8@mail.gmail.com>
References: <1178032754.21684.20.camel@poste10-105.int.infoglobe.ca>
	<5a0a9d6f0705010941xfcca727l32f7bac835f5b8e8@mail.gmail.com>
Message-ID: <1178050099.21684.27.camel@poste10-105.int.infoglobe.ca>

The script from the 1.2.9 source works with my slon_tools.conf (it
creates the paths directives). I will stick to 1.2.1 for now since it is
packaged by debian but I may use the 1.2.9 tarball somewhere down the
way.

Thanks,

Dominique

On Tue, 2007-05-01 at 09:41 -0700, Andrew Hammond wrote:
> On 5/1/07, Dominique Gagnon <dgagnon@tc2l.ca> wrote:
>         Hi,
>         
>         The perl script slonik_init_cluster packaged in slony1-bin
>         1.2.1 (etch)
> 
> 
> Is there some reason you're running 1.2.1? Slony is currently at
> 1.2.9. and quite a few bugs, some of them rather serious, have been
> fixed in the last 8 releases. 
> 
> 
> 
>         doesn't create everything necessary in order for replication
>         to work
>         correctly when a slon_tools.conf based on the example included
>         is used.
>         
>         In particular, The paths and listens directives are not set
>         and it seems
>         to be related to a perl array ($VIA) used in
>         slonik_init_cluster which 
>         is not defined anywhere.
>         
>         My slon_tools.conf:  http://paste.uni.cc/15012
> 
> 
> I don't see anything obviously wrong, but then I don't use the
> slon_tools.conf stuff. 
> 
> 
> Andrew
> 
> 

From andrew.george.hammond at gmail.com  Tue May  1 14:54:20 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Tue May  1 14:54:45 2007
Subject: [Slony1-general] problem with slonik_init_cluster
In-Reply-To: <1178050099.21684.27.camel@poste10-105.int.infoglobe.ca>
References: <1178032754.21684.20.camel@poste10-105.int.infoglobe.ca>
	<5a0a9d6f0705010941xfcca727l32f7bac835f5b8e8@mail.gmail.com>
	<1178050099.21684.27.camel@poste10-105.int.infoglobe.ca>
Message-ID: <5a0a9d6f0705011454u30aef320g361e1c2020134585@mail.gmail.com>

On 5/1/07, Dominique Gagnon <dgagnon@tc2l.ca> wrote:
>
> The script from the 1.2.9 source works with my slon_tools.conf (it
> creates the paths directives). I will stick to 1.2.1 for now since it is
> packaged by debian but I may use the 1.2.9 tarball somewhere down the
>
way.



1.2.1 is fine for testing / getting a feel of the system, but before you
decide to put real data into it, I'd strongly recommend that you upgrade to
1.2.latest.

Andrew



Thanks,
>
> Dominique
>
> On Tue, 2007-05-01 at 09:41 -0700, Andrew Hammond wrote:
> > On 5/1/07, Dominique Gagnon <dgagnon@tc2l.ca> wrote:
> >         Hi,
> >
> >         The perl script slonik_init_cluster packaged in slony1-bin
> >         1.2.1 (etch)
> >
> >
> > Is there some reason you're running 1.2.1? Slony is currently at
> > 1.2.9. and quite a few bugs, some of them rather serious, have been
> > fixed in the last 8 releases.
> >
> >
> >
> >         doesn't create everything necessary in order for replication
> >         to work
> >         correctly when a slon_tools.conf based on the example included
> >         is used.
> >
> >         In particular, The paths and listens directives are not set
> >         and it seems
> >         to be related to a perl array ($VIA) used in
> >         slonik_init_cluster which
> >         is not defined anywhere.
> >
> >         My slon_tools.conf:  http://paste.uni.cc/15012
> >
> >
> > I don't see anything obviously wrong, but then I don't use the
> > slon_tools.conf stuff.
> >
> >
> > Andrew
> >
> >
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070501/=
e7873187/attachment.htm
From ajs at crankycanuck.ca  Wed May  2 09:08:18 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed May  2 09:08:41 2007
Subject: [Slony1-general] problem with slonik_init_cluster
In-Reply-To: <1178050099.21684.27.camel@poste10-105.int.infoglobe.ca>
References: <1178032754.21684.20.camel@poste10-105.int.infoglobe.ca>
	<5a0a9d6f0705010941xfcca727l32f7bac835f5b8e8@mail.gmail.com>
	<1178050099.21684.27.camel@poste10-105.int.infoglobe.ca>
Message-ID: <20070502160818.GF11506@phlogiston.dyndns.org>

On Tue, May 01, 2007 at 04:08:19PM -0400, Dominique Gagnon wrote:
> creates the paths directives). I will stick to 1.2.1 for now since it is
> packaged by debian but I may use the 1.2.9 tarball somewhere down the
> way.

I've suggested to someone else on this list that "the one packaged by
Debian" is an _extremely bad_ way to select the Slony version to use. 
If you actually need replication, and need it to work, you are going
to need to learn how to keep up with the latest stable releases of
the code.  

Slony works, and the project members try hard to make sure that
releases are reliable and useful, but it's a small community working
on a very young code base.  Normally, that doesn't matter, and
Debian's conservatism in what to put into the stable release is
extremely valuable.  In this case, however, the conservatism is a
possible recipe for serious bugs that can break your replication
system.  That probably means that, if you must use Debian packages,
you should learn how to build them from the sources for Slony.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
Users never remark, "Wow, this software may be buggy and hard 
to use, but at least there is a lot of code underneath."
		--Damien Katz
From andreas at kostyrka.org  Wed May  2 09:43:20 2007
From: andreas at kostyrka.org (Andreas Kostyrka)
Date: Wed May  2 09:42:35 2007
Subject: [Slony1-general] problem with slonik_init_cluster
In-Reply-To: <20070502160818.GF11506@phlogiston.dyndns.org>
References: <1178032754.21684.20.camel@poste10-105.int.infoglobe.ca>	<5a0a9d6f0705010941xfcca727l32f7bac835f5b8e8@mail.gmail.com>	<1178050099.21684.27.camel@poste10-105.int.infoglobe.ca>
	<20070502160818.GF11506@phlogiston.dyndns.org>
Message-ID: <4638BFA8.7070807@kostyrka.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



Andrew Sullivan wrote:
> On Tue, May 01, 2007 at 04:08:19PM -0400, Dominique Gagnon wrote:
>> creates the paths directives). I will stick to 1.2.1 for now since it is
>> packaged by debian but I may use the 1.2.9 tarball somewhere down the
>> way.
> 
> I've suggested to someone else on this list that "the one packaged by
> Debian" is an _extremely bad_ way to select the Slony version to use. 
> If you actually need replication, and need it to work, you are going
> to need to learn how to keep up with the latest stable releases of
> the code.  
Exactly. Replication is "serious" work. And slony is extremly easy to
install from the tarballs, no need for deb files.

> Slony works, and the project members try hard to make sure that
> releases are reliable and useful, but it's a small community working
> on a very young code base.  Normally, that doesn't matter, and
> Debian's conservatism in what to put into the stable release is
> extremely valuable.  In this case, however, the conservatism is a
> possible recipe for serious bugs that can break your replication
> system.  That probably means that, if you must use Debian packages,
> you should learn how to build them from the sources for Slony.

;)

Andreas
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.2 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFGOL+oHJdudm4KnO0RArXIAJ9Iec+RhOEYFHJb01hGRInhnSI5HACfSYkf
OcJB/eli4wbzSBkcGIfrjrE=
=N81Q
-----END PGP SIGNATURE-----
From cbbrowne at ca.afilias.info  Wed May  2 12:14:04 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed May  2 12:14:17 2007
Subject: [Slony1-general] Re: [Slony1-patches] PostgreSQL 8.3 compatibilty
In-Reply-To: <4631B6E2.5050304@postgresql.org>
References: <4630C427.6090700@postgresql.org>
	<4631211C.7080400@ca.afilias.info>
	<4631B6E2.5050304@postgresql.org>
Message-ID: <4638E2FC.1050203@ca.afilias.info>

Dave Page wrote:
> Christopher Browne wrote:
> > cbbrowne@dba2:Slony-I/CMD/slony1-HEAD/tests> cat
> > /tmp/slony-regress.c12207/slonik.log
> > <stdin>:4: PGRES_FATAL_ERROR load '$libdir/slony1_funcs';  - ERROR:
> > could not load library "/opt/OXRS/dbs/pgsql82/lib/slony1_funcs.so":
> > /opt/OXRS/dbs/pgsql82/lib/slony1_funcs.so: undefined symbol:
> SET_VARSIZE
> > <stdin>:4: Error: the extension for the Slony-I C functions cannot be
> > loaded in database 'dbname=slonyregress1 host=localhost user=cbbrowne
> > port=5882'<stdin>:4: ERROR: no admin conninfo for node 134594856
> >
> > Note: this problem is consistent across both branches (e.g. - HEAD
> and 1.2).
> >
> > I suspect that this breaks something about support of older versions.
> > At some point, that may be acceptable for CVS HEAD, but I rather think
> > it won't be OK for 1.2 :-).
> >
> > I don't have time to explore that this afternoon; hopefully our
> > timezones differ sufficiently appropriately that you either see this
> > tonight or tomorrow morning.
>
> Sorry - thinko on my part combined with lack of testing on 8.2 (too
> intent on making 8.3 work :-( )
>
> Updated patches attached, each minus the extra = char that they were
> previously sporting!
>
> Regards, Dave
>
Sorry it took a while to test this; I ran the test:

   ./run_test.sh test1

against both the 1.2 and HEAD branches,

against Debian/testing

against PostgreSQL versions 7.4.5, 8.2.3, and today's CVS HEAD (e.g. -
8.3 'real, real beta').

I'll want to run against 8.0 and 8.1 as well, at some point; running 6
regression tests seems enough to indicate that it can't be too terribly
broken :-), especially when we have successful test runs on every other
major version, and it worked on both older and newer versions...

I'll be checking the changes into CVS for both branches shortly.

Thanks, Dave!
From lavalamp at spiritual-machines.org  Thu May  3 10:25:44 2007
From: lavalamp at spiritual-machines.org (Brian A. Seklecki)
Date: Thu May  3 10:25:58 2007
Subject: [Slony1-general] Net-SNMP AgentX 
Message-ID: <20070503132214.L67008@arbitor.digitalfreaks.org>


Wow how did I miss this?  FBSD Ports makes me lazy I guess.

  --with-netsnmp=<dir>   Enable snmp support <dir> is the 
location of net-snmp-config. **EXPERIMENTAL**

HISTORY-1.1:

Check in first round of minimal net-snmp sub-agentx support, net-snmp 
5.1.x is the minimum supported version.  At the moment all you can do via 
SNMP is get/set the log level.  SNMP support is not enabled or compiled in 
by default. Run configure with --with-netsnmp to enable support.

--

That's really wicked-cool; I'll have to experiment with it in our lab. 
I've been bugging Bill Moran incessantly about getting AgentX for Slony 
and PostgreSQL.  Pragmatically, it really is the best way to add enhanced 
management information abstraction.

l8*
 	-lava (Brian A. Seklecki - Pittsburgh, PA, USA)
 	       http://www.spiritual-machines.org/
From andrew.george.hammond at gmail.com  Thu May  3 20:25:11 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Thu May  3 20:25:43 2007
Subject: [Slony1-general] RE: Settings table IDs
In-Reply-To: <2CC69F840555CB43B04195F218CCB57F8F4CF2@COENGEX01.cctus.com>
References: <20070426190003.A9AA229043A@main.slony.info>
	<2CC69F840555CB43B04195F218CCB57F8F4CF2@COENGEX01.cctus.com>
Message-ID: <5a0a9d6f0705032025u323caeb8j414dcfe8113d7682@mail.gmail.com>

On 4/26/07, Melvin Davidson <mdavidson@cctus.com> wrote:
>
> Andrew commented
> >It seems silly to encourage people to adopt a particular ordering when
> >1) we have no evidence to demonstrate that ordering matters
> >2) we haven't identified even a theoretical failure case that is based
> on
> >ordering
> >3) we can't reasonably believe that a given ordering is superior for
> >avoiding an unknown issue
>
> Actually, I have encountered a case where the sequence DOES MATTER!
>
> I have two tables with referencial Foreign Key constraints to each
> other.
> Don't ask why, because that is the way it was when I accepted the
> position
> and was told I could not change it. However if I find whom it was that
> did
> it, I am sure they will require hospitalization for some trauma they
> will
> sustain. :)
>
> The bottom line is, the table that has an insert done on it first MUST
> have
> a lower id than then second one, as subscribing will fail if the order
> is
> reversed.



Hi Melvin,

I too am very interested to see this example, especially if it reliably
produces the failure condition. If you can't provide the actual code, can
you please provide a detailed description so that I can implement it as a
test? As best as I can tell from your description above, the schema looks
something like this:

CREATE TABLE a (
  a_id serial PRIMARY KEY,
  b_id integer NOT NULL
);

CREATE TABLE b (
  b_id serial PRIMARY KEY,
  a_id integer NOT NULL REFERENCES a
);

ALTER TABLE a ADD CONSTRAINT a_b_id_fkey FOREIGN KEY a_id REFERENCES a;

This creates two tables with foreign keys into each other. Does that
accurately describe the situation you observed? If so, what is the order
necessary for subscription to succeed? I'd really appreciate your help with
this. It's been a minor intellectual irritant for a few years now and I'd
really like to get it nailed down and clearly understood.

Andrew
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070503/=
b55e5665/attachment.htm
From pergesu at gmail.com  Fri May  4 04:23:19 2007
From: pergesu at gmail.com (Pat Maddox)
Date: Fri May  4 04:24:08 2007
Subject: [Slony1-general] Using monit to keep slony running
Message-ID: <810a540e0705040423u3411454er671d5e899daf1994@mail.gmail.com>

I'm using monit to handle a number of our services, and want to use it
for slon as well.  Here's the config I've created:

check process slony
  with pidfile /var/run/slony.pid
  start program = "/usr/local/bin/slon -p /var/run/slony.pid twistage
'dbname=twist_prod user=slony host=localhost' >> /var/log/slony.log &"
  stop program = "/bin/kill -TERM `/bin/cat /var/run/slony.pid`"
  group slony

Running "monit start all" works fine and the slon daemon starts up and
does its thing.  However I'm not able to stop slon.  monit just sits
for a while, and then says that it failed to stop slon.  If I run the
kill command myself from the command line though it works perfectly.
Not sure what the difference is - how can I make slon stop using
monit?

Pat
From pergesu at gmail.com  Fri May  4 04:56:51 2007
From: pergesu at gmail.com (Pat Maddox)
Date: Fri May  4 04:57:47 2007
Subject: [Slony1-general] Re: Using monit to keep slony running
In-Reply-To: <810a540e0705040423u3411454er671d5e899daf1994@mail.gmail.com>
References: <810a540e0705040423u3411454er671d5e899daf1994@mail.gmail.com>
Message-ID: <810a540e0705040456r3084818l63dd3cc2dd641836@mail.gmail.com>

On 5/4/07, Pat Maddox <pergesu@gmail.com> wrote:
> I'm using monit to handle a number of our services, and want to use it
> for slon as well.  Here's the config I've created:
>
> check process slony
>   with pidfile /var/run/slony.pid
>   start program = "/usr/local/bin/slon -p /var/run/slony.pid twistage
> 'dbname=twist_prod user=slony host=localhost' >> /var/log/slony.log &"
>   stop program = "/bin/kill -TERM `/bin/cat /var/run/slony.pid`"
>   group slony
>
> Running "monit start all" works fine and the slon daemon starts up and
> does its thing.  However I'm not able to stop slon.  monit just sits
> for a while, and then says that it failed to stop slon.  If I run the
> kill command myself from the command line though it works perfectly.
> Not sure what the difference is - how can I make slon stop using
> monit?
>
> Pat
>

I ended up doing
stop program = "/bin/sh -c '/bin/kill -s SIGTERM `/bin/cat /var/run/slony.pid`'"

instead and it works fine.

My only other question now is can I be sure that slony is still
replicating properly as long as it's running?  The docs say that older
versions were very fragile and would crash if they encountered any
problem.  That'd be fine with my setup because I'm using monit, it'd
just get restarted.  In 1.2 though it's supposedly a lot less
fragile...but under what conditions will the daemon run but not be
operating properly?  Can that happen at all?  Assume that the
configuration is okay (i.e. nothing boneheaded like forgetting to
subscribe a set).

Pat
From aljeux at free.fr  Fri May  4 07:11:23 2007
From: aljeux at free.fr (Alain Peyrat)
Date: Fri May  4 07:11:31 2007
Subject: [Slony1-general] How to reduce st_lag_time ?
Message-ID: <1178287883.463b3f0b765e6@imp.free.fr>

Hi,

I'm running a simple replication between a master and a slave. The
slave is planned as a switchover host in case of big troubles on
the server (ie: slave can be stopped but not the master).

My environnment:

Postgres: 7.4
Slony: 1.2.6

It seems that my replication is no more active and the tables
sl_log_1/2 are now very big (sl_log_1 is around 6GB and 3.3e+06
tuples). Value st_lag_time is very important.

I have tried to restart the slon (master and slave) but the
st_last_received_event_ts is not changing (or only once for 3 sec).

On the master slon log, I have (1 is master, 2 is slave).

2007-05-04 15:54:31 CEST DEBUG2 remoteListenThread_2: queue event 2,712628 SYNC
2007-05-04 15:54:31 CEST DEBUG2 remoteListenThread_2: UNLISTEN
2007-05-04 15:54:31 CEST DEBUG2 remoteWorkerThread_2: Received event 2,712628
SYNC
2007-05-04 15:54:31 CEST DEBUG3 calc sync size - last time: 1 last length: 10166
ideal: 5 proposed size: 3
2007-05-04 15:54:31 CEST DEBUG2 remoteWorkerThread_2: SYNC 712628 processing
2007-05-04 15:54:31 CEST DEBUG2 remoteWorkerThread_2: no sets need syncing for
this event
2007-05-04 15:54:32 CEST DEBUG2 localListenThread: Received event 1,1553949 SYNC
2007-05-04 15:54:32 CEST DEBUG2 syncThread: new sl_action_seq 6356493 - SYNC
1553950

So, I'm seeing line with 'no sets need syncing for this event'.

Can someone help me understanding what's wrong ?

Thanks in advance,
Alain.
From cbbrowne at ca.afilias.info  Fri May  4 08:03:31 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri May  4 08:03:41 2007
Subject: [Slony1-general] How to reduce st_lag_time ?
In-Reply-To: <1178287883.463b3f0b765e6@imp.free.fr> (Alain Peyrat's message of
	"Fri, 04 May 2007 16:11:23 +0200")
References: <1178287883.463b3f0b765e6@imp.free.fr>
Message-ID: <60ps5gbph8.fsf@dba2.int.libertyrms.com>

Alain Peyrat <aljeux@free.fr> writes:
> Hi,
>
> I'm running a simple replication between a master and a slave. The
> slave is planned as a switchover host in case of big troubles on
> the server (ie: slave can be stopped but not the master).
>
> My environnment:
>
> Postgres: 7.4
> Slony: 1.2.6
>
> It seems that my replication is no more active and the tables
> sl_log_1/2 are now very big (sl_log_1 is around 6GB and 3.3e+06
> tuples). Value st_lag_time is very important.
>
> I have tried to restart the slon (master and slave) but the
> st_last_received_event_ts is not changing (or only once for 3 sec).
>
> On the master slon log, I have (1 is master, 2 is slave).
>
> 2007-05-04 15:54:31 CEST DEBUG2 remoteListenThread_2: queue event 2,712628 SYNC
> 2007-05-04 15:54:31 CEST DEBUG2 remoteListenThread_2: UNLISTEN
> 2007-05-04 15:54:31 CEST DEBUG2 remoteWorkerThread_2: Received event 2,712628
> SYNC
> 2007-05-04 15:54:31 CEST DEBUG3 calc sync size - last time: 1 last length: 10166
> ideal: 5 proposed size: 3
> 2007-05-04 15:54:31 CEST DEBUG2 remoteWorkerThread_2: SYNC 712628 processing
> 2007-05-04 15:54:31 CEST DEBUG2 remoteWorkerThread_2: no sets need syncing for
> this event
> 2007-05-04 15:54:32 CEST DEBUG2 localListenThread: Received event 1,1553949 SYNC
> 2007-05-04 15:54:32 CEST DEBUG2 syncThread: new sl_action_seq 6356493 - SYNC
> 1553950
>
> So, I'm seeing line with 'no sets need syncing for this event'.
>
> Can someone help me understanding what's wrong ?

You may find it useful to look at the following page for explanations for log entries:

<http://slony.info/adminguide/slony1-1.2.6/doc/adminguide/loganalysis.html>

The interesting log is not the one for the origin node; you can expect
it to be mighty sparse, and when that node receives events from other
nodes, it is very much to be expected that it will indicate "no sets
need syncing for this event."

More interesting will be the contents of the log file for the
subscriber node; THAT will show a lot more action, and show log
entries describing the work being done to pull and apply updates from
the origin.

But none of this is showing off anything that would be considered a
"problem," and you won't see anything in the logs that will make it
evident why you can't get the lag time down.

The relevant slon option is "sync_interval_timeout", which is either
controlled via that name in a "slon.conf" file, or via the command
line "-s" option, which determines how long the slon sleeps between
checking to see if there have been updates made that would warrant
generating a SYNC event.

I believe that the default for that is 10000, which represents 10s.
If you reduced the value, that would cause SYNC events to be generated
more frequently, which I would expect to reduce st_lag_time for you.
-- 
"cbbrowne","@","ca.afilias.info"
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From aljeux at free.fr  Fri May  4 08:16:07 2007
From: aljeux at free.fr (Alain Peyrat)
Date: Fri May  4 08:16:25 2007
Subject: [Slony1-general] How to reduce st_lag_time ?
In-Reply-To: <60ps5gbph8.fsf@dba2.int.libertyrms.com>
References: <1178287883.463b3f0b765e6@imp.free.fr>
	<60ps5gbph8.fsf@dba2.int.libertyrms.com>
Message-ID: <1178291767.463b4e3722973@imp.free.fr>

Hi,

I tried to change the default value to 100ms to test, but this has no impact on
the lag, it is still increasing. The st_last_received_event_ts does not change.

Here is the logs on the slave host (-d 3):

2007-05-04 17:12:12 CEST CONFIG main: slon version 1.2.6 starting up
2007-05-04 17:12:12 CEST DEBUG2 slon: watchdog process started
2007-05-04 17:12:12 CEST DEBUG2 slon: watchdog ready - pid = 22165
2007-05-04 17:12:12 CEST DEBUG2 slon: worker process created - pid = 22166
2007-05-04 17:12:12 CEST CONFIG main: local node id = 2
2007-05-04 17:12:12 CEST DEBUG2 main: main process started
2007-05-04 17:12:12 CEST CONFIG main: launching sched_start_mainloop
2007-05-04 17:12:12 CEST CONFIG main: loading current cluster configuration
2007-05-04 17:12:12 CEST CONFIG storeNode: no_id=1 no_comment='Node 1'
2007-05-04 17:12:12 CEST DEBUG2 setNodeLastEvent: no_id=1 event_seq=629312
2007-05-04 17:12:12 CEST CONFIG storePath: pa_server=1 pa_client=2
pa_conninfo="dbname=gforge host=xxx user=slonik" pa_connretry=10
2007-05-04 17:12:12 CEST CONFIG storeListen: li_origin=1 li_receiver=2
li_provider=1
2007-05-04 17:12:12 CEST CONFIG storeSet: set_id=1 set_origin=1 set_comment='All
gforge tables'
2007-05-04 17:12:12 CEST WARN   remoteWorker_wakeup: node 1 - no worker thread
2007-05-04 17:12:12 CEST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + worker
signaled)
2007-05-04 17:12:12 CEST CONFIG storeSubscribe: sub_set=1 sub_provider=1
sub_forward='f'
2007-05-04 17:12:12 CEST WARN   remoteWorker_wakeup: node 1 - no worker thread
2007-05-04 17:12:12 CEST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + worker
signaled)
2007-05-04 17:12:12 CEST CONFIG enableSubscription: sub_set=1
2007-05-04 17:12:12 CEST WARN   remoteWorker_wakeup: node 1 - no worker thread
2007-05-04 17:12:12 CEST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + worker
signaled)
2007-05-04 17:12:13 CEST DEBUG2 main: last local event sequence = 713075
2007-05-04 17:12:13 CEST CONFIG main: configuration complete - starting threads
2007-05-04 17:12:13 CEST DEBUG1 localListenThread: thread starts
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=22155
2007-05-04 17:12:13 CEST CONFIG enableNode: no_id=1
2007-05-04 17:12:13 CEST DEBUG1 main: running scheduler mainloop
2007-05-04 17:12:13 CEST DEBUG1 remoteWorkerThread_1: thread starts
2007-05-04 17:12:13 CEST DEBUG1 remoteListenThread_1: thread starts
2007-05-04 17:12:13 CEST DEBUG2 remoteListenThread_1: start listening for event
origin 1
2007-05-04 17:12:13 CEST DEBUG1 cleanupThread: thread starts
2007-05-04 17:12:13 CEST DEBUG1 syncThread: thread starts
2007-05-04 17:12:13 CEST DEBUG1 remoteWorkerThread_1: helper thread for provider
1 created
2007-05-04 17:12:13 CEST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 713076
2007-05-04 17:12:13 CEST DEBUG1 remoteListenThread_1: connected to
'dbname=gforge host=xxx user=slonik'
2007-05-04 17:12:13 CEST DEBUG2 localListenThread: Received event 2,713076 SYNC
2007-05-04 17:12:23 CEST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 713077
2007-05-04 17:12:24 CEST DEBUG2 localListenThread: Received event 2,713077 SYNC
2007-05-04 17:12:33 CEST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 713078
2007-05-04 17:12:33 CEST DEBUG2 localListenThread: Received event 2,713078 SYNC
2007-05-04 17:12:41 CEST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 713079
2007-05-04 17:12:42 CEST DEBUG2 localListenThread: Received event 2,713079 SYNC
2007-05-04 17:12:49 CEST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 713080
2007-05-04 17:12:50 CEST DEBUG2 localListenThread: Received event 2,713080 SYNC
2007-05-04 17:12:58 CEST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 713081
2007-05-04 17:12:58 CEST DEBUG2 localListenThread: Received event 2,713081 SYNC
2007-05-04 17:13:06 CEST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 713082
2007-05-04 17:13:07 CEST DEBUG2 localListenThread: Received event 2,713082 SYNC
2007-05-04 17:13:14 CEST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 713083
[...]

Any idea ?

Thanks,
Alain.

Selon Christopher Browne <cbbrowne@ca.afilias.info>:

> Alain Peyrat <aljeux@free.fr> writes:
> > Hi,
> >
> > I'm running a simple replication between a master and a slave. The
> > slave is planned as a switchover host in case of big troubles on
> > the server (ie: slave can be stopped but not the master).
> >
> > My environnment:
> >
> > Postgres: 7.4
> > Slony: 1.2.6
> >
> > It seems that my replication is no more active and the tables
> > sl_log_1/2 are now very big (sl_log_1 is around 6GB and 3.3e+06
> > tuples). Value st_lag_time is very important.
> >
> > I have tried to restart the slon (master and slave) but the
> > st_last_received_event_ts is not changing (or only once for 3 sec).
> >
> > On the master slon log, I have (1 is master, 2 is slave).
> >
> > 2007-05-04 15:54:31 CEST DEBUG2 remoteListenThread_2: queue event 2,712628
> SYNC
> > 2007-05-04 15:54:31 CEST DEBUG2 remoteListenThread_2: UNLISTEN
> > 2007-05-04 15:54:31 CEST DEBUG2 remoteWorkerThread_2: Received event
> 2,712628
> > SYNC
> > 2007-05-04 15:54:31 CEST DEBUG3 calc sync size - last time: 1 last length:
> 10166
> > ideal: 5 proposed size: 3
> > 2007-05-04 15:54:31 CEST DEBUG2 remoteWorkerThread_2: SYNC 712628
> processing
> > 2007-05-04 15:54:31 CEST DEBUG2 remoteWorkerThread_2: no sets need syncing
> for
> > this event
> > 2007-05-04 15:54:32 CEST DEBUG2 localListenThread: Received event 1,1553949
> SYNC
> > 2007-05-04 15:54:32 CEST DEBUG2 syncThread: new sl_action_seq 6356493 -
> SYNC
> > 1553950
> >
> > So, I'm seeing line with 'no sets need syncing for this event'.
> >
> > Can someone help me understanding what's wrong ?
>
> You may find it useful to look at the following page for explanations for log
> entries:
>
> <http://slony.info/adminguide/slony1-1.2.6/doc/adminguide/loganalysis.html>
>
> The interesting log is not the one for the origin node; you can expect
> it to be mighty sparse, and when that node receives events from other
> nodes, it is very much to be expected that it will indicate "no sets
> need syncing for this event."
>
> More interesting will be the contents of the log file for the
> subscriber node; THAT will show a lot more action, and show log
> entries describing the work being done to pull and apply updates from
> the origin.
>
> But none of this is showing off anything that would be considered a
> "problem," and you won't see anything in the logs that will make it
> evident why you can't get the lag time down.
>
> The relevant slon option is "sync_interval_timeout", which is either
> controlled via that name in a "slon.conf" file, or via the command
> line "-s" option, which determines how long the slon sleeps between
> checking to see if there have been updates made that would warrant
> generating a SYNC event.
>
> I believe that the default for that is 10000, which represents 10s.
> If you reduced the value, that would cause SYNC events to be generated
> more frequently, which I would expect to reduce st_lag_time for you.
> --
> "cbbrowne","@","ca.afilias.info"
> <http://dba2.int.libertyrms.com/>
> Christopher Browne
> (416) 673-4124 (land)
>


From aljeux at free.fr  Fri May  4 11:38:15 2007
From: aljeux at free.fr (AlJeux)
Date: Fri May  4 11:38:34 2007
Subject: [Slony1-general] How to reduce st_lag_time ?
In-Reply-To: <1178291767.463b4e3722973@imp.free.fr>
References: <1178287883.463b3f0b765e6@imp.free.fr>	<60ps5gbph8.fsf@dba2.int.libertyrms.com>
	<1178291767.463b4e3722973@imp.free.fr>
Message-ID: <463B7D97.4080905@free.fr>

In fact, after setting this value to the slave and the master, the 
synchronisation has restarted.

Thank you again for your help.
Alain.

Alain Peyrat a ?crit :
> Hi,
> 
> I tried to change the default value to 100ms to test, but this has no impact on
> the lag, it is still increasing. The st_last_received_event_ts does not change.
> 
> Here is the logs on the slave host (-d 3):
> 
> 2007-05-04 17:12:12 CEST CONFIG main: slon version 1.2.6 starting up
> 2007-05-04 17:12:12 CEST DEBUG2 slon: watchdog process started
> 2007-05-04 17:12:12 CEST DEBUG2 slon: watchdog ready - pid = 22165
> 2007-05-04 17:12:12 CEST DEBUG2 slon: worker process created - pid = 22166
> 2007-05-04 17:12:12 CEST CONFIG main: local node id = 2
> 2007-05-04 17:12:12 CEST DEBUG2 main: main process started
> 2007-05-04 17:12:12 CEST CONFIG main: launching sched_start_mainloop
> 2007-05-04 17:12:12 CEST CONFIG main: loading current cluster configuration
> 2007-05-04 17:12:12 CEST CONFIG storeNode: no_id=1 no_comment='Node 1'
> 2007-05-04 17:12:12 CEST DEBUG2 setNodeLastEvent: no_id=1 event_seq=629312
> 2007-05-04 17:12:12 CEST CONFIG storePath: pa_server=1 pa_client=2
> pa_conninfo="dbname=gforge host=xxx user=slonik" pa_connretry=10
> 2007-05-04 17:12:12 CEST CONFIG storeListen: li_origin=1 li_receiver=2
> li_provider=1
> 2007-05-04 17:12:12 CEST CONFIG storeSet: set_id=1 set_origin=1 set_comment='All
> gforge tables'
> 2007-05-04 17:12:12 CEST WARN   remoteWorker_wakeup: node 1 - no worker thread
> 2007-05-04 17:12:12 CEST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + worker
> signaled)
> 2007-05-04 17:12:12 CEST CONFIG storeSubscribe: sub_set=1 sub_provider=1
> sub_forward='f'
> 2007-05-04 17:12:12 CEST WARN   remoteWorker_wakeup: node 1 - no worker thread
> 2007-05-04 17:12:12 CEST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + worker
> signaled)
> 2007-05-04 17:12:12 CEST CONFIG enableSubscription: sub_set=1
> 2007-05-04 17:12:12 CEST WARN   remoteWorker_wakeup: node 1 - no worker thread
> 2007-05-04 17:12:12 CEST DEBUG2 sched_wakeup_node(): no_id=1 (0 threads + worker
> signaled)
> 2007-05-04 17:12:13 CEST DEBUG2 main: last local event sequence = 713075
> 2007-05-04 17:12:13 CEST CONFIG main: configuration complete - starting threads
> 2007-05-04 17:12:13 CEST DEBUG1 localListenThread: thread starts
> NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=22155
> 2007-05-04 17:12:13 CEST CONFIG enableNode: no_id=1
> 2007-05-04 17:12:13 CEST DEBUG1 main: running scheduler mainloop
> 2007-05-04 17:12:13 CEST DEBUG1 remoteWorkerThread_1: thread starts
> 2007-05-04 17:12:13 CEST DEBUG1 remoteListenThread_1: thread starts
> 2007-05-04 17:12:13 CEST DEBUG2 remoteListenThread_1: start listening for event
> origin 1
> 2007-05-04 17:12:13 CEST DEBUG1 cleanupThread: thread starts
> 2007-05-04 17:12:13 CEST DEBUG1 syncThread: thread starts
> 2007-05-04 17:12:13 CEST DEBUG1 remoteWorkerThread_1: helper thread for provider
> 1 created
> 2007-05-04 17:12:13 CEST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 713076
> 2007-05-04 17:12:13 CEST DEBUG1 remoteListenThread_1: connected to
> 'dbname=gforge host=xxx user=slonik'
> 2007-05-04 17:12:13 CEST DEBUG2 localListenThread: Received event 2,713076 SYNC
> 2007-05-04 17:12:23 CEST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 713077
> 2007-05-04 17:12:24 CEST DEBUG2 localListenThread: Received event 2,713077 SYNC
> 2007-05-04 17:12:33 CEST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 713078
> 2007-05-04 17:12:33 CEST DEBUG2 localListenThread: Received event 2,713078 SYNC
> 2007-05-04 17:12:41 CEST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 713079
> 2007-05-04 17:12:42 CEST DEBUG2 localListenThread: Received event 2,713079 SYNC
> 2007-05-04 17:12:49 CEST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 713080
> 2007-05-04 17:12:50 CEST DEBUG2 localListenThread: Received event 2,713080 SYNC
> 2007-05-04 17:12:58 CEST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 713081
> 2007-05-04 17:12:58 CEST DEBUG2 localListenThread: Received event 2,713081 SYNC
> 2007-05-04 17:13:06 CEST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 713082
> 2007-05-04 17:13:07 CEST DEBUG2 localListenThread: Received event 2,713082 SYNC
> 2007-05-04 17:13:14 CEST DEBUG2 syncThread: new sl_action_seq 1 - SYNC 713083
> [...]
> 
> Any idea ?
> 
> Thanks,
> Alain.
> 
> Selon Christopher Browne <cbbrowne@ca.afilias.info>:
> 
>> Alain Peyrat <aljeux@free.fr> writes:
>>> Hi,
>>>
>>> I'm running a simple replication between a master and a slave. The
>>> slave is planned as a switchover host in case of big troubles on
>>> the server (ie: slave can be stopped but not the master).
>>>
>>> My environnment:
>>>
>>> Postgres: 7.4
>>> Slony: 1.2.6
>>>
>>> It seems that my replication is no more active and the tables
>>> sl_log_1/2 are now very big (sl_log_1 is around 6GB and 3.3e+06
>>> tuples). Value st_lag_time is very important.
>>>
>>> I have tried to restart the slon (master and slave) but the
>>> st_last_received_event_ts is not changing (or only once for 3 sec).
>>>
>>> On the master slon log, I have (1 is master, 2 is slave).
>>>
>>> 2007-05-04 15:54:31 CEST DEBUG2 remoteListenThread_2: queue event 2,712628
>> SYNC
>>> 2007-05-04 15:54:31 CEST DEBUG2 remoteListenThread_2: UNLISTEN
>>> 2007-05-04 15:54:31 CEST DEBUG2 remoteWorkerThread_2: Received event
>> 2,712628
>>> SYNC
>>> 2007-05-04 15:54:31 CEST DEBUG3 calc sync size - last time: 1 last length:
>> 10166
>>> ideal: 5 proposed size: 3
>>> 2007-05-04 15:54:31 CEST DEBUG2 remoteWorkerThread_2: SYNC 712628
>> processing
>>> 2007-05-04 15:54:31 CEST DEBUG2 remoteWorkerThread_2: no sets need syncing
>> for
>>> this event
>>> 2007-05-04 15:54:32 CEST DEBUG2 localListenThread: Received event 1,1553949
>> SYNC
>>> 2007-05-04 15:54:32 CEST DEBUG2 syncThread: new sl_action_seq 6356493 -
>> SYNC
>>> 1553950
>>>
>>> So, I'm seeing line with 'no sets need syncing for this event'.
>>>
>>> Can someone help me understanding what's wrong ?
>> You may find it useful to look at the following page for explanations for log
>> entries:
>>
>> <http://slony.info/adminguide/slony1-1.2.6/doc/adminguide/loganalysis.html>
>>
>> The interesting log is not the one for the origin node; you can expect
>> it to be mighty sparse, and when that node receives events from other
>> nodes, it is very much to be expected that it will indicate "no sets
>> need syncing for this event."
>>
>> More interesting will be the contents of the log file for the
>> subscriber node; THAT will show a lot more action, and show log
>> entries describing the work being done to pull and apply updates from
>> the origin.
>>
>> But none of this is showing off anything that would be considered a
>> "problem," and you won't see anything in the logs that will make it
>> evident why you can't get the lag time down.
>>
>> The relevant slon option is "sync_interval_timeout", which is either
>> controlled via that name in a "slon.conf" file, or via the command
>> line "-s" option, which determines how long the slon sleeps between
>> checking to see if there have been updates made that would warrant
>> generating a SYNC event.
>>
>> I believe that the default for that is 10000, which represents 10s.
>> If you reduced the value, that would cause SYNC events to be generated
>> more frequently, which I would expect to reduce st_lag_time for you.
>> --
>> "cbbrowne","@","ca.afilias.info"
>> <http://dba2.int.libertyrms.com/>
>> Christopher Browne
>> (416) 673-4124 (land)
>>
> 
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

From drees76 at gmail.com  Fri May  4 12:00:45 2007
From: drees76 at gmail.com (David Rees)
Date: Fri May  4 12:01:05 2007
Subject: [Slony1-general] Re: Using monit to keep slony running
In-Reply-To: <810a540e0705040456r3084818l63dd3cc2dd641836@mail.gmail.com>
References: <810a540e0705040423u3411454er671d5e899daf1994@mail.gmail.com>
	<810a540e0705040456r3084818l63dd3cc2dd641836@mail.gmail.com>
Message-ID: <72dbd3150705041200l222fe564r238ba4b4a0624896@mail.gmail.com>

On 5/4/07, Pat Maddox <pergesu@gmail.com> wrote:
> My only other question now is can I be sure that slony is still
> replicating properly as long as it's running?  The docs say that older
> versions were very fragile and would crash if they encountered any
> problem.  That'd be fine with my setup because I'm using monit, it'd
> just get restarted.  In 1.2 though it's supposedly a lot less
> fragile...but under what conditions will the daemon run but not be
> operating properly?  Can that happen at all?  Assume that the
> configuration is okay (i.e. nothing boneheaded like forgetting to
> subscribe a set).

I would suggest using something like the MRTG monitoring script as
described here:

http://slony.info/adminguide/slony1-1.2.6/doc/adminguide/monitoring.html#SLONYMRTG

When the lag gets too big, generate an alert to check on replication.

There are a number of other possible solutions described as well.

I recently had a case where I had to restart the replicate DB to clear
up a replication error, so not always restarting slon will fix things
(BTW, if anyone is interested in that I can dig up the logs and try to
figure out what happened in a new thread).

-Dave
From drees76 at gmail.com  Fri May  4 12:07:58 2007
From: drees76 at gmail.com (David Rees)
Date: Fri May  4 12:08:16 2007
Subject: [Slony1-general] How to reduce st_lag_time ?
In-Reply-To: <60ps5gbph8.fsf@dba2.int.libertyrms.com>
References: <1178287883.463b3f0b765e6@imp.free.fr>
	<60ps5gbph8.fsf@dba2.int.libertyrms.com>
Message-ID: <72dbd3150705041207w63cca609o98f976395e5dca9b@mail.gmail.com>

On 5/4/07, Christopher Browne <cbbrowne@ca.afilias.info> wrote:
> The relevant slon option is "sync_interval_timeout", which is either
> controlled via that name in a "slon.conf" file, or via the command
> line "-s" option, which determines how long the slon sleeps between
> checking to see if there have been updates made that would warrant
> generating a SYNC event.
>
> I believe that the default for that is 10000, which represents 10s.
> If you reduced the value, that would cause SYNC events to be generated
> more frequently, which I would expect to reduce st_lag_time for you.

How much overhead does significantly reducing the sync interval pose
on the servers? What values are people having good luck with for their
use?

BTW, maybe I'm reading the docs wrong, but would sync_interval be more
appropriate to set low than sync_interval_timeout? Maybe I'm missing
something?

-Dave
From lavalamp at spiritual-machines.org  Wed May  9 13:08:41 2007
From: lavalamp at spiritual-machines.org (Brian A. Seklecki)
Date: Wed May  9 13:09:06 2007
Subject: [Slony1-general] Re: Assuming node id = 1 (STORE NODE, etc.)
In-Reply-To: <20070327170402.C6132@arbitor.digitalfreaks.org>
References: <20070327170402.C6132@arbitor.digitalfreaks.org>
Message-ID: <20070509160752.D2394@arbitor.digitalfreaks.org>


This bug looks fixed in 1.3.x code from CVS/SVN; just be sure to to 
explicitly declare EVENT NODE in STORE NODE.

Thank you all.

~BAS


On Tue, 27 Mar 2007, Brian A. Seklecki wrote:

> In Two places:
>
> line 334 src/slonik/slonik.c:
>
>                        case STMT_STORE_NODE:
>                                {
>                                        SlonikStmt_store_node *stmt =
>                                        (SlonikStmt_store_node *) hdr;
>
>                                        if (stmt->ev_origin < 0)
>                                        {
>                                                stmt->ev_origin = 1;
>                                        }
>
> and line 611 src/slonik/parser.y:
>
> stmt_store_node         : lno K_STORE K_NODE option_list
>                                        {
>                                 SlonikStmt_store_node *new;
>                    statement_option opt[] = {
>                                    STMT_OPTION_INT( O_ID, -1 ),
>                                    STMT_OPTION_STR( O_COMMENT, NULL ),
>                                    STMT_OPTION_YN( O_SPOOLNODE, 0 ),
>                                    STMT_OPTION_INT( O_EVENT_NODE, 1 ),
>                                    STMT_OPTION_END
>                                 };
>
>
>
> This works for 99.9% of situations, but it may be more pragmatic to use the 
> node ID of the least number value instead of static "1".  E.g., default in 
> the .y to "-1", and in the .c, to check for "-1" or ...undefined in conf = 
> default (-1), thus check some algorithm getLowestDefinedNodeID();
>
> The work-around for this is to explicitly declare:
> ... "event node = [some_node_id_other_than_1_here]"
>
> Which is also really inconsistent because in many other commands this would 
> be referred to as "node id"; that's a separate issue.
>
> Is this feature request/ticket worthy?  Also, are we presently using gborg or 
> pgfoundry for issue tracking?
>
> TIA, ~BAS
>
> --
>
>
> l8*
> 	-lava (Brian A. Seklecki - Pittsburgh, PA, USA)
> 	       http://www.spiritual-machines.org/
>
> "...from back in the heady days when "helpdesk" meant nothing, "diskquota"
> meant everything, and lives could be bought and sold for a couple of pages
> of laser printout - and frequently were."
>

l8*
 	-lava (Brian A. Seklecki - Pittsburgh, PA, USA)
 	       http://www.spiritual-machines.org/

     "Guilty? Yeah. But he knows it. I mean, you're guilty.
     You just don't know it. So who's really in jail?"
     ~James Maynard Keenan

From dmitry.koterov at gmail.com  Fri May 11 15:47:14 2007
From: dmitry.koterov at gmail.com (Dmitry Koterov)
Date: Fri May 11 15:47:24 2007
Subject: [Slony1-general] How to reduce the time data appears in the slave?
Message-ID: <d7df81620705111547h358ff7d0s1c37a2443718d239@mail.gmail.com>

Hello.

I read that Slony uses LISTEN/NOTIFY pgsql features to implement immediate
spreading of all the master changes to subsribers:
http://www.postgresql.org/docs/8.2/interactive/sql-notify.html
But seems information is spreaded not so quickly as I expect. E.g., when I
post something to the master, changes may appear on slave with 2-3 seconds
delay.

I could decrease sync_interval to 100 ms (for example) and it helps, BUT in
this case slon daemon from the slave begins to bomb the master with the
query:

select last_value from "_moikrug_cluster".sl_log_status
(10 queries per seconds).

It is not good for master's CPU to process such large number of queries (if
I have 10 slaves - 100 queries/s, aargh!).

So, the question is: how to make the Slony to update slaves as fast as
possible without bombing the master with periodical queries? And - what is
the detailed meaning of sync_interval?

P.S.
I tried to watch postgres logs and noticed that slave runs LISTEN/UNLISTEN
periodically, but the number of UNLISTEN is greater than the number of
LISTEN. So - maybe my configuration is simply broken, and slon does not
listen anything at all? Please say what debug information should I post
here.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070512/=
bf3627d4/attachment.htm
From dmitry at koterov.ru  Fri May 11 15:50:12 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Fri May 11 15:50:25 2007
Subject: [Slony1-general] How to reduce the time data appears in the slave?
In-Reply-To: <d7df81620705111547h358ff7d0s1c37a2443718d239@mail.gmail.com>
References: <d7df81620705111547h358ff7d0s1c37a2443718d239@mail.gmail.com>
Message-ID: <d7df81620705111550s1adea9a1wd8bf797d8625ca6f@mail.gmail.com>

Hello.

I read that Slony uses LISTEN/NOTIFY pgsql features to implement immediate
spreading of all the master changes to subsribers:
http://www.postgresql.org/docs/8.2/interactive/sql-notify.html
But seems information is spreaded not so quickly as I expect. E.g., when I
post something to the master, changes may appear on slave with 2-3 seconds
delay.

I could decrease sync_interval to 100 ms (for example) and it helps, BUT in
this case slon daemon from the slave begins to bomb the master with the
query:

select last_value from "_moikrug_cluster".sl_log_status
(10 queries per seconds).

It is not good for master's CPU to process such large number of queries (if
I have 10 slaves - 100 queries/s, aargh!).

So, the question is: how to make the Slony to update slaves as fast as
possible without bombing the master with periodical queries? And - what is
the detailed meaning of sync_interval?

P.S.
I tried to watch postgres logs and noticed that slave runs LISTEN/UNLISTEN
periodically, but the number of UNLISTEN is greater than the number of
LISTEN. So - maybe my configuration is simply broken, and slon does not
listen anything at all? Please say what debug information should I post
here.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070512/=
1c90cb31/attachment.htm
From ajs at crankycanuck.ca  Fri May 11 16:09:42 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri May 11 16:10:06 2007
Subject: [Slony1-general] How to reduce the time data appears in the slave?
In-Reply-To: <d7df81620705111547h358ff7d0s1c37a2443718d239@mail.gmail.com>
References: <d7df81620705111547h358ff7d0s1c37a2443718d239@mail.gmail.com>
Message-ID: <20070511230942.GC20618@phlogiston.dyndns.org>

On Sat, May 12, 2007 at 02:47:14AM +0400, Dmitry Koterov wrote:
> So, the question is: how to make the Slony to update slaves as fast as
> possible without bombing the master with periodical queries? And - what is
> the detailed meaning of sync_interval?

There isn't a way to update the slaves as fast as possible without
the queries, although more recent code reduces the LISTEN/NOTIFY
dependency.  But this is _async_ replication.  If you want perfect
copies, it can't do what you want.  It's always going to lag some.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
Everything that happens in the world happens at some place.
		--Jane Jacobs 
From jason at buberel.org  Fri May 11 22:54:25 2007
From: jason at buberel.org (Jason L. Buberel)
Date: Fri May 11 22:54:39 2007
Subject: [Slony1-general] after upgrade to 1.2.9: FATAL syncThread error
Message-ID: <46455691.9080801@buberel.org>

I am running a slony cluster against three built-from-src postgres-8.1.4 =

servers. I had been running 1.2.6 successfully.

After installing the new slony, I then restarted each of the databases =

to make sure they picked up the new libs.

But after issuing the updgrade commands:

    cluster name =3D $CLUSTERNAME;
    node 1 admin conninfo =3D 'dbname=3D$SRV1DBNAME port=3D$SRV1PORT =

host=3D$SRV1HOST user=3D$REPLICATIONUSER';
    node 2 admin conninfo =3D 'dbname=3D$SRV2DBNAME port=3D$SRV2PORT =

host=3D$SRV2HOST user=3D$REPLICATIONUSER';
    node 3 admin conninfo =3D 'dbname=3D$SRV3DBNAME port=3D$SRV3PORT =

host=3D$SRV3HOST user=3D$REPLICATIONUSER';

    update functions (id =3D 1);
    update functions (id =3D 2);
    update functions (id =3D 3);

    restart node 1;
    restart node 2;
    restart node 3;

Then restarting a slon daemon on node 3, I see the following fatal error:

May 12 00:41:47 srv3 slon[8573]: [42-1] 2007-05-12 00:41:47 CDT CONFIG =

main: configuration complete - starting threads
May 12 00:41:47 srv3 postgres[8587]: [64-1] NOTICE:  Slony-I: cleanup =

stale sl_nodelock entry for pid=3D8440
May 12 00:41:47 srv3 slon[8573]: [44-1] 2007-05-12 00:41:47 CDT CONFIG =

enableNode: no_id=3D1
May 12 00:41:48 srv3 slon[8573]: [47-1] 2007-05-12 00:41:47 CDT CONFIG =

enableNode: no_id=3D2
May 12 00:41:50 srv3 slon[8573]: [54-1] 2007-05-12 00:41:50 CDT FATAL  =

syncThread: "select =

"_srv1_srv2_cluster".createEvent('_srv1_srv2_cluster', 'SYNC', NULL);" -
May 12 00:41:50 srv3 slon[8573]: [54-2]  server closed the connection =

unexpectedly
May 12 00:41:50 srv3 slon[8573]: [54-3]         This probably means the =

server terminated abnormally
May 12 00:41:50 srv3 slon[8573]: [54-4]         before or while =

processing the request.
May 12 00:41:50 srv3 slon[8573]: [58-1] 2007-05-12 00:41:50 CDT INFO   =

remoteListenThread_1: disconnecting from 'dbname=3Daltos_research port=3D54=
33
May 12 00:41:50 srv3 slon[8573]: [58-2]  host=3Dsrv1.altosresearch.com =

user=3Dpostgres'
May 12 00:41:50 srv3 postgres[7320]: [64-1] LOG:  server process (PID =

8597) was terminated by signal 11
May 12 00:41:50 srv3 postgres[7320]: [65-1] LOG:  terminating any other =

active server processes

What is the most likely cause of this error?

Thanks,
Jason

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070511/=
5d475a7d/attachment.htm
From ajs at crankycanuck.ca  Sat May 12 05:33:02 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Sat May 12 05:33:22 2007
Subject: [Slony1-general] after upgrade to 1.2.9: FATAL syncThread error
In-Reply-To: <46455691.9080801@buberel.org>
References: <46455691.9080801@buberel.org>
Message-ID: <20070512123302.GG24306@phlogiston.dyndns.org>

On Fri, May 11, 2007 at 10:54:25PM -0700, Jason L. Buberel wrote:
> "_srv1_srv2_cluster".createEvent('_srv1_srv2_cluster', 'SYNC', NULL);" -
> May 12 00:41:50 srv3 slon[8573]: [54-2]  server closed the connection 
> unexpectedly
> May 12 00:41:50 srv3 slon[8573]: [54-3]         This probably means the 
> server terminated abnormally
> May 12 00:41:50 srv3 slon[8573]: [54-4]         before or while 
> processing the request.

> What is the most likely cause of this error?

It may not be an error on Slony's part.  Slony seems to think the
back end crashed.  What do your postgres logs say?

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
A certain description of men are for getting out of debt, yet are
against all taxes for raising money to pay it off.
		--Alexander Hamilton
From jason at buberel.org  Sat May 12 06:42:55 2007
From: jason at buberel.org (Jason L. Buberel)
Date: Sat May 12 06:43:08 2007
Subject: [Slony1-general] after upgrade to 1.2.9: FATAL syncThread error
In-Reply-To: <20070512123302.GG24306@phlogiston.dyndns.org>
References: <46455691.9080801@buberel.org>
	<20070512123302.GG24306@phlogiston.dyndns.org>
Message-ID: <4645C45F.9000409@buberel.org>

Andrew,

[note: I love that domain name on your email address, btw]

Not much, honestly. Just prior to invoking slony, I see this (after an =

earlier slony-induced restart, after which I did confirm that I could =

connect via psql and execute queries normally):

May 12 00:42:41 srv3 postgres[8694]: [87-1] LOG:  database system was =

not properly shut down; automatic recovery in progress
May 12 00:42:41 srv3 postgres[8694]: [88-1] LOG:  redo starts at 28/25DB3B6C
May 12 00:42:41 srv3 postgres[8694]: [89-1] LOG:  record with zero =

length at 28/25DB6AE8
May 12 00:42:41 srv3 postgres[8694]: [90-1] LOG:  redo done at 28/25DB6AC0
May 12 00:42:41 srv3 postgres[8694]: [91-1] LOG:  database system is ready
May 12 00:42:41 srv3 postgres[8694]: [92-1] LOG:  transaction ID wrap =

limit is 2147484146, limited by database "postgres"
May 12 00:42:51 srv3 slon[8571]: [1-1] 2007-05-12 00:42:51 CDT CONFIG =

main: slon version 1.2.9 starting up
May 12 00:42:51 srv3 slon[8698]: [2-1] 2007-05-12 00:42:51 CDT CONFIG =

main: local node id =3D 3

[... then, all heck breaks loose, and then the restart ...]

May 12 00:42:53 srv3 slon[8698]: [54-1] 2007-05-12 00:42:53 CDT FATAL  =

syncThread: "select =

"_srv1_srv2_cluster".createEvent('_srv1_srv2_cluster', 'SYNC', NULL);" -
May 12 00:42:53 srv3 slon[8698]: [54-2]  server closed the connection =

unexpectedly
May 12 00:42:53 srv3 slon[8698]: [54-3]         This probably means the =

server terminated abnormally
May 12 00:42:53 srv3 slon[8698]: [54-4]         before or while =

processing the request.
May 12 00:42:53 srv3 postgres[7320]: [82-1] LOG:  server process (PID =

8714) was terminated by signal 11
May 12 00:42:53 srv3 postgres[7320]: [83-1] LOG:  terminating any other =

active server processes
May 12 00:42:53 srv3 postgres[8704]: [83-1] WARNING:  terminating =

connection because of crash of another server process
May 12 00:42:53 srv3 postgres[8704]: [83-2] DETAIL:  The postmaster has =

commanded this server process to roll back the current transaction and
exit, because another server
May 12 00:42:53 srv3 postgres[8704]: [83-3]  process exited abnormally =

and possibly corrupted shared memory.
May 12 00:42:53 srv3 postgres[8704]: [83-4] HINT:  In a moment you =

should be able to reconnect to the database and repeat your command.
May 12 00:42:53 srv3 postgres[8713]: [82-1] WARNING:  terminating =

connection because of crash of another server process
May 12 00:42:53 srv3 slon[8698]: [56-1] 2007-05-12 00:42:53 CDT INFO   =

remoteListenThread_1: disconnecting from 'dbname=3Daltos_research port=3D54=
33
May 12 00:42:53 srv3 postgres[8712]: [82-1] WARNING:  terminating =

connection because of crash of another server process
May 12 00:42:53 srv3 postgres[8713]: [82-2] DETAIL:  The postmaster has =

commanded this server process to roll back the current transaction and
exit, because another server
May 12 00:42:53 srv3 postgres[8713]: [82-3]  process exited abnormally =

and possibly corrupted shared memory.
May 12 00:42:53 srv3 postgres[8710]: [82-1] WARNING:  terminating =

connection because of crash of another server process
May 12 00:42:53 srv3 postgres[8712]: [82-2] DETAIL:  The postmaster has =

commanded this server process to roll back the current transaction and
exit, because another server
May 12 00:42:53 srv3 postgres[8713]: [82-4] HINT:  In a moment you =

should be able to reconnect to the database and repeat your command.
May 12 00:42:53 srv3 slon[8698]: [56-2]  host=3Dsrv1.altosresearch.com =

user=3Dpostgres'
May 12 00:42:53 srv3 postgres[8710]: [82-2] DETAIL:  The postmaster has =

commanded this server process to roll back the current transaction and
exit, because another server
May 12 00:42:53 srv3 postgres[8712]: [82-3]  process exited abnormally =

and possibly corrupted shared memory.
May 12 00:42:53 srv3 postgres[8710]: [82-3]  process exited abnormally =

and possibly corrupted shared memory.
May 12 00:42:53 srv3 postgres[8712]: [82-4] HINT:  In a moment you =

should be able to reconnect to the database and repeat your command.
May 12 00:42:53 srv3 postgres[8710]: [82-4] HINT:  In a moment you =

should be able to reconnect to the database and repeat your command.
May 12 00:42:53 srv3 postgres[7320]: [84-1] LOG:  all server processes =

terminated; reinitializing
May 12 00:42:53 srv3 postgres[8715]: [85-1] LOG:  database system was =

interrupted at 2007-05-12 00:42:41 CDT


Andrew Sullivan wrote:
> It may not be an error on Slony's part.  Slony seems to think the
> back end crashed.  What do your postgres logs say?
>   =

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070512/=
413dd044/attachment-0001.htm
From dmitry at koterov.ru  Sat May 12 06:52:36 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Sat May 12 06:52:40 2007
Subject: [Slony1-general] How to reduce the time data appears in the slave?
In-Reply-To: <20070511230942.GC20618@phlogiston.dyndns.org>
References: <d7df81620705111547h358ff7d0s1c37a2443718d239@mail.gmail.com>
	<20070511230942.GC20618@phlogiston.dyndns.org>
Message-ID: <d7df81620705120652p43f94b27wca0b8372e868aa38@mail.gmail.com>

Hello.

But please, detalize how Slony uses LISTEN/NOTIFY and why, if it uses
LISTEN/NOTIFY, it still generates poll queries instead of sitting and
waiting for a NOTIFY event. I cannot find this information in the Slony
documentation.

My main goal is NOT to reduce the lag to zero, but - reduce it as much as
possible without bombing the master by polling queries. E.g. lag 0.2s is
allowed, but 5 polling queries per second is too much.

On 5/12/07, Andrew Sullivan <ajs@crankycanuck.ca> wrote:
>
> On Sat, May 12, 2007 at 02:47:14AM +0400, Dmitry Koterov wrote:
> > So, the question is: how to make the Slony to update slaves as fast as
> > possible without bombing the master with periodical queries? And - what
> is
> > the detailed meaning of sync_interval?
>
> There isn't a way to update the slaves as fast as possible without
> the queries, although more recent code reduces the LISTEN/NOTIFY
> dependency.  But this is _async_ replication.  If you want perfect
> copies, it can't do what you want.  It's always going to lag some.
>
> A
>
> --
> Andrew Sullivan  | ajs@crankycanuck.ca
> Everything that happens in the world happens at some place.
>                 --Jane Jacobs
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070512/=
d20d0f34/attachment.htm
From jason at buberel.org  Sat May 12 15:03:18 2007
From: jason at buberel.org (Jason L. Buberel)
Date: Sat May 12 15:03:32 2007
Subject: [Slony1-general] Re: after upgrade to 1.2.9: FATAL syncThread error
In-Reply-To: <20070512194959.GC25221@phlogiston.dyndns.org>
References: <46455691.9080801@buberel.org>
	<20070512123302.GG24306@phlogiston.dyndns.org>
	<4645C351.6020300@buberel.org>
	<20070512194959.GC25221@phlogiston.dyndns.org>
Message-ID: <464639A6.7090508@buberel.org>

Interestingly: Another cluster configuration from an 8.1.4 -> 8.2.4 =

server seems to be working just fine. The main difference is that this =

second cluster configuration was created/installed from scratch.

My plan now is simply to drop the slon schema on all nodes and recreate =

the whole thing.

I'll let you know how that goes. If you have any other suggestions on =

how to get to the root cause of the problem described below I would be =

happy to try them out.

Regards
Jason

Andrew Sullivan wrote:
> On Sat, May 12, 2007 at 06:38:25AM -0700, Jason L. Buberel wrote:
>   =

>> May 12 00:42:53 srv3 slon[8698]: [54-1] 2007-05-12 00:42:53 CDT FATAL  =

>> syncThread: "select =

>> "_srv1_srv2_cluster".createEvent('_srv1_srv2_cluster', 'SYNC', NULL);" -
>>     =

>
> It looks like the createEvent() function is blowing up. . . =

>
>   =

>> May 12 00:42:53 srv3 slon[8698]: [54-2]  server closed the connection =

>> unexpectedly
>> May 12 00:42:53 srv3 slon[8698]: [54-3]         This probably means the =

>> server terminated abnormally
>> May 12 00:42:53 srv3 slon[8698]: [54-4]         before or while =

>> processing the request.
>> May 12 00:42:53 srv3 postgres[7320]: [82-1] LOG:  server process (PID =

>> 8714) was terminated by signal 11
>>     =

>
> with sig 11.  You could look at the corefile that oughta be dumped. =

> But my guess is that the slony functions haven't been compiled
> correctly here.
>
> A
>
>   =

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070512/=
ec4555b6/attachment.htm
From ajs at crankycanuck.ca  Sun May 13 11:32:45 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Sun May 13 11:33:13 2007
Subject: [Slony1-general] How to reduce the time data appears in the slave?
In-Reply-To: <d7df81620705120652p43f94b27wca0b8372e868aa38@mail.gmail.com>
References: <d7df81620705111547h358ff7d0s1c37a2443718d239@mail.gmail.com>
	<20070511230942.GC20618@phlogiston.dyndns.org>
	<d7df81620705120652p43f94b27wca0b8372e868aa38@mail.gmail.com>
Message-ID: <20070513183245.GA27803@phlogiston.dyndns.org>

On Sat, May 12, 2007 at 05:52:36PM +0400, Dmitry Koterov wrote:
> But please, detalize how Slony uses LISTEN/NOTIFY and why, if it uses
> LISTEN/NOTIFY, it still generates poll queries instead of sitting and
> waiting for a NOTIFY event. I cannot find this information in the Slony
> documentation.

Which version are you using?  In the past, we found that too much
reliance on LISTEN/NOTIFY tended to cause bloat in pg_listener and a
different set of problems.  So 1.2.x reduced that cost, at the
expense of additional polls.  Those polls should be cheap.  I'm
mostly worried about. . .

> possible without bombing the master by polling queries. E.g. lag 0.2s is
> allowed, but 5 polling queries per second is too much.

. . .this.  Why is it too much?  Anyway, if that's too much, then why
not set the lag to the level you'd find acceptable?

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
This work was visionary and imaginative, and goes to show that visionary
and imaginative work need not end up well. 
		--Dennis Ritchie
From dmitry at koterov.ru  Sun May 13 15:33:45 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Sun May 13 15:33:58 2007
Subject: [Slony1-general] How to reduce the time data appears in the slave?
In-Reply-To: <20070513183245.GA27803@phlogiston.dyndns.org>
References: <d7df81620705111547h358ff7d0s1c37a2443718d239@mail.gmail.com>
	<20070511230942.GC20618@phlogiston.dyndns.org>
	<d7df81620705120652p43f94b27wca0b8372e868aa38@mail.gmail.com>
	<20070513183245.GA27803@phlogiston.dyndns.org>
Message-ID: <d7df81620705131533t4e9607d9iccb3f1e16a42fc26@mail.gmail.com>

Thanks for answers!

> But please, detalize how Slony uses LISTEN/NOTIFY and why, if it uses
> > LISTEN/NOTIFY, it still generates poll queries instead of sitting and
> > waiting for a NOTIFY event. I cannot find this information in the Slony
> > documentation.
>
> Which version are you using?  In the past, we found that too much
> reliance on LISTEN/NOTIFY tended to cause bloat in pg_listener and a
> different set of problems.  So 1.2.x reduced that cost, at the
> expense of additional polls.  Those polls should be cheap.  I'm
> mostly worried about. . .

We use 1.2.4.
Poll is cheap if there is a small number of slaves. But if I have 10 or more
slaves...
By the way, if the poll is used now, what role still have LISTEN/NOTIFY?

> possible without bombing the master by polling queries. E.g. lag 0.2s is
> > allowed, but 5 polling queries per second is too much.
> . . .this.  Why is it too much?  Anyway, if that's too much, then why
> not set the lag to the level you'd find acceptable?

Assume that I set the lag to 1s and have 10 slaves.
So, there are 10 polling queries per second, and the slave updation lag is >
1s (I need 0.2s lag and no more ideally).

All this causes that if I use Slony in the production website, I have to
bind the user's session to the master database for the time of 1-2 seconds
after each user's update. Assume that a user fills some form and posts it to
the script on a server. This script processes the form, adds information to
the database and redirects to the resulting page (in most cases this page
contains some part of added data). After such redirect I cannot use a Slave
for 1-2 seconds because of the updation lag, so I have to use Master
connection. And Master loading grows. The more we reduce ths updation lag,
the less Master is loaded by such user's queries, that's why I want to
decrease the lag as much as possible.

I have the following idea: write 2 stored functions in C (as pg contrib):

- ipc_listen(name): blocks until the signal is received
- ipc_notify(name): sends a signal

They are analogs of LISTEN/NOTIFY, but use standard IPC instead of
pg_listener. Then I:

- create a SELECT RULE and make all the slave readings from sl_log_status
wait for ipc_listen('log_status');
- create a TRIGGER and make all writings to sl_log_status to call
ipc_notify('log_status')

So, if anybody reads from sl_log_status, it blocks until anything is written
to sl_log_status. Seems it will be fully transparent for Slony, but
decreases the amount of polling queries. (It is an idea only, practical
implementation may need to be more detailed.) What do you think about it?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070514/=
3568e379/attachment.htm
From cbbrowne at ca.afilias.info  Sun May 13 18:57:11 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Sun May 13 18:57:23 2007
Subject: [Slony1-general] How to reduce the time data appears in the slave?
In-Reply-To: <d7df81620705131533t4e9607d9iccb3f1e16a42fc26@mail.gmail.com>
	(Dmitry Koterov's message of "Mon, 14 May 2007 02:33:45 +0400")
References: <d7df81620705111547h358ff7d0s1c37a2443718d239@mail.gmail.com>
	<20070511230942.GC20618@phlogiston.dyndns.org>
	<d7df81620705120652p43f94b27wca0b8372e868aa38@mail.gmail.com>
	<20070513183245.GA27803@phlogiston.dyndns.org>
	<d7df81620705131533t4e9607d9iccb3f1e16a42fc26@mail.gmail.com>
Message-ID: <60646wp3pk.fsf@dba2.int.libertyrms.com>

"Dmitry Koterov" <dmitry@koterov.ru> writes:
> So, if anybody reads from sl_log_status, it blocks until anything is
> written to sl_log_status. Seems it will be fully transparent for
> Slony, but decreases the amount of polling queries. (It is an idea
> only, practical implementation may need to be more detailed.) What
> do you think about it?

It smells to me as though you're trying to optimize the wrong problem.

The "pg_listener problem" is one that changes in 1.2 *mostly*
rectifies; the costs associated with NOTIFY/LISTEN should be dropped
to a dull roar in 1.2.  (There are further optimizations in CVS HEAD,
but it shouldn't be *highly* relevant.)

What hasn't been touched, and in a sense, can't be, is to reduce the
load of processing the queries against the "master" node that pull
replication data to feed to the subscribers.

I suspect that it is THOSE queries that are causing you trouble, as
opposed to anything about NOTIFY/LISTEN.

If you have nine nodes all feeding off the master, that's going to put
considerable load on the master node.  And THAT would easily cause
things to slow down on the master, and cause replicas to lag.

The fewer nodes that feed directly from the origin, the less load that
you put on the origin, and the easier it should be able to feed them.
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From bwillits at cox.net  Sun May 13 22:08:13 2007
From: bwillits at cox.net (Bill Willits)
Date: Sun May 13 22:08:59 2007
Subject: [Slony1-general] How to reduce the time data appears in the slave?
References: <d7df81620705111547h358ff7d0s1c37a2443718d239@mail.gmail.com><20070511230942.GC20618@phlogiston.dyndns.org><d7df81620705120652p43f94b27wca0b8372e868aa38@mail.gmail.com><20070513183245.GA27803@phlogiston.dyndns.org><d7df81620705131533t4e9607d9iccb3f1e16a42fc26@mail.gmail.com>
	<60646wp3pk.fsf@dba2.int.libertyrms.com>
Message-ID: <004701c795e5$e2195970$b501a8c0@bwlaptop>

I think there are 2 questions in Dmitry's email.
1) How to minimize the workload on the master while still having a low time 
lag between master and slave
and
2) How to coordinate or synchronize the master/slave so the application can 
reliably update the master and query the slave for the just-updated data 
without getting old/stale data.

On 1, it sounds like the workload is directly proportional to the number of 
direct slaves the master has and somewhat proportional to the 
sync_interval/time lag setting.  Correct me if I'm wrong here.

This second question is one that I have also been struggling with.  We would 
like to direct all inserts/updates/deletes to the master but send all 
selects to the slave(s).  However, our app routinely makes updates and 
immediately queries back the (just-updated) data.  Any suggestions on how to 
do this without a bogus and unreliable wait/sleep call?  Does Dmitry's idea 
make sense??

Thanks,
Bill

----- Original Message ----- 
From: "Christopher Browne" <cbbrowne@ca.afilias.info>
To: <dmitry@koterov.ru>
Cc: <slony1-general@lists.slony.info>
Sent: Sunday, May 13, 2007 6:57 PM
Subject: Re: [Slony1-general] How to reduce the time data appears in the 
slave?


> "Dmitry Koterov" <dmitry@koterov.ru> writes:
>> So, if anybody reads from sl_log_status, it blocks until anything is
>> written to sl_log_status. Seems it will be fully transparent for
>> Slony, but decreases the amount of polling queries. (It is an idea
>> only, practical implementation may need to be more detailed.) What
>> do you think about it?
>
> It smells to me as though you're trying to optimize the wrong problem.
>
> The "pg_listener problem" is one that changes in 1.2 *mostly*
> rectifies; the costs associated with NOTIFY/LISTEN should be dropped
> to a dull roar in 1.2.  (There are further optimizations in CVS HEAD,
> but it shouldn't be *highly* relevant.)
>
> What hasn't been touched, and in a sense, can't be, is to reduce the
> load of processing the queries against the "master" node that pull
> replication data to feed to the subscribers.
>
> I suspect that it is THOSE queries that are causing you trouble, as
> opposed to anything about NOTIFY/LISTEN.
>
> If you have nine nodes all feeding off the master, that's going to put
> considerable load on the master node.  And THAT would easily cause
> things to slow down on the master, and cause replicas to lag.
>
> The fewer nodes that feed directly from the origin, the less load that
> you put on the origin, and the easier it should be able to feed them.
> -- 
> select 'cbbrowne' || '@' || 'ca.afilias.info';
> <http://dba2.int.libertyrms.com/>
> Christopher Browne
> (416) 673-4124 (land)
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general 

From ajs at crankycanuck.ca  Mon May 14 04:47:07 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon May 14 04:47:44 2007
Subject: [Slony1-general] How to reduce the time data appears in the slave?
In-Reply-To: <d7df81620705131533t4e9607d9iccb3f1e16a42fc26@mail.gmail.com>
References: <d7df81620705111547h358ff7d0s1c37a2443718d239@mail.gmail.com>
	<20070511230942.GC20618@phlogiston.dyndns.org>
	<d7df81620705120652p43f94b27wca0b8372e868aa38@mail.gmail.com>
	<20070513183245.GA27803@phlogiston.dyndns.org>
	<d7df81620705131533t4e9607d9iccb3f1e16a42fc26@mail.gmail.com>
Message-ID: <20070514114707.GA29488@phlogiston.dyndns.org>

On Mon, May 14, 2007 at 02:33:45AM +0400, Dmitry Koterov wrote:
> They are analogs of LISTEN/NOTIFY, but use standard IPC instead of
> pg_listener. Then I:

I'd be astonished if pg_listener were your main problem.

> So, if anybody reads from sl_log_status, it blocks until anything is written
> to sl_log_status. Seems it will be fully transparent for Slony, but
> decreases the amount of polling queries. (It is an idea only, practical
> implementation may need to be more detailed.) What do you think about it?

It sounds like a way to make everything a lot slower.  

One way you could improve your notification about whether a slave is
up to date enough for you is to add a sequence.  When you start your
transaction, you select nextval() from the sequence.  You can poll
from time to time on the replicas to see whether the sequence is >=
the value you have.  If so, that replica is "up to date enough" for
you.  But my real feeling is that Slony is the wrong tool for the job
you're trying to use it for. 

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
When my information changes, I alter my conclusions.  What do you do sir?
		--attr. John Maynard Keynes
From ajs at crankycanuck.ca  Mon May 14 04:47:19 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon May 14 04:47:45 2007
Subject: [Slony1-general] How to reduce the time data appears in the slave?
In-Reply-To: <004701c795e5$e2195970$b501a8c0@bwlaptop>
References: <60646wp3pk.fsf@dba2.int.libertyrms.com>
	<004701c795e5$e2195970$b501a8c0@bwlaptop>
Message-ID: <20070514114719.GB29488@phlogiston.dyndns.org>

On Sun, May 13, 2007 at 10:08:13PM -0700, Bill Willits wrote:

> 2) How to coordinate or synchronize the master/slave so the application can 
> reliably update the master and query the slave for the just-updated data 
> without getting old/stale data.

You can't.  That's what I figured he was trying to do in my first
email, where I emphasised that this is async replication.  

In a fully async system like this, you just _cannot_ have read-write
queries dependent on data that is on the replicas, unless you're
willing to have your clients do a lot of extra work to make sure that
when you look at the replica, you are looking at one that has your
changes committed.  Practically speaking, anything that is "real-time
sensitive" needs to talk to a complete, up to date version of the
database, and that means you have to talk to the database that has
all the data, or else you have to figure out a way where sending the
wrong data back is acceptable.  Slony-I is just not designed for the
former kind of use (but if you can figure out ways where stale data
doesn't matter, it works ok).  It's not multimaster (if it were,
different sessions could write in different databases, and you could
bind a given session to a given back end, so you'd always see at
least your own changes).  It's also not synchronous.

A
-- 
Andrew Sullivan  | ajs@crankycanuck.ca
I remember when computers were frustrating because they *did* exactly what 
you told them to.  That actually seems sort of quaint now.
		--J.D. Baldwin
From dmitry at koterov.ru  Mon May 14 04:56:12 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Mon May 14 04:56:29 2007
Subject: [Slony1-general] How to reduce the time data appears in the slave?
In-Reply-To: <20070514114707.GA29488@phlogiston.dyndns.org>
References: <d7df81620705111547h358ff7d0s1c37a2443718d239@mail.gmail.com>
	<20070511230942.GC20618@phlogiston.dyndns.org>
	<d7df81620705120652p43f94b27wca0b8372e868aa38@mail.gmail.com>
	<20070513183245.GA27803@phlogiston.dyndns.org>
	<d7df81620705131533t4e9607d9iccb3f1e16a42fc26@mail.gmail.com>
	<20070514114707.GA29488@phlogiston.dyndns.org>
Message-ID: <d7df81620705140456s59332964w4e2c12209c2a4409@mail.gmail.com>

>
> > So, if anybody reads from sl_log_status, it blocks until anything is
> written
> > to sl_log_status. Seems it will be fully transparent for Slony, but
> > decreases the amount of polling queries. (It is an idea only, practical
> > implementation may need to be more detailed.) What do you think about
> it?
>
> It sounds like a way to make everything a lot slower.


Why? Where is the slowdown?

It reduces the number of polling queries without affecting the speed of a
slave reaction... And does not affect pg_listeners, so - there is no problem
with pg_listeners vacuuming. Where am I wrong?

One way you could improve your notification about whether a slave is
> up to date enough for you is to add a sequence.  When you start your
> transaction, you select nextval() from the sequence.

I think "select last_value from "_moikrug_cluster".sl_log_status" query may
also be used to check if the slave is "up to date enough". But it is not the
main problem - slave checking. The main problem for me is to reduce the
number of polling queries without increasing the slave lag time.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070514/=
e754a67a/attachment.htm
From ajs at crankycanuck.ca  Mon May 14 06:25:16 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon May 14 06:25:25 2007
Subject: [Slony1-general] How to reduce the time data appears in the slave?
In-Reply-To: <d7df81620705140456s59332964w4e2c12209c2a4409@mail.gmail.com>
References: <d7df81620705111547h358ff7d0s1c37a2443718d239@mail.gmail.com>
	<20070511230942.GC20618@phlogiston.dyndns.org>
	<d7df81620705120652p43f94b27wca0b8372e868aa38@mail.gmail.com>
	<20070513183245.GA27803@phlogiston.dyndns.org>
	<d7df81620705131533t4e9607d9iccb3f1e16a42fc26@mail.gmail.com>
	<20070514114707.GA29488@phlogiston.dyndns.org>
	<d7df81620705140456s59332964w4e2c12209c2a4409@mail.gmail.com>
Message-ID: <20070514132516.GJ29488@phlogiston.dyndns.org>

On Mon, May 14, 2007 at 03:56:12PM +0400, Dmitry Koterov wrote:
> >> So, if anybody reads from sl_log_status, it blocks until anything is
> >written
> >> to sl_log_status. 

> >It sounds like a way to make everything a lot slower.
> 
> 
> Why? Where is the slowdown?

If you have many slaves trying to read, they're all blocked.  This is
effectively going to single-thread all the replicas.  It will be
slower.

> main problem - slave checking. The main problem for me is to reduce the
> number of polling queries without increasing the slave lag time.

And as people keep telling you, that just isn't possible _by design_. 
The polling is what keeps the slaves in sync.  I understand what you
are trying to do, but it wasn't a target use case for Slony-I (in
fact, we explicitly considered and rejected this case, AFAIR).  Not
all replication strategies are appropriate for every use case, and
this one is not appropriate for yours, I think.  Likely what you
really need is something like Postgres-R.  Or maybe statement
replication.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
This work was visionary and imaginative, and goes to show that visionary
and imaginative work need not end up well. 
		--Dennis Ritchie
From dmitry at koterov.ru  Mon May 14 06:42:38 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Mon May 14 06:42:44 2007
Subject: [Slony1-general] How to reduce the time data appears in the slave?
In-Reply-To: <20070514132516.GJ29488@phlogiston.dyndns.org>
References: <d7df81620705111547h358ff7d0s1c37a2443718d239@mail.gmail.com>
	<20070511230942.GC20618@phlogiston.dyndns.org>
	<d7df81620705120652p43f94b27wca0b8372e868aa38@mail.gmail.com>
	<20070513183245.GA27803@phlogiston.dyndns.org>
	<d7df81620705131533t4e9607d9iccb3f1e16a42fc26@mail.gmail.com>
	<20070514114707.GA29488@phlogiston.dyndns.org>
	<d7df81620705140456s59332964w4e2c12209c2a4409@mail.gmail.com>
	<20070514132516.GJ29488@phlogiston.dyndns.org>
Message-ID: <d7df81620705140642x59a0e270o628397fabb31fe89@mail.gmail.com>

>
> > >> So, if anybody reads from sl_log_status, it blocks until anything is
> > >written
> > >> to sl_log_status.
> > >It sounds like a way to make everything a lot slower.
> > Why? Where is the slowdown?
> If you have many slaves trying to read, they're all blocked.  This is
> effectively going to single-thread all the replicas.  It will be
> slower.

Hmmm... But in postgres read operations never blocks other operations (by
default) because of versioning. So, while all the replicas are blocked
waiting for a new event, they are still able to process a lot of read
queries coming from the site.

Or you mean that after unblocking ALL the replicas will simultaneously begin
to read new events from the Master and slow him down greatly? Please
detalize, it is not clear enough for me yet...

Thanks.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070514/=
8fd64ebc/attachment.htm
From ajs at crankycanuck.ca  Mon May 14 07:12:50 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon May 14 07:13:04 2007
Subject: [Slony1-general] How to reduce the time data appears in the slave?
In-Reply-To: <d7df81620705140642x59a0e270o628397fabb31fe89@mail.gmail.com>
References: <d7df81620705111547h358ff7d0s1c37a2443718d239@mail.gmail.com>
	<20070511230942.GC20618@phlogiston.dyndns.org>
	<d7df81620705120652p43f94b27wca0b8372e868aa38@mail.gmail.com>
	<20070513183245.GA27803@phlogiston.dyndns.org>
	<d7df81620705131533t4e9607d9iccb3f1e16a42fc26@mail.gmail.com>
	<20070514114707.GA29488@phlogiston.dyndns.org>
	<d7df81620705140456s59332964w4e2c12209c2a4409@mail.gmail.com>
	<20070514132516.GJ29488@phlogiston.dyndns.org>
	<d7df81620705140642x59a0e270o628397fabb31fe89@mail.gmail.com>
Message-ID: <20070514141250.GA29980@phlogiston.dyndns.org>

On Mon, May 14, 2007 at 05:42:38PM +0400, Dmitry Koterov wrote:
> >
> >> >> So, if anybody reads from sl_log_status, it blocks until anything is
> >> >written
> >> >> to sl_log_status.

> Hmmm... But in postgres read operations never blocks other operations (by
> default) because of versioning. So, while all the replicas are blocked
> waiting for a new event, they are still able to process a lot of read
> queries coming from the site.

I must misunderstand you, then.  What you wrote (above) is that
you're going to block readers until something writes.  That suggests
to me that you're taking some sort of read-blocking lock (you can do
that: LOCK TABLE IN ACCESS EXCLUSIVE MODE will in fact block
everything).

I guess I still don't understand how any of this helps you, though.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
The plural of anecdote is not data.
		--Roger Brinner
From dmitry at koterov.ru  Mon May 14 07:53:30 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Mon May 14 07:53:36 2007
Subject: [Slony1-general] How to reduce the time data appears in the slave?
In-Reply-To: <20070514141250.GA29980@phlogiston.dyndns.org>
References: <d7df81620705111547h358ff7d0s1c37a2443718d239@mail.gmail.com>
	<20070511230942.GC20618@phlogiston.dyndns.org>
	<d7df81620705120652p43f94b27wca0b8372e868aa38@mail.gmail.com>
	<20070513183245.GA27803@phlogiston.dyndns.org>
	<d7df81620705131533t4e9607d9iccb3f1e16a42fc26@mail.gmail.com>
	<20070514114707.GA29488@phlogiston.dyndns.org>
	<d7df81620705140456s59332964w4e2c12209c2a4409@mail.gmail.com>
	<20070514132516.GJ29488@phlogiston.dyndns.org>
	<d7df81620705140642x59a0e270o628397fabb31fe89@mail.gmail.com>
	<20070514141250.GA29980@phlogiston.dyndns.org>
Message-ID: <d7df81620705140753p59adfe27r6202345fce1f6fab@mail.gmail.com>

No, of course I meant NOT global readers blocking. I DO NOT NEED zero lag. I
need only SHORT lag (e.g. 0.1 s), BUT without bombing the Master by polling
queries when the number of slaves is 20 (20/0.1 =3D 200 queries/s is too
much!).

I'll explain everything from scratch. Suppose we have 2 stored functions:

1. ipc_listen(): blocks the current session (session!) until somebody called
ipc_notify().
2. ipc_notify(): send a message to ALL who are waiting inside blocking
ipc_listen() call.

We could write these functions as postgres contrib (in C), using named
pipes. It is about 50 lines of C code, no more. Note that these functions DO
NOT USE standart postgres LISTEN/NOTIFY mechanism, so the do not bloat
pg_listeners table at all.

Now let's consider the following algorythm.

1. Slave is always connected to the master, so it runs ipc_listen(). This
call blocks its session.
2. Master, after it modifies an event table, in addition calls ipc_notify()
to make all slaves to wake up.
3. All the slaves wake up from ipc_listen() (exit from this stored function)
and continue their work: read events from the table as usual etc. - as it
realized in Slony now.
4. On the next round all the slaves begin from (1).

This algorythm avoids a lot of polling queries, because we generate next
polling query only if the previous one is finished. It also reduces the lag
between master and slave synchronization, because changes are spreaded in a
short time after the master is added a row to its event table.

So, the only difference from today's Slony implementation is to call
ipc_listen() before each polling query and run ipc_notify() just after an
event were inserted to the master's table.

I do not understand why this solution is inappropriate. Please explain.


On 5/14/07, Andrew Sullivan <ajs@crankycanuck.ca> wrote:
>
> On Mon, May 14, 2007 at 05:42:38PM +0400, Dmitry Koterov wrote:
> > >
> > >> >> So, if anybody reads from sl_log_status, it blocks until anything
> is
> > >> >written
> > >> >> to sl_log_status.
>
> > Hmmm... But in postgres read operations never blocks other operations
> (by
> > default) because of versioning. So, while all the replicas are blocked
> > waiting for a new event, they are still able to process a lot of read
> > queries coming from the site.
>
> I must misunderstand you, then.  What you wrote (above) is that
> you're going to block readers until something writes.  That suggests
> to me that you're taking some sort of read-blocking lock (you can do
> that: LOCK TABLE IN ACCESS EXCLUSIVE MODE will in fact block
> everything).
>
> I guess I still don't understand how any of this helps you, though.
>
> A
>
> --
> Andrew Sullivan  | ajs@crankycanuck.ca
> The plural of anecdote is not data.
>                 --Roger Brinner
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070514/=
86cf974f/attachment.htm
From leif at renesys.com  Mon May 14 08:11:06 2007
From: leif at renesys.com (Leif Bergman)
Date: Mon May 14 08:11:11 2007
Subject: [Slony1-general] release versions
Message-ID: <20070514151106.GA10365@renesys.com>


Hello,

I am somewhat new to slony, and I am confused about the version
numbers.  Which would be considered the latest stable version?

Thanks much,

Leif Bergman
Renesys
From andrew.george.hammond at gmail.com  Mon May 14 10:23:13 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Mon May 14 10:23:21 2007
Subject: [Slony1-general] release versions
In-Reply-To: <20070514151106.GA10365@renesys.com>
References: <20070514151106.GA10365@renesys.com>
Message-ID: <5a0a9d6f0705141023o3e7a6691pad26ac846f5afbf2@mail.gmail.com>

1.2.9 is the latest stable release and has been out for a few months now.
Generally you want to go with the latest release available, unless you
happen to be running an archaic version of PostgreSQL. However, the project
has had issues with release quality, so you might want to wait a week or so
before trying a fresh release in production. Heck, you might even want to
test it yourself before deploying.

On that subject, while we have the existing mailing lists for issue reports,
I'm not aware of any forum where we collect positive results: "I
installed/upgraded to slonyversion on (os,pgversion), ... to replicate a
database of x tables at about y gb and it appears to be working after z
days". I realize that such testimonials are of limited use, but it'd be
interesting to get an idea of how it's being used. QA strikes me as
something that the community can collaborate on, so I think it'd make good
sense to at least start collecting positive results somewhere.

Andrew


On 5/14/07, Leif Bergman <leif@renesys.com> wrote:
>
>
> Hello,
>
> I am somewhat new to slony, and I am confused about the version
> numbers.  Which would be considered the latest stable version?
>
> Thanks much,
>
> Leif Bergman
> Renesys
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070514/=
c3cb35a0/attachment.htm
From andrew.george.hammond at gmail.com  Mon May 14 10:48:27 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Mon May 14 10:48:34 2007
Subject: [Slony1-general] Re: after upgrade to 1.2.9: FATAL syncThread
	error
In-Reply-To: <464639A6.7090508@buberel.org>
References: <46455691.9080801@buberel.org>
	<20070512123302.GG24306@phlogiston.dyndns.org>
	<4645C351.6020300@buberel.org>
	<20070512194959.GC25221@phlogiston.dyndns.org>
	<464639A6.7090508@buberel.org>
Message-ID: <5a0a9d6f0705141048t5fc9c066o86ead3c4961bf229@mail.gmail.com>

On 5/12/07, Jason L. Buberel <jason@buberel.org> wrote:
>
>  Interestingly: Another cluster configuration from an 8.1.4 -> 8.2.4serve=
r seems to be working just fine. The main difference is that this
> second cluster configuration was created/installed from scratch.
>


I was going to ask if you'd run the regression tests on your binary after
building slony (since bad slony shared libs would certainly be a good way to
cause PostgreSQL to crash), but if you are using the same binary
successfully in another configuration, then that's probably not the problem.


My plan now is simply to drop the slon schema on all nodes and recreate the
> whole thing.
>


I'll let you know how that goes. If you have any other suggestions on how to
> get to the root cause of the problem described below I would be happy to =
try
> them out.
>


How did your cluster rebuild go?

Andrew



Regards
> Jason
>
> Andrew Sullivan wrote:
>
> On Sat, May 12, 2007 at 06:38:25AM -0700, Jason L. Buberel wrote:
>
>  May 12 00:42:53 srv3 slon[8698]: [54-1] 2007-05-12 00:42:53 CDT FATAL
> syncThread: "select
> "_srv1_srv2_cluster".createEvent('_srv1_srv2_cluster', 'SYNC', NULL);" -
>
>  It looks like the createEvent() function is blowing up. . .
>
>    May 12 00:42:53 srv3 slon[8698]: [54-2]  server closed the connection
> unexpectedly
> May 12 00:42:53 srv3 slon[8698]: [54-3]         This probably means the
> server terminated abnormally
> May 12 00:42:53 srv3 slon[8698]: [54-4]         before or while
> processing the request.
> May 12 00:42:53 srv3 postgres[7320]: [82-1] LOG:  server process (PID
> 8714) was terminated by signal 11
>
>  with sig 11.  You could look at the corefile that oughta be dumped.
> But my guess is that the slony functions haven't been compiled
> correctly here.
>
> A
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070514/=
1ae0f79f/attachment-0001.htm
From jason at buberel.org  Tue May 15 13:18:23 2007
From: jason at buberel.org (jason@buberel.org)
Date: Tue May 15 13:18:35 2007
Subject: [Slony1-general] Re: after upgrade to 1.2.9: FATAL syncThread error
References: <46455691.9080801@buberel.org>
	<20070512123302.GG24306@phlogiston.dyndns.org>
	<4645C351.6020300@buberel.org>
	<20070512194959.GC25221@phlogiston.dyndns.org>
	<464639A6.7090508@buberel.org>
	<5a0a9d6f0705141048t5fc9c066o86ead3c4961bf229@mail.gmail.com>
Message-ID: <cone.1179260303.861857.12732.0@lenny.buberel.org>

Good news - after dropping the slony schema on both nodes and recreating, 
everything seems to be working well.

Thanks for listening,
Jason


Andrew Hammond writes:
>    My plan now is simply to drop the slon schema on all nodes and recreate 
>    the whole thing.
> 
>    I'll let you know how that goes. If you have any other suggestions on 
>    how to get to the root cause of the problem described below I would be 
>    happy to try them out.
> 
> 
> 
> How did your cluster rebuild go?
> 
> Andrew
 
From jason at buberel.org  Tue May 15 14:20:54 2007
From: jason at buberel.org (jason@buberel.org)
Date: Tue May 15 14:21:04 2007
Subject: [Slony1-general] Am I going to end up in deadlock hell for this?
Message-ID: <cone.1179264054.255418.12732.0@lenny.buberel.org>

Consider the following configuration:

Cluster #1: prod_backup_cluster: Used to replicate all tables and all data 
from my the 'production_db' database on srv1 to the 
'prod_backup' database running on srv3:

srv1:production_db --prod_backup_cluster--> srv3:prod_backup

Cluster #2: prod_trxn_cluster: Used to replicate certain customer 
transaction-related tables:

srv1:production_db --prod_trxn_cluster---> srv3:prod_trxns

Question #1: What are the chances that the two slony processes, each of 
which will be reading from the same master database (srv1:production_db), 
will find themselves in a deadlock situation when the application makes 
updates to the data hosted in srv1:production_db?

Question #2: What would be the advantages of making this a single cluster 
with three nodes and two sets (one set coverall all tables for the 'backup' 
and another set of just the transactional tables)?

Now, we add a third cluster to the configuration, just to make everything a 
bit more complicated:

Cluster #3: stats_cluster: Used to replicate the results of a large number 
of statistical calculations from srv2:stats_db to srv1:production_db:

srv2:stats_db --stats_cluster--> srv1:production_db

The result of this will be that data from a single table (we have a table 
called 'city_summary', for example) will go from srv2 -> srv1 (via 
stats_cluster), then from srv1 -> srv3 (via prod_backup_cluster).

Question #3: There are times where the 'city_summary' table, residing on 
srv1:production_db will be written to with data originating from 
srv2:stats_db while at the same time being replicated to srv3:prod_backup. 
All of this will happen using three distinct slony clusters. Will I end up 
in a state of continual deadlock?

Question #4: Am I totally insane, or just a big dolt?

Thanks,
Jason

From ajs at crankycanuck.ca  Tue May 15 14:35:51 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue May 15 14:36:14 2007
Subject: [Slony1-general] Am I going to end up in deadlock hell for this?
In-Reply-To: <cone.1179264054.255418.12732.0@lenny.buberel.org>
References: <cone.1179264054.255418.12732.0@lenny.buberel.org>
Message-ID: <20070515213551.GA1883@phlogiston.dyndns.org>

On Tue, May 15, 2007 at 04:20:54PM -0500, jason@buberel.org wrote:
> Consider the following configuration:
> 
> Cluster #1: prod_backup_cluster: Used to replicate all tables and all data 

> Cluster #2: prod_trxn_cluster: Used to replicate certain customer 

You can't have more than one "cluster" in slony that covers the same
tables.  You can have more than one set, not all of which are
subscribed to by all nodes.  I think this answers your questions, but
if not, please ask more.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
I remember when computers were frustrating because they *did* exactly what 
you told them to.  That actually seems sort of quaint now.
		--J.D. Baldwin
From jason at buberel.org  Tue May 15 15:01:00 2007
From: jason at buberel.org (jason@buberel.org)
Date: Tue May 15 15:01:12 2007
Subject: [Slony1-general] Am I going to end up in deadlock hell for this?
References: <cone.1179264054.255418.12732.0@lenny.buberel.org>
	<20070515213551.GA1883@phlogiston.dyndns.org>
Message-ID: <cone.1179266460.448062.12732.0@lenny.buberel.org>

Andrew,

I suspected as much :) That is what I get for trying to be too clever.

Thanks, and I'll try to document what I end up with once I get it working.

-jason


Andrew Sullivan writes:

> On Tue, May 15, 2007 at 04:20:54PM -0500, jason@buberel.org wrote:
>> Consider the following configuration:
>> 
>> Cluster #1: prod_backup_cluster: Used to replicate all tables and all data 
> 
>> Cluster #2: prod_trxn_cluster: Used to replicate certain customer 
> 
> You can't have more than one "cluster" in slony that covers the same
> tables.  You can have more than one set, not all of which are
> subscribed to by all nodes.  I think this answers your questions, but
> if not, please ask more.
> 
> A
> 
> -- 
> Andrew Sullivan  | ajs@crankycanuck.ca
> I remember when computers were frustrating because they *did* exactly what 
> you told them to.  That actually seems sort of quaint now.
> 		--J.D. Baldwin
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
From jason at buberel.org  Tue May 15 15:42:43 2007
From: jason at buberel.org (jason@buberel.org)
Date: Tue May 15 15:42:53 2007
Subject: [Slony1-general] Am I going to end up in deadlock hell for this?
References: <cone.1179264054.255418.12732.0@lenny.buberel.org>
	<20070515213551.GA1883@phlogiston.dyndns.org>
Message-ID: <cone.1179268963.893683.12732.0@lenny.buberel.org>

One more quick/easy question:

Given that I have one table that would appear in several sets (named 
'public.city_summary'), that will be replicated as follows:

srv2:stats_db (node 2) ->
    srv1:production_db (node 1) -> 
        srv3:production_backup (node 3)

Should that table be given the same id# in all three sets:

create set (id=1, origin=2, comment='stats set');
...
set add table (id=1, set id=1, origin=2, 
    fully qualified name='public.city_summary');
...

create set (id=2, origin=1, comment='production backup');
...
set add table (id=1, set id=2, origin=2,
    fully qualified name='public.city_summary');

Or would each table definition in each set need globally unique identifier?

Thanks,
jason


Andrew Sullivan writes:

> On Tue, May 15, 2007 at 04:20:54PM -0500, jason@buberel.org wrote:
>> Consider the following configuration:
>> 
>> Cluster #1: prod_backup_cluster: Used to replicate all tables and all data 
> 
>> Cluster #2: prod_trxn_cluster: Used to replicate certain customer 
> 
> You can't have more than one "cluster" in slony that covers the same
> tables.  You can have more than one set, not all of which are
> subscribed to by all nodes.  I think this answers your questions, but
> if not, please ask more.
> 
> A
> 
> -- 
> Andrew Sullivan  | ajs@crankycanuck.ca
> I remember when computers were frustrating because they *did* exactly what 
> you told them to.  That actually seems sort of quaint now.
> 		--J.D. Baldwin
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
From dmitry at koterov.ru  Wed May 16 03:57:38 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Wed May 16 03:57:54 2007
Subject: [Slony1-general] How to determine if the slave is up to date?
Message-ID: <d7df81620705160357n47fa5f6fk31cf8c1be7bb9915@mail.gmail.com>

Hello.

Is it possible to determine that a slave has up-to-date status according to
the master's status at some time?

E.g. I want to post some update to the master at the moment A, remember its
"up-to-date counter" and a second or two later (moment B) - check if the
slave has greater or equal value of this counter. If so, I suppose than at
the moment B slave is "up-to-date" relative to the master at the moment A.

In previous Slony version I could use "select last_value from
"_moikrug_cluster".sl_log_status" to fetch such counter from the master, but
in the recent Slony version this counter seems to be always zero.

I understant that I could implement this functionality manually (create a
sequence and increment it on the master with later checking at the slave),
but possibly Slony has already such sequence implemented?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070516/=
22ec0e4c/attachment.htm
From dmitry at koterov.ru  Wed May 16 04:21:03 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Wed May 16 04:21:19 2007
Subject: [Slony1-general] Re: How to determine if the slave is up to date?
In-Reply-To: <d7df81620705160357n47fa5f6fk31cf8c1be7bb9915@mail.gmail.com>
References: <d7df81620705160357n47fa5f6fk31cf8c1be7bb9915@mail.gmail.com>
Message-ID: <d7df81620705160421t7889912co6322f02bfc6e6ec7@mail.gmail.com>

Oh, I suppose that this counter is the number of last processed/generated
SYNC event.
So, POSSIBLY the question is:
1. how to determine the last SYNC number generated by the master?
2. how th determine the last SYNC number processed by the slave?

On 5/16/07, Dmitry Koterov <dmitry@koterov.ru> wrote:
>
> Hello.
>
> Is it possible to determine that a slave has up-to-date status according
> to the master's status at some time?
>
> E.g. I want to post some update to the master at the moment A, remember
> its "up-to-date counter" and a second or two later (moment B) - check if =
the
> slave has greater or equal value of this counter. If so, I suppose than at
> the moment B slave is "up-to-date" relative to the master at the moment A.
>
> In previous Slony version I could use "select last_value from
> "_moikrug_cluster".sl_log_status" to fetch such counter from the master, =
but
> in the recent Slony version this counter seems to be always zero.
>
> I understant that I could implement this functionality manually (create a
> sequence and increment it on the master with later checking at the slave),
> but possibly Slony has already such sequence implemented?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070516/=
07d605cb/attachment.htm
From arosenthal at AtlantaHand.com  Wed May 16 06:05:52 2007
From: arosenthal at AtlantaHand.com (Al Rosenthal)
Date: Wed May 16 06:05:52 2007
Subject: [Slony1-general] Slony on windows wireless hangs.
Message-ID: <000801c797ba$eec1fbb0$b900a8c0@Tablet>

Hello,

I have a small network running windows.  Postgres 8.2.3 is the db and slony=
 version 1.2.9 is set up.

When I started replication my first db, I followed the examples and it work=
ed perfectly.  One master, one slave, db size of 300Mb.  Then I noticed tha=
t replication often fell behind.  If I watched the statistics info on pgAdm=
in III, it would report that the hanging event was a SYNC and often it woul=
d hang for several hours.  If I ignored the problem long enough, it would c=
atch up, run for a while, and then hang.  (Long enough was usually 12 - 24 =
hours).

My second db was larger, about 22 Gigs.  I tried for almost a month to repl=
icate it.  It never went through COPY_SET to completion.  I read everything=
 I could on line and tried every permutation of the slony.conf files.  I tr=
ied setting up with scripts and pgAdmin III.  It didn't matter.  Finally I =
remembered that wireless often drops the connection for a second or two at =
a time.  I can't rewire the office but did get one long cord, connect the w=
ireless router to one machine, and I was able to replicate the large db.

I still have the problem of both db's replication hanging for several hours=
.  I can only guess that slony is intolerant of even momentary network outa=
ges.  If I restart slony (net stop slony-i, net start slony-i) from the com=
mand line, it always catches up.

My question, is there any way of adjusting settings to make slony more tole=
rant?  Or am I looking in the wrong direction?  I know I could write anothe=
r service as a .NET program and implement ManagementEventWatcher to see whe=
n a connection resets and then restart slony-i that way, but that seems clu=
nky.

Any suggestions?


Thanks.


Al
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070516/=
2b004c47/attachment-0001.htm
From cbbrowne at ca.afilias.info  Wed May 16 06:45:59 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed May 16 06:46:04 2007
Subject: [Slony1-general] How to determine if the slave is up to date?
In-Reply-To: <d7df81620705160357n47fa5f6fk31cf8c1be7bb9915@mail.gmail.com>
	(Dmitry Koterov's message of "Wed, 16 May 2007 14:57:38 +0400")
References: <d7df81620705160357n47fa5f6fk31cf8c1be7bb9915@mail.gmail.com>
Message-ID: <603b1wvq3s.fsf@dba2.int.libertyrms.com>

"Dmitry Koterov" <dmitry@koterov.ru> writes:
> Hello.
> Is it possible to determine that a slave has up-to-date status according to the master's status at some time?
> E.g. I want to post some update to the master at the moment A, remember its "up-to-date counter" and a second or two later (moment B) - check if the slave has greater
> or equal value of this counter. If so, I suppose than at the moment B slave is "up-to-date" relative to the master at the moment A.
> In previous Slony version I could use "select last_value from "_moikrug_cluster".sl_log_status" to fetch such counter from the master, but in the recent Slony version
> this counter seems to be always zero.
> I understant that I could implement this functionality manually (create a sequence and increment it on the master with later checking at the slave), but possibly Slony
> has already such sequence implemented?

Look at the view "sl_status", on the origin node for a given set.

The view lists what has been confirmed by all the other nodes, and
even includes a count of how many events each subscriber is behind.
-- 
output = ("cbbrowne" "@" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From ajs at crankycanuck.ca  Wed May 16 07:52:06 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed May 16 07:52:25 2007
Subject: [Slony1-general] Am I going to end up in deadlock hell for this?
In-Reply-To: <cone.1179268963.893683.12732.0@lenny.buberel.org>
References: <cone.1179264054.255418.12732.0@lenny.buberel.org>
	<20070515213551.GA1883@phlogiston.dyndns.org>
	<cone.1179268963.893683.12732.0@lenny.buberel.org>
Message-ID: <20070516145206.GA4036@phlogiston.dyndns.org>

On Tue, May 15, 2007 at 05:42:43PM -0500, jason@buberel.org wrote:
> One more quick/easy question:
> 
> Given that I have one table that would appear in several sets (named 

No, it won't.  A table must appear in exactly one set.

If you want that table to appear on three members, then you put it in
the all_members set.  If you want a table to appear only on the foo
member, then you put it in the foo_members set.  If you want it to
appear on (say) foo and bar but not baz, then you put it in the
foobar_members set.  Something like that.  I think you need to
re-read the manual about sets.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
When my information changes, I alter my conclusions.  What do you do sir?
		--attr. John Maynard Keynes
From ajs at crankycanuck.ca  Wed May 16 08:00:49 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed May 16 08:00:57 2007
Subject: [Slony1-general] Slony on windows wireless hangs.
In-Reply-To: <000801c797ba$eec1fbb0$b900a8c0@Tablet>
References: <000801c797ba$eec1fbb0$b900a8c0@Tablet>
Message-ID: <20070516150049.GB4036@phlogiston.dyndns.org>

On Wed, May 16, 2007 at 09:05:52AM -0400, Al Rosenthal wrote:

> My question, is there any way of adjusting settings to make slony
> more tolerant?  Or am I looking in the wrong direction?  I know I

Slony is just using standard PostgreSQL connections.  So your problem
basically comes down to your connection failing and not picking up
where it left off.  This suggests to me that the network isn't really
working the way it ought to be.  The connection is not just getting
interrupted, but the reconnection seems not to be picking back up the
same connection, which means that you don't get TCP retries and the
like.  What happens in that case is that you have to wait for the TCP
timeout to happen, at which point Postgres will notice the connection
broke.  You could lower that timeout in the operating system, I
suppose.

That said, Slony was really designed to work in a
high-availability, data-centre-type environment: it's really not
intended to be used across flakey networks (and just about every
wireless network I can think of falls into the flakey category to
some degree).  I'd think twice about using it like you're trying to
do.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
A certain description of men are for getting out of debt, yet are
against all taxes for raising money to pay it off.
		--Alexander Hamilton
From decibel at decibel.org  Wed May 16 09:20:02 2007
From: decibel at decibel.org (Jim C. Nasby)
Date: Wed May 16 09:20:15 2007
Subject: [Slony1-general] Slony on windows wireless hangs.
In-Reply-To: <20070516150049.GB4036@phlogiston.dyndns.org>
References: <000801c797ba$eec1fbb0$b900a8c0@Tablet>
	<20070516150049.GB4036@phlogiston.dyndns.org>
Message-ID: <20070516162002.GO14548@decibel.org>

On Wed, May 16, 2007 at 11:00:49AM -0400, Andrew Sullivan wrote:
> On Wed, May 16, 2007 at 09:05:52AM -0400, Al Rosenthal wrote:
> 
> > My question, is there any way of adjusting settings to make slony
> > more tolerant?  Or am I looking in the wrong direction?  I know I
> 
> Slony is just using standard PostgreSQL connections.  So your problem
> basically comes down to your connection failing and not picking up
> where it left off.  This suggests to me that the network isn't really
> working the way it ought to be.  The connection is not just getting
> interrupted, but the reconnection seems not to be picking back up the
> same connection, which means that you don't get TCP retries and the
> like.  What happens in that case is that you have to wait for the TCP
> timeout to happen, at which point Postgres will notice the connection
> broke.  You could lower that timeout in the operating system, I
> suppose.

The tcp_keepalives_* configuration parameters allow you to do this
easily from within PostgreSQL (I think they're new it 8.2).
-- 
Jim C. Nasby, Database Architect                decibel@decibel.org 
Give your computer some brain candy! www.distributed.net Team #1828

Windows: "Where do you want to go today?"
Linux: "Where do you want to go tomorrow?"
FreeBSD: "Are you guys coming, or what?"
From dbarthel at usedeverywhere.com  Wed May 16 09:58:17 2007
From: dbarthel at usedeverywhere.com (Don Barthel)
Date: Wed May 16 09:58:22 2007
Subject: [Slony1-general] Slony Replication Slow
Message-ID: <93f47c720705160958t7f8cfa3dp78f9a0f0db3d6bcc@mail.gmail.com>

I have one subscriber, one master.

Running 'select * from sl_status;' on the master I see:

<<
 st_origin | st_received | st_last_event |      st_last_event_ts
| st_last_received |    st_last_received_ts    |
st_last_received_event_ts  | st_lag_num_events |   st_lag_time
-----------+-------------+---------------+----------------------------+------------------+---------------------------+-------
---------------------+-------------------+-----------------
         1 |           2 |       1713287 | 2007-05-16 11:46:39.741258
|          1708951 | 2007-05-16 11:45:30.50751 | 2007-05-16
06:37:55.004498 |              4336 | 05:08:45.916761
>>

I.e.
st_last_received_event_ts == '2007-05-16 06:37:55.004498' and
st_lag_num_events == 4336 and
st_lag_time == '05:08:45.916761' (5 hours?)

Slony is controlled from the subscriber. I have restarted slony from
the subscriber (clumsily with 'kill' then 'slon.sh restart' because
'slon.sh stop' didn't seem to stop it).

Events are getting processed but very slowly such that st_lag_time is
increasing.

Every night I update a field in 100,000 records and slony can't seem
to quite catch up.

The load averages on the two database servers is 'normal' which means
low for the master and moderate for the subscriber.

Suggestion? Thanks!

- Don Barthel
From pgsql at j-davis.com  Wed May 16 11:05:11 2007
From: pgsql at j-davis.com (Jeff Davis)
Date: Wed May 16 11:05:29 2007
Subject: [Slony1-general] Slony Replication Slow
In-Reply-To: <93f47c720705160958t7f8cfa3dp78f9a0f0db3d6bcc@mail.gmail.com>
References: <93f47c720705160958t7f8cfa3dp78f9a0f0db3d6bcc@mail.gmail.com>
Message-ID: <1179338711.24902.158.camel@dogma.v10.wvs>

On Wed, 2007-05-16 at 11:58 -0500, Don Barthel wrote:
> I have one subscriber, one master.
> 
> Running 'select * from sl_status;' on the master I see:
> 
> <<
>  st_origin | st_received | st_last_event |      st_last_event_ts
> | st_last_received |    st_last_received_ts    |
> st_last_received_event_ts  | st_lag_num_events |   st_lag_time
> -----------+-------------+---------------+----------------------------+------------------+---------------------------+-------
> ---------------------+-------------------+-----------------
>          1 |           2 |       1713287 | 2007-05-16 11:46:39.741258
> |          1708951 | 2007-05-16 11:45:30.50751 | 2007-05-16
> 06:37:55.004498 |              4336 | 05:08:45.916761
> >>
> 
> I.e.
> st_last_received_event_ts == '2007-05-16 06:37:55.004498' and
> st_lag_num_events == 4336 and
> st_lag_time == '05:08:45.916761' (5 hours?)
> 
> Slony is controlled from the subscriber. I have restarted slony from
> the subscriber (clumsily with 'kill' then 'slon.sh restart' because
> 'slon.sh stop' didn't seem to stop it).
> 
> Events are getting processed but very slowly such that st_lag_time is
> increasing.
> 
> Every night I update a field in 100,000 records and slony can't seem
> to quite catch up.
> 

Slony doesn't handle transactions that change a lot of data very well.
Slony uses the same mechanism to replicate a single-tuple INSERT as an
unqualified UPDATE on 100M records.

If you can break up transactions into smaller chunks that will improve
the lag time a lot.

However, 100K tuples isn't that much really, so maybe your problem comes
from somewhere else? Do your indexes fit in the shared buffers on the
subscriber?

Regards,
	Jeff Davis

From jason at buberel.org  Wed May 16 11:35:14 2007
From: jason at buberel.org (jason@buberel.org)
Date: Wed May 16 11:35:28 2007
Subject: [Slony1-general] Log warnings of incorrect escape characters?
References: <93f47c720705160958t7f8cfa3dp78f9a0f0db3d6bcc@mail.gmail.com>
	<1179338711.24902.158.camel@dogma.v10.wvs>
Message-ID: <cone.1179340514.683765.2342.0@lenny.buberel.org>

Saw this in logs today coming from a cluster running slony-1.2.9 against 
pg-8.2.4:

2007-05-16 13:29:08 CDT DEBUG1 cleanupThread:    0.203 seconds for delete logs
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=7035
CONTEXT:  SQL statement "SELECT  "_ar_primary".cleanupNodelock()"
        PL/pgSQL function "cleanupevent" line 77 at perform
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=7035
CONTEXT:  SQL statement "SELECT  "_ar_primary".cleanupNodelock()"
PL/pgSQL function "cleanupevent" line 77 at perform
2007-05-16 13:30:35 CDT DEBUG1 cleanupThread:    0.019 seconds for 
cleanupEvent()
2007-05-16 13:30:35 CDT DEBUG1 cleanupThread:    0.017 seconds for delete 
logs
WARNING:  nonstandard use of \\ in a string literal at character 99
HINT:  Use the escape string syntax for backslashes, e.g., E'\\'.
WARNING:  nonstandard use of \\ in a string literal at character 1251
HINT:  Use the escape string syntax for backslashes, e.g., E'\\'.

Everything appears to be working correctly, but wasn't sure if this was an 
'expected' message to see in log file.

-jason


From jason at buberel.org  Wed May 16 11:48:04 2007
From: jason at buberel.org (jason@buberel.org)
Date: Wed May 16 11:48:10 2007
Subject: [Slony1-general] Re: Am I going to end up in deadlock hell for this?
References: <cone.1179264054.255418.12732.0@lenny.buberel.org>
	<20070515213551.GA1883@phlogiston.dyndns.org>
	<cone.1179268963.893683.12732.0@lenny.buberel.org>
	<20070516145206.GA4036@phlogiston.dyndns.org>
Message-ID: <cone.1179341284.288043.2342.0@lenny.buberel.org>

Thank you for the kindly RTFM :)

To quote from the documentation: This ID must be unique across all sets; you 
cannot have two tables in the same cluster with the same ID.

Which leads me to ask whether or not either of the following two statements 
are true of slony:

Within a single cluster, a table can only be a member of one set.

- or -

Within a single cluster, a table can be a member of more than one set as 
long as each of those sets has the same origin node.

Thanks,
jason


Andrew Sullivan writes:

> On Tue, May 15, 2007 at 05:42:43PM -0500, jason@buberel.org wrote:
>> One more quick/easy question:
>> 
>> Given that I have one table that would appear in several sets (named 
> 
> No, it won't.  A table must appear in exactly one set.
> 
> If you want that table to appear on three members, then you put it in
> the all_members set.  If you want a table to appear only on the foo
> member, then you put it in the foo_members set.  If you want it to
> appear on (say) foo and bar but not baz, then you put it in the
> foobar_members set.  Something like that.  I think you need to
> re-read the manual about sets.
> 
> A
> 
> -- 
> Andrew Sullivan  | ajs@crankycanuck.ca
> When my information changes, I alter my conclusions.  What do you do sir?
> 		--attr. John Maynard Keynes
From cbbrowne at ca.afilias.info  Wed May 16 12:18:28 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed May 16 12:18:39 2007
Subject: [Slony1-general] Re: Am I going to end up in deadlock hell for
	this?
In-Reply-To: <cone.1179341284.288043.2342.0@lenny.buberel.org>
	(jason@buberel.org's message of "Wed, 16 May 2007 13:48:04 -0500")
References: <cone.1179264054.255418.12732.0@lenny.buberel.org>
	<20070515213551.GA1883@phlogiston.dyndns.org>
	<cone.1179268963.893683.12732.0@lenny.buberel.org>
	<20070516145206.GA4036@phlogiston.dyndns.org>
	<cone.1179341284.288043.2342.0@lenny.buberel.org>
Message-ID: <60d510tw57.fsf@dba2.int.libertyrms.com>

jason@buberel.org writes:
> Thank you for the kindly RTFM :)
>
> To quote from the documentation: This ID must be unique across all
> sets; you cannot have two tables in the same cluster with the same ID.
>
> Which leads me to ask whether or not either of the following two
> statements are true of slony:
>
> Within a single cluster, a table can only be a member of one set.
>
> - or -
>
> Within a single cluster, a table can be a member of more than one set
> as long as each of those sets has the same origin node.

The first statement is the true one, and it actually furthermore
crosses clusters...

A table can only successfully be a member of one set.
-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@" [name;tld];;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From ajs at crankycanuck.ca  Wed May 16 12:21:36 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed May 16 12:21:53 2007
Subject: [Slony1-general] Re: Am I going to end up in deadlock hell for
	this?
In-Reply-To: <cone.1179341284.288043.2342.0@lenny.buberel.org>
References: <cone.1179264054.255418.12732.0@lenny.buberel.org>
	<20070515213551.GA1883@phlogiston.dyndns.org>
	<cone.1179268963.893683.12732.0@lenny.buberel.org>
	<20070516145206.GA4036@phlogiston.dyndns.org>
	<cone.1179341284.288043.2342.0@lenny.buberel.org>
Message-ID: <20070516192136.GP4440@phlogiston.dyndns.org>

On Wed, May 16, 2007 at 01:48:04PM -0500, jason@buberel.org wrote:

> To quote from the documentation: This ID must be unique across all sets; 
> you cannot have two tables in the same cluster with the same ID.

Right, that's a requirement that table IDs be globally unique to the
cluster.

> Within a single cluster, a table can only be a member of one set.

Yes, this is true.  Interestingly, I don't find this easily in the
docs (I haven't looked carefully, but I'm surprised that it doesn't
leap out at me in the discussion of sets).  So my apologies for
suggesting you read the manual more carefully.

You can see this is true, however, by looking at the table definition
of _slonyschema.sl_table:

 \d _copy_test.sl_table
    Table "_copy_test.sl_table"
   Column    |  Type   | Modifiers 
-------------+---------+-----------
 tab_id      | integer | not null
 tab_reloid  | oid     | not null
 tab_relname | name    | not null
 tab_nspname | name    | not null
 tab_set     | integer | 
 tab_idxname | name    | not null
 tab_altered | boolean | not null
 tab_comment | text    | 
Indexes:
    "sl_table-pkey" primary key, btree (tab_id)
    "sl_table_tab_reloid_key" unique, btree (tab_reloid)
Foreign-key constraints:
    "tab_set-set_id-ref" FOREIGN KEY (tab_set) REFERENCES
_copy_test.sl_set(set_id)

As you can see, tab_reloid (which is the oid from pg_class for the
table) is unique, and there's only room for one tab_set value.  So
each table must be in exactly one set.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
The whole tendency of modern prose is away from concreteness.
		--George Orwell
From dbarthel at usedeverywhere.com  Wed May 16 13:10:28 2007
From: dbarthel at usedeverywhere.com (Don Barthel)
Date: Wed May 16 13:10:36 2007
Subject: [Slony1-general] Slony Replication Slow
In-Reply-To: <1179338711.24902.158.camel@dogma.v10.wvs>
References: <93f47c720705160958t7f8cfa3dp78f9a0f0db3d6bcc@mail.gmail.com>
	<1179338711.24902.158.camel@dogma.v10.wvs>
Message-ID: <93f47c720705161310l6d077ae6w48be8ff475c377e2@mail.gmail.com>

Jeff:

Thanks for your help.

I have some big/complicated indexes on one particular table and since
I have lots of memory to spare I've just now increased my
shared_buffers from 20000 to 40000 (and SHMMAX at the OS level
appropriately) and reload'ed postgresql but things are still
progressing slowly.

I notice in my /var/log/messages on the origin (and not on the subscriber) this:

May 16 14:55:26 coliseum postgres[21817]: [1-1] ERROR:  duplicate key
violates unique constraint "sl_nodelock-pkey"

sl=slon, right?

Doing this: select * from "_usedstuff".sl_nodelock; I get:

 nl_nodeid | nl_conncnt | nl_backendpid
-----------+------------+---------------
         1 |          0 |         18083
         2 |         10 |         18089
         2 |         11 |         18090

Only three records. And I notice that the index on this table is:
"sl_nodelock-pkey" PRIMARY KEY, btree (nl_nodeid, nl_conncnt)

This looks like a problem to me but I cannot fathom where to start to
fix it. Any clues? Thanks in advance!

- Don Barthel


On 5/16/07, Jeff Davis <pgsql@j-davis.com> wrote:
> On Wed, 2007-05-16 at 11:58 -0500, Don Barthel wrote:
> > I have one subscriber, one master.
> >
> > Running 'select * from sl_status;' on the master I see:
> >
> > <<
> >  st_origin | st_received | st_last_event |      st_last_event_ts
> > | st_last_received |    st_last_received_ts    |
> > st_last_received_event_ts  | st_lag_num_events |   st_lag_time
> > -----------+-------------+---------------+----------------------------+------------------+---------------------------+-------
> > ---------------------+-------------------+-----------------
> >          1 |           2 |       1713287 | 2007-05-16 11:46:39.741258
> > |          1708951 | 2007-05-16 11:45:30.50751 | 2007-05-16
> > 06:37:55.004498 |              4336 | 05:08:45.916761
> > >>
> >
> > I.e.
> > st_last_received_event_ts == '2007-05-16 06:37:55.004498' and
> > st_lag_num_events == 4336 and
> > st_lag_time == '05:08:45.916761' (5 hours?)
> >
> > Slony is controlled from the subscriber. I have restarted slony from
> > the subscriber (clumsily with 'kill' then 'slon.sh restart' because
> > 'slon.sh stop' didn't seem to stop it).
> >
> > Events are getting processed but very slowly such that st_lag_time is
> > increasing.
> >
> > Every night I update a field in 100,000 records and slony can't seem
> > to quite catch up.
> >
>
> Slony doesn't handle transactions that change a lot of data very well.
> Slony uses the same mechanism to replicate a single-tuple INSERT as an
> unqualified UPDATE on 100M records.
>
> If you can break up transactions into smaller chunks that will improve
> the lag time a lot.
>
> However, 100K tuples isn't that much really, so maybe your problem comes
> from somewhere else? Do your indexes fit in the shared buffers on the
> subscriber?
>
> Regards,
>         Jeff Davis
>
>
From cbbrowne at ca.afilias.info  Wed May 16 13:39:28 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed May 16 13:39:38 2007
Subject: [Slony1-general] Slony Replication Slow
In-Reply-To: <93f47c720705161310l6d077ae6w48be8ff475c377e2@mail.gmail.com>
References: <93f47c720705160958t7f8cfa3dp78f9a0f0db3d6bcc@mail.gmail.com>	<1179338711.24902.158.camel@dogma.v10.wvs>
	<93f47c720705161310l6d077ae6w48be8ff475c377e2@mail.gmail.com>
Message-ID: <464B6C00.8040708@ca.afilias.info>

Don Barthel wrote:
> Jeff:
>
> Thanks for your help.
>
> I have some big/complicated indexes on one particular table and since
> I have lots of memory to spare I've just now increased my
> shared_buffers from 20000 to 40000 (and SHMMAX at the OS level
> appropriately) and reload'ed postgresql but things are still
> progressing slowly.
>
> I notice in my /var/log/messages on the origin (and not on the
> subscriber) this:
>
> May 16 14:55:26 coliseum postgres[21817]: [1-1] ERROR:  duplicate key
> violates unique constraint "sl_nodelock-pkey"
>
> sl=slon, right?
>
> Doing this: select * from "_usedstuff".sl_nodelock; I get:
>
> nl_nodeid | nl_conncnt | nl_backendpid
> -----------+------------+---------------
>         1 |          0 |         18083
>         2 |         10 |         18089
>         2 |         11 |         18090
>
> Only three records. And I notice that the index on this table is:
> "sl_nodelock-pkey" PRIMARY KEY, btree (nl_nodeid, nl_conncnt)
>
> This looks like a problem to me but I cannot fathom where to start to
> fix it. Any clues? Thanks in advance!
This would normally result from trying to start up a second slon process
when one is already running.

The constraint on sl_nodelock prevents two slon processes from trying to
work on things at the same time, and consequently mussing things up.

The error message is more an indication that Slony-I *prevented*
something bad from happening...
From dmitry at koterov.ru  Wed May 16 13:50:56 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Wed May 16 13:51:04 2007
Subject: [Slony1-general] How to determine if the slave is up to date?
In-Reply-To: <603b1wvq3s.fsf@dba2.int.libertyrms.com>
References: <d7df81620705160357n47fa5f6fk31cf8c1be7bb9915@mail.gmail.com>
	<603b1wvq3s.fsf@dba2.int.libertyrms.com>
Message-ID: <d7df81620705161350o3a291ceah60d74b71708c5564@mail.gmail.com>

Thanks!
Seems it is what I was looking for:

1. Current state of the Master:
master# STATE :=3D (select st_last_event from _cluster.sl_status limit 1)

2. Current slave ID:
slave# SLAVE_ID :=3D (select _cluster.getlocalnodeid('_cluster'));

3. If the slave is more up-to-date than master:
master# (select min(st_last_received) from _cluster.sl_status where
st_received =3D SLAVE_ID) >=3D STATE


On 5/16/07, Christopher Browne <cbbrowne@ca.afilias.info> wrote:
>
> "Dmitry Koterov" <dmitry@koterov.ru> writes:
> > Hello.
> > Is it possible to determine that a slave has up-to-date status according
> to the master's status at some time?
> > E.g. I want to post some update to the master at the moment A, remember
> its "up-to-date counter" and a second or two later (moment B) - check if =
the
> slave has greater
> > or equal value of this counter. If so, I suppose than at the moment B
> slave is "up-to-date" relative to the master at the moment A.
> > In previous Slony version I could use "select last_value from
> "_moikrug_cluster".sl_log_status" to fetch such counter from the master, =
but
> in the recent Slony version
> > this counter seems to be always zero.
> > I understant that I could implement this functionality manually (create
> a sequence and increment it on the master with later checking at the slav=
e),
> but possibly Slony
> > has already such sequence implemented?
>
> Look at the view "sl_status", on the origin node for a given set.
>
> The view lists what has been confirmed by all the other nodes, and
> even includes a count of how many events each subscriber is behind.
> --
> output =3D ("cbbrowne" "@" "ca.afilias.info")
> <http://dba2.int.libertyrms.com/>
> Christopher Browne
> (416) 673-4124 (land)
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070517/=
837d4c31/attachment.htm
From cbbrowne at ca.afilias.info  Wed May 16 14:13:40 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed May 16 14:13:50 2007
Subject: [Slony1-general] How to determine if the slave is up to date?
In-Reply-To: <d7df81620705161350o3a291ceah60d74b71708c5564@mail.gmail.com>
	(Dmitry Koterov's message of "Thu, 17 May 2007 00:50:56 +0400")
References: <d7df81620705160357n47fa5f6fk31cf8c1be7bb9915@mail.gmail.com>
	<603b1wvq3s.fsf@dba2.int.libertyrms.com>
	<d7df81620705161350o3a291ceah60d74b71708c5564@mail.gmail.com>
Message-ID: <604pmctqt7.fsf@dba2.int.libertyrms.com>

"Dmitry Koterov" <dmitry@koterov.ru> writes:
> Thanks!
> Seems it is what I was looking for:
> 1. Current state of the Master:
> master# STATE := (select st_last_event from _cluster.sl_status limit 1)

Sounds good!

> 2. Current slave ID:
> slave# SLAVE_ID := (select _cluster.getlocalnodeid('_cluster'));

Yes, that would be how to determine it...

> 3. If the slave is more up-to-date than master:
> master# (select min(st_last_received) from _cluster.sl_status where st_received = SLAVE_ID) >= STATE

It should never be possible for a subscriber to be more up to date
than the origin.

Firstly, that logically doesn't make sense.  (At least, when we're not
in a universe where time travels backwards, objects 'fall' *away* from
gravity wells, and such...)

Secondly, implementation-wise, it can't happen because the subscriber
can't ask for events until the origin has actually generated them.
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From dbarthel at usedeverywhere.com  Wed May 16 14:41:20 2007
From: dbarthel at usedeverywhere.com (Don Barthel)
Date: Wed May 16 14:41:30 2007
Subject: [Slony1-general] Slony Replication Slow
In-Reply-To: <464B6C00.8040708@ca.afilias.info>
References: <93f47c720705160958t7f8cfa3dp78f9a0f0db3d6bcc@mail.gmail.com>
	<1179338711.24902.158.camel@dogma.v10.wvs>
	<93f47c720705161310l6d077ae6w48be8ff475c377e2@mail.gmail.com>
	<464B6C00.8040708@ca.afilias.info>
Message-ID: <93f47c720705161441g4a46ceb3hb35900d124a83b25@mail.gmail.com>

Christopher:

> The error message is more an indication that Slony-I *prevented*
> something bad from happening...

OK, good to know. Is there anything I can to to 'encourage' slony to
catch up quicker? Its really moving llike molasses.
st_last_received_event_ts has advanced only 12 minutes in the past
hour and a half. Restarting slony didn't help.

Disconnecting the subscriber application server from the application?
Running some sort of slonik script? Rebooting? Kicking slony in the
pants?

As always, thanks for the brain cycles!

- Don Barthel


On 5/16/07, Christopher Browne <cbbrowne@ca.afilias.info> wrote:
> Don Barthel wrote:
> > Jeff:
> >
> > Thanks for your help.
> >
> > I have some big/complicated indexes on one particular table and since
> > I have lots of memory to spare I've just now increased my
> > shared_buffers from 20000 to 40000 (and SHMMAX at the OS level
> > appropriately) and reload'ed postgresql but things are still
> > progressing slowly.
> >
> > I notice in my /var/log/messages on the origin (and not on the
> > subscriber) this:
> >
> > May 16 14:55:26 coliseum postgres[21817]: [1-1] ERROR:  duplicate key
> > violates unique constraint "sl_nodelock-pkey"
> >
> > sl=slon, right?
> >
> > Doing this: select * from "_usedstuff".sl_nodelock; I get:
> >
> > nl_nodeid | nl_conncnt | nl_backendpid
> > -----------+------------+---------------
> >         1 |          0 |         18083
> >         2 |         10 |         18089
> >         2 |         11 |         18090
> >
> > Only three records. And I notice that the index on this table is:
> > "sl_nodelock-pkey" PRIMARY KEY, btree (nl_nodeid, nl_conncnt)
> >
> > This looks like a problem to me but I cannot fathom where to start to
> > fix it. Any clues? Thanks in advance!
> This would normally result from trying to start up a second slon process
> when one is already running.
>
> The constraint on sl_nodelock prevents two slon processes from trying to
> work on things at the same time, and consequently mussing things up.
>
> The error message is more an indication that Slony-I *prevented*
> something bad from happening...
>
From pgsql at j-davis.com  Wed May 16 15:56:05 2007
From: pgsql at j-davis.com (Jeff Davis)
Date: Wed May 16 15:56:22 2007
Subject: [Slony1-general] Slony Replication Slow
In-Reply-To: <93f47c720705161310l6d077ae6w48be8ff475c377e2@mail.gmail.com>
References: <93f47c720705160958t7f8cfa3dp78f9a0f0db3d6bcc@mail.gmail.com>
	<1179338711.24902.158.camel@dogma.v10.wvs>
	<93f47c720705161310l6d077ae6w48be8ff475c377e2@mail.gmail.com>
Message-ID: <1179356165.24902.171.camel@dogma.v10.wvs>

On Wed, 2007-05-16 at 15:10 -0500, Don Barthel wrote:
> I have some big/complicated indexes on one particular table and since
> I have lots of memory to spare I've just now increased my
> shared_buffers from 20000 to 40000 (and SHMMAX at the OS level
> appropriately) and reload'ed postgresql but things are still
> progressing slowly.
> 

Shared buffers on the origin or the subscriber? Shared buffers might not
matter in this case anyway.

A Slony subscriber reads from the log on the origin and then writes the
records on the subscriber in separate statements. For any bulk operation
on the origin (like a big UPDATE statement) that translates into a lot
of little statements on the subscriber. Usually, each of those little
statements uses an index, because that's fastest for an action on a
single tuple.

If you don't have enough memory to hold the index, that translates into
bad performance.

Tell us more about the situation. Is the index fitting in shared memory?
How big is the table? How much physical memory? Try to hunt down which
events are taking a long time to SYNC. What other types of bulk
operations are you doing? Any big UPDATEs or DELETEs?

Regards,
	Jeff Davis

From dbarthel at usedeverywhere.com  Wed May 16 16:47:13 2007
From: dbarthel at usedeverywhere.com (Don Barthel)
Date: Wed May 16 16:47:22 2007
Subject: [Slony1-general] Slony Replication Slow
In-Reply-To: <1179356165.24902.171.camel@dogma.v10.wvs>
References: <93f47c720705160958t7f8cfa3dp78f9a0f0db3d6bcc@mail.gmail.com>
	<1179338711.24902.158.camel@dogma.v10.wvs>
	<93f47c720705161310l6d077ae6w48be8ff475c377e2@mail.gmail.com>
	<1179356165.24902.171.camel@dogma.v10.wvs>
Message-ID: <93f47c720705161647n547287daqa7932b1dde872adf@mail.gmail.com>

Jeff:

Thanks for responding! I have edited out some bits to keep this as
brief as possible.

> Shared buffers on the origin or the subscriber? Shared buffers might not
> matter in this case anyway.

I upped the shared_buffers on both the origin and subscriber from
20,000 to 40,000 then did a reload (not a restart) in Postgres on both
the origin and the subscriber.

> A Slony subscriber reads from the log on the origin and then writes the
> records on the subscriber in separate statements. For any bulk operation
> on the origin (like a big UPDATE statement) that translates into a lot
> of little statements on the subscriber. Usually, each of those little
> statements uses an index, because that's fastest for an action on a
> single tuple.

My mass update was comprised of all individual, one record, updates
per transaction.

> If you don't have enough memory to hold the index, that translates into
> bad performance.
>
> Tell us more about the situation. Is the index fitting in shared memory?
> How big is the table? How much physical memory?

Thanks for the pointed questions. I didn't think any of this was
relevant until I checked. Two of my 12 indexes are bigger (as measured
by 'relpages') than the table itself. These are 8k pages right?

SELECT relname, reltuples, relpages FROM pg_class ORDER BY relpages DESC ;
             relname             |  reltuples  | relpages
---------------------------------+-------------+----------
 indx_tsearch2a                  |       72559 |    66324
 indx_tsearch2c                  |       40761 |    44640
 used_ad                         |      335542 |    38646

(used_ad is the table, the other two are partial indexes.)

So, my shared_buffers is 40,000 and two of my indexes are bigger than
that. The total size of all 12 indexes is some 324,000 'relpages'.

*** Is shared_buffers comparable to 'relpages'? If so, is it practical
to bump shared_buffers to 325,000? That's just over 2.5GB by my
calculation - I have 3GB on the machine.

> Try to hunt down which events are taking a long time to SYNC.

Please, what would allow me to measure that?

> What other types of bulk operations are you doing? Any big UPDATEs or DELETEs?

Nothing else big during the day, just a steady constant stream.

Thanks again, and in advance, for your sage advice.

- Don Barthel
From pgsql at j-davis.com  Wed May 16 17:26:00 2007
From: pgsql at j-davis.com (Jeff Davis)
Date: Wed May 16 17:26:19 2007
Subject: [Slony1-general] Slony Replication Slow
In-Reply-To: <93f47c720705161647n547287daqa7932b1dde872adf@mail.gmail.com>
References: <93f47c720705160958t7f8cfa3dp78f9a0f0db3d6bcc@mail.gmail.com>
	<1179338711.24902.158.camel@dogma.v10.wvs>
	<93f47c720705161310l6d077ae6w48be8ff475c377e2@mail.gmail.com>
	<1179356165.24902.171.camel@dogma.v10.wvs>
	<93f47c720705161647n547287daqa7932b1dde872adf@mail.gmail.com>
Message-ID: <1179361560.24902.221.camel@dogma.v10.wvs>

On Wed, 2007-05-16 at 18:47 -0500, Don Barthel wrote:
> My mass update was comprised of all individual, one record, updates
> per transaction.

If each transaction only modifies one record, mass updates aren't the
problem.

> SELECT relname, reltuples, relpages FROM pg_class ORDER BY relpages DESC ;
>              relname             |  reltuples  | relpages
> ---------------------------------+-------------+----------
>  indx_tsearch2a                  |       72559 |    66324
>  indx_tsearch2c                  |       40761 |    44640
>  used_ad                         |      335542 |    38646
> 

First, those numbers might not be updated unless you run ANALYZE.
Second, those other two indexes aren't the indexes used by Slony. The
most important index is the index Slony is using, which is the primary
key by default (which is certainly not those tsearch2 indexes).

See if the primary key of the table fits into shared_buffers. If you're
doing a lot of reporting from that machine constantly the primary key
might be pushed out anyway, which could still be your problem even if
the primary key is smaller than the other indexes.

However, if user_ad is only 300K tuples, that doesn't quite make sense.
I just checked on one of my tables with 30M tuples and the index is only
86K pages. From that, I would guess that your primary key index is less
than 1000 pages. Is that about right?

If that's true and slony is working constantly, that index would
probably be mostly in memory.

Are there other things happening on the subscriber that might be taxing
the I/O system too much? If those tsearch indexes are being used heavily
that might be responsible.

Also, just look at some system stats. What is the bottleneck, is it
disk? If so, what is the disk doing? Is the most intensive process the
postgres process that slony is connected to, or is it something else?

> *** Is shared_buffers comparable to 'relpages'? If so, is it practical

Yes.

> to bump shared_buffers to 325,000? That's just over 2.5GB by my
> calculation - I have 3GB on the machine.
> 

I don't recommend making a jump like that. If you use more than 50% of
the physical memory for shared_buffers you should really do benchmarks
to see if that's actually helping you.

The extra memory is not going to waste -- the OS is caching disk also,
so there's a good chance you don't have to go to disk on many of the
reads.

> > Try to hunt down which events are taking a long time to SYNC.
> 
> Please, what would allow me to measure that?
> 

If they are all one-record updates or similar, it's probably not any one
SYNC, but just the total quantity. I was looking out for any "DELETE
FROM mytable;" or "UPDATE mytable SET foo=foo+1;" type queries that
would really take a long time to SYNC. It doesn't look like that's your
problem though.

Regards,
	Jeff Davis





From drees76 at gmail.com  Wed May 16 18:31:24 2007
From: drees76 at gmail.com (David Rees)
Date: Wed May 16 18:31:35 2007
Subject: [Slony1-general] Bug in cleanupNodelock prevents slony startup
Message-ID: <72dbd3150705161831n559d161al7ee2c60c59db25bb@mail.gmail.com>

I upgraded my dev setup from pg 8.2.3 to pg 8.2.4 and slony 1.2.8 to
1.2.9 today and it went smoothly, except that one slony node failed to
come up with the messages:

FATAL  localListenThread: "select "_rep1".cleanupNodelock(); insert
into "_rep1".sl_nodelock values (    1, 0, "pg_catalog".
pg_backend_pid()); " - ERROR:  duplicate key violates unique
constraint "sl_nodelock-pkey"
FATAL  Do you already have a slon running against this node?
FATAL  Or perhaps a residual idle backend connection from a dead slon?

I checked the process list and found no extra slon daemons or residual
backend connections, so then I took a look at the sl_nodelock table
for the affected node and noticed that the nl_backendpid listed for
that node happened to be the same as the pid for the currently running
pg daemon itself so the cleanupnodelock function wasn't cleaning up
the entry.

After restarting postgres the affected slon node came up normally. I
suspect I could have manually cleaned out the sl_nodelock table as
well.

I don't know enough about the _Slony_I_killBackend function, but
perhaps it could be improved to detect this situation.

-Dave
From JanWieck at Yahoo.com  Wed May 16 19:51:21 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed May 16 19:51:41 2007
Subject: [Slony1-general] Slony Replication Slow
In-Reply-To: <93f47c720705161647n547287daqa7932b1dde872adf@mail.gmail.com>
References: <93f47c720705160958t7f8cfa3dp78f9a0f0db3d6bcc@mail.gmail.com>	<1179338711.24902.158.camel@dogma.v10.wvs>	<93f47c720705161310l6d077ae6w48be8ff475c377e2@mail.gmail.com>	<1179356165.24902.171.camel@dogma.v10.wvs>
	<93f47c720705161647n547287daqa7932b1dde872adf@mail.gmail.com>
Message-ID: <464BC329.2020107@Yahoo.com>

On 5/16/2007 7:47 PM, Don Barthel wrote:
> Jeff:
> 
> Thanks for responding! I have edited out some bits to keep this as
> brief as possible.
> 
>> Shared buffers on the origin or the subscriber? Shared buffers might not
>> matter in this case anyway.
> 
> I upped the shared_buffers on both the origin and subscriber from
> 20,000 to 40,000 then did a reload (not a restart) in Postgres on both
> the origin and the subscriber.

Anything that affects the shared memory configuration in Postgres 
(shared buffers, max number connections, etc.) requires a restart. A 
reload ignores those changes.

> SELECT relname, reltuples, relpages FROM pg_class ORDER BY relpages DESC ;
>              relname             |  reltuples  | relpages
> ---------------------------------+-------------+----------
>  indx_tsearch2a                  |       72559 |    66324
>  indx_tsearch2c                  |       40761 |    44640
>  used_ad                         |      335542 |    38646

Hard to believe that 300MB of data will cause nearly a GB of index 
information ... but then again this is tsearch, which I don't know too 
well. However, I would try to reindx that table and see what's left 
after that.

> 
> (used_ad is the table, the other two are partial indexes.)
> 
> So, my shared_buffers is 40,000 and two of my indexes are bigger than
> that. The total size of all 12 indexes is some 324,000 'relpages'.
> 
> *** Is shared_buffers comparable to 'relpages'? If so, is it practical
> to bump shared_buffers to 325,000? That's just over 2.5GB by my
> calculation - I have 3GB on the machine.

relpages as well as shared buffers are measured in block size, which 
defaults to 8k. So yes, they are comparable. On a 3GB machine (assuming 
this is a dedicated database server) I would start out with 125,000 
shared buffers (roughly one GB).


Jan

-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From dmitry at koterov.ru  Thu May 17 14:19:00 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Thu May 17 14:19:10 2007
Subject: [Slony1-general] How to determine if the slave is up to date?
In-Reply-To: <604pmctqt7.fsf@dba2.int.libertyrms.com>
References: <d7df81620705160357n47fa5f6fk31cf8c1be7bb9915@mail.gmail.com>
	<603b1wvq3s.fsf@dba2.int.libertyrms.com>
	<d7df81620705161350o3a291ceah60d74b71708c5564@mail.gmail.com>
	<604pmctqt7.fsf@dba2.int.libertyrms.com>
Message-ID: <d7df81620705171419t4cf48ecfp27d768113e55f196@mail.gmail.com>

Query (3) will be run in the future, of course, and STATE for it is got from
the past. So, it could happen. We do not need the time-machine to go forward
in time with usual speed. :-)

On 5/17/07, Christopher Browne <cbbrowne@ca.afilias.info> wrote:
>
> "Dmitry Koterov" <dmitry@koterov.ru> writes:
> > Thanks!
> > Seems it is what I was looking for:
> > 1. Current state of the Master:
> > master# STATE :=3D (select st_last_event from _cluster.sl_status limit =
1)
>
> Sounds good!
>
> > 2. Current slave ID:
> > slave# SLAVE_ID :=3D (select _cluster.getlocalnodeid('_cluster'));
>
> Yes, that would be how to determine it...
>
> > 3. If the slave is more up-to-date than master:
> > master# (select min(st_last_received) from _cluster.sl_status where
> st_received =3D SLAVE_ID) >=3D STATE
>
> It should never be possible for a subscriber to be more up to date
> than the origin.
>
> Firstly, that logically doesn't make sense.  (At least, when we're not
> in a universe where time travels backwards, objects 'fall' *away* from
> gravity wells, and such...)
>
> Secondly, implementation-wise, it can't happen because the subscriber
> can't ask for events until the origin has actually generated them.
> --
> select 'cbbrowne' || '@' || 'ca.afilias.info';
> <http://dba2.int.libertyrms.com/>
> Christopher Browne
> (416) 673-4124 (land)
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070518/=
bab06940/attachment.htm
From dmitry at koterov.ru  Fri May 18 00:59:17 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Fri May 18 00:59:32 2007
Subject: [Slony1-general] How to determine if the slave is up to date?
In-Reply-To: <d7df81620705161350o3a291ceah60d74b71708c5564@mail.gmail.com>
References: <d7df81620705160357n47fa5f6fk31cf8c1be7bb9915@mail.gmail.com>
	<603b1wvq3s.fsf@dba2.int.libertyrms.com>
	<d7df81620705161350o3a291ceah60d74b71708c5564@mail.gmail.com>
Message-ID: <d7df81620705180059s6367914es4b6c7d901200d1b@mail.gmail.com>

And another question: is it possible to determine the slave status from a
SLAVE, not from the master?
So, I need an analog of

select min(st_last_received) from _cluster.sl_status where st_received =3D
SLAVE_ID

query, but performed on a SLAVE, not on Master.


On 5/17/07, Dmitry Koterov <dmitry@koterov.ru> wrote:
>
> Thanks!
> Seems it is what I was looking for:
>
> 1. Current state of the Master:
> master# STATE :=3D (select st_last_event from _cluster.sl_status limit 1)
>
> 2. Current slave ID:
> slave# SLAVE_ID :=3D (select _cluster.getlocalnodeid('_cluster'));
>
> 3. If the slave is more up-to-date than master:
> master# (select min(st_last_received) from _cluster.sl_status where
> st_received =3D SLAVE_ID) >=3D STATE
>
>
> On 5/16/07, Christopher Browne <cbbrowne@ca.afilias.info> wrote:
> >
> > "Dmitry Koterov" <dmitry@koterov.ru> writes:
> > > Hello.
> > > Is it possible to determine that a slave has up-to-date status
> > according to the master's status at some time?
> > > E.g. I want to post some update to the master at the moment A,
> > remember its "up-to-date counter" and a second or two later (moment B) -
> > check if the slave has greater
> > > or equal value of this counter. If so, I suppose than at the moment B
> > slave is "up-to-date" relative to the master at the moment A.
> > > In previous Slony version I could use "select last_value from
> > "_moikrug_cluster".sl_log_status" to fetch such counter from the master=
, but
> > in the recent Slony version
> > > this counter seems to be always zero.
> > > I understant that I could implement this functionality manually
> > (create a sequence and increment it on the master with later checking a=
t the
> > slave), but possibly Slony
> > > has already such sequence implemented?
> >
> > Look at the view "sl_status", on the origin node for a given set.
> >
> > The view lists what has been confirmed by all the other nodes, and
> > even includes a count of how many events each subscriber is behind.
> > --
> > output =3D ("cbbrowne" "@" "ca.afilias.info")
> > <http://dba2.int.libertyrms.com/>
> > Christopher Browne
> > (416) 673-4124 (land)
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070518/=
359b81d0/attachment.htm
From pkalva at livedatagroup.com  Fri May 18 13:15:22 2007
From: pkalva at livedatagroup.com (Pallav Kalva)
Date: Fri May 18 13:15:45 2007
Subject: [Slony1-general] Slony Switchover
Message-ID: <464E095A.406@livedatagroup.com>

Hi,

    I am in process of upgrading my database from postgres8.0 to
postgres8.2 using slony. I have simple master to slave setup.
   
    Once slave is in sync with the master what is the process of
switchover to slave cleanly so that I can start all my applications
point to the new slave db ?

    At present we are going slony route to make our upgrade process
quick, so after the upgrade we will delete the postgres8.0 db.

   I am not sure if I need to do switchover or failover for this process ?

 
Thanks!
Pallav.

From plk.zuber at gmail.com  Fri May 18 16:30:07 2007
From: plk.zuber at gmail.com (=?UTF-8?Q?Filip_Rembia=C5=82kowski?=)
Date: Fri May 18 16:30:18 2007
Subject: [Slony1-general] Slony Switchover
In-Reply-To: <464E095A.406@livedatagroup.com>
References: <464E095A.406@livedatagroup.com>
Message-ID: <92869e660705181630o546983c8maf40b42644879179@mail.gmail.com>

2007/5/18, Pallav Kalva <pkalva@livedatagroup.com>:
> Hi,
>
>     I am in process of upgrading my database from postgres8.0 to
> postgres8.2 using slony. I have simple master to slave setup.
>
>     Once slave is in sync with the master what is the process of
> switchover to slave cleanly so that I can start all my applications
> point to the new slave db ?

Have you read <http://slony.info/documentation/failover.html>?

Exactly as said there, use MOVE SET to change origin, then reconfigure apps.

IMHO, after the switchover it's good to leave replication running for
some time, for two reasons:
1) in case of problems you can switch back quickly
2) Slony will guard replicated tables on your "retired" server from
being UPDATE'd by misconfigured apps.


>     At present we are going slony route to make our upgrade process
> quick, so after the upgrade we will delete the postgres8.0 db.

remember to stop new db from replicating; you can use UNINSTALL NODE
http://slony.info/documentation/stmtuninstallnode.html

>
>    I am not sure if I need to do switchover or failover for this process ?

This is switchover. Failover assumes that other server cannot come back ever.




-- 
Filip Rembia?kowski
From dbarthel at usedeverywhere.com  Fri May 18 21:40:19 2007
From: dbarthel at usedeverywhere.com (Don Barthel)
Date: Fri May 18 21:40:31 2007
Subject: [Slony1-general] Version mismatch issues in switchover
Message-ID: <93f47c720705182140h65a9b4aewf4c460dd9cc5b78c@mail.gmail.com>

I've read about switchover here:
http://slony.info/documentation/failover.html and it all makes sense.
But I've also read that running Slony between two machines with
different Postgresql version and different Slony versions is
problematic.

Let's say I have Postgresql 8.0.7 with Slony 1.1.5 (which I have) on a
two node setup and I want to migrate to a spanking new server with the
latest Postgresql with Slony, and let's say I want to use Slony beyond
the migration.

Can I run mismatched versions for the migration?

What are the implications?

Thanks, this list is great, and Slony is great.

- Don Barthel
From jason at buberel.org  Fri May 18 21:43:57 2007
From: jason at buberel.org (Jason L. Buberel)
Date: Fri May 18 21:44:12 2007
Subject: [Slony1-general] Re: Version mismatch issues in switchover
In-Reply-To: <93f47c720705182140h65a9b4aewf4c460dd9cc5b78c@mail.gmail.com>
References: <93f47c720705182140h65a9b4aewf4c460dd9cc5b78c@mail.gmail.com>
Message-ID: <464E808D.7020404@buberel.org>

Don,

I am successfully running a slony cluster that contains two pg-8.2.4 =

nodes and one pg-8.1.4 node without problems. Infact, there is a HOWTO =

entry for performing postgres updates in just this manner.

However, slony insists that the version of slony installed and running =

on all nodes is the same.

-jason

Don Barthel wrote:
> I've read about switchover here:
> http://slony.info/documentation/failover.html and it all makes sense.
> But I've also read that running Slony between two machines with
> different Postgresql version and different Slony versions is
> problematic.
>
> Let's say I have Postgresql 8.0.7 with Slony 1.1.5 (which I have) on a
> two node setup and I want to migrate to a spanking new server with the
> latest Postgresql with Slony, and let's say I want to use Slony beyond
> the migration.
>
> Can I run mismatched versions for the migration?
>
> What are the implications?
>
> Thanks, this list is great, and Slony is great.
>
> - Don Barthel
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070518/=
2de75902/attachment.htm
From cbbrowne at ca.afilias.info  Sun May 20 19:03:02 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Sun May 20 19:03:15 2007
Subject: [Slony1-general] Problem
In-Reply-To: <a51917420705192110t257a1a08g46e859e97ccf5236@mail.gmail.com>
	(Andrew And's message of "Sun, 20 May 2007 01:10:29 -0300")
References: <a51917420705192110t257a1a08g46e859e97ccf5236@mail.gmail.com>
Message-ID: <604pm7q6g9.fsf@dba2.int.libertyrms.com>

"Andrew And" <andcop2006@gmail.com> writes:
> ? I have this problem that I show below but I don't know what I could do.?Could someone help me?
>
> ?
>
> <stdin>:4: PGRES_FATAL_ERROR select "_slave".subscribeSet(1, 1, 2, 'f');
>
> - ERROR: insert or update on table "sl_path" violates foreign key constraint "pa_client-no_id-ref" DETAIL:
>
> Key (pa_client)=(2) is not present in table "sl_node". CONTEXT:
>
> SQL statement "insert into "_slave".sl_path (pa_server, pa_client, pa_conninfo, pa_connretry) values ( $1 , $2 , '<event pending>', 10)"
>
> PL/pgSQL function "subscribeset_int" line 53 at SQL statement SQL statement "SELECT "_slave".subscribeSet_int( $1 , $2 , $3 , $4 )"
>
> PL/pgSQL function "subscribeset" line 63 at perform

Well, you cannot set up a subscription unless there is a direct path
in sl_path from the provider to the subscriber.  And a path can't
exist unless the node is aware of both nodess.

Evidently the node isn't even aware of the existence of node #2, so
there's neither a suitable node nor a suitable path.

Perhaps you haven't launched slon processes?
-- 
(reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From smarlowe at g2switchworks.com  Mon May 21 10:17:19 2007
From: smarlowe at g2switchworks.com (Scott Marlowe)
Date: Mon May 21 10:17:35 2007
Subject: [Slony1-general] Version mismatch issues in switchover
In-Reply-To: <93f47c720705182140h65a9b4aewf4c460dd9cc5b78c@mail.gmail.com>
References: <93f47c720705182140h65a9b4aewf4c460dd9cc5b78c@mail.gmail.com>
Message-ID: <4651D41F.2050904@g2switchworks.com>

Don Barthel wrote:
> I've read about switchover here:
> http://slony.info/documentation/failover.html and it all makes sense.
> But I've also read that running Slony between two machines with
> different Postgresql version and different Slony versions is
> problematic.
>
> Let's say I have Postgresql 8.0.7 with Slony 1.1.5 (which I have) on a
> two node setup and I want to migrate to a spanking new server with the
> latest Postgresql with Slony, and let's say I want to use Slony beyond
> the migration.
>
> Can I run mismatched versions for the migration?
>
> What are the implications?
>
> Thanks, this list is great, and Slony is great.

Actually, that was one of the primary things Slony was designed to let 
you do, migrate.

Note that you need to pay attention to the same issues that you would if 
you were migrating from one version of pgsql to another.  I.e. read the 
release notes for the new version to spot any obvious things that might 
affect you.  One of the most common with 8.2.x is that checking of 
values for certain encodings are now checked more closely, so some data 
that might work in an older version of pgsql might not insert into 8.2.x
From kevin at kevinkempterllc.com  Mon May 21 17:01:45 2007
From: kevin at kevinkempterllc.com (Kevin Kempter)
Date: Mon May 21 17:01:33 2007
Subject: [Slony1-general] Question(s) about replication functionality
Message-ID: <200705211801.45520.kevin@kevinkempterllc.com>

Hi List;

Just curious - can SLONY perform the following?

1) replication of specified columns within a table (i.e. we don't want to 
replicate the entire table per columns) ?

2) replication based on a where condition (i.e. we want to replicate a subset 
of the data based on where clauses for each table) ?




From kevin at kevinkempterllc.com  Mon May 21 17:04:28 2007
From: kevin at kevinkempterllc.com (Kevin Kempter)
Date: Mon May 21 17:04:15 2007
Subject: [Slony1-general] replication of views?
Message-ID: <200705211804.28124.kevin@kevinkempterllc.com>

Hi List;

Can I replicate views with SLONY ?
From shoaibmir at gmail.com  Mon May 21 17:06:11 2007
From: shoaibmir at gmail.com (Shoaib Mir)
Date: Mon May 21 17:06:21 2007
Subject: [Slony1-general] Question(s) about replication functionality
In-Reply-To: <200705211801.45520.kevin@kevinkempterllc.com>
References: <200705211801.45520.kevin@kevinkempterllc.com>
Message-ID: <bf54be870705211706nd9da8e7la8c481724952cd9e@mail.gmail.com>

I am not very sure if you can do this with Slony, but as much of I know I
don't think its possible at column level.

For you scenario write your own triggers using dblink functionality and that
will help you get this all done with out a problem.

--
Shoaib Mir
EnterpriseDB (www.enterprisedb.com)

On 5/21/07, Kevin Kempter <kevin@kevinkempterllc.com> wrote:
>
> Hi List;
>
> Just curious - can SLONY perform the following?
>
> 1) replication of specified columns within a table (i.e. we don't want to
> replicate the entire table per columns) ?
>
> 2) replication based on a where condition (i.e. we want to replicate a
> subset
> of the data based on where clauses for each table) ?
>
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070521/=
ace0c8ec/attachment.htm
From shoaibmir at gmail.com  Mon May 21 17:09:03 2007
From: shoaibmir at gmail.com (Shoaib Mir)
Date: Mon May 21 17:09:13 2007
Subject: [Slony1-general] replication of views?
In-Reply-To: <200705211804.28124.kevin@kevinkempterllc.com>
References: <200705211804.28124.kevin@kevinkempterllc.com>
Message-ID: <bf54be870705211709u5d3feaf9m4d1d10571a7f848a@mail.gmail.com>

We do not have the native materialized views support so they are like stored
as queries.

If you have the table data properly replicated between tables then that
means the views will also have the same data on both database server nodes.

--
Shoaib Mir
EnterpriseDB (www.enterprisedb.com)

On 5/21/07, Kevin Kempter <kevin@kevinkempterllc.com> wrote:
>
> Hi List;
>
> Can I replicate views with SLONY ?
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070521/=
c4d5f929/attachment.htm
From cbbrowne at ca.afilias.info  Mon May 21 19:41:39 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon May 21 19:41:53 2007
Subject: [Slony1-general] replication of views?
In-Reply-To: <200705211804.28124.kevin@kevinkempterllc.com> (Kevin Kempter's
	message of "Mon, 21 May 2007 18:04:28 -0600")
References: <200705211804.28124.kevin@kevinkempterllc.com>
Message-ID: <60abvxpokc.fsf@dba2.int.libertyrms.com>

Kevin Kempter <kevin@kevinkempterllc.com> writes:
> Can I replicate views with SLONY ?

Is a VIEW a table in which you can INSERT/DELETE/UPDATE tuples?

No, it is not.

Slony-I replicates table data.  If you have a view on a replicated
table, there will be no need to worry about replicating the view; the
underlying data will be replicated.
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From cbbrowne at ca.afilias.info  Mon May 21 19:43:23 2007
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon May 21 19:43:34 2007
Subject: [Slony1-general] Question(s) about replication functionality
In-Reply-To: <200705211801.45520.kevin@kevinkempterllc.com> (Kevin Kempter's
	message of "Mon, 21 May 2007 18:01:45 -0600")
References: <200705211801.45520.kevin@kevinkempterllc.com>
Message-ID: <60646lpohg.fsf@dba2.int.libertyrms.com>

Kevin Kempter <kevin@kevinkempterllc.com> writes:
> Just curious - can SLONY perform the following?
>
> 1) replication of specified columns within a table (i.e. we don't want to 
> replicate the entire table per columns) ?
>
> 2) replication based on a where condition (i.e. we want to replicate a subset 
> of the data based on where clauses for each table) ?

No, neither of these things has been implemented.

Something somewhat like these things ought to be possible, but you
might have better results by replicating the tables, and then
accomplishing the restrictions via a view.
-- 
output = ("cbbrowne" "@" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
From kpakalenko at gmail.com  Tue May 22 04:44:12 2007
From: kpakalenko at gmail.com (Konstantin Pakalenko)
Date: Tue May 22 04:44:28 2007
Subject: [Slony1-general] pg_dump, slony1_dump, sanity check
Message-ID: <1913126532.20070522144412@gmail.com>


  Hey guys,

  trying to backup Slave DB but unfortunately get a sanity check
  problem like this one ...

pg_dump: failed sanity check, parent table OID 3441170 of pg_rewrite entry OID 55594 not found
pg_dump: *** aborted because of error

  found some solutions which offered using slony1_dump.sh .. got ir
  running .. but i can't find the file or smthn in which it dumps all
  information ...

  and maybe there is anore solution to solve sanity check problem on
  slave DB when using slony.

  thnx alot
  Kostas

-- 
Su Pagarba,
Konstantin Pakalenko 
+37065502589
kpakalenko@gmail.com

From ajs at crankycanuck.ca  Tue May 22 08:20:16 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue May 22 08:20:34 2007
Subject: [Slony1-general] pg_dump, slony1_dump, sanity check
In-Reply-To: <1913126532.20070522144412@gmail.com>
References: <1913126532.20070522144412@gmail.com>
Message-ID: <20070522152016.GS22797@phlogiston.dyndns.org>

On Tue, May 22, 2007 at 02:44:12PM +0300, Konstantin Pakalenko wrote:
> 
> pg_dump: failed sanity check, parent table OID 3441170 of pg_rewrite entry OID 55594 not found
> pg_dump: *** aborted because of error

Right.  You can't use pg_dump on a database that has any replicated
table in it.

>   found some solutions which offered using slony1_dump.sh .. got ir
>   running .. but i can't find the file or smthn in which it dumps all
>   information ...

According to the comments in the script, it dumps to standard out. 
If you want a file, you have to redirect.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
A certain description of men are for getting out of debt, yet are
against all taxes for raising money to pay it off.
		--Alexander Hamilton
From dmitry at koterov.ru  Tue May 22 10:05:56 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Tue May 22 10:06:05 2007
Subject: [Slony1-general] Where Slony stores really changed data?
Message-ID: <d7df81620705221005w36e0e6e5nbeb18f41f6e70301@mail.gmail.com>

Hello.

Assume I perform an UPDATE for some replicated table: UPDATE tbl SET
field=3D'abcd' WHERE id=3D10. A Slony master creates an event for this upda=
te,
and each slave pulls this event and performs the replication process. The
question is: where Slony really stores an information about what data was
changed and to what it was changed?

I browsed sl_event table (and all other), but did not find any data field
containing the real "abcd" string. Seems there is no table inside the Slony
schema which holds that string "abcd". Strange! But how could it be? How
slaves know which data has to be updated while a replication process?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070522/=
8884817c/attachment.htm
From ajs at crankycanuck.ca  Tue May 22 10:17:06 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue May 22 10:17:17 2007
Subject: [Slony1-general] Where Slony stores really changed data?
In-Reply-To: <d7df81620705221005w36e0e6e5nbeb18f41f6e70301@mail.gmail.com>
References: <d7df81620705221005w36e0e6e5nbeb18f41f6e70301@mail.gmail.com>
Message-ID: <20070522171706.GC23486@phlogiston.dyndns.org>

On Tue, May 22, 2007 at 09:05:56PM +0400, Dmitry Koterov wrote:

> question is: where Slony really stores an information about what data was
> changed and to what it was changed?

sl_log_[12].  Only one of them is active at a given time.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
Unfortunately reformatting the Internet is a little more painful 
than reformatting your hard drive when it gets out of whack.
		--Scott Morris
From dmitry at koterov.ru  Wed May 23 07:17:55 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Wed May 23 07:17:59 2007
Subject: [Slony1-general] How to obtain a detailed error message from slonik?
Message-ID: <d7df81620705230717h7da172fey5179939071f45def@mail.gmail.com>

Hello.

When an error happened in the SQL script executed via slonik, it prints only

DDL Statement failed - PGRES_FATAL_ERROR

How to print the full error message including the database answer, e.g.
"ERROR:  syntax error at or near "sss" at character 3"? Is it possible?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070523/=
f4ad44e3/attachment.htm
From ajs at crankycanuck.ca  Wed May 23 07:36:24 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed May 23 07:36:39 2007
Subject: [Slony1-general] How to obtain a detailed error message from
	slonik?
In-Reply-To: <d7df81620705230717h7da172fey5179939071f45def@mail.gmail.com>
References: <d7df81620705230717h7da172fey5179939071f45def@mail.gmail.com>
Message-ID: <20070523143624.GC26357@phlogiston.dyndns.org>

On Wed, May 23, 2007 at 06:17:55PM +0400, Dmitry Koterov wrote:
> 
> How to print the full error message including the database answer, e.g.
> "ERROR:  syntax error at or near "sss" at character 3"? Is it possible?

Check your postgres logs, which can log this if your logging settings
are correct.  (But yes, more detail in this error message might be
nice.)

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
Unfortunately reformatting the Internet is a little more painful 
than reformatting your hard drive when it gets out of whack.
		--Scott Morris
From dmitry at koterov.ru  Wed May 23 13:32:35 2007
From: dmitry at koterov.ru (Dmitry Koterov)
Date: Wed May 23 13:32:44 2007
Subject: [Slony1-general] How to obtain a detailed error message from
	slonik?
In-Reply-To: <20070523143624.GC26357@phlogiston.dyndns.org>
References: <d7df81620705230717h7da172fey5179939071f45def@mail.gmail.com>
	<20070523143624.GC26357@phlogiston.dyndns.org>
Message-ID: <d7df81620705231332i7c6277c5he35af93788377e66@mail.gmail.com>

Postgres logs are situated on a different machine(s), I run slonik from a
separate host without Postgres installed.

So, I cannot make slonik to show detailed error messages to the console,
correct? It seems a little strange, because slonik itself receives such
messages from the server when it runs a query...

On 5/23/07, Andrew Sullivan <ajs@crankycanuck.ca> wrote:
>
> On Wed, May 23, 2007 at 06:17:55PM +0400, Dmitry Koterov wrote:
> >
> > How to print the full error message including the database answer, e.g.
> > "ERROR:  syntax error at or near "sss" at character 3"? Is it possible?
>
> Check your postgres logs, which can log this if your logging settings
> are correct.  (But yes, more detail in this error message might be
> nice.)
>
> A
>
> --
> Andrew Sullivan  | ajs@crankycanuck.ca
> Unfortunately reformatting the Internet is a little more painful
> than reformatting your hard drive when it gets out of whack.
>                 --Scott Morris
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070524/=
378bf93e/attachment.htm
From ajs at crankycanuck.ca  Wed May 23 14:13:42 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed May 23 14:14:10 2007
Subject: [Slony1-general] How to obtain a detailed error message from
	slonik?
In-Reply-To: <d7df81620705231332i7c6277c5he35af93788377e66@mail.gmail.com>
References: <d7df81620705230717h7da172fey5179939071f45def@mail.gmail.com>
	<20070523143624.GC26357@phlogiston.dyndns.org>
	<d7df81620705231332i7c6277c5he35af93788377e66@mail.gmail.com>
Message-ID: <20070523211342.GD27381@phlogiston.dyndns.org>

On Thu, May 24, 2007 at 12:32:35AM +0400, Dmitry Koterov wrote:
> Postgres logs are situated on a different machine(s), I run slonik from a
> separate host without Postgres installed.
> 
> So, I cannot make slonik to show detailed error messages to the console,
> correct? It seems a little strange, because slonik itself receives such
> messages from the server when it runs a query...

The server will still log the query that caused the problem if it's
configured to do so.  But yes, I think it'd be nice to have some way
of telling slonik "echo all the error messages if you get them". 
AFAIR there isn't a way to do that.

A

-- 
Andrew Sullivan  | ajs@crankycanuck.ca
This work was visionary and imaginative, and goes to show that visionary
and imaginative work need not end up well. 
		--Dennis Ritchie
From aaron.randall at visionoss.com  Mon May 28 07:48:45 2007
From: aaron.randall at visionoss.com (Aaron Randall)
Date: Mon May 28 07:46:13 2007
Subject: [Slony1-general] How to rotate Slony log files
Message-ID: <465AEBCD.1060704@visionoss.com>

Hi All,

I was wondering if anyone knew of a way of using logrotate to handle the 
Slony log files?  The standard Slony config gives each log file a 
different name which includes the date. This doesn't work with logrotate 
which expects the log to have a consistent and well known name.

Many thanks,

Aaron
From mdavidson at cctus.com  Tue May 29 07:15:05 2007
From: mdavidson at cctus.com (Melvin Davidson)
Date: Tue May 29 07:15:01 2007
Subject: [Slony1-general] RE: How to rotate Slony log files
In-Reply-To: <20070528190004.33AE9290BF2@main.slony.info>
Message-ID: <2CC69F840555CB43B04195F218CCB57F8F599F@COENGEX01.cctus.com>

I use a manual script to rotate the logs. Basically it's a simple stop,
rename and restart,
the contents of which is below:

echo Stopping and Restarting slon 1
date
pkill slon
sleep 1
SAVELOG=slon1.log`date +%C%y%m%d%H%M%S`
mv slon1.log $SAVELOG
/home/pgsql/slony/app1/start_node_1_bg.sh

Note: I have a separate script for the master and each slave.
ie: on master, the log name is slon1.log, 
    on slave 101..slon101.log
    ...
    on slave 301..slon301.log


-----Original Message-----
From: slony1-general-bounces@lists.slony.info
[mailto:slony1-general-bounces@lists.slony.info] On Behalf Of
slony1-general-request@lists.slony.info
Sent: Monday, May 28, 2007 1:00 PM
To: slony1-general@lists.slony.info
Subject: Slony1-general Digest, Vol 3, Issue 23

Send Slony1-general mailing list submissions to
	slony1-general@lists.slony.info

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.slony.info/mailman/listinfo/slony1-general
or, via email, send a message with subject or body 'help' to
	slony1-general-request@lists.slony.info

You can reach the person managing the list at
	slony1-general-owner@lists.slony.info

When replying, please edit your Subject line so it is more specific than
"Re: Contents of Slony1-general digest..."


Today's Topics:

   1. How to rotate Slony log files (Aaron Randall)


----------------------------------------------------------------------

Message: 1
Date: Mon, 28 May 2007 14:48:45 +0000
From: Aaron Randall <aaron.randall@visionoss.com>
Subject: [Slony1-general] How to rotate Slony log files
To: slony1-general@lists.slony.info
Message-ID: <465AEBCD.1060704@visionoss.com>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Hi All,

I was wondering if anyone knew of a way of using logrotate to handle the
Slony log files?  The standard Slony config gives each log file a
different name which includes the date. This doesn't work with logrotate
which expects the log to have a consistent and well known name.

Many thanks,

Aaron


------------------------------

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general


End of Slony1-general Digest, Vol 3, Issue 23
*********************************************
From bnichols at ca.afilias.info  Tue May 29 07:57:46 2007
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Tue May 29 07:57:52 2007
Subject: [Slony1-general] How to rotate Slony log files
In-Reply-To: <465AEBCD.1060704@visionoss.com>
References: <465AEBCD.1060704@visionoss.com>
Message-ID: <1180450666.22088.128.camel@dba5.int.libertyrms.com>

On Mon, 2007-05-28 at 14:48 +0000, Aaron Randall wrote:
> Hi All,
> 
> I was wondering if anyone knew of a way of using logrotate to handle the 
> Slony log files?  The standard Slony config gives each log file a 
> different name which includes the date. This doesn't work with logrotate 
> which expects the log to have a consistent and well known name.

We use Cfengine to do it.  Before that, we used Apache's rotatelogs.

-- 
Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.

From lavalamp at spiritual-machines.org  Tue May 29 09:10:02 2007
From: lavalamp at spiritual-machines.org (Brian A. Seklecki)
Date: Tue May 29 09:10:12 2007
Subject: [Slony1-general] How to rotate Slony log files
In-Reply-To: <465AEBCD.1060704@visionoss.com>
References: <465AEBCD.1060704@visionoss.com>
Message-ID: <20070529120823.G5929@arbitor.digitalfreaks.org>


Don't burn your time.  Just use syslog and a local facility.

   syslog 2
   syslog_facility LOCAL2


~BAS

On Mon, 28 May 2007, Aaron Randall wrote:

> Hi All,
>
> I was wondering if anyone knew of a way of using logrotate to handle the 
> Slony log files?  The standard Slony config gives each log file a different 
> name which includes the date. This doesn't work with logrotate which expects 
> the log to have a consistent and well known name.
>
> Many thanks,
>
> Aaron
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>

l8*
 	-lava (Brian A. Seklecki - Pittsburgh, PA, USA)
 	       http://www.spiritual-machines.org/

     "Guilty? Yeah. But he knows it. I mean, you're guilty.
     You just don't know it. So who's really in jail?"
     ~James Maynard Keenan

From andrew.george.hammond at gmail.com  Tue May 29 11:42:20 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Tue May 29 11:42:28 2007
Subject: [Slony1-general] How to rotate Slony log files
In-Reply-To: <465AEBCD.1060704@visionoss.com>
References: <465AEBCD.1060704@visionoss.com>
Message-ID: <5a0a9d6f0705291142i35764094vdaf5a05e5b1276e2@mail.gmail.com>

The slon_mkservice.sh script in HEAD pipes the output of slons through DJB's
multilog. This is yet another solution to the problem you're talking about.
logrotate, while standard on a number of systems, is a poor solution IMHO.

Andrew


On 5/28/07, Aaron Randall <aaron.randall@visionoss.com> wrote:
>
> Hi All,
>
> I was wondering if anyone knew of a way of using logrotate to handle the
> Slony log files?  The standard Slony config gives each log file a
> different name which includes the date. This doesn't work with logrotate
> which expects the log to have a consistent and well known name.
>
> Many thanks,
>
> Aaron
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070529/=
e7cf1ede/attachment.htm
From andrew.george.hammond at gmail.com  Tue May 29 12:03:30 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Tue May 29 12:03:38 2007
Subject: [Slony1-general] Version mismatch issues in switchover
In-Reply-To: <93f47c720705182140h65a9b4aewf4c460dd9cc5b78c@mail.gmail.com>
References: <93f47c720705182140h65a9b4aewf4c460dd9cc5b78c@mail.gmail.com>
Message-ID: <5a0a9d6f0705291203s66f0e6cek723f43109b1dc27c@mail.gmail.com>

On 5/18/07, Don Barthel <dbarthel@usedeverywhere.com> wrote:
>
> I've read about switchover here:
> http://slony.info/documentation/failover.html and it all makes sense.
> But I've also read that running Slony between two machines with
> different Postgresql version and different Slony versions is
> problematic.
>

Different PostgreSQL versions is relatively trivial and supported under
slony. As another reply mentions, there may be issues with data which is
more closely checked in 8.2. However this isn't slony specific, and you'd be
crazy not to test your application against the newer version of before
implementing a major version upgrade.

Running different versions of slony between two machines is not supported.
It might be made to work, but it'd be a silly thing to do. Of course, you
don't need to upgrade slony at the same time you upgrade PostgreSQL. I'd
suggest you upgrade slony first (although personally I'd wait for 1.2.10),
and concurrently test your application against 8.2. Then subscribe your new
8.2 system and monitor it for a while until you're comfortable with running
it as a master. Finally, move the set and re-point your application during a
maintenance window.

Andrew
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070529/=
8db760e4/attachment.htm
From andrew.george.hammond at gmail.com  Tue May 29 12:07:15 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Tue May 29 12:07:23 2007
Subject: [Slony1-general] Question(s) about replication functionality
In-Reply-To: <60646lpohg.fsf@dba2.int.libertyrms.com>
References: <200705211801.45520.kevin@kevinkempterllc.com>
	<60646lpohg.fsf@dba2.int.libertyrms.com>
Message-ID: <5a0a9d6f0705291207p3f00e8dek484c312408c38188@mail.gmail.com>

On 5/21/07, Christopher Browne <cbbrowne@ca.afilias.info> wrote:
>
> Kevin Kempter <kevin@kevinkempterllc.com> writes:
> > Just curious - can SLONY perform the following?
> >
> > 1) replication of specified columns within a table (i.e. we don't want
> to
> > replicate the entire table per columns) ?
> >
> > 2) replication based on a where condition (i.e. we want to replicate a
> subset
> > of the data based on where clauses for each table) ?
>
> No, neither of these things has been implemented.
>
> Something somewhat like these things ought to be possible, but you
> might have better results by replicating the tables, and then
> accomplishing the restrictions via a view.


PostgreSQL supports table partitioning. Slony replicates tables. Partition
your table based on the where condition and then replicate only the
appropriate child table.

To replicate only specified columns, consider a materialized view. However
be aware that you will pay a non-trivial price on the origin system.

Andrew
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070529/=
d3ea4832/attachment.htm
From pergesu at gmail.com  Tue May 29 12:27:22 2007
From: pergesu at gmail.com (Pat Maddox)
Date: Tue May 29 12:27:34 2007
Subject: [Slony1-general] How to rotate Slony log files
In-Reply-To: <5a0a9d6f0705291142i35764094vdaf5a05e5b1276e2@mail.gmail.com>
References: <465AEBCD.1060704@visionoss.com>
	<5a0a9d6f0705291142i35764094vdaf5a05e5b1276e2@mail.gmail.com>
Message-ID: <810a540e0705291227wa44c658t92271847b24087c0@mail.gmail.com>

Why is logrotate a poor solution?  I'm using logrotate with the
copytruncate setting and it works fine.

Pat


On 5/29/07, Andrew Hammond <andrew.george.hammond@gmail.com> wrote:
> The slon_mkservice.sh script in HEAD pipes the output of slons through DJB's
> multilog. This is yet another solution to the problem you're talking about.
> logrotate, while standard on a number of systems, is a poor solution IMHO.
>
> Andrew
>
>
>
> On 5/28/07, Aaron Randall <aaron.randall@visionoss.com> wrote:
> > Hi All,
> >
> > I was wondering if anyone knew of a way of using logrotate to handle the
> > Slony log files?  The standard Slony config gives each log file a
> > different name which includes the date. This doesn't work with logrotate
> > which expects the log to have a consistent and well known name.
> >
> > Many thanks,
> >
> > Aaron
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general@lists.slony.info
> >
> http://lists.slony.info/mailman/listinfo/slony1-general
> >
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
From pergesu at gmail.com  Wed May 30 16:53:44 2007
From: pergesu at gmail.com (Pat Maddox)
Date: Wed May 30 16:53:54 2007
Subject: [Slony1-general] How do I remove all traces of slony from my
	database?
Message-ID: <810a540e0705301653wb29339fw5510787afeb9878a@mail.gmail.com>

I set up a db cluster and now I want to stop the replication.  I ran
"uninstall node" on both nodes and stopped the slony daemons.  Is that
all I need to do or is there anything else I need to do?  I want there
to be no traces of slony remaining in my system.

Thanks,
Pat
From wmoran at collaborativefusion.com  Wed May 30 18:00:23 2007
From: wmoran at collaborativefusion.com (Bill Moran)
Date: Wed May 30 18:00:40 2007
Subject: [Slony1-general] How do I remove all traces of slony from my
	database?
In-Reply-To: <810a540e0705301653wb29339fw5510787afeb9878a@mail.gmail.com>
References: <810a540e0705301653wb29339fw5510787afeb9878a@mail.gmail.com>
Message-ID: <20070530210023.522eef45.wmoran@collaborativefusion.com>

"Pat Maddox" <pergesu@gmail.com> wrote:
>
> I set up a db cluster and now I want to stop the replication.  I ran
> "uninstall node" on both nodes and stopped the slony daemons.  Is that
> all I need to do or is there anything else I need to do?  I want there
> to be no traces of slony remaining in my system.

The only thing your missing is deinstalling the Slony binaries, which
is the job of whatever packaging system you use.

-- 
Bill Moran
Collaborative Fusion Inc.

wmoran@collaborativefusion.com
Phone: 412-422-3463x4023

****************************************************************
IMPORTANT: This message contains confidential information
and is intended only for the individual named. If the reader of
this message is not an intended recipient (or the individual
responsible for the delivery of this message to an intended
recipient), please be advised that any re-use, dissemination,
distribution or copying of this message is prohibited. Please
notify the sender immediately by e-mail if you have received
this e-mail by mistake and delete this e-mail from your system.
E-mail transmission cannot be guaranteed to be secure or
error-free as information could be intercepted, corrupted, lost,
destroyed, arrive late or incomplete, or contain viruses. The
sender therefore does not accept liability for any errors or
omissions in the contents of this message, which arise as a
result of e-mail transmission.
****************************************************************
From open at immo.ru  Wed May 30 22:33:48 2007
From: open at immo.ru (open)
Date: Wed May 30 22:34:11 2007
Subject: [Slony1-general] how can i clean sl_events?
Message-ID: <465E5E3C.1080505@immo.ru>

Hi All :)
I'm using Slony1-1.2.9 and PostgreSQL8.0.11, and i have a problem with 
sl_event,
when i setup the replication (with the slon_tools) everything be OK,
but after then, the engineer of the our partner setup firerwall, and we 
have connection only from slave to master, and no connection from master 
to slave....
at first look with the replication data it's OK, but after the 
reconfigure firewall (when we have connection both side from slave to 
master and from master to slave)
and restart slon daemon, i have such messages in LOGS on SLAVE


May 30 14:31:36 stalingrad slon_mmedia[19197]: [533-1] 2007-05-30 
14:31:36 MSD [19175] ERROR  remoteWorkerThread_3: "notify 
"_multimedia_Event"; notify "_multimedia_Confirm"; insert
May 30 14:31:36 stalingrad slon_mmedia[19197]: [533-2]  into 
"_multimedia".sl_event     (ev_origin, ev_seqno, ev_timestamp,      
ev_minxid, ev_maxxid, ev_xip, ev_type     ) values
May 30 14:31:36 stalingrad slon_mmedia[19197]: [533-3]  ('3', '121342', 
'2007-05-30 12:13:12.08808', '6006148', '6006154', '''6006148''', 
'SYNC'); insert into "_multimedia".sl_confirm
May 30 14:31:36 stalingrad slon_mmedia[19197]: [533-4]  ^I(con_origin, 
con_received, con_seqno, con_timestamp)    values (3, 2, '121342', 
now()); notify "_multimedia_Event"; notify
May 30 14:31:36 stalingrad slon_mmedia[19197]: [533-5]  
"_multimedia_Confirm"; insert into "_multimedia".sl_event     
(ev_origin, ev_seqno, ev_timestamp,      ev_minxid, ev_maxxid,
May 30 14:31:36 stalingrad slon_mmedia[19197]: [533-6]  ev_xip, 
ev_type     ) values ('3', '121343', '2007-05-30 12:13:22.098347', 
'6006184', '6006185', '', 'SYNC'); insert into
May 30 14:31:36 stalingrad slon_mmedia[19197]: [533-7]  
"_multimedia".sl_confirm ^I(con_origin, con_received, con_seqno, 
con_timestamp)    values (3, 2, '121343', now()); notify
May 30 14:31:36 stalingrad slon_mmedia[19197]: [533-8]  
"_multimedia_Event"; notify "_multimedia_Confirm"; insert into 
"_multimedia".sl_event     (ev_origin, ev_seqno, ev_timestamp,
May 30 14:31:36 stalingrad slon_mmedia[19197]: [533-9]     ev_minxid, 
ev_maxxid, ev_xip, ev_type     ) values ('3', '121344', '2007-05-30 
12:13:32.098602', '6006214', '6006227',
May 30 14:31:36 stalingrad slon_mmedia[19197]: [533-10]  '''6006214''', 
'SYNC'); insert into "_multimedia".sl_confirm ^I(con_origin, 
con_received, con_seqno, con_timestamp)    values
May 30 14:31:36 stalingrad slon_mmedia[19197]: [533-11]  (3, 2, 
'121344', now()); commit transaction;" PGRES_FATAL_ERROR ERROR:  
duplicate key violates unique constraint
May 30 14:31:36 stalingrad slon_mmedia[19197]: [533-12]  "sl_event-pkey"
May 30 14:31:36 stalingrad slon_mmedia[19194]: [534-1] 2007-05-30 
14:31:36 MSD [19175] DEBUG2 remoteListenThread_1: queue event 3,121383 SYNC

I'm manually deleting 5-6 rows from sl_event on slave and data 
replication OK,
BUT I HAVE ~300K rows in sl_event on master, and that not cleaning....
i see this on master logs
May 31 07:33:31 vps137 slon_mmedia[7337]: [251775-1] DEBUG3 
cleanupThread: minxid: 86393132
May 31 07:33:33 vps137 slon_mmedia[7337]: [251776-1] DEBUG2 
cleanupThread:    2.622 seconds for vacuuming
May 31 08:09:45 vps137 slon_mmedia[7337]: [260208-1] DEBUG3 
cleanupThread: minxid: 86528109
May 31 08:09:48 vps137 slon_mmedia[7337]: [260217-1] DEBUG2 
cleanupThread:    2.899 seconds for vacuuming
May 31 08:44:27 vps137 slon_mmedia[7337]: [268320-1] DEBUG3 
cleanupThread: minxid: 86717718
May 31 08:44:29 vps137 slon_mmedia[7337]: [268344-1] DEBUG2 
cleanupThread:    3.335 seconds for vacuuming
May 31 09:21:15 vps137 slon_mmedia[7337]: [276864-1] DEBUG3 
cleanupThread: minxid: 86814805
May 31 09:21:18 vps137 slon_mmedia[7337]: [276877-1] DEBUG2 
cleanupThread:    2.771 seconds for vacuuming

I'm right that cleanupThread call PL/SQL function cleanupevent() ???
I'm not understant why the number of rows in sl_event more and more, and 
they not cleanup......

in view sl_status i see the huge lags....

[root@vps135 ~]# psql multimedia postgres -c "select * from 
_multimedia.sl_status"
 st_origin | st_received | st_last_event |      st_last_event_ts      | 
st_last_received |    st_last_received_ts     | 
st_last_received_event_ts  | st_lag_num_events |      st_lag_time      
-----------+-------------+---------------+----------------------------+------------------+----------------------------+----------------------------+-------------------+------------------------
         1 |           2 |        124097 | 2007-05-31 09:30:45.389851 
|           124097 | 2007-05-31 09:30:53.626126 | 2007-05-31 
09:30:45.389851 |                 0 | 00:00:13.345121
         1 |           3 |        124097 | 2007-05-31 09:30:45.389851 
|           124097 | 2007-05-31 09:30:39.250211 | 2007-05-31 
09:30:45.389851 |                 0 | 00:00:13.345121
         1 |           6 |        124097 | 2007-05-31 09:30:45.389851 
|            69838 | 2007-05-24 09:29:13.589914 | 2007-05-24 
09:29:12.580015 |             54259 | 7 days 00:01:46.154957
         1 |           4 |        124097 | 2007-05-31 09:30:45.389851 
|           124097 | 2007-05-31 10:19:44.532959 | 2007-05-31 
09:30:45.389851 |                 0 | 00:00:13.345121
         1 |           5 |        124097 | 2007-05-31 09:30:45.389851 
|           124097 | 2007-05-31 09:30:09.487824 | 2007-05-31 
09:30:45.389851 |                 0 | 00:00:13.345121
(5 rows)

How can i clean sl_status ???

PS sorry for my english.
thanks,
Open












From ajs at crankycanuck.ca  Thu May 31 04:29:50 2007
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu May 31 04:30:20 2007
Subject: [Slony1-general] how can i clean sl_events?
In-Reply-To: <465E5E3C.1080505@immo.ru>
References: <465E5E3C.1080505@immo.ru>
Message-ID: <20070531112950.GD19397@phlogiston.dyndns.org>

On Thu, May 31, 2007 at 09:33:48AM +0400, open wrote:
> when i setup the replication (with the slon_tools) everything be OK,
> but after then, the engineer of the our partner setup firerwall, and we 
> have connection only from slave to master, and no connection from master 
> to slave....

Then it's broken.  Tell the "engineer" to fix the network so that it
works the way it needs to.  It is well-documented that every active
node in a Slony cluster must be able to talk to the other nodes.  As
far as Slony knows, the events haven't arrived on the replica.  If
you fix the firewall, the problem should go away on its own.

A


-- 
Andrew Sullivan  | ajs@crankycanuck.ca
I remember when computers were frustrating because they *did* exactly what 
you told them to.  That actually seems sort of quaint now.
		--J.D. Baldwin
From open at immo.ru  Thu May 31 04:42:50 2007
From: open at immo.ru (open)
Date: Thu May 31 04:43:28 2007
Subject: [Slony1-general] how can i clean sl_events?
In-Reply-To: <20070531112950.GD19397@phlogiston.dyndns.org>
References: <465E5E3C.1080505@immo.ru>
	<20070531112950.GD19397@phlogiston.dyndns.org>
Message-ID: <465EB4BA.3070700@immo.ru>

thx for the answer.

1) the"engineer" fix it yesterday, but it was broken about 6 day.
2) I have 5 slave, and I don't wont (and it's impossible) setup the 
connect between all slaves,
i think that enough have a direct connect between master and each 
slaves, and from each slave  to master.

So now we have connect about a day, but number of event (sl_event) is 
grow....





Andrew Sullivan wrote:
> On Thu, May 31, 2007 at 09:33:48AM +0400, open wrote:
>   
>> when i setup the replication (with the slon_tools) everything be OK,
>> but after then, the engineer of the our partner setup firerwall, and we 
>> have connection only from slave to master, and no connection from master 
>> to slave....
>>     
>
> Then it's broken.  Tell the "engineer" to fix the network so that it
> works the way it needs to.  It is well-documented that every active
> node in a Slony cluster must be able to talk to the other nodes.  As
> far as Slony knows, the events haven't arrived on the replica.  If
> you fix the firewall, the problem should go away on its own.
>
> A
>
>
>   

From JanWieck at Yahoo.com  Thu May 31 06:02:53 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu May 31 06:03:07 2007
Subject: [Slony1-general] how can i clean sl_events?
In-Reply-To: <465EB4BA.3070700@immo.ru>
References: <465E5E3C.1080505@immo.ru>	<20070531112950.GD19397@phlogiston.dyndns.org>
	<465EB4BA.3070700@immo.ru>
Message-ID: <465EC77D.1040807@Yahoo.com>

On 5/31/2007 7:42 AM, open wrote:
> thx for the answer.
> 
> 1) the"engineer" fix it yesterday, but it was broken about 6 day.
> 2) I have 5 slave, and I don't wont (and it's impossible) setup the 
> connect between all slaves,
> i think that enough have a direct connect between master and each 
> slaves, and from each slave  to master.

That would only be true if all subscribers are direct subscribers that 
use the origin as data provider. The problem with this setup is that if 
you ever need to switchover or failover to one of the subscribers, you 
will have to abandon all others (of fix the network connectivity before 
doing so).

> So now we have connect about a day, but number of event (sl_event) is 
> grow....

Check the output of the sl_status view over a couple of minutes. If the 
column st_last_event for any of your subscribers is not advancing, the 
event confirmations from that node still don't make it back to the 
origin and your engineer has more to fix.


Jan

> 
> 
> 
> 
> 
> Andrew Sullivan wrote:
>> On Thu, May 31, 2007 at 09:33:48AM +0400, open wrote:
>>   
>>> when i setup the replication (with the slon_tools) everything be OK,
>>> but after then, the engineer of the our partner setup firerwall, and we 
>>> have connection only from slave to master, and no connection from master 
>>> to slave....
>>>     
>>
>> Then it's broken.  Tell the "engineer" to fix the network so that it
>> works the way it needs to.  It is well-documented that every active
>> node in a Slony cluster must be able to talk to the other nodes.  As
>> far as Slony knows, the events haven't arrived on the replica.  If
>> you fix the firewall, the problem should go away on its own.
>>
>> A
>>
>>
>>   
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From open at immo.ru  Thu May 31 06:44:26 2007
From: open at immo.ru (open)
Date: Thu May 31 06:44:48 2007
Subject: [Slony1-general] how can i clean sl_events?
In-Reply-To: <465EC77D.1040807@Yahoo.com>
References: <465E5E3C.1080505@immo.ru>	<20070531112950.GD19397@phlogiston.dyndns.org>
	<465EB4BA.3070700@immo.ru> <465EC77D.1040807@Yahoo.com>
Message-ID: <465ED13A.9030700@immo.ru>


Yes, i know about this problem, but the mission of my replication is  a  
sync critical data to remote servers,  and the soft on the remote 
servers work with local data.
> That would only be true if all subscribers are direct subscribers that 
> use the origin as data provider. The problem with this setup is that 
> if you ever need to switchover or failover to one of the subscribers, 
> you will have to abandon all others (of fix the network connectivity 
> before doing so).
>
>> So now we have connect about a day, but number of event (sl_event) is 
>> grow....
>
> Check the output of the sl_status view over a couple of minutes. If 
> the column st_last_event for any of your subscribers is not advancing, 
> the event confirmations from that node still don't make it back to the 
> origin and your engineer has more to fix.
.....bad news, sl_status looks nornally....
st_last_event  increases normally....
have you any ideas ?

WBR, open.

[root@vps135 ~]# psql multimedia postgres -c "select * from 
_multimedia.sl_status"
 st_origin | st_received | st_last_event |      st_last_event_ts      | 
st_last_received |    st_last_received_ts     | 
st_last_received_event_ts  | st_lag_num_events |      st_lag_time      
-----------+-------------+---------------+----------------------------+------------------+----------------------------+----------------------------+-------------------+------------------------
         1 |           2 |        125597 | 2007-05-31 17:34:11.315586 
|           125596 | 2007-05-31 17:34:00.400391 | 2007-05-31 
17:33:51.299999 |                 1 | 00:00:39.454171
         1 |           3 |        125597 | 2007-05-31 17:34:11.315586 
|           125597 | 2007-05-31 17:34:12.837879 | 2007-05-31 
17:34:11.315586 |                 0 | 00:00:19.438584
         1 |           6 |        125597 | 2007-05-31 17:34:11.315586 
|            69838 | 2007-05-24 09:29:13.589914 | 2007-05-24 
09:29:12.580015 |             55759 | 7 days 08:05:18.174155
         1 |           4 |        125597 | 2007-05-31 17:34:11.315586 
|           125597 | 2007-05-31 18:23:10.653676 | 2007-05-31 
17:34:11.315586 |                 0 | 00:00:19.438584
         1 |           5 |        125597 | 2007-05-31 17:34:11.315586 
|           125597 | 2007-05-31 17:33:29.585481 | 2007-05-31 
17:34:11.315586 |                 0 | 00:00:19.438584
(5 rows)

[root@vps135 ~]# psql multimedia postgres -c "select * from 
_multimedia.sl_status"
 st_origin | st_received | st_last_event |      st_last_event_ts      | 
st_last_received |    st_last_received_ts     | 
st_last_received_event_ts  | st_lag_num_events |        st_lag_time        
-----------+-------------+---------------+----------------------------+------------------+----------------------------+----------------------------+-------------------+----------------------------
         1 |           2 |        125598 | 2007-05-31 17:34:31.419948 
|           125597 | 2007-05-31 17:34:21.424266 | 2007-05-31 
17:34:11.315586 |                 1 | 00:00:28.263841
         1 |           3 |        125598 | 2007-05-31 17:34:31.419948 
|           125597 | 2007-05-31 17:34:12.837879 | 2007-05-31 
17:34:11.315586 |                 1 | 00:00:28.263841
         1 |           6 |        125598 | 2007-05-31 17:34:31.419948 
|            69838 | 2007-05-24 09:29:13.589914 | 2007-05-24 
09:29:12.580015 |             55760 | 7 days 08:05:26.9994119999
         1 |           4 |        125598 | 2007-05-31 17:34:31.419948 
|           125598 | 2007-05-31 18:23:31.422288 | 2007-05-31 
17:34:31.419948 |                 0 | 00:00:08.159479
         1 |           5 |        125598 | 2007-05-31 17:34:31.419948 
|           125597 | 2007-05-31 17:33:29.585481 | 2007-05-31 
17:34:11.315586 |                 1 | 00:00:28.263841
(5 rows)

[root@vps135 ~]# psql multimedia postgres -c "select * from 
_multimedia.sl_status"
 st_origin | st_received | st_last_event |      st_last_event_ts      | 
st_last_received |    st_last_received_ts     | 
st_last_received_event_ts  | st_lag_num_events |      st_lag_time      
-----------+-------------+---------------+----------------------------+------------------+----------------------------+----------------------------+-------------------+------------------------
         1 |           2 |        125618 | 2007-05-31 17:41:12.190197 
|           125617 | 2007-05-31 17:41:01.543596 | 2007-05-31 
17:40:52.162198 |                 1 | 00:00:31.501594
         1 |           3 |        125618 | 2007-05-31 17:41:12.190197 
|           125618 | 2007-05-31 17:41:04.928925 | 2007-05-31 
17:41:12.190197 |                 0 | 00:00:11.473595
         1 |           6 |        125618 | 2007-05-31 17:41:12.190197 
|            69838 | 2007-05-24 09:29:13.589914 | 2007-05-24 
09:29:12.580015 |             55780 | 7 days 08:12:11.083777
         1 |           4 |        125618 | 2007-05-31 17:41:12.190197 
|           125618 | 2007-05-31 18:30:16.192276 | 2007-05-31 
17:41:12.190197 |                 0 | 00:00:11.473595
         1 |           5 |        125618 | 2007-05-31 17:41:12.190197 
|           125618 | 2007-05-31 17:40:31.441314 | 2007-05-31 
17:41:12.190197 |                 0 | 00:00:11.473595
(5 rows)


From JanWieck at Yahoo.com  Thu May 31 08:52:30 2007
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu May 31 08:52:38 2007
Subject: [Slony1-general] how can i clean sl_events?
In-Reply-To: <465ED13A.9030700@immo.ru>
References: <465E5E3C.1080505@immo.ru>	<20070531112950.GD19397@phlogiston.dyndns.org>	<465EB4BA.3070700@immo.ru>
	<465EC77D.1040807@Yahoo.com> <465ED13A.9030700@immo.ru>
Message-ID: <465EEF3E.70205@Yahoo.com>

On 5/31/2007 9:44 AM, open wrote:
>> Check the output of the sl_status view over a couple of minutes. If 
>> the column st_last_event for any of your subscribers is not advancing, 
>> the event confirmations from that node still don't make it back to the 
>> origin and your engineer has more to fix.
> .....bad news, sl_status looks nornally....
> st_last_event  increases normally....
> have you any ideas ?

Ooops ... my fault ... I meant the column st_received. As it looks to 
me, node 6 is either not replicating, or the confirmations don't make it 
back to the origin. The origin thinks it is stuck on event 69838.


Jan


> 
> WBR, open.
> 
> [root@vps135 ~]# psql multimedia postgres -c "select * from 
> _multimedia.sl_status"
>  st_origin | st_received | st_last_event |      st_last_event_ts      | 
> st_last_received |    st_last_received_ts     | 
> st_last_received_event_ts  | st_lag_num_events |      st_lag_time      
> -----------+-------------+---------------+----------------------------+------------------+----------------------------+----------------------------+-------------------+------------------------
>          1 |           2 |        125597 | 2007-05-31 17:34:11.315586 
> |           125596 | 2007-05-31 17:34:00.400391 | 2007-05-31 
> 17:33:51.299999 |                 1 | 00:00:39.454171
>          1 |           3 |        125597 | 2007-05-31 17:34:11.315586 
> |           125597 | 2007-05-31 17:34:12.837879 | 2007-05-31 
> 17:34:11.315586 |                 0 | 00:00:19.438584
>          1 |           6 |        125597 | 2007-05-31 17:34:11.315586 
> |            69838 | 2007-05-24 09:29:13.589914 | 2007-05-24 
> 09:29:12.580015 |             55759 | 7 days 08:05:18.174155
>          1 |           4 |        125597 | 2007-05-31 17:34:11.315586 
> |           125597 | 2007-05-31 18:23:10.653676 | 2007-05-31 
> 17:34:11.315586 |                 0 | 00:00:19.438584
>          1 |           5 |        125597 | 2007-05-31 17:34:11.315586 
> |           125597 | 2007-05-31 17:33:29.585481 | 2007-05-31 
> 17:34:11.315586 |                 0 | 00:00:19.438584
> (5 rows)
> 
> [root@vps135 ~]# psql multimedia postgres -c "select * from 
> _multimedia.sl_status"
>  st_origin | st_received | st_last_event |      st_last_event_ts      | 
> st_last_received |    st_last_received_ts     | 
> st_last_received_event_ts  | st_lag_num_events |        st_lag_time        
> -----------+-------------+---------------+----------------------------+------------------+----------------------------+----------------------------+-------------------+----------------------------
>          1 |           2 |        125598 | 2007-05-31 17:34:31.419948 
> |           125597 | 2007-05-31 17:34:21.424266 | 2007-05-31 
> 17:34:11.315586 |                 1 | 00:00:28.263841
>          1 |           3 |        125598 | 2007-05-31 17:34:31.419948 
> |           125597 | 2007-05-31 17:34:12.837879 | 2007-05-31 
> 17:34:11.315586 |                 1 | 00:00:28.263841
>          1 |           6 |        125598 | 2007-05-31 17:34:31.419948 
> |            69838 | 2007-05-24 09:29:13.589914 | 2007-05-24 
> 09:29:12.580015 |             55760 | 7 days 08:05:26.9994119999
>          1 |           4 |        125598 | 2007-05-31 17:34:31.419948 
> |           125598 | 2007-05-31 18:23:31.422288 | 2007-05-31 
> 17:34:31.419948 |                 0 | 00:00:08.159479
>          1 |           5 |        125598 | 2007-05-31 17:34:31.419948 
> |           125597 | 2007-05-31 17:33:29.585481 | 2007-05-31 
> 17:34:11.315586 |                 1 | 00:00:28.263841
> (5 rows)
> 
> [root@vps135 ~]# psql multimedia postgres -c "select * from 
> _multimedia.sl_status"
>  st_origin | st_received | st_last_event |      st_last_event_ts      | 
> st_last_received |    st_last_received_ts     | 
> st_last_received_event_ts  | st_lag_num_events |      st_lag_time      
> -----------+-------------+---------------+----------------------------+------------------+----------------------------+----------------------------+-------------------+------------------------
>          1 |           2 |        125618 | 2007-05-31 17:41:12.190197 
> |           125617 | 2007-05-31 17:41:01.543596 | 2007-05-31 
> 17:40:52.162198 |                 1 | 00:00:31.501594
>          1 |           3 |        125618 | 2007-05-31 17:41:12.190197 
> |           125618 | 2007-05-31 17:41:04.928925 | 2007-05-31 
> 17:41:12.190197 |                 0 | 00:00:11.473595
>          1 |           6 |        125618 | 2007-05-31 17:41:12.190197 
> |            69838 | 2007-05-24 09:29:13.589914 | 2007-05-24 
> 09:29:12.580015 |             55780 | 7 days 08:12:11.083777
>          1 |           4 |        125618 | 2007-05-31 17:41:12.190197 
> |           125618 | 2007-05-31 18:30:16.192276 | 2007-05-31 
> 17:41:12.190197 |                 0 | 00:00:11.473595
>          1 |           5 |        125618 | 2007-05-31 17:41:12.190197 
> |           125618 | 2007-05-31 17:40:31.441314 | 2007-05-31 
> 17:41:12.190197 |                 0 | 00:00:11.473595
> (5 rows)
> 
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
#======================================================================#
# It's easier to get forgiveness for being wrong than for being right. #
# Let's break this rule - forgive me.                                  #
#================================================== JanWieck@Yahoo.com #
From andrew.george.hammond at gmail.com  Thu May 31 10:36:18 2007
From: andrew.george.hammond at gmail.com (Andrew Hammond)
Date: Thu May 31 10:36:26 2007
Subject: [Slony1-general] how can i clean sl_events?
In-Reply-To: <465EEF3E.70205@Yahoo.com>
References: <465E5E3C.1080505@immo.ru>
	<20070531112950.GD19397@phlogiston.dyndns.org>
	<465EB4BA.3070700@immo.ru> <465EC77D.1040807@Yahoo.com>
	<465ED13A.9030700@immo.ru> <465EEF3E.70205@Yahoo.com>
Message-ID: <5a0a9d6f0705311036k1a96defycaaf53c53fb46024@mail.gmail.com>

On 5/31/07, Jan Wieck <JanWieck@yahoo.com> wrote:
>
> On 5/31/2007 9:44 AM, open wrote:
> >> Check the output of the sl_status view over a couple of minutes. If
> >> the column st_last_event for any of your subscribers is not advancing,
> >> the event confirmations from that node still don't make it back to the
> >> origin and your engineer has more to fix.
> > .....bad news, sl_status looks nornally....
> > st_last_event  increases normally....
> > have you any ideas ?
>
> Ooops ... my fault ... I meant the column st_received. As it looks to
> me, node 6 is either not replicating, or the confirmations don't make it
> back to the origin. The origin thinks it is stuck on event 69838.


I wonder if that has anything to do with the following.

"I'm manually deleting 5-6 rows from sl_event on slave and data
replication OK,"

Did you delete these rows on node 6 and not on any other node?
Did you keep a copy of the rows you deleted, or at least note their event
number?

Andrew
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20070531/=
d8b09def/attachment.htm
From open at immo.ru  Thu May 31 22:24:21 2007
From: open at immo.ru (open)
Date: Thu May 31 22:24:58 2007
Subject: [Slony1-general] how can i clean sl_events?
In-Reply-To: <5a0a9d6f0705311036k1a96defycaaf53c53fb46024@mail.gmail.com>
References: <465E5E3C.1080505@immo.ru>	
	<20070531112950.GD19397@phlogiston.dyndns.org>	
	<465EB4BA.3070700@immo.ru> <465EC77D.1040807@Yahoo.com>	
	<465ED13A.9030700@immo.ru> <465EEF3E.70205@Yahoo.com>
	<5a0a9d6f0705311036k1a96defycaaf53c53fb46024@mail.gmail.com>
Message-ID: <465FAD85.30900@immo.ru>

Andrew Hammond wrote:
> Did you delete these rows on node 6 and not on any other node?
> Did you keep a copy of the rows you deleted, or at least note their 
> event number?
>
> Andrew

i delete 4 row and i have a copy of the rows, how it's help me ?

From open at immo.ru  Thu May 31 22:38:41 2007
From: open at immo.ru (open)
Date: Thu May 31 22:39:15 2007
Subject: [Slony1-general] how can i clean sl_events?
In-Reply-To: <465EEF3E.70205@Yahoo.com>
References: <465E5E3C.1080505@immo.ru>	<20070531112950.GD19397@phlogiston.dyndns.org>	<465EB4BA.3070700@immo.ru>
	<465EC77D.1040807@Yahoo.com> <465ED13A.9030700@immo.ru>
	<465EEF3E.70205@Yahoo.com>
Message-ID: <465FB0E1.3080900@immo.ru>

...it's wonderful but the replication set data is replicating to node 6.....
...i'm don't understand why the confirmation don't make it back to the 
origin, now node 6 have a connect to node 1 (master)
what can i do that master forget this 1 event and work normally ?


multimedia=# select count(*) from _multimedia.sl_event where ev_origin =1;
 count
-------
 58671
(1 row)

multimedia=# select count(*) from _multimedia.sl_event where ev_origin =2;
 count
-------
 65063
(1 row)

multimedia=# select count(*) from _multimedia.sl_event where ev_origin =3;
 count
-------
 66414
(1 row)

multimedia=# select count(*) from _multimedia.sl_event where ev_origin =4;
 count
-------
 65970
(1 row)

multimedia=# select count(*) from _multimedia.sl_event where ev_origin =5;
 count
-------
 68250
(1 row)

multimedia=# select count(*) from _multimedia.sl_event where ev_origin =6;
 count
-------
     1
(1 row)

multimedia=#







>
> Ooops ... my fault ... I meant the column st_received. As it looks to 
> me, node 6 is either not replicating, or the confirmations don't make 
> it back to the origin. The origin thinks it is stuck on event 69838.
>
>
> Jan
>
>
>>
>> WBR, open.
>>
>> [root@vps135 ~]# psql multimedia postgres -c "select * from 
>> _multimedia.sl_status"
>>  st_origin | st_received | st_last_event |      st_last_event_ts      
>> | st_last_received |    st_last_received_ts     | 
>> st_last_received_event_ts  | st_lag_num_events |      
>> st_lag_time      
>> -----------+-------------+---------------+----------------------------+------------------+----------------------------+----------------------------+-------------------+------------------------ 
>>
>>          1 |           2 |        125597 | 2007-05-31 17:34:11.315586 
>> |           125596 | 2007-05-31 17:34:00.400391 | 2007-05-31 
>> 17:33:51.299999 |                 1 | 00:00:39.454171
>>          1 |           3 |        125597 | 2007-05-31 17:34:11.315586 
>> |           125597 | 2007-05-31 17:34:12.837879 | 2007-05-31 
>> 17:34:11.315586 |                 0 | 00:00:19.438584
>>          1 |           6 |        125597 | 2007-05-31 17:34:11.315586 
>> |            69838 | 2007-05-24 09:29:13.589914 | 2007-05-24 
>> 09:29:12.580015 |             55759 | 7 days 08:05:18.174155
>>          1 |           4 |        125597 | 2007-05-31 17:34:11.315586 
>> |           125597 | 2007-05-31 18:23:10.653676 | 2007-05-31 
>> 17:34:11.315586 |                 0 | 00:00:19.438584
>>          1 |           5 |        125597 | 2007-05-31 17:34:11.315586 
>> |           125597 | 2007-05-31 17:33:29.585481 | 2007-05-31 
>> 17:34:11.315586 |                 0 | 00:00:19.438584
>> (5 rows)
>>
>> [root@vps135 ~]# psql multimedia postgres -c "select * from 
>> _multimedia.sl_status"
>>  st_origin | st_received | st_last_event |      st_last_event_ts      
>> | st_last_received |    st_last_received_ts     | 
>> st_last_received_event_ts  | st_lag_num_events |        
>> st_lag_time        
>> -----------+-------------+---------------+----------------------------+------------------+----------------------------+----------------------------+-------------------+---------------------------- 
>>
>>          1 |           2 |        125598 | 2007-05-31 17:34:31.419948 
>> |           125597 | 2007-05-31 17:34:21.424266 | 2007-05-31 
>> 17:34:11.315586 |                 1 | 00:00:28.263841
>>          1 |           3 |        125598 | 2007-05-31 17:34:31.419948 
>> |           125597 | 2007-05-31 17:34:12.837879 | 2007-05-31 
>> 17:34:11.315586 |                 1 | 00:00:28.263841
>>          1 |           6 |        125598 | 2007-05-31 17:34:31.419948 
>> |            69838 | 2007-05-24 09:29:13.589914 | 2007-05-24 
>> 09:29:12.580015 |             55760 | 7 days 08:05:26.9994119999
>>          1 |           4 |        125598 | 2007-05-31 17:34:31.419948 
>> |           125598 | 2007-05-31 18:23:31.422288 | 2007-05-31 
>> 17:34:31.419948 |                 0 | 00:00:08.159479
>>          1 |           5 |        125598 | 2007-05-31 17:34:31.419948 
>> |           125597 | 2007-05-31 17:33:29.585481 | 2007-05-31 
>> 17:34:11.315586 |                 1 | 00:00:28.263841
>> (5 rows)
>>
>> [root@vps135 ~]# psql multimedia postgres -c "select * from 
>> _multimedia.sl_status"
>>  st_origin | st_received | st_last_event |      st_last_event_ts      
>> | st_last_received |    st_last_received_ts     | 
>> st_last_received_event_ts  | st_lag_num_events |      
>> st_lag_time      
>> -----------+-------------+---------------+----------------------------+------------------+----------------------------+----------------------------+-------------------+------------------------ 
>>
>>          1 |           2 |        125618 | 2007-05-31 17:41:12.190197 
>> |           125617 | 2007-05-31 17:41:01.543596 | 2007-05-31 
>> 17:40:52.162198 |                 1 | 00:00:31.501594
>>          1 |           3 |        125618 | 2007-05-31 17:41:12.190197 
>> |           125618 | 2007-05-31 17:41:04.928925 | 2007-05-31 
>> 17:41:12.190197 |                 0 | 00:00:11.473595
>>          1 |           6 |        125618 | 2007-05-31 17:41:12.190197 
>> |            69838 | 2007-05-24 09:29:13.589914 | 2007-05-24 
>> 09:29:12.580015 |             55780 | 7 days 08:12:11.083777
>>          1 |           4 |        125618 | 2007-05-31 17:41:12.190197 
>> |           125618 | 2007-05-31 18:30:16.192276 | 2007-05-31 
>> 17:41:12.190197 |                 0 | 00:00:11.473595
>>          1 |           5 |        125618 | 2007-05-31 17:41:12.190197 
>> |           125618 | 2007-05-31 17:40:31.441314 | 2007-05-31 
>> 17:41:12.190197 |                 0 | 00:00:11.473595
>> (5 rows)
>>
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>
>

