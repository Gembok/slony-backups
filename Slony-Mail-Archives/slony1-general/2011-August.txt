From mu at forsa.de  Mon Aug  1 06:12:09 2011
From: mu at forsa.de (marmu)
Date: Mon, 1 Aug 2011 06:12:09 -0700 (PDT)
Subject: [Slony1-general] subscribeSet() the receiver does not exist
 receiver id:2
In-Reply-To: <4E316F37.8080105@ca.afilias.info>
References: <32138395.post@talk.nabble.com> <4E316F37.8080105@ca.afilias.info>
Message-ID: <32169512.post@talk.nabble.com>


>> Then I get this message:
>> Starting SLONY slave(fquest)<stdin>:4: NOTICE:  subscribe set:
>> omit_copy=f
>> <stdin>:4: PGRES_FATAL_ERROR select "_fquest".subscribeSet(1, 1, 2, 'f',
>> 'f');  - ERROR:  Slony-I: subscribeSet() the receiver does not exist
>> receiver id:2

>You need to add something like

>store node(id=2, event node=1);

>before the subscribe set command in your
>fquest-slave-setupmaker script

thanks a lot, this solved the "receiver does not exist"-problem. both master
and slave start without any errors now.


>> Further I get this error when clicking on the cluster in pgadmin:
>> 2011-07-26 10:58:42 CEST fquest postgres ERROR:  relation "pg_listener"
>> does
>> not exist at character 25
>> 2011-07-26 10:58:42 CEST fquest postgres STATEMENT:  SELECT listenerpid
>> FROM
>> pg_listener WHERE relname = '_fquest_Event'

>This error is unrelated, but might be because the version of pgadmin you 
>have is older than the version of postgresql you are using.
>
>AI will also warn you that many (maybe even the last released one) 
>versions of pgadmin don't properly work with slony 2.0.x

read about that already. will keep it in mind. but pgadmin is for now the
best way for me to check what is going on. further comparing the old and the
new server replication wise.



What still bothers me, is that the replication set on the master-DB states
24 tables. this is ok. but the replication set on the slave does not state
any abonnements/subscriptions (to these tables). I think there should be an
abonnement/subscription. like this (on the old server - pgadmin sql field):

-- subscribe replication set

 SELECT _fquest.subscribeset(1, 1, 2, false);

thanks for your help, really appreciated.

Cheers,
Marcus
-- 
View this message in context: http://old.nabble.com/subscribeSet%28%29-the-receiver-does-not-exist-receiver-id%3A2-tp32138395p32169512.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.


From francescoboccacci at libero.it  Tue Aug  2 08:44:38 2011
From: francescoboccacci at libero.it (Francesco Boccacci)
Date: Tue, 2 Aug 2011 15:44:38 +0000 (UTC)
Subject: [Slony1-general] Invitation to connect on LinkedIn
Message-ID: <3595972.1769677.1312299878938.JavaMail.app@ela4-app0131.prod>

LinkedIn
------------



   
I'd like to add you to my professional network on LinkedIn.

- Francesco

Francesco Boccacci
R&D developer at Navionics s.p.a 
Florence Area, Italy

Confirm that you know Francesco Boccacci
https://www.linkedin.com/e/-8dhz-gqv1gr2v-2a/isd/3732900460/eoPbSnL0/


 
-- 
(c) 2011, LinkedIn Corporation
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110802/82656d11/attachment.htm 

From ssinger at ca.afilias.info  Wed Aug  3 12:53:56 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 03 Aug 2011 15:53:56 -0400
Subject: [Slony1-general] try { DROP NODE(...)} in 2.1.0
Message-ID: <4E39A754.6040601@ca.afilias.info>

In doing some testing against the 2.1.0 beta I tried executing some 
slonik code like

try {
   drop node(id=3, event node=1);
}
on error {
   echo "node already gone";
}
store node(id=3, event node=1);

A script like this will fail in the current 2.1.0 betas and this 
shouldn't surprise anyone who has read about the new features in 2.1.0

A drop node requires that the cluster be somewhat caught up (at least to 
the extent that any events from the drop'd node that have been confirmed 
elsewhere are confirmed everywhere).  This means that drop node has an 
implicit 'wait for event' before it.   However you can't have 'wait for 
events' inside a try block.

I am surprised that no one else has stumbled upon this while testing 2.1.0

The options I see are

1) Accept that you can't do that type of thing anymore (using try blocks 
as control flow structures)

2)  Have some way of moving the 'wait for event' to the 'try' statement 
instead of the first statement in the try block (details and syntax 
proposals or even a patch welcome)

3) A better idea

Steve




From ichbinrene at gmail.com  Fri Aug  5 13:38:58 2011
From: ichbinrene at gmail.com (Rene Romero Benavides)
Date: Fri, 05 Aug 2011 15:38:58 -0500
Subject: [Slony1-general] Segmentation fault when subscribing a node to a
	replication set
Message-ID: <4E3C54E2.8040409@gmail.com>

Hello everybody and greetings from M?xico City.

*slony version: 2.0.7 with postgresql 9.0.4 running on debian squeeze 
(and same issue arises with opensuse 11.04)*
I get the following message when trying to subscribe a node to a 
replication set:

*<stdin>:4: row number 0 is out of range 0..-1
Segmentation fault*

the slony instructions (generated with perltools) that caused this error:/
/-----------------------------------------------------------------------------------------------------
/cluster name = slony_cluster;
  node 1 admin conninfo='host=10.0.0.142 dbname=pgbench user=postgres 
port=5432 password=postgres';
  node 2 admin conninfo='host=10.0.0.140 dbname=pgbenchslave 
user=postgres port=5432 password=postgres';
   try {
     subscribe set (id = 1, provider = 1, receiver = 2, forward = yes);
   }
   on error {
     exit 1;
   }
   echo 'Subscribed nodes to set 1';/
-----------------------------------------------------------------------------------------------------

And my slon_tools.conf
-----------------------------------------------------------------------------------------------------

if ($ENV{"SLONYNODES"}) {
     require $ENV{"SLONYNODES"};
} else {

     $CLUSTER_NAME = 'slony_cluster';

     $LOGDIR = '/var/log/slony1';

     $MASTERNODE = 1;

     $DEBUGLEVEL = 2;

     add_node(node     => 1,
          host     => '10.0.0.142',
          dbname   => 'pgbench',
          port     => 5432,
          user     => 'postgres',
              password => 'postgres');

     add_node(node     => 2,
          host     => '10.0.0.140',
          dbname   => 'pgbenchslave',
          port     => 5432,
          user     => 'postgres',
              password => 'postgres');
}


$SLONY_SETS = {
     "set1" => {
     "set_id" => 1,
     "table_id"    => 1,
     "sequence_id" => 1,
     "pkeyedtables" => [
                                 'pgbench_tellers',
                                 'pgbench_history',
                                 'pgbench_accounts',
                                 'pgbench_branches'
                            ],
     },
};

if ($ENV{"SLONYSET"}) {
     require $ENV{"SLONYSET"};
}
# Please do not add or change anything below this point.
1;
-----------------------------------------------------------------------------------------------------
When I fire up the script I get:
2011-08-05 15:26:20 CDT LOG:  unexpected EOF on client connection
on the slon logs from the master node (where I'm calling it from)

I've checked connectivity between nodes and everything is alright. I've 
ran other scripts such as slonik_uninstall_nodes, slonik_drop_node, and 
slonik_add_node and they're executed successfully.

The cluster was previously initialized with:
-----------------------------------------------------------------------------------------------------------------------
# INIT CLUSTER
cluster name = slony_cluster;
  node 1 admin conninfo='host=10.0.0.142 dbname=pgbench user=postgres 
port=5432 password=postgres';
  node 2 admin conninfo='host=10.0.0.140 dbname=pgbenchslave 
user=postgres port=5432 password=postgres';
   init cluster (id = 1, comment = 'Node 1 - pgbench at 10.0.0.142');

# STORE NODE
   store node (id = 2, event node = 1, comment = 'Node 2 - 
pgbenchslave at 10.0.0.140');
   echo 'Set up replication nodes';

# STORE PATH
   echo 'Next: configure paths for each node/origin';
   store path (server = 1, client = 2, conninfo = 'host=10.0.0.142 
dbname=pgbench user=postgres port=5432 password=postgres');
   store path (server = 2, client = 1, conninfo = 'host=10.0.0.140 
dbname=pgbenchslave user=postgres port=5432 password=postgres');
   echo 'Replication nodes prepared';
   echo 'Please start a slon replication daemon for each node';
-----------------------------------------------------------------------------------------------------------------------
The slon daemon is running on both nodes.

Any help will be highly appreciated. Thanks



-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110805/c4060571/attachment.htm 

From ssinger at ca.afilias.info  Fri Aug  5 14:09:37 2011
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 05 Aug 2011 17:09:37 -0400
Subject: [Slony1-general] Segmentation fault when subscribing a node to
 a replication set
In-Reply-To: <4E3C54E2.8040409@gmail.com>
References: <4E3C54E2.8040409@gmail.com>
Message-ID: <4E3C5C11.4050505@ca.afilias.info>

On 11-08-05 04:38 PM, Rene Romero Benavides wrote:
> Hello everybody and greetings from M?xico City.
>
> *slony version: 2.0.7 with postgresql 9.0.4 running on debian squeeze
> (and same issue arises with opensuse 11.04)*
> I get the following message when trying to subscribe a node to a
> replication set:
>
> *<stdin>:4: row number 0 is out of range 0..-1
> Segmentation fault*
>

I don't see a 'create set(....)'  slonik command anywhere in the below 
output.  The altperl script slonik_create_set command generates this. 
Did you forget to run it befure you tried subscribing?

(if this is the case and slonik is generating a segmentation fault 
instead of a useful error message then that is a bug)

Steve

> the slony instructions (generated with perltools) that caused this error:/
> /-----------------------------------------------------------------------------------------------------
> /cluster name = slony_cluster;
> node 1 admin conninfo='host=10.0.0.142 dbname=pgbench user=postgres
> port=5432 password=postgres';
> node 2 admin conninfo='host=10.0.0.140 dbname=pgbenchslave user=postgres
> port=5432 password=postgres';
> try {
> subscribe set (id = 1, provider = 1, receiver = 2, forward = yes);
> }
> on error {
> exit 1;
> }
> echo 'Subscribed nodes to set 1';/
> -----------------------------------------------------------------------------------------------------
>
> And my slon_tools.conf
> -----------------------------------------------------------------------------------------------------
>
> if ($ENV{"SLONYNODES"}) {
> require $ENV{"SLONYNODES"};
> } else {
>
> $CLUSTER_NAME = 'slony_cluster';
>
> $LOGDIR = '/var/log/slony1';
>
> $MASTERNODE = 1;
>
> $DEBUGLEVEL = 2;
>
> add_node(node => 1,
> host => '10.0.0.142',
> dbname => 'pgbench',
> port => 5432,
> user => 'postgres',
> password => 'postgres');
>
> add_node(node => 2,
> host => '10.0.0.140',
> dbname => 'pgbenchslave',
> port => 5432,
> user => 'postgres',
> password => 'postgres');
> }
>
>
> $SLONY_SETS = {
> "set1" => {
> "set_id" => 1,
> "table_id" => 1,
> "sequence_id" => 1,
> "pkeyedtables" => [
> 'pgbench_tellers',
> 'pgbench_history',
> 'pgbench_accounts',
> 'pgbench_branches'
> ],
> },
> };
>
> if ($ENV{"SLONYSET"}) {
> require $ENV{"SLONYSET"};
> }
> # Please do not add or change anything below this point.
> 1;
> -----------------------------------------------------------------------------------------------------
> When I fire up the script I get:
> 2011-08-05 15:26:20 CDT LOG: unexpected EOF on client connection
> on the slon logs from the master node (where I'm calling it from)
>
> I've checked connectivity between nodes and everything is alright. I've
> ran other scripts such as slonik_uninstall_nodes, slonik_drop_node, and
> slonik_add_node and they're executed successfully.
>
> The cluster was previously initialized with:
> -----------------------------------------------------------------------------------------------------------------------
> # INIT CLUSTER
> cluster name = slony_cluster;
> node 1 admin conninfo='host=10.0.0.142 dbname=pgbench user=postgres
> port=5432 password=postgres';
> node 2 admin conninfo='host=10.0.0.140 dbname=pgbenchslave user=postgres
> port=5432 password=postgres';
> init cluster (id = 1, comment = 'Node 1 - pgbench at 10.0.0.142');
>
> # STORE NODE
> store node (id = 2, event node = 1, comment = 'Node 2 -
> pgbenchslave at 10.0.0.140');
> echo 'Set up replication nodes';
>
> # STORE PATH
> echo 'Next: configure paths for each node/origin';
> store path (server = 1, client = 2, conninfo = 'host=10.0.0.142
> dbname=pgbench user=postgres port=5432 password=postgres');
> store path (server = 2, client = 1, conninfo = 'host=10.0.0.140
> dbname=pgbenchslave user=postgres port=5432 password=postgres');
> echo 'Replication nodes prepared';
> echo 'Please start a slon replication daemon for each node';
> -----------------------------------------------------------------------------------------------------------------------
> The slon daemon is running on both nodes.
>
> Any help will be highly appreciated. Thanks
>
>
>
>
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


From ichbinrene at gmail.com  Fri Aug  5 14:44:26 2011
From: ichbinrene at gmail.com (Rene Romero Benavides)
Date: Fri, 05 Aug 2011 16:44:26 -0500
Subject: [Slony1-general] Segmentation fault when subscribing a node to
 a replication set
In-Reply-To: <4E3C5C11.4050505@ca.afilias.info>
References: <4E3C54E2.8040409@gmail.com> <4E3C5C11.4050505@ca.afilias.info>
Message-ID: <4E3C643A.1060406@gmail.com>

My bad, I just started using slony and bypassed that common sense 
implication =-D. Thanks for the guide and for bearing with newbies. No 
bug whatsoever.

El 05/08/11 16:09, Steve Singer escribi?:
> On 11-08-05 04:38 PM, Rene Romero Benavides wrote:
>> Hello everybody and greetings from M?xico City.
>>
>> *slony version: 2.0.7 with postgresql 9.0.4 running on debian squeeze
>> (and same issue arises with opensuse 11.04)*
>> I get the following message when trying to subscribe a node to a
>> replication set:
>>
>> *<stdin>:4: row number 0 is out of range 0..-1
>> Segmentation fault*
>>
>
> I don't see a 'create set(....)'  slonik command anywhere in the below 
> output.  The altperl script slonik_create_set command generates this. 
> Did you forget to run it befure you tried subscribing?
>
> (if this is the case and slonik is generating a segmentation fault 
> instead of a useful error message then that is a bug)
>
> Steve
>
>> the slony instructions (generated with perltools) that caused this 
>> error:/
>> /----------------------------------------------------------------------------------------------------- 
>>
>> /cluster name = slony_cluster;
>> node 1 admin conninfo='host=10.0.0.142 dbname=pgbench user=postgres
>> port=5432 password=postgres';
>> node 2 admin conninfo='host=10.0.0.140 dbname=pgbenchslave user=postgres
>> port=5432 password=postgres';
>> try {
>> subscribe set (id = 1, provider = 1, receiver = 2, forward = yes);
>> }
>> on error {
>> exit 1;
>> }
>> echo 'Subscribed nodes to set 1';/
>> ----------------------------------------------------------------------------------------------------- 
>>
>>
>> And my slon_tools.conf
>> ----------------------------------------------------------------------------------------------------- 
>>
>>
>> if ($ENV{"SLONYNODES"}) {
>> require $ENV{"SLONYNODES"};
>> } else {
>>
>> $CLUSTER_NAME = 'slony_cluster';
>>
>> $LOGDIR = '/var/log/slony1';
>>
>> $MASTERNODE = 1;
>>
>> $DEBUGLEVEL = 2;
>>
>> add_node(node => 1,
>> host => '10.0.0.142',
>> dbname => 'pgbench',
>> port => 5432,
>> user => 'postgres',
>> password => 'postgres');
>>
>> add_node(node => 2,
>> host => '10.0.0.140',
>> dbname => 'pgbenchslave',
>> port => 5432,
>> user => 'postgres',
>> password => 'postgres');
>> }
>>
>>
>> $SLONY_SETS = {
>> "set1" => {
>> "set_id" => 1,
>> "table_id" => 1,
>> "sequence_id" => 1,
>> "pkeyedtables" => [
>> 'pgbench_tellers',
>> 'pgbench_history',
>> 'pgbench_accounts',
>> 'pgbench_branches'
>> ],
>> },
>> };
>>
>> if ($ENV{"SLONYSET"}) {
>> require $ENV{"SLONYSET"};
>> }
>> # Please do not add or change anything below this point.
>> 1;
>> ----------------------------------------------------------------------------------------------------- 
>>
>> When I fire up the script I get:
>> 2011-08-05 15:26:20 CDT LOG: unexpected EOF on client connection
>> on the slon logs from the master node (where I'm calling it from)
>>
>> I've checked connectivity between nodes and everything is alright. I've
>> ran other scripts such as slonik_uninstall_nodes, slonik_drop_node, and
>> slonik_add_node and they're executed successfully.
>>
>> The cluster was previously initialized with:
>> ----------------------------------------------------------------------------------------------------------------------- 
>>
>> # INIT CLUSTER
>> cluster name = slony_cluster;
>> node 1 admin conninfo='host=10.0.0.142 dbname=pgbench user=postgres
>> port=5432 password=postgres';
>> node 2 admin conninfo='host=10.0.0.140 dbname=pgbenchslave user=postgres
>> port=5432 password=postgres';
>> init cluster (id = 1, comment = 'Node 1 - pgbench at 10.0.0.142');
>>
>> # STORE NODE
>> store node (id = 2, event node = 1, comment = 'Node 2 -
>> pgbenchslave at 10.0.0.140');
>> echo 'Set up replication nodes';
>>
>> # STORE PATH
>> echo 'Next: configure paths for each node/origin';
>> store path (server = 1, client = 2, conninfo = 'host=10.0.0.142
>> dbname=pgbench user=postgres port=5432 password=postgres');
>> store path (server = 2, client = 1, conninfo = 'host=10.0.0.140
>> dbname=pgbenchslave user=postgres port=5432 password=postgres');
>> echo 'Replication nodes prepared';
>> echo 'Please start a slon replication daemon for each node';
>> ----------------------------------------------------------------------------------------------------------------------- 
>>
>> The slon daemon is running on both nodes.
>>
>> Any help will be highly appreciated. Thanks
>>
>>
>>
>>
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>


From cbbrowne at afilias.info  Fri Aug  5 14:49:09 2011
From: cbbrowne at afilias.info (Christopher Browne)
Date: Fri, 5 Aug 2011 17:49:09 -0400
Subject: [Slony1-general] Segmentation fault when subscribing a node to
 a replication set
In-Reply-To: <4E3C5C11.4050505@ca.afilias.info>
References: <4E3C54E2.8040409@gmail.com>
	<4E3C5C11.4050505@ca.afilias.info>
Message-ID: <CANfbgbY=7u-yMyfwaVLrS1MPZ5cqM3Oxq4EXQoqHXH3hqgsO0g@mail.gmail.com>

On Fri, Aug 5, 2011 at 5:09 PM, Steve Singer <ssinger at ca.afilias.info> wrote:
> On 11-08-05 04:38 PM, Rene Romero Benavides wrote:
>> Hello everybody and greetings from M?xico City.
>>
>> *slony version: 2.0.7 with postgresql 9.0.4 running on debian squeeze
>> (and same issue arises with opensuse 11.04)*
>> I get the following message when trying to subscribe a node to a
>> replication set:
>>
>> *<stdin>:4: row number 0 is out of range 0..-1
>> Segmentation fault*
>>
>
> I don't see a 'create set(....)' ?slonik command anywhere in the below
> output. ?The altperl script slonik_create_set command generates this.
> Did you forget to run it befure you tried subscribing?
>
> (if this is the case and slonik is generating a segmentation fault
> instead of a useful error message then that is a bug)

I don't think I'd call this a *severe* bug, but it seems to me that
"row number 0 is out of range..." isn't quite the most intuitive error
message of all time.

That's presumably taking place somewhere inside the Postgres "stack",
as that description isn't found anywhere in the Slony code base.

If we can get a stack trace from the Postgres logs indicating
specifically where it was executing when this happened, it might be
possible to toss in a more descriptive error message, presumably
somewhere in the subscribe set code.

From dilrajssokhi at gmail.com  Fri Aug  5 16:22:46 2011
From: dilrajssokhi at gmail.com (Dilraj Singh)
Date: Fri, 5 Aug 2011 16:22:46 -0700
Subject: [Slony1-general] Uninterrupted Slony Replication
Message-ID: <CAHFJsA9PkvFO27RUVoO-hXnrZzHtb1M93z9JgQ+AL6H7p02doA@mail.gmail.com>

Hi,

I am using postgresql-8.4 and slony1-1.2.0.3 and i have been able implement
a 4 node replication cluster where nodes communicate successfully with each
other. The way i have went about this is that i have written scripts (say
cluster_setup.sh and subscribe.sh) to be run with slonik. Like run the
script cluster_setup on the master node and then slon daemon's on all the 4
nodes with necessary connection information and finally run subscribe.sh on
the master node again. This works perfectly fine and even when i kill some
of the slons on the different machines, if i start slon again, the
replication at that node picks up where it was left before.

After this i tried automating the whole process so that in case of a network
disconnect/power failure/reboot the replication can continue to work as
normal. So instead of running slon's manually on each machine, i placed a
script having 'bash -U postgres -c "./slon conninfo=" ' command in init.d
directory for each machine. After having all the database replication
running again, i rebooted one of the machines but i could not have the
database replication restored after that. The node which was acting as a
provider to the rebooted machine started showing this error:

2011-08-05 09:25:40 PDTERROR  remoteListenThread_3: "select con_origin,
con_received,     max(con_seqno) as con_seqno,     max(con_timestamp) as
con_timestamp from "_four_node_rep_cluster20".sl_confirm where con_received
<> 2 group by con_origin, con_received" 2011-08-05 09:25:42 PDTERROR
remoteListenThread_3: "select ev_origin, ev_seqno, ev_timestamp,
ev_snapshot,        "pg_catalog".txid_snapshot_xmin(ev_snapshot),
"pg_catalog".txid_snapshot_xmax(ev_snapshot),        ev_type,
ev_data1, ev_data2,        ev_data3, ev_data4,        ev_data5,
ev_data6,        ev_data7, ev_data8 from "_four_node_rep_cluster20".sl_event
e where (e.ev_origin = '3' and e.ev_seqno > '5000000005') or (e.ev_origin =
'4' and e.ev_seqno > '5000000039') order by e.ev_origin, e.ev_seqno limit
40" - no connection to the server

and then the replication wont start working again till the time i reboot all
the nodes. I am guessing it might be the case that the provider node gets
reinitialized on rebooting thats why the replication starts again. I know
slony is used for automated database replication so i was wondering whether
there is any way in which i can make this work without rebooting all the
nodes, which will be inconvenient if the number of nodes increase or for
production server

Any inputs on the above error will be greatly appreciated.

Regards
Dilraj Singh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110805/d19da692/attachment-0001.htm 

From ssinger_pg at sympatico.ca  Sat Aug  6 08:42:37 2011
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Sat, 6 Aug 2011 11:42:37 -0400
Subject: [Slony1-general] Uninterrupted Slony Replication
In-Reply-To: <CAHFJsA9PkvFO27RUVoO-hXnrZzHtb1M93z9JgQ+AL6H7p02doA@mail.gmail.com>
References: <CAHFJsA9PkvFO27RUVoO-hXnrZzHtb1M93z9JgQ+AL6H7p02doA@mail.gmail.com>
Message-ID: <BLU0-SMTP82CF5CECDADEAF4EF6B7C6AC3F0@phx.gbl>

On Fri, 5 Aug 2011, Dilraj Singh wrote:

> Hi,
> 
> I am using postgresql-8.4 and slony1-1.2.0.3 and i have been able implement
> a 4 node replication cluster where nodes communicate successfully with each

Try upgrading to 2.0.7 and see if it fixes your problem.

1) 2.0.3 has a bug (unrelated to your current issue) that isn't present in 
2.0.2 or 2.0.4 so that release should be avoided

2) 2.0.7 has some fixes related to recovering from dropped connections that 
might fix your issue, the error you paste below looks familiar.

<snip>

> 2011-08-05 09:25:40 PDTERROR? remoteListenThread_3: "select con_origin,
> con_received,???? max(con_seqno) as con_seqno,???? max(con_timestamp) as
> con_timestamp from "_four_node_rep_cluster20".sl_confirm where con_received
> <> 2 group by con_origin, con_received" 2011-08-05 09:25:42 PDTERROR?
> remoteListenThread_3: "select ev_origin, ev_seqno, ev_timestamp,???????
> ev_snapshot,??????? "pg_catalog".txid_snapshot_xmin(ev_snapshot),???????
> "pg_catalog".txid_snapshot_xmax(ev_snapshot),??????? ev_type,???????
> ev_data1, ev_data2,??????? ev_data3, ev_data4,??????? ev_data5,
> ev_data6,??????? ev_data7, ev_data8 from "_four_node_rep_cluster20".sl_event
> e where (e.ev_origin = '3' and e.ev_seqno > '5000000005') or (e.ev_origin =
> '4' and e.ev_seqno > '5000000039') order by e.ev_origin, e.ev_seqno limit
> 40" - no connection to the server
> 
> and then the replication wont start working again till the time i reboot all
> the nodes. I am guessing it might be the case that the provider node gets
> reinitialized on rebooting thats why the replication starts again. I know
> slony is used for automated database replication so i was wondering whether
> there is any way in which i can make this work without rebooting all the
> nodes, which will be inconvenient if the number of nodes increase or for
> production server
> 
> Any inputs on the above error will be greatly appreciated.
> 
> Regards
> Dilraj Singh
> 
>

From dilrajssokhi at gmail.com  Mon Aug  8 16:55:24 2011
From: dilrajssokhi at gmail.com (Dilraj Singh)
Date: Mon, 8 Aug 2011 16:55:24 -0700
Subject: [Slony1-general] Uninterrupted Slony Replication
In-Reply-To: <BLU0-SMTP82CF5CECDADEAF4EF6B7C6AC3F0@phx.gbl>
References: <CAHFJsA9PkvFO27RUVoO-hXnrZzHtb1M93z9JgQ+AL6H7p02doA@mail.gmail.com>
	<BLU0-SMTP82CF5CECDADEAF4EF6B7C6AC3F0@phx.gbl>
Message-ID: <CAHFJsA_CFONC6e-=y1B6mOuDcE07KgEyseMq0DwBQ1CfKS2oYg@mail.gmail.com>

Hi,

Yup, it works for 2.0.7. Thanks.

But i tried for version 2.0.4 also, still its giving the same errors. We are
little bit inclined to use version 2.0.4 as it is current version available
with apt-get on debian and hence can be updated easily using apt-get. So is
there any way i can make this work in the version 2.0.4 too?

Also, I noticed that on rebooting the machine, it does not even work when i
kill the slon process started on reboot and manually start the slon process
like ./slon conninfo=.

Regards
Dilraj Singh

On Sat, Aug 6, 2011 at 8:42 AM, Steve Singer <ssinger_pg at sympatico.ca>wrote:

> On Fri, 5 Aug 2011, Dilraj Singh wrote:
>
>  Hi,
>>
>> I am using postgresql-8.4 and slony1-1.2.0.3 and i have been able
>> implement
>> a 4 node replication cluster where nodes communicate successfully with
>> each
>>
>
> Try upgrading to 2.0.7 and see if it fixes your problem.
>
> 1) 2.0.3 has a bug (unrelated to your current issue) that isn't present in
> 2.0.2 or 2.0.4 so that release should be avoided
>
> 2) 2.0.7 has some fixes related to recovering from dropped connections that
> might fix your issue, the error you paste below looks familiar.
>
> <snip>
>
>
>  2011-08-05 09:25:40 PDTERROR  remoteListenThread_3: "select con_origin,
>> con_received,     max(con_seqno) as con_seqno,     max(con_timestamp) as
>> con_timestamp from "_four_node_rep_cluster20".sl_**confirm where
>> con_received
>> <> 2 group by con_origin, con_received" 2011-08-05 09:25:42 PDTERROR
>> remoteListenThread_3: "select ev_origin, ev_seqno, ev_timestamp,
>> ev_snapshot,        "pg_catalog".txid_snapshot_**
>> xmin(ev_snapshot),
>> "pg_catalog".txid_snapshot_**xmax(ev_snapshot),        ev_type,
>> ev_data1, ev_data2,        ev_data3, ev_data4,        ev_data5,
>> ev_data6,        ev_data7, ev_data8 from "_four_node_rep_cluster20".sl_**
>> event
>> e where (e.ev_origin = '3' and e.ev_seqno > '5000000005') or (e.ev_origin
>> =
>> '4' and e.ev_seqno > '5000000039') order by e.ev_origin, e.ev_seqno limit
>> 40" - no connection to the server
>>
>> and then the replication wont start working again till the time i reboot
>> all
>> the nodes. I am guessing it might be the case that the provider node gets
>> reinitialized on rebooting thats why the replication starts again. I know
>> slony is used for automated database replication so i was wondering
>> whether
>> there is any way in which i can make this work without rebooting all the
>> nodes, which will be inconvenient if the number of nodes increase or for
>> production server
>>
>> Any inputs on the above error will be greatly appreciated.
>>
>> Regards
>> Dilraj Singh
>>
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110808/ef5b3a89/attachment.htm 

From ssinger_pg at sympatico.ca  Mon Aug  8 17:08:19 2011
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Mon, 8 Aug 2011 20:08:19 -0400
Subject: [Slony1-general] Uninterrupted Slony Replication
In-Reply-To: <CAHFJsA_CFONC6e-=y1B6mOuDcE07KgEyseMq0DwBQ1CfKS2oYg@mail.gmail.com>
References: <CAHFJsA9PkvFO27RUVoO-hXnrZzHtb1M93z9JgQ+AL6H7p02doA@mail.gmail.com>
	<BLU0-SMTP82CF5CECDADEAF4EF6B7C6AC3F0@phx.gbl>
	<CAHFJsA_CFONC6e-=y1B6mOuDcE07KgEyseMq0DwBQ1CfKS2oYg@mail.gmail.com>
Message-ID: <BLU0-SMTP101CA77E86086D8C7368776AC200@phx.gbl>

On Mon, 8 Aug 2011, Dilraj Singh wrote:

> Hi,
> 
> Yup, it works for 2.0.7. Thanks.
> 
> But i tried for version 2.0.4 also, still its giving the same errors. We are
> little bit inclined to use version 2.0.4 as it is current version available
> with apt-get on debian and hence can be updated easily using apt-get. So is
> there any way i can make this work in the version 2.0.4 too?
> 
> Also, I noticed that on rebooting the machine, it does not even work when i
> kill the slon process started on reboot and manually start the slon process
> like ./slon conninfo=.


Once your network and postgresql instances are up you should just be able to 
restart all of your slon processes and replication should resume (with 
2.0.4) it should recover from the dropped connections when slon is 
restarting.

How are you starting slon?  Are you using a slon.conf file or passing the 
conninfo on the command line? (you need to be doing one of the two).

Steve



> 
> Regards
> Dilraj Singh
> 
> On Sat, Aug 6, 2011 at 8:42 AM, Steve Singer <ssinger_pg at sympatico.ca>
> wrote:
>       On Fri, 5 Aug 2011, Dilraj Singh wrote:
>
>             Hi,
>
>             I am using postgresql-8.4 and slony1-1.2.0.3 and i
>             have been able implement
>             a 4 node replication cluster where nodes communicate
>             successfully with each
> 
> 
> Try upgrading to 2.0.7 and see if it fixes your problem.
> 
> 1) 2.0.3 has a bug (unrelated to your current issue) that isn't
> present in 2.0.2 or 2.0.4 so that release should be avoided
> 
> 2) 2.0.7 has some fixes related to recovering from dropped connections
> that might fix your issue, the error you paste below looks familiar.
> 
> <snip>
> 
>
>       2011-08-05 09:25:40 PDTERROR? remoteListenThread_3:
>       "select con_origin,
>       con_received,???? max(con_seqno) as con_seqno,????
>       max(con_timestamp) as
>       con_timestamp from "_four_node_rep_cluster20".sl_confirm
>       where con_received
>       <> 2 group by con_origin, con_received" 2011-08-05
>       09:25:42 PDTERROR?
>       remoteListenThread_3: "select ev_origin, ev_seqno,
>       ev_timestamp,???????
>       ev_snapshot,???????
>       "pg_catalog".txid_snapshot_xmin(ev_snapshot),???????
>       "pg_catalog".txid_snapshot_xmax(ev_snapshot),???????
>       ev_type,???????
>       ev_data1, ev_data2,??????? ev_data3, ev_data4,???????
>       ev_data5,
>       ev_data6,??????? ev_data7, ev_data8 from
>       "_four_node_rep_cluster20".sl_event
>       e where (e.ev_origin = '3' and e.ev_seqno > '5000000005')
>       or (e.ev_origin =
>       '4' and e.ev_seqno > '5000000039') order by e.ev_origin,
>       e.ev_seqno limit
>       40" - no connection to the server
>
>       and then the replication wont start working again till the
>       time i reboot all
>       the nodes. I am guessing it might be the case that the
>       provider node gets
>       reinitialized on rebooting thats why the replication
>       starts again. I know
>       slony is used for automated database replication so i was
>       wondering whether
>       there is any way in which i can make this work without
>       rebooting all the
>       nodes, which will be inconvenient if the number of nodes
>       increase or for
>       production server
>
>       Any inputs on the above error will be greatly appreciated.
>
>       Regards
>       Dilraj Singh
> 
> 
> 
> 
>

From dilrajssokhi at gmail.com  Mon Aug  8 20:22:30 2011
From: dilrajssokhi at gmail.com (Dilraj Singh)
Date: Mon, 8 Aug 2011 20:22:30 -0700
Subject: [Slony1-general] Uninterrupted Slony Replication
In-Reply-To: <BLU0-SMTP101CA77E86086D8C7368776AC200@phx.gbl>
References: <CAHFJsA9PkvFO27RUVoO-hXnrZzHtb1M93z9JgQ+AL6H7p02doA@mail.gmail.com>
	<BLU0-SMTP82CF5CECDADEAF4EF6B7C6AC3F0@phx.gbl>
	<CAHFJsA_CFONC6e-=y1B6mOuDcE07KgEyseMq0DwBQ1CfKS2oYg@mail.gmail.com>
	<BLU0-SMTP101CA77E86086D8C7368776AC200@phx.gbl>
Message-ID: <CAHFJsA-SyAF8_CEKJzVTHDCoytWOD-Uo+OktH=ThW+_ZMYVZQw@mail.gmail.com>

Hi Steve,

I have placed a script in the /etc/init.d folder of my debian machine which
has the commands as

#!/bin/sh
bash -u postgres -c '/usr/lib/postgresql/8.4/bin/pg_ctl start -D
/var/lib/postgresql/8.4/main" '
bash -u postgres -c '/usr/bin/slon conninfo= "dbname=testdb user=postgres" '

I have configured this script on each of the 4 machines to run at the the
reboot time which will start the database and then will run the slon
process. I am passing conninfo on the command line itself and before doing
the reboot, i have also made the cluster_setup and subscriptions for the
four nodes. So its like replication is going on when i do reboot on one of
the machines.

As you pointed out, this all procedure works fine in 2.0.7, but fails in the
version 2.0.4. Also while seeing the output of the                    *ps
aux | grep postgres* command at the times of broken and not-broken
connection, i can see the entries for the processes related to database of
the other machines (which are connected to it as described in subscription
script) in the not-broken connection whereas broken connection (after
reboot) has only local database entries in the command output.

Thanks for helping me out. :)

Regards
Dilraj Singh

On Mon, Aug 8, 2011 at 5:08 PM, Steve Singer <ssinger_pg at sympatico.ca>wrote:

> Once your network and postgresql instances are up you should just be able
> to restart all of your slon processes and replication should resume (with
> 2.0.4) it should recover from the dropped connections when slon is
> restarting.
>
> How are you starting slon?  Are you using a slon.conf file or passing the
> conninfo on the command line? (you need to be doing one of the two).
>
> Steve
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110808/2d1ec316/attachment.htm 

From ssinger_pg at sympatico.ca  Mon Aug  8 20:33:38 2011
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Mon, 8 Aug 2011 23:33:38 -0400
Subject: [Slony1-general] Uninterrupted Slony Replication
In-Reply-To: <CAHFJsA-SyAF8_CEKJzVTHDCoytWOD-Uo+OktH=ThW+_ZMYVZQw@mail.gmail.com>
References: <CAHFJsA9PkvFO27RUVoO-hXnrZzHtb1M93z9JgQ+AL6H7p02doA@mail.gmail.com>
	<BLU0-SMTP82CF5CECDADEAF4EF6B7C6AC3F0@phx.gbl>
	<CAHFJsA_CFONC6e-=y1B6mOuDcE07KgEyseMq0DwBQ1CfKS2oYg@mail.gmail.com>
	<BLU0-SMTP101CA77E86086D8C7368776AC200@phx.gbl>
	<CAHFJsA-SyAF8_CEKJzVTHDCoytWOD-Uo+OktH=ThW+_ZMYVZQw@mail.gmail.com>
Message-ID: <BLU0-SMTP1306ECF9873CEA839C72BEAC200@phx.gbl>

On Mon, 8 Aug 2011, Dilraj Singh wrote:

> Hi Steve,
> I have placed a script in the /etc/init.d folder of my debian machine which
> has the commands as

To restart slon manually after your rebooted node i back up try

slon four_node_rep_cluster20 'dbname=testdb user=postgres'

on all the other nodes.

> 
> #!/bin/sh
> bash -u postgres -c '/usr/lib/postgresql/8.4/bin/pg_ctl start -D
> /var/lib/postgresql/8.4/main" '
> bash -u postgres -c '/usr/bin/slon conninfo= "dbname=testdb user=postgres" '

What the above line does is start slon with a cluster name of 'conninfo='
in your previous email you pasted output that indicated that your 
clustername is 'four_node_rep_cluster20'

I suspect that the slon started but your init script isn't actually the slon 
instance doing the work but you have somethign somewhere else that is 
starting up the slon with the clustername 'four_node_rep_cluster20'  I 
suspect that other slon instance recovers properly from the reboot of the 
remote node (since 2.0.7 tends to recover properly) while with 2.0.4 you 
need to manually correctly restart the remote slons



> 
> I have configured this script on each of the 4 machines to run at the the
> reboot time which will start the database and then will run the slon
> process. I am passing conninfo on the command line itself and before doing
> the reboot, i have also made the cluster_setup and subscriptions for the
> four nodes. So its like replication is going on when i do reboot on one of
> the machines.?
> 
> As you pointed out, this all procedure works fine in 2.0.7, but fails in the
> version 2.0.4. Also while seeing the output of the ? ? ? ? ? ? ? ? ? ?ps aux
> | grep postgres command at the times of broken and not-broken connection, i
> can see the entries for the processes related to database of the other
> machines (which are connected to it as described in subscription script) in
> the not-broken connection whereas broken connection (after reboot) has only
> local database entries in the command output.
> 
> Thanks for helping me out. :)
> 
> Regards
> Dilraj Singh
> 
> On Mon, Aug 8, 2011 at 5:08 PM, Steve Singer <ssinger_pg at sympatico.ca>
> wrote:
>       Once your network and postgresql instances are up you should
>       just be able to restart all of your slon processes and
>       replication should resume (with 2.0.4) it should recover from
>       the dropped connections when slon is restarting.
> 
> How are you starting slon? ?Are you using a slon.conf file or passing
> the conninfo on the command line? (you need to be doing one of the
> two).
> 
> Steve
> 
> 
>

From dilrajssokhi at gmail.com  Mon Aug  8 21:09:06 2011
From: dilrajssokhi at gmail.com (Dilraj Singh)
Date: Mon, 8 Aug 2011 21:09:06 -0700
Subject: [Slony1-general] Uninterrupted Slony Replication
In-Reply-To: <BLU0-SMTP1306ECF9873CEA839C72BEAC200@phx.gbl>
References: <CAHFJsA9PkvFO27RUVoO-hXnrZzHtb1M93z9JgQ+AL6H7p02doA@mail.gmail.com>
	<BLU0-SMTP82CF5CECDADEAF4EF6B7C6AC3F0@phx.gbl>
	<CAHFJsA_CFONC6e-=y1B6mOuDcE07KgEyseMq0DwBQ1CfKS2oYg@mail.gmail.com>
	<BLU0-SMTP101CA77E86086D8C7368776AC200@phx.gbl>
	<CAHFJsA-SyAF8_CEKJzVTHDCoytWOD-Uo+OktH=ThW+_ZMYVZQw@mail.gmail.com>
	<BLU0-SMTP1306ECF9873CEA839C72BEAC200@phx.gbl>
Message-ID: <CAHFJsA-D1TqKdrfT1pLygpbdWtNKM9MfPE2227dn7D0KK76kgg@mail.gmail.com>

Hi Steve,

Yeah, i am sorry but i missed the clustername definition while writing the
mail. Thanks for pointing that out. It will definitely not work without me
defining the cluster name same as the one in the cluster setup and
subscription scripts. Exact initialization script is :

#!/bin/sh
bash -u postgres -c '/usr/lib/postgresql/8.4/bin/**pg_ctl start
-D /var/lib/postgresql/8.4/main" '
bash -u postgres -c '/usr/bin/slon four_node_replication_cluster20
"dbname=testdb user=postgres" '

As i said in my earlier mail, I even manually started slon processes on the
rebooted machine, but even then replication does not start.

Regards
Dilraj Singh


On Mon, Aug 8, 2011 at 8:33 PM, Steve Singer <ssinger_pg at sympatico.ca>wrote:

> On Mon, 8 Aug 2011, Dilraj Singh wrote:
>
>  Hi Steve,
>> I have placed a script in the /etc/init.d folder of my debian machine
>> which
>> has the commands as
>>
>
> To restart slon manually after your rebooted node i back up try
>
> slon four_node_rep_cluster20 'dbname=testdb user=postgres'
>
> on all the other nodes.
>
>
>
>> #!/bin/sh
>> bash -u postgres -c '/usr/lib/postgresql/8.4/bin/**pg_ctl start -D
>> /var/lib/postgresql/8.4/main" '
>> bash -u postgres -c '/usr/bin/slon conninfo= "dbname=testdb user=postgres"
>> '
>>
>
> What the above line does is start slon with a cluster name of 'conninfo='
> in your previous email you pasted output that indicated that your
> clustername is 'four_node_rep_cluster20'
>
> I suspect that the slon started but your init script isn't actually the
> slon instance doing the work but you have somethign somewhere else that is
> starting up the slon with the clustername 'four_node_rep_cluster20'  I
> suspect that other slon instance recovers properly from the reboot of the
> remote node (since 2.0.7 tends to recover properly) while with 2.0.4 you
> need to manually correctly restart the remote slons
>
>
>
>
>
>> I have configured this script on each of the 4 machines to run at the the
>> reboot time which will start the database and then will run the slon
>> process. I am passing conninfo on the command line itself and before doing
>> the reboot, i have also made the cluster_setup and subscriptions for the
>> four nodes. So its like replication is going on when i do reboot on one of
>> the machines.
>>
>> As you pointed out, this all procedure works fine in 2.0.7, but fails in
>> the
>> version 2.0.4. Also while seeing the output of the                    ps
>> aux
>> | grep postgres command at the times of broken and not-broken connection,
>> i
>> can see the entries for the processes related to database of the other
>> machines (which are connected to it as described in subscription script)
>> in
>> the not-broken connection whereas broken connection (after reboot) has
>> only
>> local database entries in the command output.
>>
>> Thanks for helping me out. :)
>>
>> Regards
>> Dilraj Singh
>>
>> On Mon, Aug 8, 2011 at 5:08 PM, Steve Singer <ssinger_pg at sympatico.ca>
>> wrote:
>>      Once your network and postgresql instances are up you should
>>      just be able to restart all of your slon processes and
>>      replication should resume (with 2.0.4) it should recover from
>>      the dropped connections when slon is restarting.
>>
>> How are you starting slon?  Are you using a slon.conf file or passing
>> the conninfo on the command line? (you need to be doing one of the
>> two).
>>
>> Steve
>>
>>
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20110808/141e49d4/attachment-0001.htm 

From ssinger_pg at sympatico.ca  Tue Aug  9 04:26:10 2011
From: ssinger_pg at sympatico.ca (Steve Singer)
Date: Tue, 9 Aug 2011 07:26:10 -0400
Subject: [Slony1-general] Uninterrupted Slony Replication
In-Reply-To: <CAHFJsA-D1TqKdrfT1pLygpbdWtNKM9MfPE2227dn7D0KK76kgg@mail.gmail.com>
References: <CAHFJsA9PkvFO27RUVoO-hXnrZzHtb1M93z9JgQ+AL6H7p02doA@mail.gmail.com>
	<BLU0-SMTP82CF5CECDADEAF4EF6B7C6AC3F0@phx.gbl>
	<CAHFJsA_CFONC6e-=y1B6mOuDcE07KgEyseMq0DwBQ1CfKS2oYg@mail.gmail.com>
	<BLU0-SMTP101CA77E86086D8C7368776AC200@phx.gbl>
	<CAHFJsA-SyAF8_CEKJzVTHDCoytWOD-Uo+OktH=ThW+_ZMYVZQw@mail.gmail.com>
	<BLU0-SMTP1306ECF9873CEA839C72BEAC200@phx.gbl>
	<CAHFJsA-D1TqKdrfT1pLygpbdWtNKM9MfPE2227dn7D0KK76kgg@mail.gmail.com>
Message-ID: <BLU0-SMTP1001F9CC22CDCC61274C6F5AC200@phx.gbl>

On Mon, 8 Aug 2011, Dilraj Singh wrote:

> Hi Steve,
> Yeah, i am sorry but i missed the clustername definition while writing the
> mail. Thanks for pointing that out. It will definitely not work without me
> defining the cluster name same as the one in the cluster setup and
> subscription scripts. Exact initialization script is :
> ?
> #!/bin/sh
> bash -u postgres -c '/usr/lib/postgresql/8.4/bin/pg_ctl start
> -D?/var/lib/postgresql/8.4/main" '
> bash -u postgres -c '/usr/bin/slon four_node_replication_cluster20
> "dbname=testdb user=postgres" '
> 
> As i said in my earlier mail, I even manually started slon processes on the
> rebooted machine, but even then replication does not start.
>

You need to restart the slon processes on all the *other* machines, not the 
rebooted.  The slon process on the reboted one gets restart by the act of 
rebooting.

Steve



> Regards
> Dilraj Singh
> 
> 
> On Mon, Aug 8, 2011 at 8:33 PM, Steve Singer <ssinger_pg at sympatico.ca>
> wrote:
>       On Mon, 8 Aug 2011, Dilraj Singh wrote:
>
>       Hi Steve,
>       I have placed a script in the /etc/init.d folder of my
>       debian machine which
>       has the commands as
> 
> 
> To restart slon manually after your rebooted node i back up try
> 
> slon four_node_rep_cluster20 'dbname=testdb user=postgres'
> 
> on all the other nodes.
> 
>
>       #!/bin/sh
>       bash -u postgres -c '/usr/lib/postgresql/8.4/bin/pg_ctl
>       start -D
>       /var/lib/postgresql/8.4/main" '
>       bash -u postgres -c '/usr/bin/slon conninfo=
>       "dbname=testdb user=postgres" '
> 
> 
> What the above line does is start slon with a cluster name of
> 'conninfo='
> in your previous email you pasted output that indicated that your
> clustername is 'four_node_rep_cluster20'
> 
> I suspect that the slon started but your init script isn't actually
> the slon instance doing the work but you have somethign somewhere else
> that is starting up the slon with the clustername
> 'four_node_rep_cluster20' ?I suspect that other slon instance recovers
> properly from the reboot of the remote node (since 2.0.7 tends to
> recover properly) while with 2.0.4 you need to manually correctly
> restart the remote slons
> 
> 
> 
> 
>
>       I have configured this script on each of the 4 machines to
>       run at the the
>       reboot time which will start the database and then will
>       run the slon
>       process. I am passing conninfo on the command line itself
>       and before doing
>       the reboot, i have also made the cluster_setup and
>       subscriptions for the
>       four nodes. So its like replication is going on when i do
>       reboot on one of
>       the machines.?
>
>       As you pointed out, this all procedure works fine in
>       2.0.7, but fails in the
>       version 2.0.4. Also while seeing the output of the ? ? ? ?
>       ? ? ? ? ? ?ps aux
>       | grep postgres command at the times of broken and
>       not-broken connection, i
>       can see the entries for the processes related to database
>       of the other
>       machines (which are connected to it as described in
>       subscription script) in
>       the not-broken connection whereas broken connection (after
>       reboot) has only
>       local database entries in the command output.
>
>       Thanks for helping me out. :)
>
>       Regards
>       Dilraj Singh
>
>       On Mon, Aug 8, 2011 at 5:08 PM, Steve Singer
>       <ssinger_pg at sympatico.ca>
>       wrote:
>       ? ? ?Once your network and postgresql instances are up you
>       should
>       ? ? ?just be able to restart all of your slon processes
>       and
>       ? ? ?replication should resume (with 2.0.4) it should
>       recover from
>       ? ? ?the dropped connections when slon is restarting.
>
>       How are you starting slon? ?Are you using a slon.conf file
>       or passing
>       the conninfo on the command line? (you need to be doing
>       one of the
>       two).
>
>       Steve
> 
> 
> 
> 
> 
>

From stuart at stuartbishop.net  Tue Aug  9 04:37:56 2011
From: stuart at stuartbishop.net (Stuart Bishop)
Date: Tue, 9 Aug 2011 18:37:56 +0700
Subject: [Slony1-general] 1 cluster or 5?
Message-ID: <CADmi=6PtF9vaOybL2Lxru7yJN0wAkcwc2z_mOT94mgzF_wN8wQ@mail.gmail.com>

I need to replicate some new databases.

We have 5 shards, each with an identical schema. Each of these will be
replicated to a slave (5 masters, 5 slaves).

Do I need 5 separate clusters, or can I do this with a single cluster
and 5 replication sets? I'll be using PG 8.4 and Slony-I 2.0.7.

I suspect I need 5 separate clusters, and I'm less likely to blow my
foot off with this setup, but I'm soliciting opinions before I proceed
:-)

-- 
Stuart Bishop <stuart at stuartbishop.net>
http://www.stuartbishop.net/

From bench at silentmedia.com  Tue Aug  9 09:15:47 2011
From: bench at silentmedia.com (Ben Chobot)
Date: Tue, 9 Aug 2011 09:15:47 -0700
Subject: [Slony1-general] 1 cluster or 5?
In-Reply-To: <CADmi=6PtF9vaOybL2Lxru7yJN0wAkcwc2z_mOT94mgzF_wN8wQ@mail.gmail.com>
References: <CADmi=6PtF9vaOybL2Lxru7yJN0wAkcwc2z_mOT94mgzF_wN8wQ@mail.gmail.com>
Message-ID: <EBB339B3-8166-4DBB-9802-213F349688E6@silentmedia.com>

On Aug 9, 2011, at 4:37 AM, Stuart Bishop wrote:

> I need to replicate some new databases.
> 
> We have 5 shards, each with an identical schema. Each of these will be
> replicated to a slave (5 masters, 5 slaves).
> 
> Do I need 5 separate clusters, or can I do this with a single cluster
> and 5 replication sets? I'll be using PG 8.4 and Slony-I 2.0.7.
> 
> I suspect I need 5 separate clusters, and I'm less likely to blow my
> foot off with this setup, but I'm soliciting opinions before I proceed
> :-)

Unfortunately the terminology gets overloaded. Do you mean you have 5 different PG clusters, and want a slave for each? Or do you have 1 PG cluster with 5 different schemas in it (which happen to be identical) and want a slave for each schema?

If you have multiple PG clusters, you need multiple slony clusters. If not, then you don't. A replication set can span schemas if you wish it to, and you can also have multiple replication sets per schema.

From cbbrowne at afilias.info  Tue Aug  9 09:49:42 2011
From: cbbrowne at afilias.info (Christopher Browne)
Date: Tue, 9 Aug 2011 12:49:42 -0400
Subject: [Slony1-general] 1 cluster or 5?
In-Reply-To: <CADmi=6PtF9vaOybL2Lxru7yJN0wAkcwc2z_mOT94mgzF_wN8wQ@mail.gmail.com>
References: <CADmi=6PtF9vaOybL2Lxru7yJN0wAkcwc2z_mOT94mgzF_wN8wQ@mail.gmail.com>
Message-ID: <CANfbgbZooxej99Yk_bFsNKz3fFA7F+Z2Fn7Dy=FhcPbVqATNKA@mail.gmail.com>

On Tue, Aug 9, 2011 at 7:37 AM, Stuart Bishop <stuart at stuartbishop.net> wrote:
> I need to replicate some new databases.
>
> We have 5 shards, each with an identical schema. Each of these will be
> replicated to a slave (5 masters, 5 slaves).
>
> Do I need 5 separate clusters, or can I do this with a single cluster
> and 5 replication sets? I'll be using PG 8.4 and Slony-I 2.0.7.
>
> I suspect I need 5 separate clusters, and I'm less likely to blow my
> foot off with this setup, but I'm soliciting opinions before I proceed
> :-)

By "identical schema," does that mean you have 5 databases with
identical schemas:

for db in db1 db2 db3 db4 db5; do
createdb ${db}; psql -d ${db} -c "create table public.t1 (id serial
primary key);"
done

Or one database, with several namespaces, with identical schemas within that:
createdb db
for i in sc1 sc2 sc3 sc4 sc5; do
psql -d db -c "create namespace ${i};"
psql -d db -c "create table ${i}.t1 (id serial primary key);"
done

If it looks like the latter, where the schemas are distinguishable by
virtue of namespace, then it would be pretty reasonable to have 1
cluster with 5 replication sets.

if, on the other hand, it looks like the former, where it's only the
database name that tells things apart, I'd be not too keen to try to
have just one big cluster, as there's a pretty big "foot-gun."

Let me describe the shape of the "foot-gun"...

The problem that can occur here is, since the names of tables
otherwise match, you could accidentally alter subscriptions in such a
way as to have cross-talk between sets, and accidentally have "set #5"
pulling data from "set #2", thereby mixing things in a way that you'd
probably have hoped would be immiscible.

Mind you, it shouldn't be possible to have subscriptions to "node 1's
public.t1" and "node 3's public.t1" subscribed to anywhere
simultaneously, because:
 i) Postgres only allows one table to be called public.t1 in the
subscriber database (let's say it's db6), and this combines with...
 ii) Slony requires an sl_table entry for each time a table gets
subscribed, and has a unique index on sl_table.tab_reloid.
That should nicely prevent "crosstalk."  You can't have 2
subscriptions involving the same target table on a node at the same
time.  I think that means you're reasonably protected from corruption
of replication for this kind of case.

But it doesn't prevent an onlooker from getting confused, and that's
the factor I'd put higher on my list.

One of the "nice to have" features would be the ability to rename
tables on a subscriber.  That would mean that the schema could be, on
the "master" nodes, as described in that first "for" loop, but then,
on a subscriber, the schema might look like what's in the second "for"
loop.  In that case, it's quite possible that you'd have just 6 nodes
in the cluster:
a) Five "master" nodes for the 5 systems, and
b) One "subscriber" that consolidates the data from all 5 systems.

The trouble with this feature is less about doing it and more about
coming up with a parsimonious way to describe the configuration.

