From m1n3s6 at live.com  Tue Sep  1 08:44:55 2009
From: m1n3s6 at live.com (Manish N)
Date: Tue Sep  1 08:45:07 2009
Subject: [Slony1-general] Slony Failback Issues
Message-ID: <SNT103-W5202C1AAD2490361BBA615E7F10@phx.gbl>


Hey,

I'm trying to test failover-failback between two nodes in a typical Master-=
Slave setup.

I do not wish to completely abandon my master & then get it in sync laster =
on ( via doing a pg_dump n stuff ) in case of any issues, instead I would j=
ust do a slonik_move_set to my slave which is relatively in sync with maste=
r & start replication.

I do a slonik_move_set from node1 ( master )  to node2 ( slave ) & start re=
plication everything works fine, however i can't do a failback using slonik=
_move_set from node2 back to node1, it results into following errors which =
were completely odd IMO.

slonik_move_set --config /etc/slon_tools_failover.conf set1 1 2 | slonik

<stdin>:4: Locking down set 1 on node 1

<stdin>:5: PGRES_FATAL_ERROR select
"_replication_failover".lockSet(1); select
"_replication_failover".getMaxXid();  - ERROR:  Slony-I: set 1 is
already locked

Post manually unlocking set when tried to move set i got following error, w=
here as all the updates ran on replicated tables were received on node2

<stdin>:5: PGRES_FATAL_ERROR select "_test_cluster".moveSet(1, 2);  - ERROR=
:  Slony-I: set 1 is not subscribed by node 2

P.S. I did change the node IDs as required once a set  is moved and the Sub=
scribers are also properly updated in slony's schemas on both the nodes.

Can someone please let me know what i'm doing wrong if any.

- Manish

_________________________________________________________________
Sports, news, fashion and entertainment. Pick it all up in a package called=
 MSN India
http://in.msn.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090901/=
fdac27d0/attachment.htm
From ajs at crankycanuck.ca  Tue Sep  1 10:24:41 2009
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue Sep  1 10:25:00 2009
Subject: [Slony1-general] Slony Failback Issues
In-Reply-To: <SNT103-W5202C1AAD2490361BBA615E7F10@phx.gbl>
References: <SNT103-W5202C1AAD2490361BBA615E7F10@phx.gbl>
Message-ID: <20090901172441.GL14929@shinkuro.com>

On Tue, Sep 01, 2009 at 09:14:55PM +0530, Manish N wrote:

> I'm trying to test failover-failback between two nodes in a typical Master-Slave setup.
> 
> I do not wish to completely abandon my master & then get it in sync
> laster on ( via doing a pg_dump n stuff ) in case of any issues,
> instead I would just do a slonik_move_set to my slave which is
> relatively in sync with master & start replication.

Are both nodes up when you do this?

A

-- 
Andrew Sullivan
ajs@crankycanuck.ca
From lai at clustersolutions.net  Tue Sep  1 14:39:56 2009
From: lai at clustersolutions.net (lai@clustersolutions.net)
Date: Tue Sep  1 14:52:55 2009
Subject: [Slony1-general] IP Change
In-Reply-To: <20090901172441.GL14929@shinkuro.com>
References: <SNT103-W5202C1AAD2490361BBA615E7F10@phx.gbl>
	<20090901172441.GL14929@shinkuro.com>
Message-ID: <4151.75.25.11.233.1251841196.squirrel@www.clustersolutions.net>

Hello, we are moving our office and our IP will change. According to the
doc, it seems all I would need to do is to modify the config file with the
new IP, and then issue slonik_subscribe_set and that should do the update?

You comments are greatly appreciated. Thanks!

Tim


From jks at selectacast.net  Tue Sep  1 15:03:26 2009
From: jks at selectacast.net (Joseph S)
Date: Tue Sep  1 15:03:57 2009
Subject: [Slony1-general] Vacuum of sl_1 and 2 logs. (postgres)
In-Reply-To: <87vdlkpeup.fsf@dba2.int.libertyrms.com>
References: <8a547c840907221134p29d896b0r1cd38a5cf3bd3080@mail.gmail.com>	<871vo8qxli.fsf@dba2.int.libertyrms.com>	<8a547c840907221402p4fe844edw642e687e6294acb2@mail.gmail.com>
	<87vdlkpeup.fsf@dba2.int.libertyrms.com>
Message-ID: <4A9D9A2E.5040008@selectacast.net>

Slony does know when its about to truncate a table though.  I ran into 
this case with a bloated table:

NOTICE:  Slony-I: log switch to sl_log_2 still in progress - sl_log_1 
not truncated

WARNING:  relation "_crod.sl_log_1" contains more than "max_fsm_pages" 
pages with useful free space
HINT:  Consider compacting this relation or increasing the configuration 
parameter "max_fsm_pages".

What happened was that replication broke for a while. After I got it 
going again there ended up being a lot of deleted rows in sl_log_1, 
which was about to get truncated when slony got caught up.  Meanwhile my 
database has been really slow today as sl_log_1 got vacuumed again and 
again.

Before I found this thread I was going to ask the list if it was a good 
idea to disable autovacuum for the sl_log_X tables since they keep 
getting truncated.  I can't figure out how often a logswitch happens, or 
if there is any way to configure it, and I can't figure out how slony 
decides to launch its own vacuum.

Christopher Browne wrote:
> Tory M Blue <tmblue@gmail.com> writes:
>   
>> On Wed, Jul 22, 2009 at 1:19 PM, Christopher
>> Browne<cbbrowne@ca.afilias.info> wrote:
>>     
>>> Tory M Blue <tmblue@gmail.com> writes:
>>>       
>>>> So I've noticed recently that I'm vacuuming the sl_?.log files with
>>>> postgres and this doesn't appear right. The fact is slon has it's own
>>>> process for dealing with this and I believe it's a clean truncate.
>>>>         
>>> I would actually counsel taking the opposite approach, that it may be
>>> preferable for autovacuum to handle vacuuming the Slony-I tables than
>>> for Slony-I to do it itself.
>>>
>>> Autovacuum should have a better capability to cope with the dual factors
>>> of:
>>>  a) Needing to vacuum some tables "even more often", as well as
>>>  b) Needing to not vacuum some tables very often.
>>>
>>> In principle, we could make the cleanup thread in Slony-I smarter, but
>>> that would duplicate the good work that has gone into the PostgreSQL
>>> built-in...
>>>       
>> Ahh good info, although I would think that a postgres vacuum, using
>> delete's would be worse than a slon truncate of said table once
>> everything was replicated?
>>
>> I have major index bloat and looking for anything and everything that
>> could help with it.
>>     
>
> I'll illustrate with a couple examples...
>
> Consider the case where we have Slony-I do the vacuuming itself...
>
> Comparison #1: sl_log_1
>
>  - Every 3 iterations of the cleanup thread, by default, every 30
>    minutes, it would vacuum all of its tables, bloated or not.
>
>    Supposing sl_log_1 has lots of junk in it (deleted tuples or not),
>    lots of time will be spent vacuuming it every 30 minutes,
>    needed/useful or not.
>
> On the other hand, if the autovac thread handles this, then...
>
>  - If you're running Slony-I 2.0, where only TRUNCATE is used, there
>    should never be a material # of dead tuples in sl_log_1,
>    so you can expect it to *NEVER* vacuum sl_log_1.
>
>    Winner: autovacuum :-)
>
> Comparison #2: sl_confirm
>
>    This table gets trimmed fairly often.  But let's suppose it's
>    getting pretty bloated...
>
>    - If we use Slony-I cleanup thread to vacuum, then it'll vacuum it
>      every 30 minutes, regardless of usefulness.
>
>    - If we use autovac, then:
>
>        a) autovac may vacuum it *more* often, if it's getting tuples
>           trimmed frequently
>
>        b) On the other hand, if tuples don't get trimmed out for 2
>           hours due to something holding onto data, then there will
>           be NO deletes/updates to sl_confirm, and autovac won't
>           bother vacuuming it.
>
>    In either case, I expect autovacuum to be preferable.
>
> There are various pathological cases characterized by the above where
> "have the Slony-I cleanup thread do it" is distinctly inferior to "use
> autovacuum."
>
> I'm *STRONGLY* disinclined to try to improve the cleanup thread's
> logic in this regard; relevant development effort would be *WAY*
> better spent improving autovacuum, as that would be helpful to more
> than just Slony-I users.
>   
From cbbrowne at ca.afilias.info  Tue Sep  1 15:39:38 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Sep  1 15:40:07 2009
Subject: [Slony1-general] IP Change
In-Reply-To: <4151.75.25.11.233.1251841196.squirrel@www.clustersolutions.net>
	(lai@clustersolutions.net's message of "Tue, 1 Sep 2009 14:39:56 -0700
	(PDT)")
References: <SNT103-W5202C1AAD2490361BBA615E7F10@phx.gbl>
	<20090901172441.GL14929@shinkuro.com>
	<4151.75.25.11.233.1251841196.squirrel@www.clustersolutions.net>
Message-ID: <87ab1eguhh.fsf@dba2.int.libertyrms.com>

lai@clustersolutions.net writes:
> Hello, we are moving our office and our IP will change. According to the
> doc, it seems all I would need to do is to modify the config file with the
> new IP, and then issue slonik_subscribe_set and that should do the update?

You shouldn't need any revisions of subscriptions; the changes would
take place in two places:

a) Whatever process you use to start up the slons needs to be aware of
the new IP addresses for the nodes that they manage.

b) If the addresses of the nodes change, then you'll need to resubmit
the STORE PATH requests to indicate how the slons talk to *other* nodes
than their own.

If you used named hosts, then it mightn't be necessary to change Slony-I
configuration in any way - you might just need to do a DNS or
/etc/hosts.conf change to indicate the new IP addresses for the hosts.
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From m1n3s6 at live.com  Tue Sep  1 20:30:51 2009
From: m1n3s6 at live.com (Manish N)
Date: Tue Sep  1 20:31:31 2009
Subject: [Slony1-general] Slony Failback Issues
In-Reply-To: <20090901172441.GL14929@shinkuro.com>
References: <SNT103-W5202C1AAD2490361BBA615E7F10@phx.gbl>
	<20090901172441.GL14929@shinkuro.com>
Message-ID: <SNT103-W16E94EB37415873EA65D79E7F00@phx.gbl>



> Date: Tue, 1 Sep 2009 13:24:41 -0400
> From: ajs@crankycanuck.ca
> To: slony1-general@lists.slony.info
> Subject: Re: [Slony1-general] Slony Failback Issues
> =

> On Tue, Sep 01, 2009 at 09:14:55PM +0530, Manish N wrote:
> =

> > I'm trying to test failover-failback between two nodes in a typical Mas=
ter-Slave setup.
> > =

> > I do not wish to completely abandon my master & then get it in sync
> > laster on ( via doing a pg_dump n stuff ) in case of any issues,
> > instead I would just do a slonik_move_set to my slave which is
> > relatively in sync with master & start replication.
> =

> Are both nodes up when you do this?
> =

> A

Yup, both nodes are up.



- Manish



_________________________________________________________________
One stop at MSN India to catch up with what=92s hot in the world around you=
 today
http://in.msn.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090902/=
2c134e35/attachment.htm
From kleptog at gmail.com  Wed Sep  2 01:34:06 2009
From: kleptog at gmail.com (Martijn van Oosterhout)
Date: Wed Sep  2 01:35:01 2009
Subject: [Slony1-general] Autogenerating slonik preamble
Message-ID: <2fc2c5f10909020134j53095655ge115b8125cdb164e@mail.gmail.com>

When you use a slonik you need to provide a preamble each time
defining the nodes and how to connect to them. It seems to me that
this information is already present in each node, thus if you can
connect to any node you can extract the necessary information to find
the other nodes. I suppose the objection is that path from one node to
another is not necessarily globally the same?

I've been thinking of a script that reads a simple config file listing
the nodes with paths and how they are connected. The script would then
generate a slonik script that executes the necessary command to take
the cluster from the current configuration to the specified
configuration. Has anyone heard of such a script?

Have a nice day,
-- 
Martijn van Oosterhout <kleptog@gmail.com> http://svana.org/kleptog/
From s.le_ray+slony1 at eutech-ssii.com  Wed Sep  2 01:40:29 2009
From: s.le_ray+slony1 at eutech-ssii.com (=?UTF-8?B?U8OpYmFzdGllbg==?= Le Ray)
Date: Wed Sep  2 01:41:32 2009
Subject: [Slony1-general] Autogenerating slonik preamble
In-Reply-To: <2fc2c5f10909020134j53095655ge115b8125cdb164e@mail.gmail.com>
References: <2fc2c5f10909020134j53095655ge115b8125cdb164e@mail.gmail.com>
Message-ID: <20090902104029.3574c5bf@thebes.eutech-ssii.com>

TGUgV2VkLCAyIFNlcCAyMDA5IDEwOjM0OjA2ICswMjAwLApNYXJ0aWpuIHZhbiBPb3N0ZXJob3V0
IDxrbGVwdG9nQGdtYWlsLmNvbT4gYSDDqWNyaXQgOgoKPiBXaGVuIHlvdSB1c2UgYSBzbG9uaWsg
eW91IG5lZWQgdG8gcHJvdmlkZSBhIHByZWFtYmxlIGVhY2ggdGltZQo+IGRlZmluaW5nIHRoZSBu
b2RlcyBhbmQgaG93IHRvIGNvbm5lY3QgdG8gdGhlbS4gSXQgc2VlbXMgdG8gbWUgdGhhdAo+IHRo
aXMgaW5mb3JtYXRpb24gaXMgYWxyZWFkeSBwcmVzZW50IGluIGVhY2ggbm9kZSwgdGh1cyBpZiB5
b3UgY2FuCj4gY29ubmVjdCB0byBhbnkgbm9kZSB5b3UgY2FuIGV4dHJhY3QgdGhlIG5lY2Vzc2Fy
eSBpbmZvcm1hdGlvbiB0byBmaW5kCj4gdGhlIG90aGVyIG5vZGVzLiBJIHN1cHBvc2UgdGhlIG9i
amVjdGlvbiBpcyB0aGF0IHBhdGggZnJvbSBvbmUgbm9kZSB0bwo+IGFub3RoZXIgaXMgbm90IG5l
Y2Vzc2FyaWx5IGdsb2JhbGx5IHRoZSBzYW1lPwo+IAo+IEkndmUgYmVlbiB0aGlua2luZyBvZiBh
IHNjcmlwdCB0aGF0IHJlYWRzIGEgc2ltcGxlIGNvbmZpZyBmaWxlIGxpc3RpbmcKPiB0aGUgbm9k
ZXMgd2l0aCBwYXRocyBhbmQgaG93IHRoZXkgYXJlIGNvbm5lY3RlZC4gVGhlIHNjcmlwdCB3b3Vs
ZCB0aGVuCj4gZ2VuZXJhdGUgYSBzbG9uaWsgc2NyaXB0IHRoYXQgZXhlY3V0ZXMgdGhlIG5lY2Vz
c2FyeSBjb21tYW5kIHRvIHRha2UKPiB0aGUgY2x1c3RlciBmcm9tIHRoZSBjdXJyZW50IGNvbmZp
Z3VyYXRpb24gdG8gdGhlIHNwZWNpZmllZAo+IGNvbmZpZ3VyYXRpb24uIEhhcyBhbnlvbmUgaGVh
cmQgb2Ygc3VjaCBhIHNjcmlwdD8KPiAKPiBIYXZlIGEgbmljZSBkYXksCgpIaSwKCndoYXQgdGhl
IGJlbmVmaXRzIGluIHJlZ2FyZHMgb2Ygc2xvbmlrX3ByaW50X3ByZWFtYmxlID8KClJlZ2FyZHMK
ClNlYmFzdGllbgotLS0tLS0tLS0tLS0tLSBuZXh0IHBhcnQgLS0tLS0tLS0tLS0tLS0KQSBub24t
dGV4dCBhdHRhY2htZW50IHdhcyBzY3J1YmJlZC4uLgpOYW1lOiBzaWduYXR1cmUuYXNjClR5cGU6
IGFwcGxpY2F0aW9uL3BncC1zaWduYXR1cmUKU2l6ZTogMTk3IGJ5dGVzCkRlc2M6IG5vdCBhdmFp
bGFibGUKVXJsIDogaHR0cDovL2xpc3RzLnNsb255LmluZm8vcGlwZXJtYWlsL3Nsb255MS1nZW5l
cmFsL2F0dGFjaG1lbnRzLzIwMDkwOTAyLzUyNzcwOTU5L3NpZ25hdHVyZS5wZ3AK
From kleptog at gmail.com  Wed Sep  2 04:03:55 2009
From: kleptog at gmail.com (Martijn van Oosterhout)
Date: Wed Sep  2 04:04:55 2009
Subject: [Slony1-general] Autogenerating slonik preamble
In-Reply-To: <20090902104029.3574c5bf@thebes.eutech-ssii.com>
References: <2fc2c5f10909020134j53095655ge115b8125cdb164e@mail.gmail.com>
	<20090902104029.3574c5bf@thebes.eutech-ssii.com>
Message-ID: <2fc2c5f10909020403l7f7a8190l1a07a2ba8212f96a@mail.gmail.com>

On Wed, Sep 2, 2009 at 10:40 AM, S?bastien Le
Ray<s.le_ray+slony1@eutech-ssii.com> wrote:
> Hi,
>
> what the benefits in regards of slonik_print_preamble ?

Print preamble already needs a config file with all the information,
so that kind of misses the point. I'm trying to find a way to manage
all the machines in a cluster without having to keep all the copies of
slon_tools.conf up to date. If the config could be generated from a
running node that would be a lot easier.

I'm thinking of going the other way and have a completely seperate
machine which is the only one that can manage the cluster.

Have a nice day,
-- 
Martijn van Oosterhout <kleptog@gmail.com> http://svana.org/kleptog/
From bart.geesink at sara.nl  Wed Sep  2 07:16:47 2009
From: bart.geesink at sara.nl (Bart Geesink)
Date: Wed Sep  2 07:16:57 2009
Subject: [Slony1-general] replicating pg_largeobjects
Message-ID: <4A9E7E4F.3090004@sara.nl>

Hi list,

We host a Postgres database for a client of ours, which is replicated 
with Slony to a second host. It occured to me that there was a size 
difference between the two databases. Reason was that some data is 
stored in the system table pg_largeobjects table. When I tried to 
replicate the table pg_catalog.pg_largeobject, Slony started to complain 
that no primary key is present on that table. Apparently, adding Is 
there a workaround for this so I can replicate the system table 
pg_largeobjects as well?

Thanks in advance,

Bart Geesink


From nagy at ecircle-ag.com  Wed Sep  2 07:29:47 2009
From: nagy at ecircle-ag.com (Csaba Nagy)
Date: Wed Sep  2 07:29:51 2009
Subject: [Slony1-general] replicating pg_largeobjects
In-Reply-To: <4A9E7E4F.3090004@sara.nl>
References: <4A9E7E4F.3090004@sara.nl>
Message-ID: <1251901787.3240.135.camel@pcd12478>

Hi Bart,

On Wed, 2009-09-02 at 16:16 +0200, Bart Geesink wrote:
> [snip] Apparently, adding Is 
> there a workaround for this so I can replicate the system table 
> pg_largeobjects as well?

As far as I know there's no way to replicate the large objects via
slony, as you can't create triggers on system tables, and slony needs
one on each replicated table to intercept the data changes.

For exactly this reason we abandoned the use of large objects and
replaced it with a custom table, similar with pg_largeobject, and some
custom code to simulate on the client side what postgres is doing when
you work with the large objects. It was in fact surprisingly little code
needed to do it... and we can now replicate our "large objects" via
slony.

Cheers,
Csaba.


From lai at clustersolutions.net  Wed Sep  2 22:48:38 2009
From: lai at clustersolutions.net (lai@clustersolutions.net)
Date: Wed Sep  2 23:02:00 2009
Subject: [Slony1-general] IP Change
In-Reply-To: <87ab1eguhh.fsf@dba2.int.libertyrms.com>
References: <SNT103-W5202C1AAD2490361BBA615E7F10@phx.gbl>
	<20090901172441.GL14929@shinkuro.com>
	<4151.75.25.11.233.1251841196.squirrel@www.clustersolutions.net>
	<87ab1eguhh.fsf@dba2.int.libertyrms.com>
Message-ID: <1360.24.24.245.248.1251956918.squirrel@www.clustersolutions.net>

Thanks! Will give that a try...Tim

> lai@clustersolutions.net writes:
>> Hello, we are moving our office and our IP will change. According to the
>> doc, it seems all I would need to do is to modify the config file with
>> the
>> new IP, and then issue slonik_subscribe_set and that should do the
>> update?
>
> You shouldn't need any revisions of subscriptions; the changes would
> take place in two places:
>
> a) Whatever process you use to start up the slons needs to be aware of
> the new IP addresses for the nodes that they manage.
>
> b) If the addresses of the nodes change, then you'll need to resubmit
> the STORE PATH requests to indicate how the slons talk to *other* nodes
> than their own.
>
> If you used named hosts, then it mightn't be necessary to change Slony-I
> configuration in any way - you might just need to do a DNS or
> /etc/hosts.conf change to indicate the new IP addresses for the hosts.
> --
> output = reverse("ofni.sailifa.ac" "@" "enworbbc")
> Christopher Browne
> "Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
> phasers on the Heffalump, Piglet, meet me in transporter room three"
>


From nettreeinc at gmail.com  Thu Sep  3 02:58:30 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Thu Sep  3 02:59:26 2009
Subject: [Slony1-general] where to find data transaction log?
Message-ID: <25272928.post@talk.nabble.com>


Hi,

By doing the clustering, I am trying to find out status such as how many
records got updated, and time that it started and finished update. 

Does anyone know where I can find log that will give me these informations?
or a tool that can college those infos for me?

I have enabled useful pramaters under "runtime statistic" in
postgresql.conf. But I don't know where to find infomation that I need. 




-- 
View this message in context: http://www.nabble.com/where-to-find-data-transaction-log--tp25272928p25272928.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From mnagaraja at alcatel-lucent.com  Thu Sep  3 03:20:34 2009
From: mnagaraja at alcatel-lucent.com (Nagaraja, Madhukar (Madhukar))
Date: Thu Sep  3 03:21:35 2009
Subject: [Slony1-general] Slony caused a Deadlock after a switchover.
Message-ID: <E9F099885B445541A55D830FC62E175A0B67120994@INBANSXCHMBSA3.in.alcatel-lucent.com>

Hello,
We have been using the slony for the past few months. We are still in dev/test cycle and not yet gone to production. But recently we did a switchover operation and we got this error in the slony logs. Can you help us out in this ?? Please let me know if you need any more information. We are using slony1 2.0.1.

This is the error we got:

<stdin>:5: PGRES_FATAL_ERROR select "_evslonycluster".lockSet(1); select pg_catalog.txid_snapshot_xmax(pg_catalog.txid_current_snapshot()); - ERROR:  deadlock detected

DETAIL:  Process 12169 waits for AccessExclusiveLock on relation 17591 of database 16386; blocked by process 1987.

Process 1987 waits for AccessShareLock on relation 17480 of database 16386; blocked by process 12169.

CONTEXT:  SQL statement "create trigger "_evslonycluster_lockedset" before insert or update or delete on "public"."domaincontrollerinfo" for each row execute procedure

                                "_evslonycluster".lockedSet ('_evslonycluster');"

PL/pgSQL function "lockset" line 44 at EXECUTE statement

Thanks.
From cbbrowne at ca.afilias.info  Thu Sep  3 09:12:34 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Sep  3 09:12:49 2009
Subject: [Slony1-general] where to find data transaction log?
In-Reply-To: <25272928.post@talk.nabble.com> (roctaiwan's message of "Thu, 3
	Sep 2009 02:58:30 -0700 (PDT)")
References: <25272928.post@talk.nabble.com>
Message-ID: <87eiqof1n1.fsf@dba2.int.libertyrms.com>

roctaiwan <nettreeinc@gmail.com> writes:
> By doing the clustering, I am trying to find out status such as how many
> records got updated, and time that it started and finished update. 
>
> Does anyone know where I can find log that will give me these informations?
> or a tool that can college those infos for me?
>
> I have enabled useful pramaters under "runtime statistic" in
> postgresql.conf. But I don't know where to find infomation that I need. 

Starting in 1.2, we started reporting numbers of inserts/updates/deletes
in the slon logs; look for lines looking like:

   "remoteHelperThread_%d_%d: inserts=%d updates=%d deletes=%d\n",

In 1.2, this is reported at debug level SLON_DEBUG2.

In 2.0, this is reported at debug level SLON_DEBUG1.

See the slon log output; that will be stored whereever you choose to
store it.
-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@" [name;tld];;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Thu Sep  3 09:14:46 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Sep  3 09:14:58 2009
Subject: [Slony1-general] Autogenerating slonik preamble
In-Reply-To: <2fc2c5f10909020134j53095655ge115b8125cdb164e@mail.gmail.com>
	(Martijn van Oosterhout's message of "Wed, 2 Sep 2009 10:34:06 +0200")
References: <2fc2c5f10909020134j53095655ge115b8125cdb164e@mail.gmail.com>
Message-ID: <87ab1cf1jd.fsf@dba2.int.libertyrms.com>

Martijn van Oosterhout <kleptog@gmail.com> writes:
> I've been thinking of a script that reads a simple config file listing
> the nodes with paths and how they are connected. The script would then
> generate a slonik script that executes the necessary command to take
> the cluster from the current configuration to the specified
> configuration. Has anyone heard of such a script?


This script was set up to help with upgrades from 1.2 to 2.0; it dumps
out this sort of information, so might be useful for your purposes.

http://main.slony.info/viewcvs/viewvc.cgi/slony1-engine/tools/slonikconfdump.sh?revision=1.1.2.4&view=markup
-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@" [name;tld];;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From michael at aers.ca  Thu Sep  3 13:39:38 2009
From: michael at aers.ca (michael@aers.ca)
Date: Thu Sep  3 13:40:05 2009
Subject: [Slony1-general] Replicating 1 table from various sources
Message-ID: <6B5AF6293A289F45826220B17ABE7937FDBD6B@BORON.aers.local>

Hi everyone, I've got a situation I need some help with. I have a group
of 4 servers - two in one city and two in another. There's about 45
tables, most of which can be replicated through the whole set without
any issue. I have two tables that I want replicated from Node 1-2 and
Node 3-4, but I do not want replicated from 1-3 or 2-3. Basically, each
city needs to have its own data for these two tables and the data should
only be replicated between nodes in the same city. 

 

It seems to me that I can't have one table in two sets, even if it uses
different origin nodes. Any tips?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090903/7fb8c4c6/attachment.htm
From glynastill at yahoo.co.uk  Thu Sep  3 13:52:13 2009
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Thu Sep  3 17:37:03 2009
Subject: [Slony1-general] Autogenerating slonik preamble
In-Reply-To: <87ab1cf1jd.fsf@dba2.int.libertyrms.com>
Message-ID: <619325.40628.qm@web23605.mail.ird.yahoo.com>

--- On Thu, 3/9/09, Christopher Browne <cbbrowne@ca.afilias.info> wrote:
> 
> This script was set up to help with upgrades from 1.2 to
> 2.0; it dumps
> out this sort of information, so might be useful for your
> purposes.
> 
> http://main.slony.info/viewcvs/viewvc.cgi/slony1-engine/tools/slonikconfdump.sh?revision=1.1.2.4&view=markup

Forgive me for not following recently, but have we got any sort of rough upgrade procedure from 1.2.x to 2.0.x yet?


      
From nettreeinc at gmail.com  Thu Sep  3 20:11:10 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Thu Sep  3 20:11:52 2009
Subject: [Slony1-general] where to find data transaction log?
In-Reply-To: <87eiqof1n1.fsf@dba2.int.libertyrms.com>
References: <25272928.post@talk.nabble.com>
	<87eiqof1n1.fsf@dba2.int.libertyrms.com>
Message-ID: <25287629.post@talk.nabble.com>


Hi Chris,

Thanks for your answer. 
You said something about 1.2, 2.0 is that chapter number for Slony manual?
Can you tell me where is the default location to find slony logs? Do I have
to enable it in conf file? which one and how?



Huang


Christopher Browne wrote:
> 
> 
> Starting in 1.2, we started reporting numbers of inserts/updates/deletes
> in the slon logs; look for lines looking like:
> 
>    "remoteHelperThread_%d_%d: inserts=%d updates=%d deletes=%d\n",
> 
> In 1.2, this is reported at debug level SLON_DEBUG2.
> 
> In 2.0, this is reported at debug level SLON_DEBUG1.
> 
> See the slon log output; that will be stored whereever you choose to
> store it.
> -- 
> 
> 
> 

-- 
View this message in context: http://www.nabble.com/where-to-find-data-transaction-log--tp25272928p25287629.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Fri Sep  4 01:55:34 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Fri Sep  4 01:56:32 2009
Subject: [Slony1-general] Slony replication monitoring
In-Reply-To: <475D1DF0.8040707@dalibo.com>
References: <72136.77509.qm@web25802.mail.ukl.yahoo.com>
	<200712070655.25479.darcyb@commandprompt.com>
	<475D1DF0.8040707@dalibo.com>
Message-ID: <25290598.post@talk.nabble.com>


could you explain more detail on how did you set a new table to collect table
update times



C?dric Villemain wrote:
> 
> Darcy Buskermolen a ?crit :
>> On Friday 07 December 2007 06:52:05 Glyn Astill wrote:
>>   
>>> Hi All,
>>>
>>> Is there a way we can monitor Slony to ensure it is in sync and that
>>> it is still working?
>>>
>>> Also is there any way to validate our data?
>>>
>>> Reporting tools or shell script ideas would be ace
>>>     
>>
>> on the origin server:
>>
>> select * from _CLUSTERNAME.sl_status;
>>
>>
>>
>>   
> I used to add a table 'last_update_X' to setX, updated every second, and 
> then check the result accross the cluster and graph the diff
>>> Glyn Astill
>>>
>>>
>>>
>>>       __________________________________________________________
>>> Sent from Yahoo! Mail - a smarter inbox http://uk.mail.yahoo.com
>>>
>>>
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general@lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>     
>>
>>
>>
>>   
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/Slony-replication-monitoring-tp14213956p25290598.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From cbbrowne at ca.afilias.info  Fri Sep  4 09:26:56 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Sep  4 09:27:20 2009
Subject: [Slony1-general] Autogenerating slonik preamble
In-Reply-To: <619325.40628.qm@web23605.mail.ird.yahoo.com> (Glyn Astill's
	message of "Thu, 3 Sep 2009 20:52:13 +0000 (GMT)")
References: <619325.40628.qm@web23605.mail.ird.yahoo.com>
Message-ID: <87hbviekvj.fsf@dba2.int.libertyrms.com>

Glyn Astill <glynastill@yahoo.co.uk> writes:
> --- On Thu, 3/9/09, Christopher Browne <cbbrowne@ca.afilias.info> wrote:
>> 
>> This script was set up to help with upgrades from 1.2 to
>> 2.0; it dumps
>> out this sort of information, so might be useful for your
>> purposes.
>> 
>> http://main.slony.info/viewcvs/viewvc.cgi/slony1-engine/tools/slonikconfdump.sh?revision=1.1.2.4&view=markup
>
> Forgive me for not following recently, but have we got any sort of rough upgrade procedure from 1.2.x to 2.0.x yet?

Yup, see the lower bits of...
http://main.slony.info/documentation/slonyupgrade.html
-- 
output = ("cbbrowne" "@" "ca.afilias.info")
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Fri Sep  4 09:50:54 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Fri Sep  4 09:51:08 2009
Subject: [Slony1-general] where to find data transaction log?
In-Reply-To: <25287629.post@talk.nabble.com> (roctaiwan's message of "Thu, 3
	Sep 2009 20:11:10 -0700 (PDT)")
References: <25272928.post@talk.nabble.com>
	<87eiqof1n1.fsf@dba2.int.libertyrms.com>
	<25287629.post@talk.nabble.com>
Message-ID: <87d466ejrl.fsf@dba2.int.libertyrms.com>

roctaiwan <nettreeinc@gmail.com> writes:
> Thanks for your answer. 
> You said something about 1.2, 2.0 is that chapter number for Slony manual?
> Can you tell me where is the default location to find slony logs? Do I have
> to enable it in conf file? which one and how?

1.  By default, each slon process logs its activity to standard output.

If you capture that, then the output will go wherever you chose to
capture the output.

2.  You might configure the slon (the -f option) to log via syslog, in
which case you would look to your system's syslog implementation to see
where it puts logs.
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From nettreeinc at gmail.com  Mon Sep  7 03:20:03 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Mon Sep  7 03:20:39 2009
Subject: [Slony1-general] where to find data transaction log?
In-Reply-To: <87d466ejrl.fsf@dba2.int.libertyrms.com>
References: <25272928.post@talk.nabble.com>
	<87eiqof1n1.fsf@dba2.int.libertyrms.com>
	<25287629.post@talk.nabble.com>
	<87d466ejrl.fsf@dba2.int.libertyrms.com>
Message-ID: <25328241.post@talk.nabble.com>




Christopher Browne wrote:
> 
> roctaiwan <nettreeinc@gmail.com> writes:
>> Thanks for your answer. 
>> You said something about 1.2, 2.0 is that chapter number for Slony
>> manual?
>> Can you tell me where is the default location to find slony logs? Do I
>> have
>> to enable it in conf file? which one and how?
> 
> 1.  By default, each slon process logs its activity to standard output.
> 
> If you capture that, then the output will go wherever you chose to
> capture the output.
> 
> 2.  You might configure the slon (the -f option) to log via syslog, in
> which case you would look to your system's syslog implementation to see
> where it puts logs.
> -- 
> Christopher Browne
> 

Christ, 
When you provide me answers, please treat me as a newbee and descirbe in
more detail on how things work! this would eliminate I repeatly asking
relatively same question over and over, save some complication when people
reading our conversation. 


1. You mean when starting the slon I can do something like:
"slon sql_cluster "dbname=something user=postgres" & > logfile.log" ?

2. this is something I am not familar with. For -f option, first I don't
know where is slon's configuration file called. Second, you said I could
generate log via syslog? how does that work? 
-- 
View this message in context: http://www.nabble.com/where-to-find-data-transaction-log--tp25272928p25328241.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From gordo169 at gmail.com  Mon Sep  7 09:03:22 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Mon Sep  7 09:03:37 2009
Subject: [Slony1-general] Does every node really need a path to every other
	node?
Message-ID: <25332701.post@talk.nabble.com>


The documentation at
http://www.slony.info/documentation/slonylistenercosts.html states that 

"It is necessary to have sl_listen entries allowing connection from each
node to every other node. Most will normally not need to be [used] (sic)
very heavily, but it still means that there needs to be n(n-1) paths."

I have been running perfectly fine for several months where this is not
true.  I have a master node A with 4 sets.  Node B replicates all 4 sets and
there is of course a path between A and B.  Node C replicates 2 sets and
there is a path between A and C.  But there is no path between B and C.  

As far as I know, this simply means that I will be restricted if I try to do
things like FAILOVER or MOVE SET.  I'm thinking if I need to do that, I can
add the necessary paths at that time.  But not having the extra path from
B-C all the time saves resources.

In any case, the documentation implies that I MUST have the connection, and
that does not appear to be the case.  

Thanks,
Gordon

-- 
View this message in context: http://www.nabble.com/Does-every-node-really-need-a-path-to-every-other-node--tp25332701p25332701.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From wmoran at potentialtech.com  Mon Sep  7 09:14:02 2009
From: wmoran at potentialtech.com (Bill Moran)
Date: Mon Sep  7 09:14:29 2009
Subject: [Slony1-general] Does every node really need a path to every
	other node?
In-Reply-To: <25332701.post@talk.nabble.com>
References: <25332701.post@talk.nabble.com>
Message-ID: <20090907121402.38f6dbb1.wmoran@potentialtech.com>

Gordon Shannon <gordo169@gmail.com> wrote:
> 
> The documentation at
> http://www.slony.info/documentation/slonylistenercosts.html states that 
> 
> "It is necessary to have sl_listen entries allowing connection from each
> node to every other node. Most will normally not need to be [used] (sic)
> very heavily, but it still means that there needs to be n(n-1) paths."
> 
> I have been running perfectly fine for several months where this is not
> true.  I have a master node A with 4 sets.  Node B replicates all 4 sets and
> there is of course a path between A and B.  Node C replicates 2 sets and
> there is a path between A and C.  But there is no path between B and C.  
> 
> As far as I know, this simply means that I will be restricted if I try to do
> things like FAILOVER or MOVE SET.  I'm thinking if I need to do that, I can
> add the necessary paths at that time.  But not having the extra path from
> B-C all the time saves resources.

Which resources?  What are you talking about?  What resources get hogged by
a single config option?

> In any case, the documentation implies that I MUST have the connection, and
> that does not appear to be the case.  

Well, you said it yourself that you don't care about switchover working and
that you'll fix it if it's needed.  So, the key here is that your Slony
config is broken in a way that you find acceptable.  Thus your setup is OK
for your particular (strange) need, and the documentation is correct as well.

-- 
Bill Moran
http://www.potentialtech.com
From nettreeinc at gmail.com  Tue Sep  8 03:21:12 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Sep  8 03:21:59 2009
Subject: [Slony1-general] where to find data transaction log?
In-Reply-To: <87eiqof1n1.fsf@dba2.int.libertyrms.com>
References: <25272928.post@talk.nabble.com>
	<87eiqof1n1.fsf@dba2.int.libertyrms.com>
Message-ID: <25343388.post@talk.nabble.com>


I have pay attention on the remotehelperthread log and I have this
"2009-09-08 18:13:25 CST DEBUG2 remoteHelperThread_1_1: inserts=110105
updates=0 deletes=110005"

what does it mean insert=110105? does that tells me number of records got
insert and deletes, in this case 110105-110005=100 which says there should
have 100 records in my table? (this is correct. I do only have 100 records
in that table)
The time up there, is that the exact time for this slaves end SYNC all
records? 

if I want to find out when did it started where should I pay attention to?

Huang





Christopher Browne wrote:
> 
> Starting in 1.2, we started reporting numbers of inserts/updates/deletes
> in the slon logs; look for lines looking like:
> 
>    "remoteHelperThread_%d_%d: inserts=%d updates=%d deletes=%d\n",
> 
> In 1.2, this is reported at debug level SLON_DEBUG2.
> 
> In 2.0, this is reported at debug level SLON_DEBUG1.
> 
> See the slon log output; that will be stored whereever you choose to
> store it.
> 
> 

-- 
View this message in context: http://www.nabble.com/where-to-find-data-transaction-log--tp25272928p25343388.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From michael at aers.ca  Tue Sep  8 08:53:39 2009
From: michael at aers.ca (michael@aers.ca)
Date: Tue Sep  8 08:56:38 2009
Subject: [Slony1-general] Does every node really need a path to every
	othernode?
In-Reply-To: <25332701.post@talk.nabble.com>
References: <25332701.post@talk.nabble.com>
Message-ID: <6B5AF6293A289F45826220B17ABE7937FDBDCF@BORON.aers.local>

Speaking as a relative newbie to Slony, that's my understanding as well
but I think the amount of resources saved would be extremely minor any
way you look at it and I'm not sure why you'd want to have something
that needs to be configured at the time of failover when it could have
been done long before. I'd rather have everything pre-configured.

-----Original Message-----
From: slony1-general-bounces@lists.slony.info
[mailto:slony1-general-bounces@lists.slony.info] On Behalf Of Gordon
Shannon
Sent: Monday, September 07, 2009 9:03 AM
To: slony1-general@lists.slony.info
Subject: [Slony1-general] Does every node really need a path to every
othernode?


The documentation at
http://www.slony.info/documentation/slonylistenercosts.html states that 

"It is necessary to have sl_listen entries allowing connection from each
node to every other node. Most will normally not need to be [used] (sic)
very heavily, but it still means that there needs to be n(n-1) paths."

I have been running perfectly fine for several months where this is not
true.  I have a master node A with 4 sets.  Node B replicates all 4 sets
and
there is of course a path between A and B.  Node C replicates 2 sets and
there is a path between A and C.  But there is no path between B and C.


As far as I know, this simply means that I will be restricted if I try
to do
things like FAILOVER or MOVE SET.  I'm thinking if I need to do that, I
can
add the necessary paths at that time.  But not having the extra path
from
B-C all the time saves resources.

In any case, the documentation implies that I MUST have the connection,
and
that does not appear to be the case.  

Thanks,
Gordon

-- 
View this message in context:
http://www.nabble.com/Does-every-node-really-need-a-path-to-every-other-
node--tp25332701p25332701.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general
From cbbrowne at ca.afilias.info  Tue Sep  8 09:46:45 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Sep  8 09:47:00 2009
Subject: [Slony1-general] where to find data transaction log?
In-Reply-To: <25343388.post@talk.nabble.com> (roctaiwan's message of "Tue, 8
	Sep 2009 03:21:12 -0700 (PDT)")
References: <25272928.post@talk.nabble.com>
	<87eiqof1n1.fsf@dba2.int.libertyrms.com>
	<25343388.post@talk.nabble.com>
Message-ID: <873a6xe64q.fsf@dba2.int.libertyrms.com>

roctaiwan <nettreeinc@gmail.com> writes:
> I have pay attention on the remotehelperthread log and I have this
> "2009-09-08 18:13:25 CST DEBUG2 remoteHelperThread_1_1: inserts=110105
> updates=0 deletes=110005"
>
> what does it mean insert=110105? does that tells me number of records got
> insert and deletes, in this case 110105-110005=100 which says there should
> have 100 records in my table? (this is correct. I do only have 100 records
> in that table)

Yes, this indicates that in the present SYNC, 110105 tuples were
inserted, and 110005 tuples were deleted.  (There isn't any breakdown on
a per-table basis.)

> The time up there, is that the exact time for this slaves end SYNC all
> records? 

The timestamp indicates when this was logged, and that normally takes
place just a moment before processing of the SYNC was completed.

> if I want to find out when did it started where should I pay attention
> to?

Look for:
     "DEBUG1: remoteWorkerThread_%d: SYNC %d processing"

With 220K updates involved in this SYNC, I expect that the start time is
quite a while before the end time.

There is more documentation on reading the logs here:
   <http://main.slony.info/documentation/loganalysis.html>
-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@" [name;tld];;
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From nettreeinc at gmail.com  Thu Sep 10 00:11:12 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Thu Sep 10 00:11:43 2009
Subject: [Slony1-general] where to find data transaction log?
In-Reply-To: <873a6xe64q.fsf@dba2.int.libertyrms.com>
References: <25272928.post@talk.nabble.com>
	<87eiqof1n1.fsf@dba2.int.libertyrms.com>
	<25343388.post@talk.nabble.com>
	<873a6xe64q.fsf@dba2.int.libertyrms.com>
Message-ID: <25378427.post@talk.nabble.com>


Hi Christ,

I have generating 100k records and wants to know how much time took for my
Master and a Slave to complete the process. I have attached logs one from
each of them, one is called  slon_master.log and slon_DB1.log. from what you
have told me on where should I looking at to find the start time and ending
time. But the story seems like a little bit different on master. I insert
100k records to master but master don't have log like this one "DEBUG2
remoteHelperThread_1_1: inserts=110105 updates=0 deletes=110005" 

Also, since I am inserting 100k records all at once the "DEBUG1:
remoteWorkerThread_%d: SYNC %d processing" won't wait for all 10k inserted
then generated a record. It breaking up into many smaller logs, generate
each line of log for every hundreds of transactions been done. 

could you take a look at my log and pointing out from each server log, when
did the 100k transaction started and when did it end?  or can anyone else
help me find it out.
Thanks,
http://www.nabble.com/file/p25378427/slon_master.log slon_master.log 
http://www.nabble.com/file/p25378427/slon_DB1.log slon_DB1.log 




Huang



Christopher Browne wrote:
> 
> Yes, this indicates that in the present SYNC, 110105 tuples were
> inserted, and 110005 tuples were deleted.  (There isn't any breakdown on
> a per-table basis.)
> 
>> The time up there, is that the exact time for this slaves end SYNC all
>> records? 
> 
> The timestamp indicates when this was logged, and that normally takes
> place just a moment before processing of the SYNC was completed.
> 
>> if I want to find out when did it started where should I pay attention
>> to?
> 
> Look for:
>      "DEBUG1: remoteWorkerThread_%d: SYNC %d processing"
> 
> With 220K updates involved in this SYNC, I expect that the start time is
> quite a while before the end time.
> 
> There is more documentation on reading the logs here:
>    <http://main.slony.info/documentation/loganalysis.html>
> -- 
> 
> 

-- 
View this message in context: http://www.nabble.com/where-to-find-data-transaction-log--tp25272928p25378427.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From j.huber at epcom.cc  Thu Sep 10 03:18:01 2009
From: j.huber at epcom.cc (Josef Huber [EPCOM IT-Systeme])
Date: Thu Sep 10 03:18:39 2009
Subject: [Slony1-general] Slony 2.0.3RC2  Inherited Table
Message-ID: <4AA8D259.2040203@epcom.cc>

Hello,

we are using Slony2.03 RC2 and Postgresql 8.4.0 and are Running under =

Suse SLES10 (SP2), i've 8 schema's, which i sync with slony,
works fine on all normal Tables (are all synchron).

I've 4 Tables which are inherited tables, and  i get not all table =

synchron by slony, services.t_schema_objects_go is synchron, and =

services.t_schema_objects_rastplaetze is not synchron( 0 records on =

master 170 records)
any idea?

excample:
CREATE TABLE services.t_schema_objects
(
  id serial NOT NULL,
  titel character varying(255)
  CONSTRAINT t_schema_objects_id_pkey PRIMARY KEY (id),
)
WITH (
  OIDS=3DFALSE
);
ALTER TABLE services.t_schema_objects OWNER TO pvis;

CREATE TABLE services.t_schema_objects_go
(
-- Inherited from table services.t_schema_objects_go:  id integer NOT =

NULL DEFAULT nextval('services.t_schema_objects_id_seq'::regclass),
-- Inherited from table services.t_schema_objects_go:  titel character =

varying(255),
  test2 character varying(255),
  CONSTRAINT t_schema_objects_go_id_pkey PRIMARY KEY (id),
)
INHERITS (services.t_schema_objects)
WITH (
  OIDS=3DFALSE
);
ALTER TABLE services.t_schema_objects_go OWNER TO pvis;


CREATE TABLE services.t_schema_objects_rastplaetze
(
-- Inherited from table services.t_schema_objects_rastplaetze:  id =

integer NOT NULL DEFAULT =

nextval('services.t_schema_objects_id_seq'::regclass),
-- Inherited from table services.t_schema_objects_rastplaetze:  titel =

character varying(255),
  objectid integer,
   CONSTRAINT t_schema_objects_rastplaetze_id_pkey PRIMARY KEY (id),
)
INHERITS (services.t_schema_objects)
WITH (
  OIDS=3DFALSE
);
ALTER TABLE services.t_schema_objects_rastplaetze OWNER TO zaunerj;

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/x-pkcs7-signature
Size: 5226 bytes
Desc: S/MIME Cryptographic Signature
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090910=
/3812f114/smime.bin
From nettreeinc at gmail.com  Thu Sep 10 03:32:21 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Thu Sep 10 03:32:59 2009
Subject: [Slony1-general] where to find data transaction log?
In-Reply-To: <873a6xe64q.fsf@dba2.int.libertyrms.com>
References: <25272928.post@talk.nabble.com>
	<87eiqof1n1.fsf@dba2.int.libertyrms.com>
	<25343388.post@talk.nabble.com>
	<873a6xe64q.fsf@dba2.int.libertyrms.com>
Message-ID: <25380890.post@talk.nabble.com>


I found that to find the ending time on Master is the hardest... 
Anyone can tell me how? here is another 100k data insert I generated on
master.
http://www.nabble.com/file/p25380890/slon_master.log slon_master.log 

It got bunch of "SYNC" in the log. but which one is starting and which one
is the last piece of data that insert into it?

2009-09-10 18:06:22 CST DEBUG2 localListenThread: Received event 1,645 SYNC
2009-09-10 18:06:22 CST DEBUG2 remoteListenThread_2: queue event 2,204 SYNC
2009-09-10 18:06:22 CST DEBUG2 remoteListenThread_2: UNLISTEN
2009-09-10 18:06:22 CST DEBUG2 remoteWorkerThread_2: Received event 2,204
SYNC
2009-09-10 18:06:22 CST DEBUG2 calc sync size - last time: 1 last length:
10007 ideal: 5 proposed size: 3
2009-09-10 18:06:22 CST DEBUG2 remoteWorkerThread_2: SYNC 204 processing
2009-09-10 18:06:22 CST DEBUG2 remoteWorkerThread_2: no sets need syncing
for this event



Huang


Christopher Browne wrote:
> 
> roctaiwan <nettreeinc@gmail.com> writes:
>> I have pay attention on the remotehelperthread log and I have this
>> "2009-09-08 18:13:25 CST DEBUG2 remoteHelperThread_1_1: inserts=110105
>> updates=0 deletes=110005"
>>
>> what does it mean insert=110105? does that tells me number of records got
>> insert and deletes, in this case 110105-110005=100 which says there
>> should
>> have 100 records in my table? (this is correct. I do only have 100
>> records
>> in that table)
> 
> Yes, this indicates that in the present SYNC, 110105 tuples were
> inserted, and 110005 tuples were deleted.  (There isn't any breakdown on
> a per-table basis.)
> 
>> The time up there, is that the exact time for this slaves end SYNC all
>> records? 
> 
> The timestamp indicates when this was logged, and that normally takes
> place just a moment before processing of the SYNC was completed.
> 
>> if I want to find out when did it started where should I pay attention
>> to?
> 
> Look for:
>      "DEBUG1: remoteWorkerThread_%d: SYNC %d processing"
> 
> With 220K updates involved in this SYNC, I expect that the start time is
> quite a while before the end time.
> 
> There is more documentation on reading the logs here:
>    <http://main.slony.info/documentation/loganalysis.html>
> -- 
> let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@"
> [name;tld];;
> Christopher Browne
> "Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
> phasers on the Heffalump, Piglet, meet me in transporter room three"
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/where-to-find-data-transaction-log--tp25272928p25380890.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From gordo169 at gmail.com  Thu Sep 10 07:40:25 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Thu Sep 10 07:40:36 2009
Subject: [Slony1-general] Slony 2.0.3RC2  Inherited Table
In-Reply-To: <4AA8D259.2040203@epcom.cc>
References: <4AA8D259.2040203@epcom.cc>
Message-ID: <25384563.post@talk.nabble.com>


Can you confirm that it's part of the replication set?  On the subscriber
node, run this:

select * from <your_slony_schema>.sl_table where
tab_relname='t_schema_objects_rastplaetze';

If that returns 0 rows, your table isn't part of a set.  Next, confirm you
are subscribed to the set:

select * from <your_slony_schema>.sl_subscribe;

You should see sub_set=your set, sub_receiver=your subscriber node, and
sub_active=t.



Josef Huber [EPCOM IT-Systeme] wrote:
> 
> Hello,
> 
> we are using Slony2.03 RC2 and Postgresql 8.4.0 and are Running under 
> Suse SLES10 (SP2), i've 8 schema's, which i sync with slony,
> works fine on all normal Tables (are all synchron).
> 
> I've 4 Tables which are inherited tables, and  i get not all table 
> synchron by slony, services.t_schema_objects_go is synchron, and 
> services.t_schema_objects_rastplaetze is not synchron( 0 records on 
> master 170 records)
> any idea?
> 
> 

-- 
View this message in context: http://www.nabble.com/Slony-2.0.3RC2--Inherited-Table-tp25380708p25384563.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From gordo169 at gmail.com  Thu Sep 10 09:45:45 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Thu Sep 10 09:45:59 2009
Subject: [Slony1-general] Slony 2.0.3RC2  Inherited Table
In-Reply-To: <25384563.post@talk.nabble.com>
References: <4AA8D259.2040203@epcom.cc> <25384563.post@talk.nabble.com>
Message-ID: <25386909.post@talk.nabble.com>


If that all checks out, be sure you aren't inadvertently putting rows in the
base table.

On the provider node, do this:

select count(*) from ONLY services.t_schema_objects_rastplaetze;

If this is zero rows, then I suspect your data is all in the base table.




Gordon Shannon wrote:
> 
> Can you confirm that it's part of the replication set?  On the subscriber
> node, run this:
> 
> select * from <your_slony_schema>.sl_table where
> tab_relname='t_schema_objects_rastplaetze';
> 
> If that returns 0 rows, your table isn't part of a set.  Next, confirm you
> are subscribed to the set:
> 
> select * from <your_slony_schema>.sl_subscribe;
> 
> You should see sub_set=your set, sub_receiver=your subscriber node, and
> sub_active=t.
> 
> 
> 
> Josef Huber [EPCOM IT-Systeme] wrote:
>> 
>> Hello,
>> 
>> we are using Slony2.03 RC2 and Postgresql 8.4.0 and are Running under 
>> Suse SLES10 (SP2), i've 8 schema's, which i sync with slony,
>> works fine on all normal Tables (are all synchron).
>> 
>> I've 4 Tables which are inherited tables, and  i get not all table 
>> synchron by slony, services.t_schema_objects_go is synchron, and 
>> services.t_schema_objects_rastplaetze is not synchron( 0 records on 
>> master 170 records)
>> any idea?
>> 
>> 
> 
> 

-- 
View this message in context: http://www.nabble.com/Slony-2.0.3RC2--Inherited-Table-tp25380708p25386909.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From ron.arts at neonova.nl  Fri Sep 11 16:45:53 2009
From: ron.arts at neonova.nl (Ron Arts)
Date: Fri Sep 11 16:46:26 2009
Subject: [Slony1-general] where are rpm's for centos?
Message-ID: <4AAAE131.90809@neonova.nl>

Hi,

I have a strong requirement to install Slony from RPM, and according to the
Slony docs this should be possible:

(http://www.slony.info/documentation/installation.html)
4.7 states: The RPMs are available at PostgreSQL RPM Repository.

But.. they aren't. Are they still maintained? Where can I get them?
source rpms are ok as well.

Thanks,
Ron Arts

-- 
NeoNova BV
innovatieve internetoplossingen

http://www.neonova.nl  Science Park 140           1098 XG Amsterdam
info: 020-5611300      servicedesk: 020-5611302   fax: 020-5611301
KvK Amsterdam 34151241

Op dit bericht is de volgende disclaimer van toepassing:
http://www.neonova.nl/maildisclaimer
From drees76 at gmail.com  Fri Sep 11 17:31:09 2009
From: drees76 at gmail.com (David Rees)
Date: Fri Sep 11 17:31:58 2009
Subject: [Slony1-general] where are rpm's for centos?
In-Reply-To: <4AAAE131.90809@neonova.nl>
References: <4AAAE131.90809@neonova.nl>
Message-ID: <72dbd3150909111731p212a8scf3a8aa46ba87103@mail.gmail.com>

On Fri, Sep 11, 2009 at 4:45 PM, Ron Arts <ron.arts@neonova.nl> wrote:
> I have a strong requirement to install Slony from RPM, and according to the
> Slony docs this should be possible:
>
> (http://www.slony.info/documentation/installation.html)
> 4.7 states: The RPMs are available at PostgreSQL RPM Repository.
>
> But.. they aren't. Are they still maintained? Where can I get them?
> source rpms are ok as well.

Look here: http://yum.pgsqlrpms.org/srpms/

-Dave
From devrim at gunduz.org  Sat Sep 12 05:36:21 2009
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Sat Sep 12 05:38:12 2009
Subject: [Slony1-general] where are rpm's for centos?
In-Reply-To: <4AAAE131.90809@neonova.nl>
References: <4AAAE131.90809@neonova.nl>
Message-ID: <1252758981.28795.979.camel@hp-laptop2.gunduz.org>

T24gU2F0LCAyMDA5LTA5LTEyIGF0IDAxOjQ1ICswMjAwLCBSb24gQXJ0cyB3cm90ZToKPiBJIGhh
dmUgYSBzdHJvbmcgcmVxdWlyZW1lbnQgdG8gaW5zdGFsbCBTbG9ueSBmcm9tIFJQTSwgYW5kIGFj
Y29yZGluZwo+IHRvIHRoZQo+IFNsb255IGRvY3MgdGhpcyBzaG91bGQgYmUgcG9zc2libGU6Cj4g
Cj4gKGh0dHA6Ly93d3cuc2xvbnkuaW5mby9kb2N1bWVudGF0aW9uL2luc3RhbGxhdGlvbi5odG1s
KQo+IDQuNyBzdGF0ZXM6IFRoZSBSUE1zIGFyZSBhdmFpbGFibGUgYXQgUG9zdGdyZVNRTCBSUE0g
UmVwb3NpdG9yeS4KPiAKPiBCdXQuLiB0aGV5IGFyZW4ndC4gQXJlIHRoZXkgc3RpbGwgbWFpbnRh
aW5lZD8gV2hlcmUgY2FuIEkgZ2V0IHRoZW0/Cj4gc291cmNlIHJwbXMgYXJlIG9rIGFzIHdlbGwu
CgpodHRwOi8veXVtLnBnc3FscnBtcy5vcmcvcnBtY2hhcnQucGhwCgpDbGljayBvbiB0aGUgUEcg
dmVyc2lvbiB0aGF0IHlvdSB1c2UsIG9yIHRoZSBPUyB0aGF0IHlvdSB1c2UuIAoKUmVnYXJkcywK
LS0gCkRldnJpbSBHw5xORMOcWiwgUkhDRQpDb21tYW5kIFByb21wdCAtIGh0dHA6Ly93d3cuQ29t
bWFuZFByb21wdC5jb20gCmRldnJpbX5ndW5kdXoub3JnLCBkZXZyaW1+UG9zdGdyZVNRTC5vcmcs
IGRldnJpbS5ndW5kdXp+bGludXgub3JnLnRyCiAgICAgICAgICAgICAgICAgICBodHRwOi8vd3d3
Lmd1bmR1ei5vcmcKLS0tLS0tLS0tLS0tLS0gbmV4dCBwYXJ0IC0tLS0tLS0tLS0tLS0tCkEgbm9u
LXRleHQgYXR0YWNobWVudCB3YXMgc2NydWJiZWQuLi4KTmFtZTogbm90IGF2YWlsYWJsZQpUeXBl
OiBhcHBsaWNhdGlvbi9wZ3Atc2lnbmF0dXJlClNpemU6IDE5NyBieXRlcwpEZXNjOiBUaGlzIGlz
IGEgZGlnaXRhbGx5IHNpZ25lZCBtZXNzYWdlIHBhcnQKVXJsIDogaHR0cDovL2xpc3RzLnNsb255
LmluZm8vcGlwZXJtYWlsL3Nsb255MS1nZW5lcmFsL2F0dGFjaG1lbnRzLzIwMDkwOTEyLzYyMTcw
ODI2L2F0dGFjaG1lbnQucGdwCg==
From gtimmens at gmail.com  Sat Sep 12 07:01:12 2009
From: gtimmens at gmail.com (Ger Timmens)
Date: Sat Sep 12 07:38:32 2009
Subject: [Slony1-general] slony 1.2.17 expected release date ?
Message-ID: <9a53441f0909120701g6c7872fesce987f868ebffeaf@mail.gmail.com>

QWxsLAoKV2hhdCdzIHRoZSBwcm9jZXNzIG9mIHJlbGVhc2luZyAxLjIuMTcgPwoKUmVnYXJkcywK
CiAgICAgR2VyCi0tLS0tLS0tLS0tLS0tIG5leHQgcGFydCAtLS0tLS0tLS0tLS0tLQpBbiBIVE1M
IGF0dGFjaG1lbnQgd2FzIHNjcnViYmVkLi4uClVSTDogaHR0cDovL2xpc3RzLnNsb255LmluZm8v
cGlwZXJtYWlsL3Nsb255MS1nZW5lcmFsL2F0dGFjaG1lbnRzLzIwMDkwOTEyLzhlNDMyYjE2L2F0
dGFjaG1lbnQuaHRtCg==
From nettreeinc at gmail.com  Sun Sep 13 23:10:34 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Sun Sep 13 23:11:33 2009
Subject: [Slony1-general] where to find data transaction log?
In-Reply-To: <25380890.post@talk.nabble.com>
References: <25272928.post@talk.nabble.com>
	<87eiqof1n1.fsf@dba2.int.libertyrms.com>
	<25343388.post@talk.nabble.com>
	<873a6xe64q.fsf@dba2.int.libertyrms.com>
	<25380890.post@talk.nabble.com>
Message-ID: <25430775.post@talk.nabble.com>


Anyone ? I just want to know how to read the log of the insert time start and
finish on Master. 
For each process, It contain different logs on master then on slaves. 
I read the docunment the chapter that all talks about logs and I do found
also what Christopher said on how to find starting and ending time on slave,
but that doesn't applied on master. Message logs in Master and in slave are
some how different.

Appreciate for any guidence.

Huang


roctaiwan wrote:
> 
> I found that to find the ending time on Master is the hardest... 
> Anyone can tell me how? here is another 100k data insert I generated on
> master.
>  http://www.nabble.com/file/p25380890/slon_master.log slon_master.log 
> 
> It got bunch of "SYNC" in the log. but which one is starting and which one
> is the last piece of data that insert into it?
> 
> 2009-09-10 18:06:22 CST DEBUG2 localListenThread: Received event 1,645
> SYNC
> 2009-09-10 18:06:22 CST DEBUG2 remoteListenThread_2: queue event 2,204
> SYNC
> 2009-09-10 18:06:22 CST DEBUG2 remoteListenThread_2: UNLISTEN
> 2009-09-10 18:06:22 CST DEBUG2 remoteWorkerThread_2: Received event 2,204
> SYNC
> 2009-09-10 18:06:22 CST DEBUG2 calc sync size - last time: 1 last length:
> 10007 ideal: 5 proposed size: 3
> 2009-09-10 18:06:22 CST DEBUG2 remoteWorkerThread_2: SYNC 204 processing
> 2009-09-10 18:06:22 CST DEBUG2 remoteWorkerThread_2: no sets need syncing
> for this event
> 
> 
> 
> Huang
> 
> 

-- 
View this message in context: http://www.nabble.com/where-to-find-data-transaction-log--tp25272928p25430775.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From cbbrowne at ca.afilias.info  Mon Sep 14 14:07:13 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Sep 14 14:07:45 2009
Subject: [Slony1-general] where to find data transaction log?
In-Reply-To: <25430775.post@talk.nabble.com> (roctaiwan's message of "Sun, 13
	Sep 2009 23:10:34 -0700 (PDT)")
References: <25272928.post@talk.nabble.com>
	<87eiqof1n1.fsf@dba2.int.libertyrms.com>
	<25343388.post@talk.nabble.com>
	<873a6xe64q.fsf@dba2.int.libertyrms.com>
	<25380890.post@talk.nabble.com> <25430775.post@talk.nabble.com>
Message-ID: <87y6ohb5ha.fsf@dba2.int.libertyrms.com>

roctaiwan <nettreeinc@gmail.com> writes:
> Anyone ? I just want to know how to read the log of the insert time start and
> finish on Master. 

Nothing is recorded in the origin node's logs; the slon managing the
origin node has no involvement in replicating data.  All it does it to
mark SYNCs.
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From mnagaraja at alcatel-lucent.com  Tue Sep 15 00:32:41 2009
From: mnagaraja at alcatel-lucent.com (Nagaraja, Madhukar (Madhukar))
Date: Tue Sep 15 00:34:54 2009
Subject: [Slony1-general] RE: Slony caused a Deadlock after a switchover.
Message-ID: <E9F099885B445541A55D830FC62E175A0B680A42A8@INBANSXCHMBSA3.in.alcatel-lucent.com>

Hello All,

I had sent a mail regarding this slony deadlock. We have seen one more issue while doing a subscribe.
Can one of you please let me know what might be the issue ? We are deploying shortly and need some information.
Below is the error we got. This is the second time:

<stdin>:18: PGRES_FATAL_ERROR select "_evslonycluster".subscribeSet(1, 1, 2,
't');  - ERROR:  deadlock detected
DETAIL:  Process 17442 waits for ExclusiveLock on relation 17494 of database
16385; blocked by process 17433.
Process 17433 waits for ExclusiveLock on relation 17494 of database 16385;
blocked by process 17442.
CONTEXT:  SQL statement "LOCK TABLE _evslonycluster.sl_event IN EXCLUSIVE MODE;
INSERT INTO _evslonycluster.sl_event (ev_origin, ev_seqno, ev_timestamp,
ev_snapshot, ev_type, ev_data1, ev_data2, ev_data3, ev_data4, ev_data5,
ev_data6, ev_data7, ev_data8) VALUES ('1',
nextval('_evslonycluster.sl_event_seq'), now(),
"pg_catalog".txid_current_snapshot(), $1, $2, $3, $4, $5, $6, $7, $8, $9);
SELECT currval('_evslonycluster.sl_event_seq');"
PL/pgSQL function "subscribeset" line 56 at assignment


The the first time we got the error, the mail is below.

Please Help.

Thanks.


-----Original Message-----
From: Nagaraja, Madhukar (Madhukar)
Sent: Thursday, September 03, 2009 3:51 PM
To: slony1-general@lists.slony.info
Subject: Slony caused a Deadlock after a switchover.

Hello,
We have been using the slony for the past few months. We are still in dev/test cycle and not yet gone to production. But recently we did a switchover operation and we got this error in the slony logs. Can you help us out in this ?? Please let me know if you need any more information. We are using slony1 2.0.1.

This is the error we got:

<stdin>:5: PGRES_FATAL_ERROR select "_evslonycluster".lockSet(1); select pg_catalog.txid_snapshot_xmax(pg_catalog.txid_current_snapshot()); - ERROR:  deadlock detected

DETAIL:  Process 12169 waits for AccessExclusiveLock on relation 17591 of database 16386; blocked by process 1987.

Process 1987 waits for AccessShareLock on relation 17480 of database 16386; blocked by process 12169.

CONTEXT:  SQL statement "create trigger "_evslonycluster_lockedset" before insert or update or delete on "public"."domaincontrollerinfo" for each row execute procedure

                                "_evslonycluster".lockedSet ('_evslonycluster');"

PL/pgSQL function "lockset" line 44 at EXECUTE statement

Thanks.
From dan at sidhe.org  Tue Sep 15 19:27:17 2009
From: dan at sidhe.org (Dan Sugalski)
Date: Tue Sep 15 19:28:14 2009
Subject: [Slony1-general] Mixing slony versions in a cluster?
Message-ID: <a06240801c6d5fbdc7d52@[172.16.5.2]>

I've currently got a three node cluster in production running Slony 
2.0.2 and PG 8.3.7. It's ticking along pretty happily, and I'm 
looking to toss a PG 8.4.1 node in to test the version upgrade.

What I'm wondering is whether it's okay to install Slony 2.0.3 RC2 on 
the new 8.4.1 node, or whether that'll cause me trouble with mixed 
minor versions in the cluster. I'd rather not have any downtime if I 
can avoid it, especially not for what's essentially a test.
-- 
				Dan

--------------------------------------it's like this-------------------
Dan Sugalski                          even samurai
dan@sidhe.org                         have teddy bears and even
                                       teddy bears get drunk
From nettreeinc at gmail.com  Tue Sep 15 19:53:21 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Sep 15 19:54:13 2009
Subject: [Slony1-general] Slony caused a Deadlock after a switchover.
In-Reply-To: <E9F099885B445541A55D830FC62E175A0B680A42A8@INBANSXCHMBSA3.in.alcatel-lucent.com>
References: <E9F099885B445541A55D830FC62E175A0B680A42A8@INBANSXCHMBSA3.in.alcatel-lucent.com>
Message-ID: <25465109.post@talk.nabble.com>


I am not sure if this is what you looking for

http://www.nabble.com/Slony-table-locking-question-td14396700.html#a14398224
Slony-table-locking-question 


Madhukar wrote:
> 
> Hello All,
> 
> I had sent a mail regarding this slony deadlock. We have seen one more
> issue while doing a subscribe.
> Can one of you please let me know what might be the issue ? We are
> deploying shortly and need some information.
> Below is the error we got. This is the second time:
> 
> 

-- 
View this message in context: http://www.nabble.com/RE%3A-Slony-caused-a-Deadlock-after-a-switchover.-tp25449319p25465109.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From gordo169 at gmail.com  Tue Sep 15 20:33:20 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Tue Sep 15 20:34:15 2009
Subject: [Slony1-general] Mixing slony versions in a cluster?
In-Reply-To: <a06240801c6d5fbdc7d52@[172.16.5.2]>
References: <a06240801c6d5fbdc7d52@[172.16.5.2]>
Message-ID: <25465419.post@talk.nabble.com>


We are doing something very similar and it works fine.   Two 8.4 nodes and
one 8.3, with Slony 2.0.2.

The slony build has to be built against the version of Postgres on the
machine it will run on.  So the easiest way is to have a single Slony 2.0.3
build against 8.4, and run all your slon processes on one of the 8.4
servers.  That's how we run.  Or if you prefer to run distributed slons, you
can build a separate 2.0.3 Slony against 8.3 for the 8.3 server.  

You may get a warning about versions at startup, but it can be ignored so
long as you don't start using 8.4-specific data features (not sure what they
are).

Hope that helps
-gordon


Dan Sugalski wrote:
> 
> I've currently got a three node cluster in production running Slony 
> 2.0.2 and PG 8.3.7. It's ticking along pretty happily, and I'm 
> looking to toss a PG 8.4.1 node in to test the version upgrade.
> 
> What I'm wondering is whether it's okay to install Slony 2.0.3 RC2 on 
> the new 8.4.1 node
> 

-- 
View this message in context: http://www.nabble.com/Mixing-slony-versions-in-a-cluster--tp25464916p25465419.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Tue Sep 15 23:08:42 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Sep 15 23:09:52 2009
Subject: Re[Slony1-general] plicating 1 table from various sources
In-Reply-To: <6B5AF6293A289F45826220B17ABE7937FDBD6B@BORON.aers.local>
References: <6B5AF6293A289F45826220B17ABE7937FDBD6B@BORON.aers.local>
Message-ID: <25466502.post@talk.nabble.com>


You can specify that in the store path like this:
store path (server = 1, client = 2, conninfo = 'dbname=$DB host=$Node1
user=$U');
store path (server = 2, client = 1, conninfo = 'dbname=$DB host=$Node2
user=$U');

#store path (server = 2, client = 3, conninfo = 'dbname=$DB host=$Node2
user=$U');
#store path (server = 3, client = 2, conninfo = 'dbname=$DB host=$Node3
user=$U');

and do subscribe to Node that you want it to subscribe. (ex. subscribe set
(id = 1, provider = 1, receiver = 2, forward = no); and/or subscribe set (id
= 1, provider = 3, receiver = 4, forward = no);

In this case, Node 1 only communicate with node 2 and node 3 with node 4. 

But if you want all 4 nodes be able to commnicate to eachother and only
these two tables stays within their "local" then you can do that on
subscribe sets to nodes you want. ex. subscribe set (id = (your table id),
provider = 1, receiver = 2, forward = no)

Steven Huang



michael-539 wrote:
> 
> Hi everyone, I've got a situation I need some help with. I have a group
> of 4 servers - two in one city and two in another. There's about 45
> tables, most of which can be replicated through the whole set without
> any issue. I have two tables that I want replicated from Node 1-2 and
> Node 3-4, but I do not want replicated from 1-3 or 2-3. Basically, each
> city needs to have its own data for these two tables and the data should
> only be replicated between nodes in the same city. 
> 
>  
> 
> It seems to me that I can't have one table in two sets, even if it uses
> different origin nodes. Any tips?
> 
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/Replicating-1-table-from-various-sources-tp25283718p25466502.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From ron.arts at neonova.nl  Wed Sep 16 00:15:36 2009
From: ron.arts at neonova.nl (Ron Arts)
Date: Wed Sep 16 00:16:51 2009
Subject: [Slony1-general] uuid in postgresql on CentOS5
Message-ID: <4AB09098.6020002@neonova.nl>

Hi,

I downloaded:

http://yum.pgsqlrpms.org/srpms/8.4/redhat/rhel-5-i386/postgresql-8.4.1-1PGDG.rhel5.src.rpm

because I want to use slony 2.0 on CentOS 5 machines. Unfortunately the source rpm
does not build, because it requires uuid-devel which is not provided with CentOS,
presumably because the e2fsprogs package provides a library with the same name.

Does slony use the uuid functions in any way?

Thanks,
Ron

-- 
NeoNova BV
innovatieve internetoplossingen

http://www.neonova.nl  Science Park 140           1098 XG Amsterdam
info: 020-5611300      servicedesk: 020-5611302   fax: 020-5611301
KvK Amsterdam 34151241

Op dit bericht is de volgende disclaimer van toepassing:
http://www.neonova.nl/maildisclaimer
From devrim at gunduz.org  Wed Sep 16 08:10:42 2009
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Wed Sep 16 08:12:58 2009
Subject: [Slony1-general] uuid in postgresql on CentOS5
In-Reply-To: <4AB09098.6020002@neonova.nl>
References: <4AB09098.6020002@neonova.nl>
Message-ID: <1253113842.24996.24.camel@hp-laptop2.gunduz.org>

T24gV2VkLCAyMDA5LTA5LTE2IGF0IDA5OjE1ICswMjAwLCBSb24gQXJ0cyB3cm90ZToKPiBodHRw
Oi8veXVtLnBnc3FscnBtcy5vcmcvc3JwbXMvOC40L3JlZGhhdC9yaGVsLTUtaTM4Ni9wb3N0Z3Jl
c3FsLTguNC4xLTFQR0RHLnJoZWw1LnNyYy5ycG0KPiAKPiBiZWNhdXNlIEkgd2FudCB0byB1c2Ug
c2xvbnkgMi4wIG9uIENlbnRPUyA1IG1hY2hpbmVzLiBVbmZvcnR1bmF0ZWx5Cj4gdGhlIHNvdXJj
ZSBycG0gZG9lcyBub3QgYnVpbGQsIGJlY2F1c2UgaXQgcmVxdWlyZXMgdXVpZC1kZXZlbCB3aGlj
aCBpcwo+IG5vdCBwcm92aWRlZCB3aXRoIENlbnRPUywgcHJlc3VtYWJseSBiZWNhdXNlIHRoZSBl
MmZzcHJvZ3MgcGFja2FnZQo+IHByb3ZpZGVzIGEgbGlicmFyeSB3aXRoIHRoZSBzYW1lIG5hbWUu
CgpTYW1lIHJlcG9zaXRvcnkgaGFzIHV1aWQgcGFja2FnZXM6CgpodHRwOi8veXVtLnBnc3FscnBt
cy5vcmcvOC40L3JlZGhhdC9yaGVsLTVTZXJ2ZXItaTM4Ni9yZXBvdmlldy9sZXR0ZXJfdS5ncm91
cC5odG1sCgpZb3UgY2FuIGluc3RhbGwgdGhlbSB2aWEgeXVtIG9yIGRpcmVjdGx5LgoKUmVnYXJk
cywKLS0gCkRldnJpbSBHw5xORMOcWiwgUkhDRQpDb21tYW5kIFByb21wdCAtIGh0dHA6Ly93d3cu
Q29tbWFuZFByb21wdC5jb20gCmRldnJpbX5ndW5kdXoub3JnLCBkZXZyaW1+UG9zdGdyZVNRTC5v
cmcsIGRldnJpbS5ndW5kdXp+bGludXgub3JnLnRyCiAgICAgICAgICAgICAgICAgICBodHRwOi8v
d3d3Lmd1bmR1ei5vcmcKLS0tLS0tLS0tLS0tLS0gbmV4dCBwYXJ0IC0tLS0tLS0tLS0tLS0tCkEg
bm9uLXRleHQgYXR0YWNobWVudCB3YXMgc2NydWJiZWQuLi4KTmFtZTogbm90IGF2YWlsYWJsZQpU
eXBlOiBhcHBsaWNhdGlvbi9wZ3Atc2lnbmF0dXJlClNpemU6IDE5NyBieXRlcwpEZXNjOiBUaGlz
IGlzIGEgZGlnaXRhbGx5IHNpZ25lZCBtZXNzYWdlIHBhcnQKVXJsIDogaHR0cDovL2xpc3RzLnNs
b255LmluZm8vcGlwZXJtYWlsL3Nsb255MS1nZW5lcmFsL2F0dGFjaG1lbnRzLzIwMDkwOTE2LzNi
MGIwOWMyL2F0dGFjaG1lbnQucGdwCg==
From cbbrowne at ca.afilias.info  Wed Sep 16 10:33:04 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Sep 16 10:33:31 2009
Subject: [Slony1-general] uuid in postgresql on CentOS5
In-Reply-To: <4AB09098.6020002@neonova.nl> (Ron Arts's message of "Wed, 16 Sep
	2009 09:15:36 +0200")
References: <4AB09098.6020002@neonova.nl>
Message-ID: <8763bibxrj.fsf@dba2.int.libertyrms.com>

Ron Arts <ron.arts@neonova.nl> writes:
> I downloaded:
>
> http://yum.pgsqlrpms.org/srpms/8.4/redhat/rhel-5-i386/postgresql-8.4.1-1PGDG.rhel5.src.rpm
>
> because I want to use slony 2.0 on CentOS 5 machines. Unfortunately the source rpm
> does not build, because it requires uuid-devel which is not provided with CentOS,
> presumably because the e2fsprogs package provides a library with the same name.
>
> Does slony use the uuid functions in any way?

No, Slony-I does not use UUID functions...

chris@dba2:~/Slony-I/CMD> ack-grep uuid
chris@dba2:~/Slony-I/CMD> ack-grep UUID
chris@dba2:~/Slony-I/CMD> 

Nothing found...

There may be something induced via some other dependency on some other
dependency on some other dependency, but it's not inherent to Slony-I...
-- 
(reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From ariel at cafelug.org.ar  Wed Sep 16 10:35:46 2009
From: ariel at cafelug.org.ar (Ariel Wainer)
Date: Wed Sep 16 10:36:16 2009
Subject: [Slony1-general] Adding the forward flag to a set
Message-ID: <57305f9e12b26c081cdcbadae09e9cad.squirrel@www.cafelug.org.ar>

Hi everybody, I'm new to Slony1, so far I got it working. I would like now
to reshape the cluster in order to make the slave master and vice versa.
But, I noticed that when I did the subscribe set, I ommited the
forward=yes, so is it posible to (manipulating the cluster schema?) enable
the forwarding?


Regards.


-- 
Ariel Wainer
Grupo de Usuarios de Software Libre de Capital Federal
www.cafelug.org.ar

From cbbrowne at ca.afilias.info  Wed Sep 16 10:47:46 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Sep 16 10:48:14 2009
Subject: [Slony1-general] Adding the forward flag to a set
In-Reply-To: <57305f9e12b26c081cdcbadae09e9cad.squirrel@www.cafelug.org.ar>
References: <57305f9e12b26c081cdcbadae09e9cad.squirrel@www.cafelug.org.ar>
Message-ID: <4AB124C2.7040604@ca.afilias.info>

Ariel Wainer wrote:
> Hi everybody, I'm new to Slony1, so far I got it working. I would like now
> to reshape the cluster in order to make the slave master and vice versa.
> But, I noticed that when I did the subscribe set, I ommited the
> forward=yes, so is it posible to (manipulating the cluster schema?) enable
> the forwarding?
>
>
> Regards.
>
>
>   
Sure, just submit the "subscribe set" request again, via slonik, and 
specify that forwarding is on.
From ron.arts at neonova.nl  Wed Sep 16 11:26:05 2009
From: ron.arts at neonova.nl (Ron Arts)
Date: Wed Sep 16 11:26:40 2009
Subject: [Slony1-general] Where can I find the source (rpm) for
	compat-postgresql-libs?
Message-ID: <4AB12DBD.6010501@neonova.nl>

Hi,

Subject says it all.
I need to be able to create everything for CentOS5 from source rpms.

Thanks,
Ron

-- 
NeoNova BV
innovatieve internetoplossingen

http://www.neonova.nl  Science Park 140           1098 XG Amsterdam
info: 020-5611300      servicedesk: 020-5611302   fax: 020-5611301
KvK Amsterdam 34151241

Op dit bericht is de volgende disclaimer van toepassing:
http://www.neonova.nl/maildisclaimer
From kevink at consistentstate.com  Wed Sep 16 15:44:46 2009
From: kevink at consistentstate.com (Kevin Kempter)
Date: Wed Sep 16 15:45:32 2009
Subject: [Slony1-general] version compatability?
Message-ID: <200909161644.46730.kevink@consistentstate.com>

Hi All;

we're running SLONY-1 v2.0.2 and postgreSQL version 8.4.1

Were setting up SLONY log shipping to a site running SLONY-1 1.2.15 and 
PostgreSQL 8.3.7

Will this work, or might we run into compatibility issues?


Thanks in advance...
From vignesh.m at i10n.com  Thu Sep 17 03:27:50 2009
From: vignesh.m at i10n.com (Vignesh Mohan)
Date: Thu Sep 17 03:29:17 2009
Subject: [Slony1-general] Slony replication - overview
Message-ID: <25488708.post@talk.nabble.com>


Hi All

I need to know some facts of Slony 's Behaviour in this use case

* Slony running in slave and master.
* Stop slony in master and Slave
* Delete few data in tables in master
* Restart Slony in master and slave

My Question is 

Will the deletion be replicated in slave now?

Please clear my doubt
Thanks in Advance

-
Vignesh.M



-- 
View this message in context: http://www.nabble.com/Slony-replication---overview-tp25488708p25488708.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From glynastill at yahoo.co.uk  Thu Sep 17 03:39:03 2009
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Thu Sep 17 03:40:32 2009
Subject: [Slony1-general] Slony replication - overview
In-Reply-To: <25488708.post@talk.nabble.com>
Message-ID: <769881.16022.qm@web23607.mail.ird.yahoo.com>

> From: Vignesh Mohan <vignesh.m@i10n.com>
> 
> Hi All
> 
> I need to know some facts of Slony 's Behaviour in this use
> case
> 
> * Slony running in slave and master.
> * Stop slony in master and Slave
> * Delete few data in tables in master
> * Restart Slony in master and slave
> 
> My Question is 
> 
> Will the deletion be replicated in slave now?
> 

Yes.


      

From dmitry.koterov at gmail.com  Thu Sep 17 04:33:39 2009
From: dmitry.koterov at gmail.com (=?utf-8?b?0JTQvNC40YLRgNC40Lkg0JrQvtGC0LXRgNC+0LI=?=)
Date: Thu Sep 17 04:35:07 2009
Subject: [Slony1-general] Invitation to view my posts on FriendFeed
Message-ID: <20090917113339.0C9FAD8A8F9@friendfeed.com>

Skipped content of type multipart/alternative-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/png
Size: 441 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090917/c082cc40/attachment-0004.png
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/png
Size: 2912 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090917/c082cc40/attachment-0005.png
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/png
Size: 1887 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090917/c082cc40/attachment-0006.png
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 1854 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090917/c082cc40/attachment-0002.jpeg
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 1274 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090917/c082cc40/attachment-0003.jpeg
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/png
Size: 837 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090917/c082cc40/attachment-0007.png
From vignesh.m at i10n.com  Thu Sep 17 04:38:51 2009
From: vignesh.m at i10n.com (Vignesh Mohan)
Date: Thu Sep 17 04:40:22 2009
Subject: [Slony1-general] Slony replication - overview
In-Reply-To: <769881.16022.qm@web23607.mail.ird.yahoo.com>
References: <25488708.post@talk.nabble.com>
	<769881.16022.qm@web23607.mail.ird.yahoo.com>
Message-ID: <25489669.post@talk.nabble.com>




Glyn Astill wrote:
> 
>> From: Vignesh Mohan <vignesh.m@i10n.com>
>> 
>> Hi All
>> 
>> I need to know some facts of Slony 's Behaviour in this use
>> case
>> 
>> * Slony running in slave and master.
>> * Stop slony in master and Slave
>> * Delete few data in tables in master
>> * Restart Slony in master and slave
>> 
>> My Question is 
>> 
>> Will the deletion be replicated in slave now?
>> 
> 
> Yes.
> 
> 
> 
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 


Thanks a load for your prompt response.

This is raising another doubt. 

>From your answer i come to a conclusion that whenever the slony is restarted
entire db in slave is re written irrespective of changes done or not done.
Am i right ?

If i m not right .. How does slony take into consideration of the deletion
of few rows in master activity when slony is stopped.? And when started
again how does it come to know that the rows are deleted 


Also tell me the way in which we can restart the slony 

I do the this
/usr/bin/slon start

What does sloni_restart_node do ?

Thanks in Advance
-
Vignesh.M
-- 
View this message in context: http://www.nabble.com/Slony-replication---overview-tp25488708p25489669.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From cedric.villemain at dalibo.com  Thu Sep 17 05:13:20 2009
From: cedric.villemain at dalibo.com (=?iso-8859-1?q?C=E9dric_Villemain?=)
Date: Thu Sep 17 05:13:39 2009
Subject: [Slony1-general] Slony replication - overview
In-Reply-To: <25489669.post@talk.nabble.com>
References: <25488708.post@talk.nabble.com>
	<769881.16022.qm@web23607.mail.ird.yahoo.com>
	<25489669.post@talk.nabble.com>
Message-ID: <200909171413.29604.cedric.villemain@dalibo.com>

Le jeudi 17 septembre 2009, Vignesh Mohan a ?crit :
> Glyn Astill wrote:
> >> From: Vignesh Mohan <vignesh.m@i10n.com>
> If i m not right .. How does slony take into consideration of the deletion
> of few rows in master activity when slony is stopped.? And when started
> again how does it come to know that the rows are deleted

Slony use trigger-base replication, that mean that every time change occur on 
your replicated tables, it is log via trigger in a another table. The slon 
process check that and replicate those. If 'slon' process is down (like slony 
is stopped) then trigger continue to fire on insert/delete/delete :) and when 
slon start again, it check and replicate.

You'd better read the documentation.

> 
> 
> Also tell me the way in which we can restart the slony
> 
> I do the this
> /usr/bin/slon start
> 
> What does sloni_restart_node do ?
> 
> Thanks in Advance
> -
> Vignesh.M
> 


-- 
----
C?dric Villemain
Administrateur de Base de Donn?es
Cel: +33 (0)6 74 15 56 53
http://dalibo.com - http://dalibo.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 198 bytes
Desc: This is a digitally signed message part.
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090917/643d6f06/attachment.pgp
From vignesh.m at i10n.com  Thu Sep 17 07:03:14 2009
From: vignesh.m at i10n.com (Vignesh Mohan)
Date: Thu Sep 17 07:03:24 2009
Subject: [Slony1-general] Slony replication - overview
In-Reply-To: <200909171413.29604.cedric.villemain@dalibo.com>
References: <25488708.post@talk.nabble.com>
	<769881.16022.qm@web23607.mail.ird.yahoo.com>
	<25489669.post@talk.nabble.com>
	<200909171413.29604.cedric.villemain@dalibo.com>
Message-ID: <25491869.post@talk.nabble.com>




C?dric Villemain wrote:
> 
> Le jeudi 17 septembre 2009, Vignesh Mohan a ?crit :
>> Glyn Astill wrote:
>> >> From: Vignesh Mohan <vignesh.m@i10n.com>
>> If i m not right .. How does slony take into consideration of the
>> deletion
>> of few rows in master activity when slony is stopped.? And when started
>> again how does it come to know that the rows are deleted
> 
> Slony use trigger-base replication, that mean that every time change occur
> on 
> your replicated tables, it is log via trigger in a another table. The slon 
> process check that and replicate those. If 'slon' process is down (like
> slony 
> is stopped) then trigger continue to fire on insert/delete/delete :) and
> when 
> slon start again, it check and replicate.
> 
> You'd better read the documentation.
> 
>> 
>> 
>> Also tell me the way in which we can restart the slony
>> 
>> I do the this
>> /usr/bin/slon start
>> 
>> What does sloni_restart_node do ?
>> 
>> Thanks in Advance
>> -
>> Vignesh.M
>> 
> 
> 
> -- 
> ----
> C?dric Villemain
> Administrateur de Base de Donn?es
> Cel: +33 (0)6 74 15 56 53
> http://dalibo.com - http://dalibo.org
> 
>  
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 
Hi Thanks again


I need one clarification . 

Q1 : I use "/usr/bin/slon start"  and  "/usr/bin/slon stop" .... A m i
correct ?

Q2: The stop seems not to work as the process is still running when i check
that with ps aux
What cud be the issue ?

Thanks in advance

Vignesh.M 


-- 
View this message in context: http://www.nabble.com/Slony-replication---overview-tp25488708p25491869.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From cedric.villemain at dalibo.com  Thu Sep 17 07:32:55 2009
From: cedric.villemain at dalibo.com (=?utf-8?q?C=C3=A9dric_Villemain?=)
Date: Thu Sep 17 07:33:14 2009
Subject: [Slony1-general] Slony replication - overview
In-Reply-To: <25491869.post@talk.nabble.com>
References: <25488708.post@talk.nabble.com>
	<200909171413.29604.cedric.villemain@dalibo.com>
	<25491869.post@talk.nabble.com>
Message-ID: <200909171633.00538.cedric.villemain@dalibo.com>

Le jeudi 17 septembre 2009, Vignesh Mohan a ?crit :
> C?dric Villemain wrote:
> > Le jeudi 17 septembre 2009, Vignesh Mohan a ?crit :
> >> Glyn Astill wrote:
> >> >> From: Vignesh Mohan <vignesh.m@i10n.com>
> >>
> >> If i m not right .. How does slony take into consideration of the
> >> deletion
> >> of few rows in master activity when slony is stopped.? And when started
> >> again how does it come to know that the rows are deleted
> >
> > Slony use trigger-base replication, that mean that every time change
> > occur on
> > your replicated tables, it is log via trigger in a another table. The
> > slon process check that and replicate those. If 'slon' process is down
> > (like slony
> > is stopped) then trigger continue to fire on insert/delete/delete :) and
> > when
> > slon start again, it check and replicate.
> >
> > You'd better read the documentation.
> >
> >> Also tell me the way in which we can restart the slony
> >>
> >> I do the this
> >> /usr/bin/slon start
> >>
> >> What does sloni_restart_node do ?
> >>
> >> Thanks in Advance
> >> -
> >> Vignesh.M
> 
> Hi Thanks again
> 
> 
> I need one clarification .
> 
> Q1 : I use "/usr/bin/slon start"  and  "/usr/bin/slon stop" .... A m i
> correct ?

no

> 
> Q2: The stop seems not to work as the process is still running when i check
> that with ps aux
> What cud be the issue ?

http://slony.info/documentation/firstdb.html

> 
> Thanks in advance
> 
> Vignesh.M
> 


-- 
----
C?dric Villemain
Administrateur de Base de Donn?es
Cel: +33 (0)6 74 15 56 53
http://dalibo.com - http://dalibo.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 198 bytes
Desc: This is a digitally signed message part.
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090917/41b51d9d/attachment.pgp
From kevink at consistentstate.com  Thu Sep 17 07:41:45 2009
From: kevink at consistentstate.com (Kevin Kempter)
Date: Thu Sep 17 07:42:01 2009
Subject: [Slony1-general] version compatability?
In-Reply-To: <200909161644.46730.kevink@consistentstate.com>
References: <200909161644.46730.kevink@consistentstate.com>
Message-ID: <200909170841.45326.kevink@consistentstate.com>

On Wednesday 16 September 2009 16:44:46 Kevin Kempter wrote:
> Hi All;
> 
> we're running SLONY-1 v2.0.2 and postgreSQL version 8.4.1
> 
> Were setting up SLONY log shipping to a site running SLONY-1 1.2.15 and
> PostgreSQL 8.3.7
> 
> Will this work, or might we run into compatibility issues?
> 
> 
> Thanks in advance...
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 

Where do I find the details per which version(s) of PostgreSQL each version of 
SLONY is compatibly with ?
From ariel at cafelug.org.ar  Thu Sep 17 08:13:23 2009
From: ariel at cafelug.org.ar (Ariel Wainer)
Date: Thu Sep 17 08:13:41 2009
Subject: [Slony1-general] One slave, several masters
Message-ID: <10e9995933929a78659f209b8b734538.squirrel@www.cafelug.org.ar>


I have the following scenario:

dbserver1 - db1
dbserver2 - db2

dbbackup  - db1 replicated from dbserver1 via slony

I would like now to replicate db2 from dbserver2 to dbackup. The question
is about the IDs. I understand that the set, tables and sequences ids can
not be the same across databases. But how do I manage the Node Id and
cluster name?
To the "cluster" working between dbserver1 and dbbackup, these are the IDs:
dbserver1 1
dbbackup 10
clustername cname

When I init the clsuter at dbserver2, should I use cname as the cluster
name and "10" to refer to dbbackup?


Regards.

-- 
Ariel Wainer
Grupo de Usuarios de Software Libre de Capital Federal
www.cafelug.org.ar

From devrim at gunduz.org  Thu Sep 17 12:20:14 2009
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Thu Sep 17 12:22:34 2009
Subject: [Slony1-general] version compatability?
In-Reply-To: <200909170841.45326.kevink@consistentstate.com>
References: <200909161644.46730.kevink@consistentstate.com>
	<200909170841.45326.kevink@consistentstate.com>
Message-ID: <1253215214.24996.45.camel@hp-laptop2.gunduz.org>

T24gVGh1LCAyMDA5LTA5LTE3IGF0IDA4OjQxIC0wNjAwLCBLZXZpbiBLZW1wdGVyIHdyb3RlOgo+
IAo+IFdoZXJlIGRvIEkgZmluZCB0aGUgZGV0YWlscyBwZXIgd2hpY2ggdmVyc2lvbihzKSBvZiBQ
b3N0Z3JlU1FMIGVhY2gKPiB2ZXJzaW9uIG9mIAo+IFNMT05ZIGlzIGNvbXBhdGlibHkgd2l0aCA/
CgpTbG9ueSAyLjAgaXMgdXNlZCB3aXRoIFBvc3RncmVTUUwgOC4zKyBvbmx5LiBTbG9ueSAxLjIu
WCB3b3JrcyB3aXRoIDcuNCsKKElJUkMgNy4zIHN1cHBvcnQgaXMgbm90IGF2YWlsYWJsZSwgYnV0
IEkgbWF5IGJlIHdyb25nKS4KClJlZ2FyZHMsCi0tIApEZXZyaW0gR8OcTkTDnFosIFJIQ0UKQ29t
bWFuZCBQcm9tcHQgLSBodHRwOi8vd3d3LkNvbW1hbmRQcm9tcHQuY29tIApkZXZyaW1+Z3VuZHV6
Lm9yZywgZGV2cmltflBvc3RncmVTUUwub3JnLCBkZXZyaW0uZ3VuZHV6fmxpbnV4Lm9yZy50cgog
ICAgICAgICAgICAgICAgICAgaHR0cDovL3d3dy5ndW5kdXoub3JnCi0tLS0tLS0tLS0tLS0tIG5l
eHQgcGFydCAtLS0tLS0tLS0tLS0tLQpBIG5vbi10ZXh0IGF0dGFjaG1lbnQgd2FzIHNjcnViYmVk
Li4uCk5hbWU6IG5vdCBhdmFpbGFibGUKVHlwZTogYXBwbGljYXRpb24vcGdwLXNpZ25hdHVyZQpT
aXplOiAxOTcgYnl0ZXMKRGVzYzogVGhpcyBpcyBhIGRpZ2l0YWxseSBzaWduZWQgbWVzc2FnZSBw
YXJ0ClVybCA6IGh0dHA6Ly9saXN0cy5zbG9ueS5pbmZvL3BpcGVybWFpbC9zbG9ueTEtZ2VuZXJh
bC9hdHRhY2htZW50cy8yMDA5MDkxNy8zNTQ5MThlNy9hdHRhY2htZW50LnBncAo=
From devrim at gunduz.org  Fri Sep 18 10:11:29 2009
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Fri Sep 18 10:13:57 2009
Subject: [Slony1-general] Where can I find the source (rpm) for
	compat-postgresql-libs?
In-Reply-To: <4AB12DBD.6010501@neonova.nl>
References: <4AB12DBD.6010501@neonova.nl>
Message-ID: <1253293889.16067.1.camel@hp-laptop2.gunduz.org>

T24gV2VkLCAyMDA5LTA5LTE2IGF0IDIwOjI2ICswMjAwLCBSb24gQXJ0cyB3cm90ZToKPiAKPiBT
dWJqZWN0IHNheXMgaXQgYWxsLgo+IEkgbmVlZCB0byBiZSBhYmxlIHRvIGNyZWF0ZSBldmVyeXRo
aW5nIGZvciBDZW50T1M1IGZyb20gc291cmNlIHJwbXMuCgovbWUgc2NyYXRjaGVzIGhlYWQKCmNv
bXBhdC1wb3N0Z3Jlc3FsLWxpYnMgaXMgbm90IGdlbmVyYXRlZCBmcm9tIGEgc291cmNlIHRhcmJh
bGwuIEZvcgpleGFtcGxlLCBJIGV4dHJhY3QgbGlicHEuc28uKiBmcm9tIGFuIG9sZCBwZyBzb3Vy
Y2UsIGFuZCBjcmVhdGUgYmluYXJ5CnBhY2thZ2UuIFNlZSBzcGVjIGZpbGUuCgpSZWdhcmRzLAot
LSAKRGV2cmltIEfDnE5Ew5xaLCBSSENFCkNvbW1hbmQgUHJvbXB0IC0gaHR0cDovL3d3dy5Db21t
YW5kUHJvbXB0LmNvbSAKZGV2cmltfmd1bmR1ei5vcmcsIGRldnJpbX5Qb3N0Z3JlU1FMLm9yZywg
ZGV2cmltLmd1bmR1en5saW51eC5vcmcudHIKICAgICAgICAgICAgICAgICAgIGh0dHA6Ly93d3cu
Z3VuZHV6Lm9yZwotLS0tLS0tLS0tLS0tLSBuZXh0IHBhcnQgLS0tLS0tLS0tLS0tLS0KQSBub24t
dGV4dCBhdHRhY2htZW50IHdhcyBzY3J1YmJlZC4uLgpOYW1lOiBub3QgYXZhaWxhYmxlClR5cGU6
IGFwcGxpY2F0aW9uL3BncC1zaWduYXR1cmUKU2l6ZTogMTk3IGJ5dGVzCkRlc2M6IFRoaXMgaXMg
YSBkaWdpdGFsbHkgc2lnbmVkIG1lc3NhZ2UgcGFydApVcmwgOiBodHRwOi8vbGlzdHMuc2xvbnku
aW5mby9waXBlcm1haWwvc2xvbnkxLWdlbmVyYWwvYXR0YWNobWVudHMvMjAwOTA5MTgvOWU3NWUx
Y2UvYXR0YWNobWVudC5wZ3AK
From ron.arts at neonova.nl  Fri Sep 18 17:35:36 2009
From: ron.arts at neonova.nl (Ron Arts)
Date: Fri Sep 18 17:36:29 2009
Subject: [Slony1-general] where are rpm's for centos?
In-Reply-To: <1253309791.20902.24.camel@hp-laptop2.gunduz.org>
References: <4AAAE131.90809@neonova.nl>	
	<1252758981.28795.979.camel@hp-laptop2.gunduz.org>	
	<4AB0AFAF.1020905@neonova.nl>
	<1253309791.20902.24.camel@hp-laptop2.gunduz.org>
Message-ID: <4AB42758.40407@neonova.nl>

Devrim G?ND?Z schreef:
> On Wed, 2009-09-16 at 11:28 +0200, Ron Arts wrote:
>> Is there a separate location for compat-postgresql-libs? I clicked
>> around, but could not find it anywhere. Google found some on other
>> sites,
>> but I prefer the 'official' one.
> 
> compat packages don't have src.rpm.

Ok, I had been looking at the rpm -qilp output, which says:
Source RPM: compat-postgresql-libs-4-1PGDG.rhel5.src.rpm
So I went looking for that one..

Is there a specfile to be found somewhere?

Ron

-- 
NeoNova BV
innovatieve internetoplossingen

http://www.neonova.nl  Science Park 140           1098 XG Amsterdam
info: 020-5611300      servicedesk: 020-5611302   fax: 020-5611301
KvK Amsterdam 34151241

Op dit bericht is de volgende disclaimer van toepassing:
http://www.neonova.nl/maildisclaimer
From sweta.mulgavker at gmail.com  Fri Sep 18 22:06:29 2009
From: sweta.mulgavker at gmail.com (Sweta Mulgavker)
Date: Fri Sep 18 22:07:35 2009
Subject: [Slony1-general] replicating views with slony
Message-ID: <4077e7710909182206h7e2e058eif0846874dd7e0930@mail.gmail.com>

Hello all,

Does one have to replicate views also.... together with tables and
sequencies?

Thanks and Regards,
Sweta.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090919/=
5ca30cbb/attachment.htm
From stuart at stuartbishop.net  Sat Sep 19 04:04:19 2009
From: stuart at stuartbishop.net (Stuart Bishop)
Date: Sat Sep 19 04:05:46 2009
Subject: [Slony1-general] replicating views with slony
In-Reply-To: <4077e7710909182206h7e2e058eif0846874dd7e0930@mail.gmail.com>
References: <4077e7710909182206h7e2e058eif0846874dd7e0930@mail.gmail.com>
Message-ID: <6bc73d4c0909190404ve96c7c3ie91730e89fca87ef@mail.gmail.com>

On Sat, Sep 19, 2009 at 12:06 PM, Sweta Mulgavker
<sweta.mulgavker@gmail.com> wrote:
> Hello all,
>
> Does one have to replicate views also.... together with tables and
> sequencies?

No.


-- 
Stuart Bishop <stuart@stuartbishop.net>
http://www.stuartbishop.net/
From devrim at gunduz.org  Sun Sep 20 14:23:45 2009
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Sun Sep 20 14:26:10 2009
Subject: [Slony1-general] where are rpm's for centos?
In-Reply-To: <4AB42758.40407@neonova.nl>
References: <4AAAE131.90809@neonova.nl>
	<1252758981.28795.979.camel@hp-laptop2.gunduz.org>
	<4AB0AFAF.1020905@neonova.nl>
	<1253309791.20902.24.camel@hp-laptop2.gunduz.org>
	<4AB42758.40407@neonova.nl>
Message-ID: <1253481825.880.24.camel@hp-laptop2.gunduz.org>

T24gU2F0LCAyMDA5LTA5LTE5IGF0IDAyOjM1ICswMjAwLCBSb24gQXJ0cyB3cm90ZToKPiBPaywg
SSBoYWQgYmVlbiBsb29raW5nIGF0IHRoZSBycG0gLXFpbHAgb3V0cHV0LCB3aGljaCBzYXlzOgo+
IFNvdXJjZSBSUE06IGNvbXBhdC1wb3N0Z3Jlc3FsLWxpYnMtNC0xUEdERy5yaGVsNS5zcmMucnBt
Cj4gU28gSSB3ZW50IGxvb2tpbmcgZm9yIHRoYXQgb25lLi4KCldlbGwsIG9mIGNvdXJzZSB0aGVy
ZSBpcyBhIHNyYy5ycG0gLS0gYnV0IGV2ZW4gSSBjYW5ub3QgZmluZCBpdCA6KSBJCnByb2JhYmx5
IGRlbGV0ZWQgaXQgd2hpbGUgY2xlYW5pbmcgdXAgcmVwb3NpdG9yeS4KCj4gSXMgdGhlcmUgYSBz
cGVjZmlsZSB0byBiZSBmb3VuZCBzb21ld2hlcmU/CgpodHRwczovL3Byb2plY3RzLmNvbW1hbmRw
cm9tcHQuY29tL3B1YmxpYy9wZ2NvcmUvYnJvd3Nlci9ycG0vcmVkaGF0L2NvbXBhdC9zcGVjCgpS
ZWdhcmRzLAotLSAKRGV2cmltIEfDnE5Ew5xaLCBSSENFCkNvbW1hbmQgUHJvbXB0IC0gaHR0cDov
L3d3dy5Db21tYW5kUHJvbXB0LmNvbSAKZGV2cmltfmd1bmR1ei5vcmcsIGRldnJpbX5Qb3N0Z3Jl
U1FMLm9yZywgZGV2cmltLmd1bmR1en5saW51eC5vcmcudHIKICAgICAgICAgICAgICAgICAgIGh0
dHA6Ly93d3cuZ3VuZHV6Lm9yZwotLS0tLS0tLS0tLS0tLSBuZXh0IHBhcnQgLS0tLS0tLS0tLS0t
LS0KQSBub24tdGV4dCBhdHRhY2htZW50IHdhcyBzY3J1YmJlZC4uLgpOYW1lOiBub3QgYXZhaWxh
YmxlClR5cGU6IGFwcGxpY2F0aW9uL3BncC1zaWduYXR1cmUKU2l6ZTogMTk3IGJ5dGVzCkRlc2M6
IFRoaXMgaXMgYSBkaWdpdGFsbHkgc2lnbmVkIG1lc3NhZ2UgcGFydApVcmwgOiBodHRwOi8vbGlz
dHMuc2xvbnkuaW5mby9waXBlcm1haWwvc2xvbnkxLWdlbmVyYWwvYXR0YWNobWVudHMvMjAwOTA5
MjEvOWIwNWIwZjIvYXR0YWNobWVudC5wZ3AK
From peter.geoghegan86 at gmail.com  Mon Sep 21 04:47:15 2009
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Mon Sep 21 04:48:12 2009
Subject: [Slony1-general] Why does sl_status event lag grow,
	even though events *are* replicated?
Message-ID: <db471ace0909210447w3036330ercdabfa97cf35785b@mail.gmail.com>

Hello,

This is a re-hash of an earlier mail that went unanswered. Doing this
probably doesn't show decorum, but I'm at the end of my tether with
this problem. I'd appreciate any help that you can give to fix this
problem, because it's a real irritation - my production system is
affected by it. At this point, I'd welcome even wild speculation.

Replication ostensibly works fine. We replicate from a windows Master
(node 1), using Hiroshi Saito's Slony-I 2.0.2 binaries, to 2 OpenSuse
slaves (nodes 2 and 3). It's all fairly standard.

When I restart a slave database (in the following example node 2),
replication continues to work (at least as far as can be immediately
observed), but sl_status shows:

st_origin | st_received | st_last_event |   st_last_event_ts |
st_last_received |    st_last_received_ts     |
st_last_received_event_ts  | st_lag_num_events |   st_lag_time
-------------+-----------------+------------------------------------------------+----------------------------+------------------+----------------------------+----------------------------+-------------------+-----------------
1            |        3        |38689  |  "2009-07-30 12:11:51.796" |
38688;"2009-07-30 12:12:02.428316" | "2009-07-30 12:11:41.859"
|1                            |"00:00:14.015"
1            |        2        |38689  |  "2009-07-30 12:11:51.796" |
38605;"2009-07-30 11:52:35.119048" | "2009-07-30 11:58:05.734"
|84                          |"00:13:50.14"

Node 2's st_lag_num_events grows and grows, until the slony-I service
(all slon daemons) is restarted on the master, at which time it
returns to zero, just as before. This is very annoying, because
sl_status is how my application monitors the state of the replication
cluster, and when its broken it confuses users. I can restart the slon
services (slon daemons) and have the event lag return to zero, but
that's not acceptable in a production system.

Bear in mind, replication isn't broken at any point - only sl_status is.

When I run test_slony_state-dbi.pl on the master while the event lag
continues to grow, it outputs the following:

peter@peter-development-machine:~/slony1-2.0.2/tools>
./test_slony_state-dbi.pl --host=10.0.0.80 --database=lustre
--cluster=lustre_cluster --user=postgres --password=my_password
DSN: dbi:Pg:dbname=lustre;host=10.0.0.80;user=postgres;password=my_password;
===========================
Rummage for DSNs
=============================
Query:

 select p.pa_server, p.pa_conninfo
 from "_lustre_cluster".sl_path p
--   where exists (select * from "_lustre_cluster".sl_subscribe s where
--                          (s.sub_provider = p.pa_server or
s.sub_receiver = p.pa_server) and
--                          sub_active = 't')
 group by pa_server, pa_conninfo;


Tests for node 1 - DSN = dbi:Pg:dbname=lustre host=10.0.0.80
user=postgres password=my_password
========================================
pg_listener info:
Pages: 0
Tuples: 0

Size Tests
================================================
     sl_log_1         0  0.000000
     sl_log_2         0  0.000000
    sl_seqlog         0  0.000000

Listen Path Analysis
===================================================
No problems found with sl_listen

--------------------------------------------------------------------------------
Summary of event info
 Origin  Min SYNC  Max SYNC Min SYNC Age Max SYNC Age
================================================================================
    1     38605     38699     00:00:00     00:15:00    0
    2        20        20     01:08:00     01:08:00    1
    3        30        30     01:02:00     01:02:00    1


---------------------------------------------------------------------------------
Summary of sl_confirm aging
 Origin   Receiver   Min SYNC   Max SYNC  Age of latest SYNC  Age of
eldest SYNC
=================================================================================
      1          2      38605      38605      00:20:00      00:20:00    0
      1          3      38627      38698      00:00:00      00:11:00    0
      2          1         20         20      01:03:00      01:03:00    1
      2          3         20         20      01:02:00      01:02:00    1
      3          1         30         30      01:02:00      01:02:00    1
      3          2         30         30      01:08:00      01:08:00    1


------------------------------------------------------------------------------

Listing of old open connections on node 1
     Database             PID            User    Query Age
   Query
================================================================================


Tests for node 3 - DSN = dbi:Pg:dbname=lustre_slave host=10.0.0.82
user=postgres password=my_password
========================================
pg_listener info:
Pages: 0
Tuples: 0

Size Tests
================================================
     sl_log_1         0  0.000000
     sl_log_2         0  0.000000
    sl_seqlog         0  0.000000

Listen Path Analysis
===================================================
No problems found with sl_listen

--------------------------------------------------------------------------------
Summary of event info
 Origin  Min SYNC  Max SYNC Min SYNC Age Max SYNC Age
================================================================================
    1     38605     38699     00:00:00     00:15:00    0
    2        20        20     01:08:00     01:08:00    1
    3        30        30     01:02:00     01:02:00    1


---------------------------------------------------------------------------------
Summary of sl_confirm aging
 Origin   Receiver   Min SYNC   Max SYNC  Age of latest SYNC  Age of
eldest SYNC
=================================================================================
      1          2      38605      38605      00:21:00      00:21:00    0
      1          3      38629      38699      00:00:00      00:11:00    0
      2          1         20         20      01:03:00      01:03:00    1
      2          3         20         20      01:03:00      01:03:00    1
      3          1         30         30      01:03:00      01:03:00    1
      3          2         30         30      01:08:00      01:08:00    1


------------------------------------------------------------------------------

Listing of old open connections on node 3
     Database             PID            User    Query Age
   Query
================================================================================


Tests for node 2 - DSN = dbi:Pg:dbname=lustre_slave host=10.0.0.81
user=postgres password=my_password
========================================
pg_listener info:
Pages: 0
Tuples: 0

Size Tests
================================================
     sl_log_1         0  0.000000
     sl_log_2         0  0.000000
    sl_seqlog         0  0.000000

Listen Path Analysis
===================================================
No problems found with sl_listen

--------------------------------------------------------------------------------
Summary of event info
 Origin  Min SYNC  Max SYNC Min SYNC Age Max SYNC Age
================================================================================
    1     38573     38699    -00:05:00     00:15:00    0
    2        20        21     00:15:00     01:03:00    0
    3        30        30     00:57:00     00:57:00    1


---------------------------------------------------------------------------------
Summary of sl_confirm aging
 Origin   Receiver   Min SYNC   Max SYNC  Age of latest SYNC  Age of
eldest SYNC
=================================================================================
      1          2      38607      38699      00:00:00      00:15:00    0
      1          3      38573      38698     -00:05:00      00:15:00    0
      2          1         20         20      00:57:00      00:57:00    1
      2          3         20         20      00:57:00      00:57:00    1
      3          1         30         30      00:57:00      00:57:00    1
      3          2         30         30      01:02:00      01:02:00    1


------------------------------------------------------------------------------

Listing of old open connections on node 2
     Database             PID            User    Query Age
   Query
================================================================================

peter@peter-development-machine:~/slony1-2.0.2/tools>

Why is this happening?

Regards,
Peter Geoghegan
From bnichols at ca.afilias.info  Mon Sep 21 11:23:27 2009
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Mon Sep 21 11:23:54 2009
Subject: [Slony1-general] Why does sl_status event lag grow, even
	though events *are* replicated?
In-Reply-To: <db471ace0909210447w3036330ercdabfa97cf35785b@mail.gmail.com>
References: <db471ace0909210447w3036330ercdabfa97cf35785b@mail.gmail.com>
Message-ID: <1253557407.6107.54.camel@bnicholson-desktop>

On Mon, 2009-09-21 at 12:47 +0100, Peter Geoghegan wrote:
> Hello,
> 
> This is a re-hash of an earlier mail that went unanswered. Doing this
> probably doesn't show decorum, but I'm at the end of my tether with
> this problem. I'd appreciate any help that you can give to fix this
> problem, because it's a real irritation - my production system is
> affected by it. At this point, I'd welcome even wild speculation.
> 
> Replication ostensibly works fine. We replicate from a windows Master
> (node 1), using Hiroshi Saito's Slony-I 2.0.2 binaries, to 2 OpenSuse
> slaves (nodes 2 and 3). It's all fairly standard.
> 
> When I restart a slave database (in the following example node 2),
> replication continues to work (at least as far as can be immediately
> observed), but sl_status shows:
> 
> st_origin | st_received | st_last_event |   st_last_event_ts |
> st_last_received |    st_last_received_ts     |
> st_last_received_event_ts  | st_lag_num_events |   st_lag_time
> -------------+-----------------+------------------------------------------------+----------------------------+------------------+----------------------------+----------------------------+-------------------+-----------------
> 1            |        3        |38689  |  "2009-07-30 12:11:51.796" |
> 38688;"2009-07-30 12:12:02.428316" | "2009-07-30 12:11:41.859"
> |1                            |"00:00:14.015"
> 1            |        2        |38689  |  "2009-07-30 12:11:51.796" |
> 38605;"2009-07-30 11:52:35.119048" | "2009-07-30 11:58:05.734"
> |84                          |"00:13:50.14"
> 
> Node 2's st_lag_num_events grows and grows, until the slony-I service
> (all slon daemons) is restarted on the master, at which time it
> returns to zero, just as before. This is very annoying, because
> sl_status is how my application monitors the state of the replication
> cluster, and when its broken it confuses users. I can restart the slon
> services (slon daemons) and have the event lag return to zero, but
> that's not acceptable in a production system.
> 
> Bear in mind, replication isn't broken at any point - only sl_status is.
> 
> When I run test_slony_state-dbi.pl on the master while the event lag
> continues to grow, it outputs the following:
> 
> peter@peter-development-machine:~/slony1-2.0.2/tools>
> ./test_slony_state-dbi.pl --host=10.0.0.80 --database=lustre
> --cluster=lustre_cluster --user=postgres --password=my_password
> DSN: dbi:Pg:dbname=lustre;host=10.0.0.80;user=postgres;password=my_password;
> ===========================
> Rummage for DSNs
> =============================
> Query:
> 
>  select p.pa_server, p.pa_conninfo
>  from "_lustre_cluster".sl_path p
> --   where exists (select * from "_lustre_cluster".sl_subscribe s where
> --                          (s.sub_provider = p.pa_server or
> s.sub_receiver = p.pa_server) and
> --                          sub_active = 't')
>  group by pa_server, pa_conninfo;
> 
> 
> Tests for node 1 - DSN = dbi:Pg:dbname=lustre host=10.0.0.80
> user=postgres password=my_password
> ========================================
> pg_listener info:
> Pages: 0
> Tuples: 0
> 
> Size Tests
> ================================================
>      sl_log_1         0  0.000000
>      sl_log_2         0  0.000000
>     sl_seqlog         0  0.000000
> 
> Listen Path Analysis
> ===================================================
> No problems found with sl_listen
> 
> --------------------------------------------------------------------------------
> Summary of event info
>  Origin  Min SYNC  Max SYNC Min SYNC Age Max SYNC Age
> ================================================================================
>     1     38605     38699     00:00:00     00:15:00    0
>     2        20        20     01:08:00     01:08:00    1
>     3        30        30     01:02:00     01:02:00    1
> 
> 
> ---------------------------------------------------------------------------------
> Summary of sl_confirm aging
>  Origin   Receiver   Min SYNC   Max SYNC  Age of latest SYNC  Age of
> eldest SYNC
> =================================================================================
>       1          2      38605      38605      00:20:00      00:20:00    0
>       1          3      38627      38698      00:00:00      00:11:00    0
>       2          1         20         20      01:03:00      01:03:00    1
>       2          3         20         20      01:02:00      01:02:00    1
>       3          1         30         30      01:02:00      01:02:00    1
>       3          2         30         30      01:08:00      01:08:00    1
> 
> 
> ------------------------------------------------------------------------------
> 
> Listing of old open connections on node 1
>      Database             PID            User    Query Age
>    Query
> ================================================================================
> 
> 
> Tests for node 3 - DSN = dbi:Pg:dbname=lustre_slave host=10.0.0.82
> user=postgres password=my_password
> ========================================
> pg_listener info:
> Pages: 0
> Tuples: 0
> 
> Size Tests
> ================================================
>      sl_log_1         0  0.000000
>      sl_log_2         0  0.000000
>     sl_seqlog         0  0.000000
> 
> Listen Path Analysis
> ===================================================
> No problems found with sl_listen
> 
> --------------------------------------------------------------------------------
> Summary of event info
>  Origin  Min SYNC  Max SYNC Min SYNC Age Max SYNC Age
> ================================================================================
>     1     38605     38699     00:00:00     00:15:00    0
>     2        20        20     01:08:00     01:08:00    1
>     3        30        30     01:02:00     01:02:00    1
> 
> 
> ---------------------------------------------------------------------------------
> Summary of sl_confirm aging
>  Origin   Receiver   Min SYNC   Max SYNC  Age of latest SYNC  Age of
> eldest SYNC
> =================================================================================
>       1          2      38605      38605      00:21:00      00:21:00    0
>       1          3      38629      38699      00:00:00      00:11:00    0
>       2          1         20         20      01:03:00      01:03:00    1
>       2          3         20         20      01:03:00      01:03:00    1
>       3          1         30         30      01:03:00      01:03:00    1
>       3          2         30         30      01:08:00      01:08:00    1
> 
> 
> ------------------------------------------------------------------------------
> 
> Listing of old open connections on node 3
>      Database             PID            User    Query Age
>    Query
> ================================================================================
> 
> 
> Tests for node 2 - DSN = dbi:Pg:dbname=lustre_slave host=10.0.0.81
> user=postgres password=my_password
> ========================================
> pg_listener info:
> Pages: 0
> Tuples: 0
> 
> Size Tests
> ================================================
>      sl_log_1         0  0.000000
>      sl_log_2         0  0.000000
>     sl_seqlog         0  0.000000
> 
> Listen Path Analysis
> ===================================================
> No problems found with sl_listen
> 
> --------------------------------------------------------------------------------
> Summary of event info
>  Origin  Min SYNC  Max SYNC Min SYNC Age Max SYNC Age
> ================================================================================
>     1     38573     38699    -00:05:00     00:15:00    0
>     2        20        21     00:15:00     01:03:00    0
>     3        30        30     00:57:00     00:57:00    1
> 
> 
> ---------------------------------------------------------------------------------
> Summary of sl_confirm aging
>  Origin   Receiver   Min SYNC   Max SYNC  Age of latest SYNC  Age of
> eldest SYNC
> =================================================================================
>       1          2      38607      38699      00:00:00      00:15:00    0
>       1          3      38573      38698     -00:05:00      00:15:00    0
>       2          1         20         20      00:57:00      00:57:00    1
>       2          3         20         20      00:57:00      00:57:00    1
>       3          1         30         30      00:57:00      00:57:00    1
>       3          2         30         30      01:02:00      01:02:00    1
> 
> 
> ------------------------------------------------------------------------------
> 
> Listing of old open connections on node 2
>      Database             PID            User    Query Age
>    Query
> ================================================================================
> 
> peter@peter-development-machine:~/slony1-2.0.2/tools>
> 
> Why is this happening?


It's a bug. The events are making it from the origin to the receiver,
but the confirmations are not making it back.  

We've experienced the same situation many times in an all Linux
environment - but that was with Slony 1.1 

-- 
Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.


From cbbrowne at ca.afilias.info  Mon Sep 21 13:06:31 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Sep 21 13:07:11 2009
Subject: [Slony1-general] version compatability?
In-Reply-To: <1253215214.24996.45.camel@hp-laptop2.gunduz.org>
References: <200909161644.46730.kevink@consistentstate.com>	<200909170841.45326.kevink@consistentstate.com>
	<1253215214.24996.45.camel@hp-laptop2.gunduz.org>
Message-ID: <4AB7DCC7.4010103@ca.afilias.info>

Devrim G?ND?Z wrote:
> On Thu, 2009-09-17 at 08:41 -0600, Kevin Kempter wrote:
>   
>> Where do I find the details per which version(s) of PostgreSQL each
>> version of 
>> SLONY is compatibly with ?
>>     
>
> Slony 2.0 is used with PostgreSQL 8.3+ only. Slony 1.2.X works with 7.4+
> (IIRC 7.3 support is not available, but I may be wrong)
Yes, we dropped:
a) Support for 7.3 in the 1.2 branch
b) Support for anything less than 8.3 in the 2.0 branch

I'd *expect* log shipping to work across major versions; it doesn't use 
any of the aspects of functionality that would break between Slony-I 1.2 
versus 2.0 or between versions of PostgreSQL.

Unfortunately, log shipping has been troublesome functionality to get 
just right (there were quite a lot of changes in the 1.2 branch relating 
to log shipping fixes), and I haven't seen enough reports of major usage 
of it in production to be really  highly confident in it.
From cbbrowne at ca.afilias.info  Mon Sep 21 13:15:51 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Sep 21 13:16:21 2009
Subject: [Slony1-general] Slony replication - overview
In-Reply-To: <25489669.post@talk.nabble.com>
References: <25488708.post@talk.nabble.com>	<769881.16022.qm@web23607.mail.ird.yahoo.com>
	<25489669.post@talk.nabble.com>
Message-ID: <4AB7DEF7.1030208@ca.afilias.info>

Vignesh Mohan wrote:
>
> Glyn Astill wrote:
>   
>>> From: Vignesh Mohan <vignesh.m@i10n.com>
>>>
>>> Hi All
>>>
>>> I need to know some facts of Slony 's Behaviour in this use
>>> case
>>>
>>> * Slony running in slave and master.
>>> * Stop slony in master and Slave
>>> * Delete few data in tables in master
>>> * Restart Slony in master and slave
>>>
>>> My Question is 
>>>
>>> Will the deletion be replicated in slave now?
>>>
>>>       
>> Yes.
>>
>>
>>
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>>
>>     
>
>
> Thanks a load for your prompt response.
>
> This is raising another doubt. 
>
> >From your answer i come to a conclusion that whenever the slony is restarted
> entire db in slave is re written irrespective of changes done or not done.
> Am i right ?
>   
You are incorrect.
> If i m not right .. How does slony take into consideration of the deletion
> of few rows in master activity when slony is stopped.? And when started
> again how does it come to know that the rows are deleted 
>   
Slony-I captures changes to individual tuples via the log triggers.

If you want to take a peek at what it's doing, then note:
 a) Each replicated table has a trigger added to it called "logtrigger" 
(more or less; names vary a bit by Slony-I version)
 b) When updates take place to a table, tuples are recorded in sl_log_1 
or sl_log_2.

You could exercise this by looking at the contents of sl_log_1/2, then 
forcing in an update, and seeing what tuples were added.
> Also tell me the way in which we can restart the slony 
>
> I do the this
> /usr/bin/slon start
>   
You need to restart the slon process; it connects to the various 
databases, and figures out based on their state what to do next.  
Nothing too much more is necessary.
> What does sloni_restart_node do ?
>   
"restart_node" was some tooling that was important in earlier versions 
of Slony-I, where slon processes had a tendancy to get a little bit 
deranged if anything went wrong with network connectivity, and you'd 
need to "reset" things by hand.
From peter.geoghegan86 at gmail.com  Mon Sep 21 14:01:55 2009
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Mon Sep 21 14:02:26 2009
Subject: [Slony1-general] Why does sl_status event lag grow, even though 
	events *are* replicated?
In-Reply-To: <1253557407.6107.54.camel@bnicholson-desktop>
References: <db471ace0909210447w3036330ercdabfa97cf35785b@mail.gmail.com>
	<1253557407.6107.54.camel@bnicholson-desktop>
Message-ID: <db471ace0909211401y302d11fbn3ae644cb02082d2f@mail.gmail.com>

Brad,

> It's a bug. The events are making it from the origin to the receiver,
> but the confirmations are not making it back.
>
> We've experienced the same situation many times in an all Linux
> environment - but that was with Slony 1.1
>
> --
> Brad Nicholson ?416-673-4106
> Database Administrator, Afilias Canada Corp.

Thanks for letting me know.

Is this particular bug (presumably similar to but distinct from the
one brad mentions in the 1.1 branch) one that has already been fixed?

Can someone suggest a course of action to fix the problem?

Regards,
Peter Geoghegan
From lawrenceg at globalitcreations.com  Tue Sep 22 03:31:57 2009
From: lawrenceg at globalitcreations.com (Lawrence Giam)
Date: Tue Sep 22 07:59:13 2009
Subject: [Slony1-general] Slony1 installation procedure
Message-ID: <F42B651E7C095744BA93C7D0D087CCD301D9FD67@gitc-mail01.globalitcreations.com>

Skipped content of type multipart/alternative-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 4414 bytes
Desc: image001.jpg
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090922/5ebf9b76/attachment.jpeg
From ktm at rice.edu  Tue Sep 22 08:29:47 2009
From: ktm at rice.edu (Kenneth Marshall)
Date: Tue Sep 22 08:30:02 2009
Subject: [Slony1-general] Slony1 2.0.3-rc2 problem between PostgreSQL 8.3.5
	-> 8.4.1
Message-ID: <20090922152947.GS22802@it.is.rice.edu>

Hi Slony community.

I am trying to set up a replication between a version 8.3.5
of PostgreSQL and an 8.4.1 version using Slony1 v. 2.0.3-rc2.
I followed the same procedure with v. 1.2.17-rc2 and it worked
fine:

1. Install Slony on the DB from source with the following
   configure command:

   for 8.3.5:
$ ./configure --prefix=/usr/site/postgresql-8.3.5 --with-pgconfigdir=/usr/site/postgresql-8.3.5/bi
n --with-pgbindir=/usr/site/postgresql-8.3.5/bin --with-pgincludedir=/usr/site/postgresql-8.3.5/incl
ude --with-pgincludeserverdir=/usr/site/postgresql-8.3.5/include/server --with-pglibdir=/usr/site/po
stgresql-8.3.5/lib --with-pgpkglibdir=/usr/site/postgresql-8.3.5/lib --with-pgsharedir=/usr/site/pos
tgresql-8.3.5/share --with-perltools

   for 8.4.5:
$ ./configure --prefix=/usr/site/postgresql-8.4.1 --with-pgconfigdir=/usr/site/postgresql-8.4.1/
bin --with-pgbindir=/usr/site/postgresql-8.4.1/bin --with-pgincludedir=/usr/site/postgresql-8.4.1/
include --with-pgincludeserverdir=/usr/site/postgresql-8.4.1/include/server --with-pglibdir=/usr/s
ite/postgresql-8.4.1/lib --with-pgpkglibdir=/usr/site/postgresql-8.4.1/lib --with-pgsharedir=/usr/
site/postgresql-8.4.1/share --with-perltools

Followed by a "make" and "make install". The files in ./bin ./lib
are updated.

2. Set up the replication using the altperl scripts:

/usr/site/postgresql-8.3.5/bin/slonik_init_cluster  | /usr/site/postgresql-8.3.5/bin/slonik
/usr/site/postgresql-8.3.5/bin/slon_start node1
/usr/site/postgresql-8.3.5/bin/slon_start node2
/usr/site/postgresql-8.3.5/bin/slonik_create_set set1 | /usr/site/postgresql-8.3.5/bin/slonik
/usr/site/postgresql-8.3.5/bin/slonik_subscribe_set set1 node2 | /usr/site/postgresql-8.3.5/bin/slonik

The nodes are defined as follows:

# The 8.3.5 node
    add_node(node     => 1,
             host     => 'localhost',
             dbname   => 'rt38',
             port     => 5432,
             user     => 'postgres',
             password => undef);

# The 8.4.1 node
    add_node(node     => 2,
             host     => '192.168.0.2',
             dbname   => 'rt38replica',
             port     => 5432,
             user     => 'postgres',
             password => undef);

The initial copy takes place but I am getting this error in
the log file for the node1 slon process:

2009-09-22 10:12:41 CDT CONFIG main: slon version 2.0.3 starting up
2009-09-22 10:12:41 CDT INFO   slon: watchdog process started
2009-09-22 10:12:41 CDT CONFIG slon: watchdog ready - pid = 29617
2009-09-22 10:12:41 CDT CONFIG main: Integer option vac_frequency = 3
2009-09-22 10:12:41 CDT CONFIG main: Integer option log_level = 0
2009-09-22 10:12:41 CDT CONFIG main: Integer option sync_interval = 1000
2009-09-22 10:12:41 CDT CONFIG main: Integer option sync_interval_timeout = 10000
2009-09-22 10:12:41 CDT CONFIG main: Integer option sync_group_maxsize = 20
2009-09-22 10:12:41 CDT CONFIG main: Integer option desired_sync_time = 60000
2009-09-22 10:12:41 CDT CONFIG main: Integer option syslog = 0
2009-09-22 10:12:41 CDT CONFIG main: Integer option quit_sync_provider = 0
2009-09-22 10:12:41 CDT CONFIG main: Integer option quit_sync_finalsync = 0
2009-09-22 10:12:41 CDT CONFIG main: Integer option sync_max_rowsize = 8192
2009-09-22 10:12:41 CDT CONFIG main: Integer option sync_max_largemem = 5242880
2009-09-22 10:12:41 CDT CONFIG main: Integer option remote_listen_timeout = 300
2009-09-22 10:12:41 CDT CONFIG main: Boolean option log_pid = 0
2009-09-22 10:12:41 CDT CONFIG main: Boolean option log_timestamp = 1
2009-09-22 10:12:41 CDT CONFIG main: Boolean option cleanup_deletelogs = 0
2009-09-22 10:12:41 CDT CONFIG main: Real option real_placeholder = 0.000000
2009-09-22 10:12:41 CDT CONFIG main: String option cluster_name = rt
2009-09-22 10:12:41 CDT CONFIG main: String option conn_info = host=localhost dbname=rt38 user=postg
res port=5432
2009-09-22 10:12:41 CDT CONFIG main: String option pid_file = [NULL]
2009-09-22 10:12:41 CDT CONFIG main: String option log_timestamp_format = %Y-%m-%d %H:%M:%S %Z
2009-09-22 10:12:41 CDT CONFIG main: String option archive_dir = [NULL]
2009-09-22 10:12:41 CDT CONFIG main: String option sql_on_connection = [NULL]
2009-09-22 10:12:41 CDT CONFIG main: String option lag_interval = [NULL]
2009-09-22 10:12:41 CDT CONFIG main: String option command_on_logarchive = [NULL]
2009-09-22 10:12:41 CDT CONFIG main: String option syslog_facility = LOCAL0
2009-09-22 10:12:41 CDT CONFIG main: String option syslog_ident = slon
2009-09-22 10:12:41 CDT CONFIG main: String option cleanup_interval = 10 minutes
2009-09-22 10:12:41 CDT CONFIG slon: worker process created - pid = 29620
2009-09-22 10:12:41 CDT CONFIG main: local node id = 1
2009-09-22 10:12:41 CDT INFO   main: main process started
2009-09-22 10:12:41 CDT CONFIG main: launching sched_start_mainloop
2009-09-22 10:12:41 CDT CONFIG main: loading current cluster configuration
2009-09-22 10:12:41 CDT CONFIG storeNode: no_id=2 no_comment='Node 2 - rt38replica@192.168.0.2'
2009-09-22 10:12:41 CDT CONFIG storePath: pa_server=2 pa_client=1 pa_conninfo="host=192.168.0.2 dbna
me=rt38replica user=postgres port=5432" pa_connretry=10
2009-09-22 10:12:41 CDT CONFIG storeListen: li_origin=2 li_receiver=1 li_provider=2
2009-09-22 10:12:41 CDT CONFIG storeSet: set_id=1 set_origin=1 set_comment='Set 1 for rt'
2009-09-22 10:12:41 CDT CONFIG main: last local event sequence = 5000001685
2009-09-22 10:12:41 CDT CONFIG main: configuration complete - starting threads
2009-09-22 10:12:41 CDT INFO   localListenThread: thread starts
2009-09-22 10:12:41 CDT CONFIG version for "host=localhost dbname=rt38 user=postgres port=5432" is 8
0305
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=29177
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=29184
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=29193
2009-09-22 10:12:41 CDT CONFIG enableNode: no_id=2
2009-09-22 10:12:41 CDT INFO   remoteWorkerThread_2: thread starts
2009-09-22 10:12:41 CDT INFO   remoteListenThread_2: thread starts
2009-09-22 10:12:41 CDT CONFIG cleanupThread: thread starts
2009-09-22 10:12:41 CDT CONFIG cleanupThread: bias = 35383
2009-09-22 10:12:41 CDT INFO   syncThread: thread starts
2009-09-22 10:12:41 CDT INFO   main: running scheduler mainloop
2009-09-22 10:12:41 CDT CONFIG version for "host=localhost dbname=rt38 user=postgres port=5432" is 8
0305
2009-09-22 10:12:41 CDT CONFIG remoteWorkerThread_2: update provider configuration
2009-09-22 10:12:41 CDT CONFIG version for "host=192.168.0.2 dbname=rt38replica user=postgres port=5
432" is 80401
2009-09-22 10:12:41 CDT CONFIG version for "host=localhost dbname=rt38 user=postgres port=5432" is 8
0305
2009-09-22 10:12:41 CDT CONFIG version for "host=localhost dbname=rt38 user=postgres port=5432" is 8
0305

If I kill and restart either the PostgreSQL 8.4.1 on node2 or the
slon processes, the _rt.sl_status on node2 will update once. Does
anyone have any ideas? What does it mean to "update provider configuration"?
They are both running 2.0.3-rc2 of Slony1. The same process works without
a problem using 1.2.17-rc2.

Regards,
Ken
From cedric.villemain at dalibo.com  Tue Sep 22 08:35:17 2009
From: cedric.villemain at dalibo.com (=?iso-8859-15?q?C=E9dric_Villemain?=)
Date: Tue Sep 22 08:35:37 2009
Subject: [Slony1-general] Slony1 installation procedure
In-Reply-To: <F42B651E7C095744BA93C7D0D087CCD301D9FD67@gitc-mail01.globalitcreations.com>
References: <F42B651E7C095744BA93C7D0D087CCD301D9FD67@gitc-mail01.globalitcreations.com>
Message-ID: <200909221735.22138.cedric.villemain@dalibo.com>

Le mardi 22 septembre 2009, Lawrence Giam a ?crit :
> Hi All,

Hi !

> 
> 
> 
> I am trying to install Slony-I on Debian Etch with kernel version
> 2.6.18-6-686 and after googling around for information, I have some
> questions to ask.
> 
> 
> 
> 1.	I tried to search the online repository using apt-get and found
> that :
> 
> 	a.	Slony1 on Debian Etch is postgresql-8.1-slony1.2.1-1
> 	b.	Slony1 on Debian Lenny is postgresql-8.3-slony1.2.15-1
> 
> 2.	Looking at the Slony-I website, I found that Slony-I has got the
> following version available which I think I will need to build from
> source.
> 
> 	a.	1.2.17-rc.tar.bz2
> 	b.	1.2.16.tar.bz2
> 
> 3.	I tried going to www.backports.org <http://www.backports.org/>
> and look for Slony-I, the version they have is
> postgresql-8.1-slony1_1.2.1-1_i386.deb
> <http://ftp.debian.org/debian/pool/main/s/slony1/postgresql-8.1-slony1_1
> .2.1-1_i386.deb>  and postgresql-8.3-slony1_1.2.16-1_i386.deb
> <http://ftp.debian.org/debian/pool/main/s/slony1/postgresql-8.1-slony1_1
> .2.1-1_i386.deb>
> 
> 
> 
> Questions:
> 
> 1.	As my production server is running on Etch and PostgreSQL 8.1, I
> want to know which version of Slony-I should I install?

the lastest, if you don't have it in debian : you can repoart/ask the slony 
debian maintener, else you can build it your self or use the debian tools to 
backport (look at "apt-build" "apt-get source -b" etc...) 

> 2.	Looking at Slony-I website, if I plan to migrate PostgreSQL on
> the production server to version 8.3. Should I upgrade the OS to Lenny
> or build PostgreSQL from source to version 8.3 with the OS remaining at
> Debian Etch?
> 3.	What is the differences between Slony-I version 2.0.x and
> version 1.2.x.

major changes ... 

> 4.	If I install from the backports repository, what are the steps
> to patch Slony to the lastest version?

I am not sure you are walking the right way...

Etch will become outdated in not so long time. (not so long in a database 
server/system life) You probably want to upgrade to Lenny anyway.

Postgresql 8.3 is more strict about CASTing, some applications needs to be re-
coded (I mean they should be compliant if they are well coded already) 

I'll install postgresql 8.4+slony2 and debian Lenny, sure. If you want 
packages, you can do your own, it is pretty simple [1]; or ./configre make 
make install which work too :).

[1] http://www.debian.org/doc/manuals/debian-
reference/ch02.en.html#_porting_a_package_to_the_stable_system
> 
> Regards,
> 
> Lawrence Giam
> 
>   <http://www.globalitcreations.com>
> ........................................................................
> ..........................
> Lawrence Giam | Global IT Creations Pte Ltd |  Network Administrator
> website: http://www.globalitcreations.com
> phone: +65 6836 4768 ext 115| fax: + 65 6836 4736 | mobile: + 65 9758
> 7448
> 


-- 
----
C?dric Villemain
Administrateur de Base de Donn?es
Cel: +33 (0)6 74 15 56 53
http://dalibo.com - http://dalibo.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 198 bytes
Desc: This is a digitally signed message part.
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090922/fc057a3c/attachment.pgp
From nettreeinc at gmail.com  Fri Sep 25 03:36:02 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Fri Sep 25 03:36:43 2009
Subject: [Slony1-general] copying large file won't get replicate
Message-ID: <25609853.post@talk.nabble.com>


Hi,

I am bulkloading (copying)about 10million of records (total size about 1G)
with replication runing. Takes me about 2h30m to load whole data into
Master, but after that I give it like another an hour or so to load into my
slave ( I only have one slave), but zero record got inserted. I then give it
overnight to do its work, which gives almost 20 hours and still nothing
happen. I examing the log on Slave and seem no obvious activity has sigh of
data inserting. (I have attached part of my log)

I do see slony replication behave as records won't show up in slave table
until all records got processed then it will load WHOLE records all at once
into the table. I observe this by monitoring slave log. 

This makes me think, before records load on the table physically and while
data is in processing, are they temporary storted in the memory and waiting
to be load into the table once done with read records from copying?

Here is my concern. I copying both 10k and 1million of records with success,
but why can't 10Million records? Is this has something to do with my memory
size, which loading 10Million of records is too much for my server to
handle?

I have 512MB memory on both master and slave. 
-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25609853.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From glynastill at yahoo.co.uk  Fri Sep 25 03:54:08 2009
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Fri Sep 25 03:54:50 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25609853.post@talk.nabble.com>
Message-ID: <348491.35682.qm@web23608.mail.ird.yahoo.com>

--- On Fri, 25/9/09, roctaiwan <nettreeinc@gmail.com> wrote:

> Here is my concern. I copying both 10k and 1million of
> records with success,
> but why can't 10Million records? Is this has something to
> do with my memory
> size, which loading 10Million of records is too much for my
> server to
> handle?
> 

If you're subscribing the slave to tables on the master that are already populated then the subscribe will use COPY so you'll only see anything on the slave when the entrie copy transaction is complete.

If you want to see them replicated in bite size chunks subscribe before populating the table on the master.



      
From JanWieck at Yahoo.com  Fri Sep 25 06:51:14 2009
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Fri Sep 25 06:52:37 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25609853.post@talk.nabble.com>
References: <25609853.post@talk.nabble.com>
Message-ID: <4ABCCAD2.4030002@Yahoo.com>

On 9/25/2009 6:36 AM, roctaiwan wrote:
> Here is my concern. I copying both 10k and 1million of records with success,
> but why can't 10Million records? Is this has something to do with my memory
> size, which loading 10Million of records is too much for my server to
> handle?

How long does it take Slony to copy those 1M rows?

Are there any error messages in the slon log?


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin
From kevink at consistentstate.com  Sat Sep 26 00:09:39 2009
From: kevink at consistentstate.com (Kevin Kempter)
Date: Sat Sep 26 00:10:20 2009
Subject: [Slony1-general] slony log shipping
Message-ID: <200909260109.39917.kevink@consistentstate.com>

Hi all;

I'm preparing to up a slony log shipping test.  I get it thus far from the 
docs:

1) I start one of the slon daemons in my existing slony cluster with the -a 
flag to generate tthe logs

2) I can run slony1_dump.sh to geterate a dump of the current 'state' of a 
node

However, I'm un clear as to how I replay both the dump from slony1_dump.sh and 
the subsequent logs into the log ship receiving node, are the dump file (from 
slony1_dump.sh) and the logs sql files that I import via psql?


Thanks in advance...
From stuart at stuartbishop.net  Sun Sep 27 04:28:22 2009
From: stuart at stuartbishop.net (Stuart Bishop)
Date: Sun Sep 27 04:29:09 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25609853.post@talk.nabble.com>
References: <25609853.post@talk.nabble.com>
Message-ID: <6bc73d4c0909270428l669291a5k47875d313cf68765@mail.gmail.com>

On Fri, Sep 25, 2009 at 5:36 PM, roctaiwan <nettreeinc@gmail.com> wrote:

> I am bulkloading (copying)about 10million of records (total size about 1G)
> with replication runing. Takes me about 2h30m to load whole data into
> Master, but after that I give it like another an hour or so to load into my
> slave ( I only have one slave), but zero record got inserted. I then give it
> overnight to do its work, which gives almost 20 hours and still nothing
> happen. I examing the log on Slave and seem no obvious activity has sigh of

I find it amusing to "select datname,current_query from
pg_stat_activity where usename='slony'" on my slave to see what it is
up to when I've got a large backlog to get through. Also looking at
the size and a arbitrary records in _clustername.sl_log_1 and
_clustername.sl_log_2.

> data inserting. (I have attached part of my log)

no you didn't, or something swallowed it ;)

> I do see slony replication behave as records won't show up in slave table
> until all records got processed then it will load WHOLE records all at once
> into the table. I observe this by monitoring slave log.

If you add all the records in a single transaction, they are added on
the slave as a single transaction too.

> This makes me think, before records load on the table physically and while
> data is in processing, are they temporary storted in the memory and waiting
> to be load into the table once done with read records from copying?

No - pending changes are serialized in a PostgreSQL table. Have a look
at what is in _yourclustername.sl_log_1 and_youclustername.sl_log_2.

> Here is my concern. I copying both 10k and 1million of records with success,
> but why can't 10Million records? Is this has something to do with my memory
> size, which loading 10Million of records is too much for my server to
> handle?
>
> I have 512MB memory on both master and slave.





-- 
Stuart Bishop <stuart@stuartbishop.net>
http://www.stuartbishop.net/
From nettreeinc at gmail.com  Sun Sep 27 08:04:04 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Sun Sep 27 08:04:14 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <348491.35682.qm@web23608.mail.ird.yahoo.com>
References: <25609853.post@talk.nabble.com>
	<348491.35682.qm@web23608.mail.ird.yahoo.com>
Message-ID: <25634374.post@talk.nabble.com>


The problem is, I don't see records show up on slaves even for long period of
waiting.



Glyn Astill wrote:
> 
> --- On Fri, 25/9/09, roctaiwan <nettreeinc@gmail.com> wrote:
> 
>> Here is my concern. I copying both 10k and 1million of
>> records with success,
>> but why can't 10Million records? Is this has something to
>> do with my memory
>> size, which loading 10Million of records is too much for my
>> server to
>> handle?
>> 
> 
> If you're subscribing the slave to tables on the master that are already
> populated then the subscribe will use COPY so you'll only see anything on
> the slave when the entrie copy transaction is complete.
> 
> If you want to see them replicated in bite size chunks subscribe before
> populating the table on the master.
> 
> 
> 
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25634374.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Sun Sep 27 08:07:37 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Sun Sep 27 08:07:46 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <4ABCCAD2.4030002@Yahoo.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
Message-ID: <25634400.post@talk.nabble.com>


It took about 30 mins to load 1M records. No error messages I observe. 



Jan Wieck wrote:
> 
> On 9/25/2009 6:36 AM, roctaiwan wrote:
>> Here is my concern. I copying both 10k and 1million of records with
>> success,
>> but why can't 10Million records? Is this has something to do with my
>> memory
>> size, which loading 10Million of records is too much for my server to
>> handle?
> 
> How long does it take Slony to copy those 1M rows?
> 
> Are there any error messages in the slon log?
> 
> 
> Jan
> 
> -- 
> Anyone who trades liberty for security deserves neither
> liberty nor security. -- Benjamin Franklin
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25634400.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From wmoran at potentialtech.com  Sun Sep 27 08:12:10 2009
From: wmoran at potentialtech.com (Bill Moran)
Date: Sun Sep 27 08:12:21 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25634400.post@talk.nabble.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25634400.post@talk.nabble.com>
Message-ID: <20090927111210.405cdb59.wmoran@potentialtech.com>

In response to roctaiwan <nettreeinc@gmail.com>:

> 
> It took about 30 mins to load 1M records. No error messages I observe. 

It would then make sense that 10M would take around 5 hours.  Have you
waited 5 hours to see if they show up after that?  Have you monitored
DB activity after inserting the 10M records to see if it's actively
working to transfer them?

> Jan Wieck wrote:
> > 
> > On 9/25/2009 6:36 AM, roctaiwan wrote:
> >> Here is my concern. I copying both 10k and 1million of records with
> >> success,
> >> but why can't 10Million records? Is this has something to do with my
> >> memory
> >> size, which loading 10Million of records is too much for my server to
> >> handle?
> > 
> > How long does it take Slony to copy those 1M rows?
> > 
> > Are there any error messages in the slon log?

-- 
Bill Moran
http://www.potentialtech.com
http://people.collaborativefusion.com/~wmoran/
From melvin6925 at yahoo.com  Sun Sep 27 08:18:30 2009
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Sun Sep 27 08:18:40 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25634400.post@talk.nabble.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25634400.post@talk.nabble.com>
Message-ID: <619437.9248.qm@web53005.mail.re2.yahoo.com>

>It took about 30 mins to load 1M records. No error messages I observe. 

If you are doing this in a single transaction
IE: 
BEGIN; insert 10m records; COMMIT;


Then the time it takes to replicate 10M would be about 5 hours.

IOW, the more records you change, the longer it takes.

However, have you checked how much physical disk space you have? If you don't have enough on the slave, then the transaction will abort and cause a rollback, which will take even longer to undo.



Melvin Davidson 



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090927/1bbb73be/attachment.htm
From nettreeinc at gmail.com  Sun Sep 27 08:19:44 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Sun Sep 27 08:19:53 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <20090927111210.405cdb59.wmoran@potentialtech.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25634400.post@talk.nabble.com>
	<20090927111210.405cdb59.wmoran@potentialtech.com>
Message-ID: <25634496.post@talk.nabble.com>


Yes, I have waited whole night (about 12 hours) but still don't see it insert
any records to slave. 

reading log and it seems its processing. But process never ended. I wil
provide more logs on both master and slave later on to provide more traces.



Bill Moran wrote:
> 
> In response to roctaiwan <nettreeinc@gmail.com>:
> 
>> 
>> It took about 30 mins to load 1M records. No error messages I observe. 
> 
> It would then make sense that 10M would take around 5 hours.  Have you
> waited 5 hours to see if they show up after that?  Have you monitored
> DB activity after inserting the 10M records to see if it's actively
> working to transfer them?
> 
> 

-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25634496.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From baoluc at gmail.com  Sun Sep 27 19:35:46 2009
From: baoluc at gmail.com (Airbus380)
Date: Sun Sep 27 19:38:09 2009
Subject: [Slony1-general] Slony-I, Slave node and trigger function
Message-ID: <25639915.post@talk.nabble.com>


Hi all,

I used two databse with slony-i, they work well. db_master1 is a master
node, and db_slave1 is a slave.

In the both databse, I have a table call staff with following structure:

CREATE TABLE staff
(
  id text NOT NULL,
  "name" text,
  age integer,
  CONSTRAINT staff_pkey PRIMARY KEY (id)
)
WITH (OIDS=FALSE);
ALTER TABLE staff OWNER TO postgres;

I test on windows xp, it is ok.

Then I created one more databse, its name is db_center.

My idea is: when slave have an operation - insert or update or delete then
the operated record must be updated to db_center, so i create a trigger
function as following:

CREATE OR REPLACE FUNCTION process_staff_audit() RETURNS TRIGGER AS
$staff_audit$
    BEGIN
        IF (TG_OP = 'DELETE') THEN
            PERFORM dblink_exec('dbname=db_center port=5432
host=192.168.1.226 user=user1 password=user1',
				'DELETE FROM staff WHERE id = ''' || OLD.id || ''';',false);
            RETURN OLD;
        ELSIF (TG_OP = 'UPDATE') THEN
            PERFORM dblink_exec('dbname=db_center port=5432
host=192.168.1.226 user=user1 password=user1',
				'UPDATE staff SET "name" = ''' || NEW.name || ''', age = ''' || NEW.age
|| ''' WHERE  id = ''' || NEW.id || ''';',false);
            RETURN NEW;
        ELSIF (TG_OP = 'INSERT') THEN
            PERFORM dblink_exec('dbname=db_center port=5432
host=192.168.1.226 user=user1 password=user1',
				'INSERT INTO staff VALUES( ''' || NEW.id ||''', ''' || NEW.name ||''',
''' || NEW.age ||''');',false);
            RETURN NEW;
        END IF;
        RETURN NULL; -- result is ignored since this is an AFTER trigger
    END;
$staff_audit$ LANGUAGE plpgsql;

CREATE TRIGGER staff_audit
AFTER INSERT OR UPDATE OR DELETE ON staff
    FOR EACH ROW EXECUTE PROCEDURE process_staff_audit();

I tested this trigger function on two database without slony-i, it worked
well.
But I used this trigger function on db_slave1 (using slony-i), it didn't
work.

Please help me to solve this problem,

Thank you very much.

Nick yahoo: lucf52
Nick skype: airbus--380

-- 
View this message in context: http://www.nabble.com/Slony-I%2C-Slave-node-and-trigger-function-tp25639915p25639915.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Sun Sep 27 19:43:22 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Sun Sep 27 19:43:52 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25634496.post@talk.nabble.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25634400.post@talk.nabble.com>
	<20090927111210.405cdb59.wmoran@potentialtech.com>
	<25634496.post@talk.nabble.com>
Message-ID: <25639944.post@talk.nabble.com>


http://www.nabble.com/file/p25639944/slon_db1.log slon_db1.log 
Here is the log I just collected on my slave after let Slony run over the
weekend, third day after inserting 10M records and my slave is still not
updated!!

I have removed one and 1/2 of records from log, which I believe is not
important. Critical message (if there is any)should of appeard long before
that. Otherwise the file is too big to attach. 

Huang

 

roctaiwan wrote:
> 
> Yes, I have waited whole night (about 12 hours) but still don't see it
> insert any records to slave. 
> 
> reading log and it seems its processing. But process never ended. I wil
> provide more logs on both master and slave later on to provide more
> traces.
> 

-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25639944.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Sun Sep 27 20:14:59 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Sun Sep 27 20:15:30 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <6bc73d4c0909270428l669291a5k47875d313cf68765@mail.gmail.com>
References: <25609853.post@talk.nabble.com>
	<6bc73d4c0909270428l669291a5k47875d313cf68765@mail.gmail.com>
Message-ID: <25640104.post@talk.nabble.com>


I checked my sl_log1 and sl_log2 there is nothing in there. 

How do you use this? 

Stuart Bishop wrote:
> 
> 
> I find it amusing to "select datname,current_query from
> pg_stat_activity where usename='slony'" on my slave to see what it is
> up to when I've got a large backlog to get through. Also looking at
> the size and a arbitrary records in _clustername.sl_log_1 and
> _clustername.sl_log_2.
> 
>> This makes me think, before records load on the table physically and
>> while
>> data is in processing, are they temporary storted in the memory and
>> waiting
>> to be load into the table once done with read records from copying?
> 
> No - pending changes are serialized in a PostgreSQL table. Have a look
> at what is in _yourclustername.sl_log_1 and_youclustername.sl_log_2.
> 
> 

-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25640104.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Sun Sep 27 20:22:20 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Sun Sep 27 20:22:52 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <619437.9248.qm@web53005.mail.re2.yahoo.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25634400.post@talk.nabble.com>
	<619437.9248.qm@web53005.mail.re2.yahoo.com>
Message-ID: <25640141.post@talk.nabble.com>


I have about 4G of physical HD spaces just for the 10M records, I believe it
should be plenty, or it isn't.... good point. I will watch out on that. 



melvin6925 wrote:
> 
>>It took about 30 mins to load 1M records. No error messages I observe. 
> 
> If you are doing this in a single transaction
> IE: 
> BEGIN; insert 10m records; COMMIT;
> 
> 
> Then the time it takes to replicate 10M would be about 5 hours.
> 
> IOW, the more records you change, the longer it takes.
> 
> However, have you checked how much physical disk space you have? If you
> don't have enough on the slave, then the transaction will abort and cause
> a rollback, which will take even longer to undo.
> 
> 
> 
> Melvin Davidson 
> 
> 
> 
>       
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25640141.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Mon Sep 28 00:17:00 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Mon Sep 28 00:17:39 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <4ABCCAD2.4030002@Yahoo.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
Message-ID: <25641479.post@talk.nabble.com>


Just got this error when I am running the COPY again today. This error appear
about 20 mins after whole 10M records inserted to Master. what does it mean?

[postgres@Slave-DB1-Slony-I ~]$ WARNING:  terminating connection because of
crash of another server process
DETAIL:  The postmaster has commanded this server process to roll back the
current transaction and exit, because another server process exited
abnormally and possibly corrupted shared memory.
HINT:  In a moment you should be able to reconnect to the database and
repeat your command.
WARNING:  terminating connection because of crash of another server process
DETAIL:  The postmaster has commanded this server process to roll back the
current transaction and exit, because another server process exited
abnormally and possibly corrupted shared memory.
HINT:  In a moment you should be able to reconnect to the database and
repeat your command.
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=925
CONTEXT:  SQL statement "SELECT  "_sql_cluster".cleanupNodelock()"
PL/pgSQL function "cleanupevent" line 77 at PERFORM




Jan Wieck wrote:
> 
> On 9/25/2009 6:36 AM, roctaiwan wrote:
>> Here is my concern. I copying both 10k and 1million of records with
>> success,
>> but why can't 10Million records? Is this has something to do with my
>> memory
>> size, which loading 10Million of records is too much for my server to
>> handle?
> 
> How long does it take Slony to copy those 1M rows?
> 
> Are there any error messages in the slon log?
> 
> 
> Jan
> 
> -- 
> Anyone who trades liberty for security deserves neither
> liberty nor security. -- Benjamin Franklin
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25641479.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From melvin6925 at yahoo.com  Mon Sep 28 06:57:18 2009
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Mon Sep 28 06:57:28 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25641479.post@talk.nabble.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25641479.post@talk.nabble.com>
Message-ID: <829566.90477.qm@web53003.mail.re2.yahoo.com>

>Just got this error when I am running the COPY again today. This error appear
>about 20 mins after whole 10M records inserted to Master. what does it mean?


And what is in the  postgresql.log? That would be your biggest clue as to the cause of the problem.


Melvin Davidson 






________________________________
From: roctaiwan <nettreeinc@gmail.com>
To: slony1-general@lists.slony.info
Sent: Monday, September 28, 2009 2:17:00 AM
Subject: Re: [Slony1-general] copying large file won't get replicate


Just got this error when I am running the COPY again today. This error appear
about 20 mins after whole 10M records inserted to Master. what does it mean?

[postgres@Slave-DB1-Slony-I ~]$ WARNING:  terminating connection because of
crash of another server process
DETAIL:  The postmaster has commanded this server process to roll back the
current transaction and exit, because another server process exited
abnormally and possibly corrupted shared memory.
HINT:  In a moment you should be able to reconnect to the database and
repeat your command.
WARNING:  terminating connection because of crash of another server process
DETAIL:  The postmaster has commanded this server process to roll back the
current transaction and exit, because another server process exited
abnormally and possibly corrupted shared memory.
HINT:  In a moment you should be able to reconnect to the database and
repeat your command.
NOTICE:  Slony-I: cleanup stale sl_nodelock entry for pid=925
CONTEXT:  SQL statement "SELECT  "_sql_cluster".cleanupNodelock()"
PL/pgSQL function "cleanupevent" line 77 at PERFORM




Jan Wieck wrote:
> 
> On 9/25/2009 6:36 AM, roctaiwan wrote:
>> Here is my concern. I copying both 10k and 1million of records with
>> success,
>> but why can't 10Million records? Is this has something to do with my
>> memory
>> size, which loading 10Million of records is too much for my server to
>> handle?
> 
> How long does it take Slony to copy those 1M rows?
> 
> Are there any error messages in the slon log?
> 
> 
> Jan
> 
> -- 
> Anyone who trades liberty for security deserves neither
> liberty nor security. -- Benjamin Franklin
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25641479.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090928/9f4e0e4f/attachment.htm
From ajs at crankycanuck.ca  Mon Sep 28 07:03:13 2009
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Mon Sep 28 07:03:27 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25641479.post@talk.nabble.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25641479.post@talk.nabble.com>
Message-ID: <20090928140312.GA536@shinkuro.com>

On Mon, Sep 28, 2009 at 12:17:00AM -0700, roctaiwan wrote:
> 
> Just got this error when I am running the COPY again today. This error appear
> about 20 mins after whole 10M records inserted to Master. what does it mean?

It means one of your postgres processes is crashing.

> 
> [postgres@Slave-DB1-Slony-I ~]$ WARNING:  terminating connection because of
> crash of another server process
  ^^^^^

Just like it says.

You need to look in the postgres logs to see what's going wrong.
Slony is a postgres client.  It's the server that's crashing.


-- 
Andrew Sullivan
ajs@crankycanuck.ca
From lawrenceg at globalitcreations.com  Mon Sep 28 01:26:47 2009
From: lawrenceg at globalitcreations.com (Lawrence Giam)
Date: Mon Sep 28 08:51:09 2009
Subject: [Slony1-general] Failover and Failback
Message-ID: <F42B651E7C095744BA93C7D0D087CCD301D9FD7B@gitc-mail01.globalitcreations.com>

Skipped content of type multipart/alternative-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 4414 bytes
Desc: image001.jpg
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090928/4fbf9f77/attachment-0001.jpeg
From plk.zuber at gmail.com  Mon Sep 28 09:20:40 2009
From: plk.zuber at gmail.com (=?UTF-8?Q?Filip_Rembia=C5=82kowski?=)
Date: Mon Sep 28 09:20:55 2009
Subject: [Slony1-general] Slony-I, Slave node and trigger function
In-Reply-To: <25639915.post@talk.nabble.com>
References: <25639915.post@talk.nabble.com>
Message-ID: <92869e660909280920l64ed93ecoe7433a17a5ab5729@mail.gmail.com>
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From jason at buberel.org  Mon Sep 28 09:24:28 2009
From: jason at buberel.org (Jason Buberel)
Date: Mon Sep 28 09:25:00 2009
Subject: [Slony1-general] Advice/Recommendations on improving Slony
	performance
Message-ID: <381af7b40909280924lb662f2fve6fac12b79ae31cd@mail.gmail.com>

I have Slony (1.2.x) running on our production cluster, where it has been
running for a few years now. The flow of data looks like this:

master -|----> slave1
        |----> slave2
        |----> slave3
         ...
        |----> slave9

Data always flows from master to each identically configured slave. There is
never any data replication between slaves or from slaves to master. All
one-way, all down-stream, all the time.

As the load on our service has grown, I've begun to see node lag counts
start to grow when machines get busy. Based on my understanding of the
documentation when the cluster was originally configured, I added storage
paths from every node to every other node (master --> slave1-9, salve1 -->
master + save2-9, etc.).

Question #1: If my flow of data is always/only from master to each slave,
can I remove the storage path's between slaves, leaving me only with master
--> slaveN and slaveN --> master? Would this decrease the communication
overhead or be beneficial in general?

Question #2: When initially configured, the DSN connection strings used to
define each node used IP addresses that were part of a 1Gbit network. Since
then, each of these machines has had an additional 10Gbit network connection
added to it. Would it be safe to stop the slon daemons, manually update the
sl_path.pa_conninfo column values to use the IP addresses of these new
network interfaces, then restart slony daemons?

Question #3: Are there other configuration parameters I can use to improve
the overall performance of the cluster?

Thanks,
jason
-- =

Jason L. Buberel
jason@buberel.org
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090928/=
3efe6a61/attachment.htm
From wmoran at potentialtech.com  Mon Sep 28 09:36:38 2009
From: wmoran at potentialtech.com (Bill Moran)
Date: Mon Sep 28 09:36:55 2009
Subject: [Slony1-general] Advice/Recommendations on improving Slony
	performance
In-Reply-To: <381af7b40909280924lb662f2fve6fac12b79ae31cd@mail.gmail.com>
References: <381af7b40909280924lb662f2fve6fac12b79ae31cd@mail.gmail.com>
Message-ID: <20090928123638.9b55bb45.wmoran@potentialtech.com>

In response to Jason Buberel <jason@buberel.org>:

> I have Slony (1.2.x) running on our production cluster, where it has been
> running for a few years now. The flow of data looks like this:
> 
> master -|----> slave1
>         |----> slave2
>         |----> slave3
>          ...
>         |----> slave9
> 
> Data always flows from master to each identically configured slave. There is
> never any data replication between slaves or from slaves to master. All
> one-way, all down-stream, all the time.
> 
> As the load on our service has grown, I've begun to see node lag counts
> start to grow when machines get busy. Based on my understanding of the
> documentation when the cluster was originally configured, I added storage
> paths from every node to every other node (master --> slave1-9, salve1 -->
> master + save2-9, etc.).
> 
> Question #1: If my flow of data is always/only from master to each slave,
> can I remove the storage path's between slaves, leaving me only with master
> --> slaveN and slaveN --> master? Would this decrease the communication
> overhead or be beneficial in general?

I doubt it would help your performance any, and it will cripple you if you
ever need to do a switchover.

> Question #2: When initially configured, the DSN connection strings used to
> define each node used IP addresses that were part of a 1Gbit network. Since
> then, each of these machines has had an additional 10Gbit network connection
> added to it. Would it be safe to stop the slon daemons, manually update the
> sl_path.pa_conninfo column values to use the IP addresses of these new
> network interfaces, then restart slony daemons?

That's the wrong approach.  Simply use slonik to redefine the paths with
store path() and they will be updated.  You won't have to restart anything.

> Question #3: Are there other configuration parameters I can use to improve
> the overall performance of the cluster?

It doesn't sound like you've identified the bottleneck yet, and that you're
assuming that it's network traffic that's the bottleneck.  If you're right,
then #2 will help.

If you're wrong, then it's CPU or disk.  If I were a betting man, I'd put
my money on disk IO as the bottleneck, as that's usually the case.  If I'm
right, then you have a few other options:
1) Buy faster CPU/disks for the servers.
2) Add an additional slave to the mix to remove some of the chore of
   replicating from the master.

Keep in mind that the master has to manage replicating data to all the
slaves, in addition to whatever work it's doing for the clients of the
database.  If you change your layout to:

master |--> slave0 |----> slave1
       |--> slave0 |----> slave2
       |--> slave0 |----> slave3
        ...
       |--> slave0 |----> slave9

Now you've reduced the replication overhead on the master by 1/9.  Since
slave0 is doing nothing _but_ replicating, it should be able to keep up
better than the master does now.

Actually, depending on what your switching fabric looks like, that change
might improve the situation even if the problem is network related.

-- 
Bill Moran
http://www.potentialtech.com
http://people.collaborativefusion.com/~wmoran/
From plk.zuber at gmail.com  Mon Sep 28 09:43:41 2009
From: plk.zuber at gmail.com (=?UTF-8?Q?Filip_Rembia=C5=82kowski?=)
Date: Mon Sep 28 09:43:55 2009
Subject: [Slony1-general] Failover and Failback
In-Reply-To: <F42B651E7C095744BA93C7D0D087CCD301D9FD7B@gitc-mail01.globalitcreations.com>
References: <F42B651E7C095744BA93C7D0D087CCD301D9FD7B@gitc-mail01.globalitcreations.com>
Message-ID: <92869e660909280943mee98430s680f82d6b8bc8c71@mail.gmail.com>
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 baoluc at gmail.com  Mon Sep 28 09:56:13 2009
From: baoluc at gmail.com (Airbus380)
Date: Mon Sep 28 10:19:39 2009
Subject: [Slony1-general] Slony-I, slave node can't get data from master
	after network fail
Message-ID: <25648985.post@talk.nabble.com>


Hi all,
Slony-I is new with me, so I have a problem as following:

I have two databse, db_master (host: 192.168.1.226) and db_slave (host:
192.168.1.5).
I config slony-i work well, all operates on data is ok. 

I using pgadminIII (on host 192.168.1.226) to connect two db, 
then I try to disable network adapter on host 192.168.1.226.

Next, I insert and edit some data on db_master and wait for minute.

After that I enable network adapter on host 192.168.1.226, the data on
db_slave.

The problem is: the data on db_slave and db_master are diffrent. On
db_slave, the data isn't updated.

Note: I use postgres on windows. I restart slony service and restart
computer 192.168.1.226.
But I can't resolve this problem.

Please help me.

Thanks for your support.

Email: baoluc@gmail.com
-- 
View this message in context: http://www.nabble.com/Slony-I%2C-slave-node-can%27t-get-data-from-master-after-network-fail-tp25648985p25648985.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From wmoran at potentialtech.com  Mon Sep 28 10:34:04 2009
From: wmoran at potentialtech.com (Bill Moran)
Date: Mon Sep 28 10:34:21 2009
Subject: [Slony1-general] Slony-I, slave node can't get data from master
	after network fail
In-Reply-To: <25648985.post@talk.nabble.com>
References: <25648985.post@talk.nabble.com>
Message-ID: <20090928133404.2b49ecce.wmoran@potentialtech.com>

In response to Airbus380 <baoluc@gmail.com>:
> 
> Hi all,
> Slony-I is new with me, so I have a problem as following:
> 
> I have two databse, db_master (host: 192.168.1.226) and db_slave (host:
> 192.168.1.5).
> I config slony-i work well, all operates on data is ok. 
> 
> I using pgadminIII (on host 192.168.1.226) to connect two db, 
> then I try to disable network adapter on host 192.168.1.226.
> 
> Next, I insert and edit some data on db_master and wait for minute.
> 
> After that I enable network adapter on host 192.168.1.226, the data on
> db_slave.
> 
> The problem is: the data on db_slave and db_master are diffrent. On
> db_slave, the data isn't updated.
> 
> Note: I use postgres on windows. I restart slony service and restart
> computer 192.168.1.226.
> But I can't resolve this problem.
> 
> Please help me.

First, did you test and verify that replication was working correctly
_before_ you simulated the network failure?

Second, how long did you wait after reconnecting?  Slony sometimes takes
a few minutes to get going again after a network failure.  Restarting
everything only makes it take longer.

Third, What's in slony's logs?

-- 
Bill Moran
http://www.potentialtech.com
http://people.collaborativefusion.com/~wmoran/
From dba at richyen.com  Mon Sep 28 14:22:05 2009
From: dba at richyen.com (Richard Yen)
Date: Mon Sep 28 14:22:28 2009
Subject: [Slony1-general] slony log shipping
In-Reply-To: <200909260109.39917.kevink@consistentstate.com>
References: <200909260109.39917.kevink@consistentstate.com>
Message-ID: <A80BD629-C55A-45AC-9D79-7096C014CCB0@richyen.com>

Replaying the dump and the subsequent logs are completely up to you.   
All of these files are plain text files, so you can replay them  
however you like.

As far as my experience goes, you need to replay the dump via psql

You can opt to use the slony_logshipper app that gets packaged in you  
slony distribution, or you can write your own app/script to handle the  
subsequent logs.

--Richard


On Sep 26, 2009, at 12:09 AM, Kevin Kempter wrote:

> Hi all;
>
> I'm preparing to up a slony log shipping test.  I get it thus far  
> from the
> docs:
>
> 1) I start one of the slon daemons in my existing slony cluster with  
> the -a
> flag to generate tthe logs
>
> 2) I can run slony1_dump.sh to geterate a dump of the current  
> 'state' of a
> node
>
> However, I'm un clear as to how I replay both the dump from  
> slony1_dump.sh and
> the subsequent logs into the log ship receiving node, are the dump  
> file (from
> slony1_dump.sh) and the logs sql files that I import via psql?
>
>
> Thanks in advance...
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

From lawrenceg at globalitcreations.com  Mon Sep 28 18:26:02 2009
From: lawrenceg at globalitcreations.com (Lawrence Giam)
Date: Mon Sep 28 20:54:23 2009
Subject: [Slony1-general] Failover and Failback
Message-ID: <F42B651E7C095744BA93C7D0D087CCD301D9FD7F@gitc-mail01.globalitcreations.com>

Skipped content of type multipart/alternative-------------- next part -----=
---------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 4414 bytes
Desc: image001.jpg
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090929=
/ca5643de/attachment-0001.jpeg
From lawrenceg at globalitcreations.com  Mon Sep 28 19:12:32 2009
From: lawrenceg at globalitcreations.com (Lawrence Giam)
Date: Mon Sep 28 20:54:23 2009
Subject: [Slony1-general] PgPool and Slony-I
Message-ID: <F42B651E7C095744BA93C7D0D087CCD301D9FD81@gitc-mail01.globalitcreations.com>

Skipped content of type multipart/alternative-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 4414 bytes
Desc: image001.jpg
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090929/8ef3edd5/attachment.jpeg
From baoluc at gmail.com  Mon Sep 28 21:14:02 2009
From: baoluc at gmail.com (Airbus380)
Date: Mon Sep 28 21:47:40 2009
Subject: [Slony1-general] Slony-I and triggers on slave.
Message-ID: <25656678.post@talk.nabble.com>


Hi All,

I configure my lab run postgres with slony,  
then write to html file and compress it to zip file and rar file as follow:

http://www.nabble.com/file/p25656678/Slony-I.zip Slony-I.zip 
http://www.nabble.com/file/p25656678/Slony-I..rar Slony-I..rar 

Please see them, and give me an idea.

Thanks you very much.

-- 
View this message in context: http://www.nabble.com/Slony-I-and-triggers-on-slave.-tp25656678p25656678.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Tue Sep 29 03:19:46 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Sep 29 03:20:31 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <20090928140312.GA536@shinkuro.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25641479.post@talk.nabble.com> <20090928140312.GA536@shinkuro.com>
Message-ID: <25660373.post@talk.nabble.com>


Ok I have received this error today, can anybody tell me why is it happening?
and how can I fix it? looks like it has something to do with cache, but I
have set my cache 256mb in postgresql.conf, isn't it not enough?

contactdb=# copy contact from '/home/postgres/5m.copy';
ERROR:  cache lookup failed for type 58783
CONTEXT:  SQL statement "INSERT INTO _sql_cluster.sl_log_1 (log_origin,
log_xid, log_tableid, log_actionseq, log_cmdtype, log_cmddata) VALUES (1,
$1, $2, nextval('_sql_cluster.sl_action_seq'), $3, $4);"

-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25660373.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Tue Sep 29 03:19:57 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Sep 29 03:20:41 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <20090928140312.GA536@shinkuro.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25641479.post@talk.nabble.com> <20090928140312.GA536@shinkuro.com>
Message-ID: <25660375.post@talk.nabble.com>


Ok I have received this error today, can anybody tell me why is it happening?
and how can I fix it? looks like it has something to do with cache, but I
have set my cache 256mb in postgresql.conf, isn't it not enough?

contactdb=# copy contact from '/home/postgres/5m.copy';
ERROR:  cache lookup failed for type 58783
CONTEXT:  SQL statement "INSERT INTO _sql_cluster.sl_log_1 (log_origin,
log_xid, log_tableid, log_actionseq, log_cmdtype, log_cmddata) VALUES (1,
$1, $2, nextval('_sql_cluster.sl_action_seq'), $3, $4);"

Also, it said something about sl_log1 but I don't see anything in that
table. it's empty
-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25660375.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From glynastill at yahoo.co.uk  Tue Sep 29 03:53:15 2009
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue Sep 29 04:16:25 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25660375.post@talk.nabble.com>
Message-ID: <22732.24995.qm@web23601.mail.ird.yahoo.com>

--- On Tue, 29/9/09, roctaiwan <nettreeinc@gmail.com> wrote:
> 
> Ok I have received this error today, can anybody tell me
> why is it happening?
> and how can I fix it? looks like it has something to do
> with cache, but I
> have set my cache 256mb in postgresql.conf, isn't it not
> enough?
> 
> contactdb=# copy contact from '/home/postgres/5m.copy';
> ERROR:? cache lookup failed for type 58783
> CONTEXT:? SQL statement "INSERT INTO
> _sql_cluster.sl_log_1 (log_origin,
> log_xid, log_tableid, log_actionseq, log_cmdtype,
> log_cmddata) VALUES (1,
> $1, $2, nextval('_sql_cluster.sl_action_seq'), $3, $4);"
> 
> Also, it said something about sl_log1 but I don't see
> anything in that
> table. it's empty


Hmm, looks like data corruption to me.  Probably something to do with the problem you posted earlier about the backend crashing.

I'd flatten that slave database and try again, obviously it'd be useful for you to find out why the server was crashing in the first place too...


Glyn

Send instant messages to your online friends http://uk.messenger.yahoo.com 
From lawrenceg at globalitcreations.com  Mon Sep 28 22:54:27 2009
From: lawrenceg at globalitcreations.com (Lawrence Giam)
Date: Tue Sep 29 08:46:25 2009
Subject: [Slony1-general] Testing failover
Message-ID: <F42B651E7C095744BA93C7D0D087CCD301D9FD8D@gitc-mail01.globalitcreations.com>

Skipped content of type multipart/alternative-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 4414 bytes
Desc: image001.jpg
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090929/72e5612e/attachment-0001.jpeg
From plk.zuber at gmail.com  Tue Sep 29 09:11:39 2009
From: plk.zuber at gmail.com (=?UTF-8?Q?Filip_Rembia=C5=82kowski?=)
Date: Tue Sep 29 09:11:51 2009
Subject: [Slony1-general] Failover and Failback
In-Reply-To: <F42B651E7C095744BA93C7D0D087CCD301D9FD7F@gitc-mail01.globalitcreations.com>
References: <F42B651E7C095744BA93C7D0D087CCD301D9FD7F@gitc-mail01.globalitcreations.com>
Message-ID: <92869e660909290911lb0f6b7bw6940fca0b323a37e@mail.gmail.com>

VyBkbml1IDI5IHdyemXFm25pYSAyMDA5IDAzOjI2IHXFvHl0a293bmlrIExhd3JlbmNlIEdpYW0g
PApsYXdyZW5jZWdAZ2xvYmFsaXRjcmVhdGlvbnMuY29tPiBuYXBpc2HFgjoKCj4gIEhpCj4KPgo+
Cj4gVGhhbmtzIGZvciB0aGUgYWR2aWNlLgo+Cj4KPgo+IFdpdGggcmVnYXJkIHRvIHRoZSBxdWVz
dGlvbiBvbiBtb3ZpbmcgYmFjayB0aGUgbWFzdGVyIHJvbGUgYmFjayB0byBOMSBhZnRlcgo+IGZ1
bGx5IHN5bmNlZCB3aXRoIE4yLiBEb2VzIHRoZSBtb3ZlX3NldCBjb21tYW5kIHBhc3MgdGhlIE1h
c3RlciByb2xlIGJhY2sKPiBjb21wbGV0ZWx5Pwo+Cnllcy4gKCB3aXRoIHJlZ2FyZHMgdG8gdGhp
cyByZXBsaWNhdGlvbiBzZXQuIHlvdSBjYW4gaGF2ZSBtb3JlIHJlcGxpY2F0aW9uCnNldHMsIGVh
Y2ggd2l0aCBpdHMgb3duIG9yaWdpbiApCgoKPgo+Cj4gQXNzdW1pbmcgdGhlIHJvbGUgYXJlIHN3
aXRjaGVkIGJhY2sgdG8gdGhlIGluaXRpYWwgc2V0dXAgc3RhZ2UsIG1lYW5pbmcgTjEKPiBpcyB0
aGUgTWFzdGVyIGFuZCBOMiBpcyB0aGUgU2xhdmUuIEFzc3VtaW5nIGlmIHdlIGhhdmUgdG8gc3Rv
cCBib3RoIHNlcnZlcnMKPiBkdWUgdG8gdW5zZWVuIGV2ZW50IGFuZCBhdCB0aGUgcG9pbnQgb2Yg
c3RhcnRpbmcgdGhlIHNsb24gZGFlbW9uIG9uIGJvdGgKPiBzZXJ2ZXJzLAo+Cj4gICAgMS4gaXMg
aXQgdGhlIHNhbWUgYXMgYmVmb3JlIChlZy4gTjEg4oCTIHNsb25fc3RhcnQgMSwgIE4yIOKAkyBz
bG9uX3N0YXJ0Cj4gICAgMik/Cj4KPiB5ZXMuIHdoeSBzaG91bGQgaXQgYmUgZGlmZmVyZW50PwoK
Pgo+ICAgIDEuIHdpbGwgdGhlIG1hc3RlciByb2xlIHN0aWxsIGJlIHdpdGggTjE/Cj4KPiB5ZXMu
CgoKKCBzbG9uIGRlYW1vbnMgcmVhZCB0aGVpciBjb25maWd1cmF0aW9uIGZyb20gdGhlIGNsdXN0
ZXIsIG5vdCBmcm9tCi9ldGMvc2xvbnkuLi4uIGNvbmZpZyBmaWxlczsgdGhlc2UgYXJlIHVzZWQg
b25seSBmb3IgaW5pdGlhbCBjb25maWd1cmF0aW9uLAphbmQgc3lzYWRtaW4gaGFzIHRvIGtlZXAg
dGhlbSB1cCB0byBkYXRlIG1hbnVhbGx5ICkKCgoKCi0tIApGaWxpcCBSZW1iaWHFgmtvd3NraQpK
SUQsbWFpbHRvOmZpbGlwLnJlbWJpYWxrb3dza2lAZ21haWwuY29tCmh0dHA6Ly9maWxpcC5yZW1i
aWFsa293c2tpLm5ldC8KLS0tLS0tLS0tLS0tLS0gbmV4dCBwYXJ0IC0tLS0tLS0tLS0tLS0tCkFu
IEhUTUwgYXR0YWNobWVudCB3YXMgc2NydWJiZWQuLi4KVVJMOiBodHRwOi8vbGlzdHMuc2xvbnku
aW5mby9waXBlcm1haWwvc2xvbnkxLWdlbmVyYWwvYXR0YWNobWVudHMvMjAwOTA5MjkvNDhjYjE5
MjkvYXR0YWNobWVudC5odG0K
From plk.zuber at gmail.com  Tue Sep 29 09:25:02 2009
From: plk.zuber at gmail.com (=?UTF-8?Q?Filip_Rembia=C5=82kowski?=)
Date: Tue Sep 29 09:25:12 2009
Subject: [Slony1-general] Testing failover
In-Reply-To: <F42B651E7C095744BA93C7D0D087CCD301D9FD8D@gitc-mail01.globalitcreations.com>
References: <F42B651E7C095744BA93C7D0D087CCD301D9FD8D@gitc-mail01.globalitcreations.com>
Message-ID: <92869e660909290925o153665n133b88fe36a07a3@mail.gmail.com>
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From michael at aers.ca  Tue Sep 29 10:38:07 2009
From: michael at aers.ca (michael@aers.ca)
Date: Tue Sep 29 10:38:22 2009
Subject: [Slony1-general] Slony init.d script help
Message-ID: <6B5AF6293A289F45826220B17ABE7937011A4991@BORON.aers.local>

I'm having some issues with my init.d scripts for slony. I have three
different clusters and init scripts for each of them. When running I see
each daemon has two processes, such as:

 

postgres 13894     1  0 10:06 ?        00:00:00 /usr/bin/slony1-cluster1
-f /etc/slony1-cluster1.conf

postgres 16149 13894  0 10:13 ?        00:00:00 /usr/bin/slony1-
cluster1 -f /etc/slony1-cluster1.conf

postgres 13774     1  0 10:06 ?        00:00:00 /usr/bin/slony1-
cluster2 -f /etc/slony1-cluster2.conf

postgres 13776 13774  0 10:06 ?        00:00:00 /usr/bin/slony1-
cluster2 -f /etc/slony1-cluster2.conf

 

 

/usr/bin/slony1-cluster1 and /usr/bin/slony1-cluster2 are symlinks to
/usr/bin/slony1

 

In the init scripts, if I run 'killproc /usr/bin/slony1' for the stop
function then both daemons are getting shut down. If I try 'killproc -p
/path/to/slony1-cluster1.pid then one of the slony1-cluster1 processes
gets terminated, but the other hangs around and seems to respawn the
first. Looking for suggestions. Here's my current stop and start code:

 

start(){

        SLON_START=$"Starting ${NAME} service: "

 

        echo -n "$SLON_START"

        $SU -l postgres -c "$SLONDAEMON -f $SLONCONF &" >> "$SLONLOG"
2>&1 < /dev/null

        sleep 2

 

        pid=`pidof -s "$SLONDAEMON"`

        if [ $pid ]

        then

                success "$SLON_START"

                touch /var/lock/subsys/${NAME}

                echo $pid > $SLONPID

                echo

        else

                failure "$SLON_START"

                echo

                script_result=1

        fi

}

 

stop(){

        echo -n $"Stopping ${NAME} service: "

        if [ $UID -ne 0 ]; then

                RETVAL=1

                failure

        else

                killproc -p $SLONPID $SLONDAEMON

                RETVAL=$?

                [ $RETVAL -eq 0 ] && rm -f /var/lock/subsys/${NAME}

        fi;

        echo

        return $RETVAL

}

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090929/c9dc7284/attachment-0001.htm
From cbbrowne at ca.afilias.info  Tue Sep 29 15:17:36 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Sep 29 15:17:55 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25660373.post@talk.nabble.com> (roctaiwan's message of "Tue, 29
	Sep 2009 03:19:46 -0700 (PDT)")
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25641479.post@talk.nabble.com> <20090928140312.GA536@shinkuro.com>
	<25660373.post@talk.nabble.com>
Message-ID: <87hbul8kfz.fsf@dba2.int.libertyrms.com>

roctaiwan <nettreeinc@gmail.com> writes:
> Ok I have received this error today, can anybody tell me why is it happening?
> and how can I fix it? looks like it has something to do with cache, but I
> have set my cache 256mb in postgresql.conf, isn't it not enough?
>
> contactdb=# copy contact from '/home/postgres/5m.copy';
> ERROR:  cache lookup failed for type 58783
> CONTEXT:  SQL statement "INSERT INTO _sql_cluster.sl_log_1 (log_origin,
> log_xid, log_tableid, log_actionseq, log_cmdtype, log_cmddata) VALUES (1,
> $1, $2, nextval('_sql_cluster.sl_action_seq'), $3, $4);"

The database looks pretty mussed up if parts of relations are missing.

I'd concur with Glyn that you probably need to reconstruct that
would-be subscriber from scratch (likely from the point of running
"initdb"), as well as that it would be useful to figure out what
caused the database to be corrupted.

Slony-I shouldn't be able to do so.

The amount of cache is pretty irrelevant to the matter.
-- 
(reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From baoluc at gmail.com  Tue Sep 29 13:33:53 2009
From: baoluc at gmail.com (Airbus380)
Date: Tue Sep 29 15:20:49 2009
Subject: [Slony1-general] Slony-I, slave node can't get data from master
	after network fail
In-Reply-To: <20090928133404.2b49ecce.wmoran@potentialtech.com>
References: <25648985.post@talk.nabble.com>
	<20090928133404.2b49ecce.wmoran@potentialtech.com>
Message-ID: <25670372.post@talk.nabble.com>


Dear Bill Moran,

Before i test slony with network fail, It run well, the master and slave
node update data together very well.

Then I simulated the network failure and slave don't have data as master.

I tested this case two times, and I have the same result, slony don't update
to slave node.

The second testing, I don't restart slony service and don't restart computer
and I wait for two hours.

But no change.

I attch slony log and system view, please see them and help me.
http://www.nabble.com/file/p25670372/postgresql-2009-09-29_000000.log
postgresql-2009-09-29_000000.log 
http://www.nabble.com/file/p25670372/system%2Bview.csv system+view.csv 

Thank you vey much.
baoluc@gmail.com



Bill Moran wrote:
> 
> In response to Airbus380 <baoluc@gmail.com>:
>> 
>> Hi all,
>> Slony-I is new with me, so I have a problem as following:
>> 
>> I have two databse, db_master (host: 192.168.1.226) and db_slave (host:
>> 192.168.1.5).
>> I config slony-i work well, all operates on data is ok. 
>> 
>> I using pgadminIII (on host 192.168.1.226) to connect two db, 
>> then I try to disable network adapter on host 192.168.1.226.
>> 
>> Next, I insert and edit some data on db_master and wait for minute.
>> 
>> After that I enable network adapter on host 192.168.1.226, the data on
>> db_slave.
>> 
>> The problem is: the data on db_slave and db_master are diffrent. On
>> db_slave, the data isn't updated.
>> 
>> Note: I use postgres on windows. I restart slony service and restart
>> computer 192.168.1.226.
>> But I can't resolve this problem.
>> 
>> Please help me.
> 
> First, did you test and verify that replication was working correctly
> _before_ you simulated the network failure?
> 
> Second, how long did you wait after reconnecting?  Slony sometimes takes
> a few minutes to get going again after a network failure.  Restarting
> everything only makes it take longer.
> 
> Third, What's in slony's logs?
> 
> -- 
> Bill Moran
> http://www.potentialtech.com
> http://people.collaborativefusion.com/~wmoran/
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/Slony-I%2C-slave-node-can%27t-get-data-from-master-after-network-fail-tp25648985p25670372.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From cbbrowne at ca.afilias.info  Tue Sep 29 15:22:41 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Sep 29 15:23:00 2009
Subject: [Slony1-general] Slony init.d script help
In-Reply-To: <6B5AF6293A289F45826220B17ABE7937011A4991@BORON.aers.local>
	(michael@aers.ca's message of "Tue, 29 Sep 2009 10:38:07 -0700")
References: <6B5AF6293A289F45826220B17ABE7937011A4991@BORON.aers.local>
Message-ID: <87d4598k7i.fsf@dba2.int.libertyrms.com>

<michael@aers.ca> writes:
> 		   :v="urn:schemas-microsoft-com:vml" xmlns:o="urn:schemas-microsoft-com:office:office"
> 	xmlns:w="urn:schemas-microsoft-com:office:word" xmlns:m="http://schemas.microsoft.com/office/2004/12/omml"
> 					 xmlns="http://www.w3.org/TR/REC-html40">
>
>    I'm having some issues with my init.d scripts for slony. I have three different clusters and init scripts for each of
> 			    them. When running I see each daemon has two processes, such as::p>
>
> 							   :p>?
>
> 	 postgres 13894???? 1? 0 10:06 ???????? 00:00:00 /usr/bin/slony1-cluster1 -f /etc/slony1-cluster1.conf:p>
>
> 	 postgres 16149 13894? 0 10:13 ???????? 00:00:00 /usr/bin/slony1- cluster1 -f /etc/slony1-cluster1.conf:p>
>
> 	 postgres 13774???? 1? 0 10:06 ???????? 00:00:00 /usr/bin/slony1- cluster2 -f /etc/slony1-cluster2.conf:p>
>
> 	 postgres 13776 13774? 0 10:06 ???????? 00:00:00 /usr/bin/slony1- cluster2 -f /etc/slony1-cluster2.conf:p>
>
> 							   :p>?
>
> 							   :p>?
>
> 		 /usr/bin/slony1-cluster1 and /usr/bin/slony1-cluster2 are symlinks to /usr/bin/slony1:p>
>
> 							   :p>?
>
> In the init scripts, if I run `killproc /usr/bin/slony1' for the stop function then both daemons are getting shut down. If
>  I try `killproc --p /path/to/slony1-cluster1.pid then one of the slony1-cluster1 processes gets terminated, but the other
>       hangs around and seems to respawn the first. Looking for suggestions. Here's my current stop and start code::p>


I presume this is on Linux?

Linux reports each thread as a separate entry in the process table,
which is why you'd see more processes than you have slons.  Notice
that 16149 is a child of 13894, which is quite consistent with this.

The PID file gets generated by the main thread, and so killing that
should also kill off any child threads.  You might want to check the
contents of the PID file to verify that.

-- 
(reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From nettreeinc at gmail.com  Tue Sep 29 18:47:03 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Sep 29 18:47:28 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <22732.24995.qm@web23601.mail.ird.yahoo.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25641479.post@talk.nabble.com> <20090928140312.GA536@shinkuro.com>
	<25660375.post@talk.nabble.com>
	<22732.24995.qm@web23601.mail.ird.yahoo.com>
Message-ID: <25673735.post@talk.nabble.com>


Glyn,
When you said flatten the slave DB, what do you mean? clean the whole DB and
leave it empty and re-sync it again from master from ground up?

How can I find out why server crashing in 'first place'?

Thanks,

Huang



Glyn Astill wrote:
> 
> Hmm, looks like data corruption to me.  Probably something to do with the
> problem you posted earlier about the backend crashing.
> 
> I'd flatten that slave database and try again, obviously it'd be useful
> for you to find out why the server was crashing in the first place too...
> 
> 
> Glyn
> 

-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25673735.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From ishii at postgresql.org  Tue Sep 29 18:50:50 2009
From: ishii at postgresql.org (Tatsuo Ishii)
Date: Tue Sep 29 18:51:17 2009
Subject: [Slony1-general] temp tables?
Message-ID: <20090930.105050.04440797.t-ishii@sraoss.co.jp>

Hi,

In my understanding, Slony-I does not allow to create/modify temp
tables on slaves. Does anybody know the reason for this?
--
Tatsuo Ishii
SRA OSS, Inc. Japan
From nettreeinc at gmail.com  Tue Sep 29 19:03:21 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Sep 29 19:03:46 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <87hbul8kfz.fsf@dba2.int.libertyrms.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25641479.post@talk.nabble.com> <20090928140312.GA536@shinkuro.com>
	<25660373.post@talk.nabble.com>
	<87hbul8kfz.fsf@dba2.int.libertyrms.com>
Message-ID: <25673843.post@talk.nabble.com>



>The database looks pretty mussed up if parts of relations are missing.

>I'd concur with Glyn that you probably need to reconstruct that
>would-be subscriber from scratch (likely from the point of running
>"initdb"), as well as that it would be useful to figure out what
>caused the database to be corrupted.

Chris,

could you explain more of what you saying here, I am not quite understand. 
Do you mean there is problem of the way I configure subscriber? My slony
topology  is very simple, because I am just doing the testing (HA). So only
one Master and one Slave. 
If you interested I have include in attachment of my cluster_setup script
http://www.nabble.com/file/p25673843/cluster_setup.sh cluster_setup.sh 

Also, how and what's best method to figure out what cause DB crashed?


>Slony-I shouldn't be able to do so.

>The amount of cache is pretty irrelevant to the matter.
 


-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25673843.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From stuart at stuartbishop.net  Tue Sep 29 19:36:42 2009
From: stuart at stuartbishop.net (Stuart Bishop)
Date: Tue Sep 29 19:37:17 2009
Subject: [Slony1-general] temp tables?
In-Reply-To: <20090930.105050.04440797.t-ishii@sraoss.co.jp>
Message-ID: <g07guv92xc4on3hs03UYAxe124vaj_firegpg@mail.gmail.com>

DQoNCk9uIFdlZCwgU2VwIDMwLCAyMDA5IGF0IDg6NTAgQU0sIFRhdHN1byBJc2hpaSA8aXNoaWlA
cG9zdGdyZXNxbC5vcmc+IHdyb3RlOg0KDQo+IEluIG15IHVuZGVyc3RhbmRpbmcsIFNsb255LUkg
ZG9lcyBub3QgYWxsb3cgdG8gY3JlYXRlL21vZGlmeSB0ZW1wDQo+IHRhYmxlcyBvbiBzbGF2ZXMu
IERvZXMgYW55Ym9keSBrbm93IHRoZSByZWFzb24gZm9yIHRoaXM/DQoNClRoaXMgaXMgaW5jb3Jy
ZWN0LiBTbG9ueS1JIGRvZXMgbm90IHN0b3AgeW91IGNyZWF0aW5nIHRlbXBvcmFyeSB0YWJsZXMg
b24gYW55IG5vZGVzLCBubyBtYXR0ZXIgaWYgdGhleSBhcmUgYSBwcm92aWRlciBmb3Igb25lIG9y
IG1vcmUgcmVwbGljYXRpb24gc2V0cywgYSBzdWJzY3JpYmVyIG9mIG9uZSBvciBtb3JlIHJlcGxp
Y2F0aW9uIHNldHMsIG9yIHNvbWUgbWl4dHVyZSBvZiBwcm92aWRlciBhbmQgc3Vic2NyaWJlci4N
Cg0KDQoNCi0tIA0KU3R1YXJ0IEJpc2hvcCA8c3R1YXJ0QHN0dWFydGJpc2hvcC5uZXQ+DQpodHRw
Oi8vd3d3LnN0dWFydGJpc2hvcC5uZXQvDQoNCi0tLS0tLS0tLS0tLS0tIG5leHQgcGFydCAtLS0t
LS0tLS0tLS0tLQpBIG5vbi10ZXh0IGF0dGFjaG1lbnQgd2FzIHNjcnViYmVkLi4uCk5hbWU6IHNp
Z25hdHVyZS5hc2MKVHlwZTogYXBwbGljYXRpb24vcGdwLXNpZ25hdHVyZQpTaXplOiAyNjIgYnl0
ZXMKRGVzYzogT3BlblBHUCBkaWdpdGFsIHNpZ25hdHVyZQpVcmwgOiBodHRwOi8vbGlzdHMuc2xv
bnkuaW5mby9waXBlcm1haWwvc2xvbnkxLWdlbmVyYWwvYXR0YWNobWVudHMvMjAwOTA5MzAvNjA5
OTAyZDkvc2lnbmF0dXJlLnBncAo=
From nettreeinc at gmail.com  Tue Sep 29 19:51:19 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Sep 29 19:51:46 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25673843.post@talk.nabble.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25641479.post@talk.nabble.com> <20090928140312.GA536@shinkuro.com>
	<25660373.post@talk.nabble.com>
	<87hbul8kfz.fsf@dba2.int.libertyrms.com>
	<25673843.post@talk.nabble.com>
Message-ID: <25674187.post@talk.nabble.com>


I just found out a error in /data/serverlog that it said "could not write to
log file: No space left on device"
But clearly I have 6.4G Disk space available (see pic attached)
http://www.nabble.com/file/p25674187/master.jpg 
I did not generating any log myself. so which log file is it talking about?!


-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25674187.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From ajs at crankycanuck.ca  Tue Sep 29 20:27:07 2009
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue Sep 29 20:27:44 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25673843.post@talk.nabble.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25641479.post@talk.nabble.com> <20090928140312.GA536@shinkuro.com>
	<25660373.post@talk.nabble.com>
	<87hbul8kfz.fsf@dba2.int.libertyrms.com>
	<25673843.post@talk.nabble.com>
Message-ID: <20090930032707.GC5228@shinkuro.com>

On Tue, Sep 29, 2009 at 07:03:21PM -0700, roctaiwan wrote:

> could you explain more of what you saying here, I am not quite understand. 
> Do you mean there is problem of the way I configure subscriber? My slony

No, what Chris is saying is that your subscriber database is somehow
badly broken.  

It sounds very much like you need to spend some time getting familiar
with Postgres administration before trying to administer Slony.  Slony
is just a client of the underlying Postgres database, and if the
Postgres system is broken, Slony can't help you.

A


-- 
Andrew Sullivan
ajs@crankycanuck.ca
From ajs at crankycanuck.ca  Tue Sep 29 20:30:27 2009
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue Sep 29 20:30:57 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25674187.post@talk.nabble.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25641479.post@talk.nabble.com> <20090928140312.GA536@shinkuro.com>
	<25660373.post@talk.nabble.com>
	<87hbul8kfz.fsf@dba2.int.libertyrms.com>
	<25673843.post@talk.nabble.com> <25674187.post@talk.nabble.com>
Message-ID: <20090930033027.GD5228@shinkuro.com>

On Tue, Sep 29, 2009 at 07:51:19PM -0700, roctaiwan wrote:

> I just found out a error in /data/serverlog that it said "could not write to
> log file: No space left on device"

I _think_ that's the Postgres transaction log (the Write Ahead Log or
WAL).  If you're out of space where that's stored, I would expect you
to be in pretty serious trouble.  Also, note that just because your
current GUI seems to think you have 6 G does not mean that there's 6 G
available to the Postgres user.

Anyway, again, this is a Posrgres basic question, and I think you need
to head over to pgsql-novice for some help.

A


-- 
Andrew Sullivan
ajs@crankycanuck.ca
From nettreeinc at gmail.com  Tue Sep 29 21:00:17 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Sep 29 21:00:47 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25674187.post@talk.nabble.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25641479.post@talk.nabble.com> <20090928140312.GA536@shinkuro.com>
	<25660373.post@talk.nabble.com>
	<87hbul8kfz.fsf@dba2.int.libertyrms.com>
	<25673843.post@talk.nabble.com> <25674187.post@talk.nabble.com>
Message-ID: <25674659.post@talk.nabble.com>


I found that if I broken the 10Million records into 4 pieces of files and
copy one at a time, then it will fix the problem. 

Again, mentioned it at very beginning. If I load whole 10M records all at
once, records won't show even for 3 days. 
-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25674659.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From melvin6925 at yahoo.com  Tue Sep 29 21:43:46 2009
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Tue Sep 29 21:44:17 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25674659.post@talk.nabble.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25641479.post@talk.nabble.com> <20090928140312.GA536@shinkuro.com>
	<25660373.post@talk.nabble.com>
	<87hbul8kfz.fsf@dba2.int.libertyrms.com>
	<25673843.post@talk.nabble.com> <25674187.post@talk.nabble.com>
	<25674659.post@talk.nabble.com>
Message-ID: <848700.17019.qm@web53007.mail.re2.yahoo.com>

>Again, mentioned it at very beginning. If I load whole 10M records all at
>once, records won't show even for 3 days. 


And again, as previously mentioned by myself and others. and your own finding in the error log, you do not have enough space for the postgresql wal file to handle 10 million inserts. 

 
Melvin Davidson 








________________________________
From: roctaiwan <nettreeinc@gmail.com>
To: slony1-general@lists.slony.info
Sent: Tuesday, September 29, 2009 11:00:17 PM
Subject: Re: [Slony1-general] copying large file won't get replicate


I found that if I broken the 10Million records into 4 pieces of files and
copy one at a time, then it will fix the problem. 

Again, mentioned it at very beginning. If I load whole 10M records all at
once, records won't show even for 3 days. 
-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25674659.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090930/3e3dc553/attachment.htm
From lawrencew00 at hotmail.com  Tue Sep 29 21:51:07 2009
From: lawrencew00 at hotmail.com (Lawrence Wong)
Date: Tue Sep 29 21:51:39 2009
Subject: [Slony1-general] Multiple Databases
Message-ID: <SNT101-W39A100DD781AEEDFDA3215BDD40@phx.gbl>


Hi,
I am trying to replicate two databases in a simple 1 master, 1 slave config=
uration.  I was successful with one database but unsuccessful with 2.  I wa=
s wondering if anyone could point out my mistake in my slonik script trying=
 to replicate 2 databases.  I have read that a simple way to go around is t=
o have the tables in the same schema but I would like to keep the databases=
 separate.  Here is my slonik script:
cluster name =3D slony_dbOne;node 1 admin conninfo=3D'host=3D192.168.112.4 =
dbname=3Ddb user=3Dpostgres password=3Dpostgres';node 2 admin conninfo=3D'h=
ost=3D192.168.112.5 dbname=3Ddb user=3Dpostgres password=3Dpostgres';
init cluster (id=3D1,comment=3D'node 1 dbOne');
#CREATE SETcreate set (id =3D 1, origin =3D 1, comment =3D 'tables');
#SET ADD TABLEset add table (set id=3D1, origin=3D1, id=3D1, fully qualifie=
d name =3D 'schema1.table2',comment=3D'');set add table (set id=3D1, origin=
=3D1, id=3D2, fully qualified name =3D 'schema1.table2',comment=3D'');
#STORE NODEstore node ( id =3D 2, comment =3D 'node 2 dbOne' );
#STORE PATHstore path ( server =3D 1, client =3D 2,	conninfo =3D 'dbname=3D=
db host=3D192.168.112.4 user=3Dpostgres password=3Dpostgres');store path ( =
server =3D 2, client =3D 1,	conninfo =3D 'dbname=3Ddb host=3D192.168.112.5 =
user=3Dpostgres password=3Dpostgres');
#STORE LISTENstore listen ( origin =3D 1, provider =3D 1, receiver =3D 2 );=
store listen ( origin =3D 2, provider =3D 2, receiver =3D 1 );
cluster name =3D slony_dbTwo;
node 3 admin conninfo=3D'host=3D192.168.112.4 dbname=3DdbTwo user=3Dpostgre=
s password=3Dpostgres';node 4 admin conninfo=3D'host=3D192.168.112.5 dbname=
=3DdbTwo user=3Dpostgres password=3Dpostgres';
init cluster (id=3D3,comment=3D'node 1 dbTwo');
#CREATE SETcreate set (id =3D 2, origin =3D 3, comment =3D 'tables');
#SET ADD TABLEset add sequence (set id=3D2, origin=3D3, id=3D1, fully quali=
fied name =3D 'schema1.table1', comment=3D'');
#STORE NODEstore node ( id =3D 4, comment =3D 'node 2 dbTwo' );
#STORE PATHstore path ( server =3D 3, client =3D 4, conninfo =3D 'dbname=3D=
dbTwo host=3D192.168.112.4 user=3Dpostgres password=3Dpostgres');store path=
 ( server =3D 4, client =3D 3, conninfo =3D 'dbname=3DdbTwo host=3D192.168.=
112.5 user=3Dpostgres password=3Dpostgres');
#STORE LISTENstore listen ( origin =3D 3, provider =3D 3, receiver =3D 4 );=
store listen ( origin =3D 4, provider =3D 4, receiver =3D 3 );
 		 	   		  =

_________________________________________________________________
Internet explorer 8 lets you browse the web faster.
http://go.microsoft.com/?linkid=3D9655582
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090930/=
f6cda629/attachment.htm
From nettreeinc at gmail.com  Tue Sep 29 22:39:15 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Sep 29 22:39:48 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <848700.17019.qm@web53007.mail.re2.yahoo.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25641479.post@talk.nabble.com> <20090928140312.GA536@shinkuro.com>
	<25660373.post@talk.nabble.com>
	<87hbul8kfz.fsf@dba2.int.libertyrms.com>
	<25673843.post@talk.nabble.com> <25674187.post@talk.nabble.com>
	<25674659.post@talk.nabble.com>
	<848700.17019.qm@web53007.mail.re2.yahoo.com>
Message-ID: <25675248.post@talk.nabble.com>


first, I don't think I had turn on WAL. The #archive_mode parameter has not
been turned on. Unless it's on naturally by this is how postgresql should
behave.
second, how much space is big enough to handle 10M inserts? I have 6.4GB
available right now. 



melvin6925 wrote:
> 
>>Again, mentioned it at very beginning. If I load whole 10M records all at
>>once, records won't show even for 3 days. 
> 
> 
> And again, as previously mentioned by myself and others. and your own
> finding in the error log, you do not have enough space for the postgresql
> wal file to handle 10 million inserts. 
> 
>  
> Melvin Davidson 
> 

-- 
View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25675248.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From glynastill at yahoo.co.uk  Wed Sep 30 01:40:23 2009
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Wed Sep 30 01:41:03 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25675248.post@talk.nabble.com>
Message-ID: <957284.29440.qm@web23604.mail.ird.yahoo.com>

--- On Wed, 30/9/09, roctaiwan <nettreeinc@gmail.com> wrote:
> 
> first, I don't think I had turn on WAL. The #archive_mode
> parameter has not
> been turned on. Unless it's on naturally by this is how
> postgresql should
> behave.
> second, how much space is big enough to handle 10M inserts?
> I have 6.4GB
> available right now. 

Nope, that parameter is for WAL archiving hence the name "archive_mode". WAL is the underpinnings of the transaction system, if you have corrupt/missing WAL then your fudged.

http://www.postgresql.org/docs/8.3/static/wal-intro.html

The top and bottom of it is you need enough disk space to accomodate the growth of the WAL.

As the others have said, you really need to start reading the postgres docs, and get youself some omre disk space.

Glyn

Send instant messages to your online friends http://uk.messenger.yahoo.com 
From Steeg at iabg.de  Wed Sep 30 03:57:21 2009
From: Steeg at iabg.de (Steeg Martin Dr.)
Date: Wed Sep 30 03:58:24 2009
Subject: [Slony1-general] Applicability of Slony in WANs and Satellite
	Networks
Message-ID: <BBDE6D332D319548BAAAC70344517B6206DCBA83@exchange03.iabg.de>

Hi there,
I'm already applying Slony-I in a small single-master / single-slave
application test environment (non-commercial) and I have some question
considering the applicability of Slony-I / Slony-II in some more general
situations.

These questions are:

Ability to provide WAN replication.
 - Is it possible to use Slony-I in wide-are network (WAN) enviroment ?
The bandwidth on the master-master link is 34 Mbit/sec, on the
master-slave link 10 MBit/sec. Both sides will have a latency of 30msec.

Robustness concerning network drop out.
 - Can (slave) nodes which are not continuosly connected to network be
used ?

Satellite Connection. This scenario considers the replication between
master and slave nodes communicating over satellite.
 - Is Slony (-I, maybe the sucessor -II) capable for Replication over
satellite network (latency 850msec, bandwith 2 MBit/sec) ?

Data consistency. In this scenario, the two master nodes are
unconnected, changes are made and new primary-key values are generated
at both sites.
 - How will the upcoming Slony-II act by time the master nodes are
reconnected ? How about conflict resolution in the case of newly
introduced, equal PK-values at each (unconneted) site ? Is there any
design proposal ?

I'd like to appreciate any response to my open questions.

Thank you very much,
 Martin
-----------------------------------------
IABG mbH
Sitz der Gesellschaft: Ottobrunn, Registergericht: Amtsgericht Muenchen, HRB 5499
Geschaeftsfuehrung: Prof. Dr.-Ing. Rudolf F. Schwarz
Vorsitzender des Aufsichtsrats: General a. D. Wolfgang Altenburg


From wmoran at potentialtech.com  Wed Sep 30 05:15:28 2009
From: wmoran at potentialtech.com (Bill Moran)
Date: Wed Sep 30 05:15:34 2009
Subject: [Slony1-general] Applicability of Slony in WANs and Satellite
	Networks
In-Reply-To: <BBDE6D332D319548BAAAC70344517B6206DCBA83@exchange03.iabg.de>
References: <BBDE6D332D319548BAAAC70344517B6206DCBA83@exchange03.iabg.de>
Message-ID: <20090930081528.974d4fab.wmoran@potentialtech.com>

In response to Steeg Martin Dr. <Steeg@iabg.de>:

> Hi there,
> I'm already applying Slony-I in a small single-master / single-slave
> application test environment (non-commercial) and I have some question
> considering the applicability of Slony-I / Slony-II in some more general
> situations.
> 
> These questions are:
> 
> Ability to provide WAN replication.

Much of this is covered in the docs:
http://slony.info/documentation/slonyintro.html#INTRODUCTION

>  - Is it possible to use Slony-I in wide-are network (WAN) enviroment ?
> The bandwidth on the master-master link is 34 Mbit/sec, on the
> master-slave link 10 MBit/sec. Both sides will have a latency of 30msec.

Yes.  We use it extensively in a WAN environment that frequently has
worse latency/bandwidth than that.

> Robustness concerning network drop out.
>  - Can (slave) nodes which are not continuosly connected to network be
> used ?

Yes, but with caveats.  If the network goes down for a short while once
a month or so, you'll be fine.  If the network is constantly dropping
out (like for hours at a time on a daily basis) Slony probably won't
work for you.

> Satellite Connection. This scenario considers the replication between
> master and slave nodes communicating over satellite.
>  - Is Slony (-I, maybe the sucessor -II) capable for Replication over
> satellite network (latency 850msec, bandwith 2 MBit/sec) ?

Where are you seeing anything about Slony-II?  AFAIK, that project has
been dead for years.

Whether or not Slony can handle your satellite network is a factor of
whether the frequency of your data change exceeds the bandwidth of your
connection.  2mb/sec is pretty tight, so I would say "no" if you've got
a DB that's being constantly updated, but "yes" if it's largely a read
only database that sees only occasional updates.

Of course, that's a sliding scale.  You'll have to test your particular
case to see if it will work under those constraints.

> Data consistency. In this scenario, the two master nodes are
> unconnected, changes are made and new primary-key values are generated
> at both sites.
>  - How will the upcoming Slony-II act by time the master nodes are
> reconnected ? How about conflict resolution in the case of newly
> introduced, equal PK-values at each (unconneted) site ? Is there any
> design proposal ?

Again, I don't think the Slony-II project is viable any more.  A google
search doesn't turn up anything about it any more.

Slony-I is not capable of multi-master replication.

-- 
Bill Moran
http://www.potentialtech.com
http://people.collaborativefusion.com/~wmoran/
From wmoran at potentialtech.com  Wed Sep 30 05:29:16 2009
From: wmoran at potentialtech.com (Bill Moran)
Date: Wed Sep 30 05:29:20 2009
Subject: [Slony1-general] Multiple Databases
In-Reply-To: <SNT101-W39A100DD781AEEDFDA3215BDD40@phx.gbl>
References: <SNT101-W39A100DD781AEEDFDA3215BDD40@phx.gbl>
Message-ID: <20090930082916.63234070.wmoran@potentialtech.com>

In response to Lawrence Wong <lawrencew00@hotmail.com>:

> 
> Hi,
> I am trying to replicate two databases in a simple 1 master, 1 slave configuration.  I was successful with one database but unsuccessful with 2.  I was wondering if anyone could point out my mistake in my slonik script trying to replicate 2 databases.  I have read that a simple way to go around is to have the tables in the same schema but I would like to keep the databases separate.  Here is my slonik script:

Wow.  Hotmail really mangled your email.  Just ran everything together.

Looking at the config, I don't see anything obviously wrong.  Here are
some guesses as to what might be causing you trouble and/or how to
debug further
1) Did you start 2 slon processes on each server?  You'll need a separate
   slon for each cluster.
2) Perhaps try splitting that config in two and submitting separately.  I
   don't know that submitting it all at once _won't_ work, but I've never
   tried it.
3) What's in slony's log files?

If I were a betting man, I'd put money that #1 is the problem.

I assume this is what the config is supposed to look like:

cluster name = slony_dbOne;
node 1 admin conninfo='host=192.168.112.4 dbname=db user=postgres password=postgres';
node 2 admin conninfo='host=192.168.112.5 dbname=db user=postgres password=postgres';
init cluster (id=1,comment='node 1 dbOne');
#CREATE SET
create set (id = 1, origin = 1, comment = 'tables');
#SET ADD TABLE
set add table (set id=1, origin=1, id=1, fully qualified name = 'schema1.table2',comment='');
set add table (set id=1, origin=1, id=2, fully qualified name = 'schema1.table2',comment='');
#STORE NODE
store node ( id = 2, comment = 'node 2 dbOne' );
#STORE PATH
store path ( server = 1, client = 2,	conninfo = 'dbname=db host=192.168.112.4 user=postgres password=postgres');
store path ( server = 2, client = 1,	conninfo = 'dbname=db host=192.168.112.5 user=postgres password=postgres');
#STORE LISTEN
store listen ( origin = 1, provider = 1, receiver = 2 );
store listen ( origin = 2, provider = 2, receiver = 1 );

cluster name = slony_dbTwo;
node 3 admin conninfo='host=192.168.112.4 dbname=dbTwo user=postgres password=postgres';
node 4 admin conninfo='host=192.168.112.5 dbname=dbTwo user=postgres password=postgres';
init cluster (id=3,comment='node 1 dbTwo');
#CREATE SET
create set (id = 2, origin = 3, comment = 'tables');
#SET ADD TABLE
set add sequence (set id=2, origin=3, id=1, fully qualified name = 'schema1.table1', comment='');
#STORE NODE
store node ( id = 4, comment = 'node 2 dbTwo' );
#STORE PATH
store path ( server = 3, client = 4, conninfo = 'dbname=dbTwo host=192.168.112.4 user=postgres password=postgres');
store path ( server = 4, client = 3, conninfo = 'dbname=dbTwo host=192.168.112.5 user=postgres password=postgres');
#STORE LISTEN
store listen ( origin = 3, provider = 3, receiver = 4 );
store listen ( origin = 4, provider = 4, receiver = 3 );

-- 
Bill Moran
http://www.potentialtech.com
http://people.collaborativefusion.com/~wmoran/
From wmoran at potentialtech.com  Wed Sep 30 06:02:18 2009
From: wmoran at potentialtech.com (Bill Moran)
Date: Wed Sep 30 06:02:27 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25675248.post@talk.nabble.com>
References: <25609853.post@talk.nabble.com> <4ABCCAD2.4030002@Yahoo.com>
	<25641479.post@talk.nabble.com> <20090928140312.GA536@shinkuro.com>
	<25660373.post@talk.nabble.com>
	<87hbul8kfz.fsf@dba2.int.libertyrms.com>
	<25673843.post@talk.nabble.com> <25674187.post@talk.nabble.com>
	<25674659.post@talk.nabble.com>
	<848700.17019.qm@web53007.mail.re2.yahoo.com>
	<25675248.post@talk.nabble.com>
Message-ID: <20090930090218.d7a118b5.wmoran@potentialtech.com>

In response to roctaiwan <nettreeinc@gmail.com>:
> 
> first, I don't think I had turn on WAL. The #archive_mode parameter has not
> been turned on. Unless it's on naturally by this is how postgresql should
> behave.
> second, how much space is big enough to handle 10M inserts? I have 6.4GB
> available right now. 

6.4G is almost certainly not enough to be replicating databases in the 10s
of millions of rows via Slony.

However, without details of your database, it's hard to say.  If each row
is a single INT field, the answer is much different than if each row is
made up of a dozen TEXT fields, each of which has War and Peace stored in
it.

When Slony replicates your 10M row transaction, it has to copy all the rows
into the table.  If your slave is configured with forwarding, it also copies
it into the sl_log table.  Each of those actions has to be WAL logged as it's
occurring.  This results in your 10M rows existing 4 times on the target
server as the action is occurring.

It's quite possible that the space was freed back up before your GUI window
could show the out of space condition.  Also, you seem to have put your
DB on the / partition, which means an out of space condition is going to
hose things royally.  Assuming your system is one big partition, you're
going to be competing against things like /tmp and /var/log, which are also
frequent culprits to fill up disks.

There's a lot of information we don't have, but I suggest you take a look at
it for your own purposes:
* How much disk space does your DB occupy?  (pg_database_size() is your
  friend here)
* How much do the larger transactions you expect to have require?  You'll
  need to set up some tests to figure it out.
* You need to get some experience watching PostgreSQL growing and shrinking
  the space it uses on disk.  Tools like MRTG can be useful to gaining that
  experience.
* 6G is not lot of disk space.

I'm frequently frustrated by people who argue "I've got plenty of space free,
10G" when it's on a 300G partition.  3% is not a lot of space, no matter how
many G it turns out to be.  If you've got 290G of data, that could trivially
expand to 310G during normal operation.  In any event, a database storing
10s of millions of rows on a partition with only 6G free is a problem, whether
that 6G is 50% free or 5% free.  (and the .4G isn't even worth mentioning)

> melvin6925 wrote:
> > 
> >>Again, mentioned it at very beginning. If I load whole 10M records all at
> >>once, records won't show even for 3 days. 
> > 
> > 
> > And again, as previously mentioned by myself and others. and your own
> > finding in the error log, you do not have enough space for the postgresql
> > wal file to handle 10 million inserts. 
> > 
> >  
> > Melvin Davidson 
> > 
> 
> -- 
> View this message in context: http://www.nabble.com/copying-large-file-won%27t-get-replicate-tp25609853p25675248.html
> Sent from the Slony-I -- General mailing list archive at Nabble.com.
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Bill Moran
http://www.potentialtech.com
http://people.collaborativefusion.com/~wmoran/
From ajs at crankycanuck.ca  Wed Sep 30 06:45:38 2009
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Sep 30 06:45:48 2009
Subject: [Slony1-general] copying large file won't get replicate
In-Reply-To: <25675248.post@talk.nabble.com>
References: <4ABCCAD2.4030002@Yahoo.com> <25641479.post@talk.nabble.com>
	<20090928140312.GA536@shinkuro.com> <25660373.post@talk.nabble.com>
	<87hbul8kfz.fsf@dba2.int.libertyrms.com>
	<25673843.post@talk.nabble.com> <25674187.post@talk.nabble.com>
	<25674659.post@talk.nabble.com>
	<848700.17019.qm@web53007.mail.re2.yahoo.com>
	<25675248.post@talk.nabble.com>
Message-ID: <20090930134538.GA5839@shinkuro.com>

On Tue, Sep 29, 2009 at 10:39:15PM -0700, roctaiwan wrote:
> 
> first, I don't think I had turn on WAL.

You can't actually turn WAL off.  Your remarks betoken several deep
misapprehensions about the way PostgreSQL works.  Without a basic
administrator's comprehension of the underlying database system, you
are never going to be successful at operating Slony.  On the contrary,
this is all going to be very painful for you (and, quite probably,
will be very unreliable) unless you get a really good grip on how
PostgreSQL works.

While Postgres doesn't demand that someone sit there twiddling the
knobs all the time just to keep it functioning reliably, it is
complicated enough that you need to understand it to operate it
successfully in production environments.  This is triply true of
Slony, which is a terribly complicated system with a great deal of
power and still a relatively primitive user interface.  If you don't
know how Postgres works and therefore can't understand what Slony is
doing, you're going to have a lousy experience of exactly the sort
you're currently enduring.  Therefore, I strongly suggest you get the
Postgres basics under control before trying to tackle replication with
Slony.

> second, how much space is big enough to handle 10M inserts? I have 6.4GB
> available right now. 

As someone already noted, 6G isn't a lot of space for many data sets.
The only real way to answer your question, however, is to calculate
what is in each row.  To do that, we'd need to see some sample data
and the database schema.

A

-- 
Andrew Sullivan
ajs@crankycanuck.ca
From ajs at crankycanuck.ca  Wed Sep 30 06:51:22 2009
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Wed Sep 30 06:51:29 2009
Subject: [Slony1-general] Applicability of Slony in WANs and Satellite
	Networks
In-Reply-To: <20090930081528.974d4fab.wmoran@potentialtech.com>
References: <BBDE6D332D319548BAAAC70344517B6206DCBA83@exchange03.iabg.de>
	<20090930081528.974d4fab.wmoran@potentialtech.com>
Message-ID: <20090930135121.GB5839@shinkuro.com>

On Wed, Sep 30, 2009 at 08:15:28AM -0400, Bill Moran wrote:

> Again, I don't think the Slony-II project is viable any more.  A google
> search doesn't turn up anything about it any more.

Well, at the very least one of the primary sponsors of the work is no
longer doing anything with it (I know because I was the person who had
finally to tell my superiors we were killing the effort), because it
was impossible to make the system work reliably with anything like the
resource consumption needed.  Practically speaking, it looked to me
like the overhead would consume any advantages as soon as you added 3
nodes.  This was not the biggest problem, but it was the one that
convinced me we were wasting Jan's efforts.

A

-- 
Andrew Sullivan
ajs@crankycanuck.ca
From cbbrowne at ca.afilias.info  Wed Sep 30 08:11:10 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Sep 30 08:11:30 2009
Subject: [Slony1-general] temp tables?
In-Reply-To: <20090930.105050.04440797.t-ishii@sraoss.co.jp>
References: <20090930.105050.04440797.t-ishii@sraoss.co.jp>
Message-ID: <4AC3750E.5000101@ca.afilias.info>

Tatsuo Ishii wrote:
> Hi,
>
> In my understanding, Slony-I does not allow to create/modify temp
> tables on slaves. Does anybody know the reason for this?
>   
Temporary tables are localized to a single connection, and are dropped 
upon closing that connection, so they're not sufficiently persistent to 
be of much use as a major participant in replication.

The only case where I'd expect to be creating temp tables on a 
subscriber is if I put a trigger on the subscriber that does some work 
involving a temp table.

I have never actually tried this, but I don't see why the following 
wouldn't work:

- A trigger function is put on a replicated table, and configured to be 
fired on the subscriber.

- This trigger function is smart enough to...
  - Create a temp table, if it's not there
  - Insert tuples into the temp table
  - Presumably, it should be prepared to immediately do whatever 
processing needs to be done on the temporary data, because you don't 
know for sure when the data will disappear

That last bit is the reason why I would expect temporary tables to be 
nearly useless; the table can't be expected to survive past "when this 
present trigger is firing."  It *MIGHT* survive longer, but you can't 
depend on that.  You can be certain that no other connection will be 
accessing the data placed in the temp table.

Thus, the usefulness is so limited that I'd be inclined to call temp 
tables "useless" in this context.
From baoluc at gmail.com  Wed Sep 30 01:14:41 2009
From: baoluc at gmail.com (Airbus380)
Date: Wed Sep 30 09:52:43 2009
Subject: [Slony1-general] Slony-I and network failure.
In-Reply-To: <92869e660909280920l64ed93ecoe7433a17a5ab5729@mail.gmail.com>
References: <25639915.post@talk.nabble.com>
	<92869e660909280920l64ed93ecoe7433a17a5ab5729@mail.gmail.com>
Message-ID: <25676756.post@talk.nabble.com>


Hi all,

Now I have a new problem about slony-i and network failure.

I setup master node in PC1 with NIC1 = 192.168.1.226, NIC2 = 192.168.1.227

NIC1 is Microsoft Loopback Adapter and cannot access internet (no gateway)

NIC2 is Realtek RTL8169/8110 Family Gigabit Ethernet NIC, and I access
internet thought NIC2.

Then I setup slave node in PC2 with NIC1 = 192.168.1.5 (only one nic)

On PC1, I use pgAdminIII connect to 192.168.1.226 and 192.168.1.5.

Next I configure slony-i for db_master and db_slave on master node and slave
node. 
Create table "staff" in two databases.
And  they work well, no problem happen.

I restart PC1 many time and no problem happen too.

Next, I disable NIC2 = 192.168.1.227 and insert some record in db_master.
And wait for minute.

Next, I enable NIC2 = 192.168.1.227, using pgAdmin on PC1 to view table
"staff" on slave node,
but slave node cannot update data from master node.

Then I wait 1 hours later, and I has the same error.

Next I restart slony service but cannot fix this error.

Next I restart computer and cannot fix this error too.

Here is pg log files:
http://www.nabble.com/file/p25676756/postgresql-2009-09-30_000000.log
postgresql-2009-09-30_000000.log 
http://www.nabble.com/file/p25676756/postgresql-2009-09-30_103703.log
postgresql-2009-09-30_103703.log 
http://www.nabble.com/file/p25676756/postgresql-2009-09-30_105055.log
postgresql-2009-09-30_105055.log 
http://www.nabble.com/file/p25676756/postgresql-2009-09-30_135154.log
postgresql-2009-09-30_135154.log 
http://www.nabble.com/file/p25676756/postgresql-2009-09-30_135449.log
postgresql-2009-09-30_135449.log 

Please help me to solve this problem

Thank you very much


Bao Luc
baoluc@gmail.com






Filip Rembia?kowski-3 wrote:
> 
> 2009/9/28 Airbus380 <baoluc@gmail.com>
> 
>>
>> Hi all,
>>
>> I used two databse with slony-i, they work well. db_master1 is a master
>> node, and db_slave1 is a slave.
>>
>> In the both databse, I have a table call staff with following structure:
>>
>> CREATE TABLE staff
>> (
>>  id text NOT NULL,
>>  "name" text,
>>  age integer,
>>  CONSTRAINT staff_pkey PRIMARY KEY (id)
>> )
>> WITH (OIDS=FALSE);
>> ALTER TABLE staff OWNER TO postgres;
>>
>>
> 
>> I test on windows xp, it is ok.
>>
>> Then I created one more databse, its name is db_center.
>>
>> My idea is: when slave have an operation - insert or update or delete
>> then
>> the operated record must be updated to db_center, so i create a trigger
>> function as following:
>>
>> CREATE OR REPLACE FUNCTION process_staff_audit() RETURNS TRIGGER AS
>> $staff_audit$
>>    BEGIN
>>        IF (TG_OP = 'DELETE') THEN
>>            PERFORM dblink_exec('dbname=db_center port=5432
>> host=192.168.1.226 user=user1 password=user1',
>>                                'DELETE FROM staff WHERE id = ''' ||
>> OLD.id
>> || ''';',false);
>>            RETURN OLD;
>>        ELSIF (TG_OP = 'UPDATE') THEN
>>            PERFORM dblink_exec('dbname=db_center port=5432
>> host=192.168.1.226 user=user1 password=user1',
>>                                'UPDATE staff SET "name" = ''' || NEW.name
>> || ''', age = ''' || NEW.age
>> || ''' WHERE  id = ''' || NEW.id || ''';',false);
>>            RETURN NEW;
>>        ELSIF (TG_OP = 'INSERT') THEN
>>            PERFORM dblink_exec('dbname=db_center port=5432
>> host=192.168.1.226 user=user1 password=user1',
>>                                'INSERT INTO staff VALUES( ''' || NEW.id
>> ||''', ''' || NEW.name ||''',
>> ''' || NEW.age ||''');',false);
>>            RETURN NEW;
>>        END IF;
>>        RETURN NULL; -- result is ignored since this is an AFTER trigger
>>    END;
>> $staff_audit$ LANGUAGE plpgsql;
>>
>> CREATE TRIGGER staff_audit
>> AFTER INSERT OR UPDATE OR DELETE ON staff
>>    FOR EACH ROW EXECUTE PROCEDURE process_staff_audit();
>>
>> I tested this trigger function on two database without slony-i, it worked
>> well.
>> But I used this trigger function on db_slave1 (using slony-i), it didn't
>> work.
>>
>>
>>
> Triggers on replicated tables are disabled by Slony-I for slave nodes.
> You would have to enable selected trigger explicitely, with STORE TRIGGER.
> http://www.slony.info/documentation/stmtstoretrigger.html
> 
> 
> 
> -- 
> Filip Rembia?kowski
> JID,mailto:filip.rembialkowski@gmail.com
> http://filip.rembialkowski.net/
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/Slony-I%2C-Slave-node-and-trigger-function-tp25639915p25676756.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From baoluc at gmail.com  Wed Sep 30 01:21:31 2009
From: baoluc at gmail.com (Airbus380)
Date: Wed Sep 30 09:52:43 2009
Subject: [Slony1-general] Slony-I and network failure.
Message-ID: <25676840.post@talk.nabble.com>


Hi all,

Now I have a new problem about slony-i and network failure.

I setup master node in PC1 with NIC1 = 192.168.1.226, NIC2 = 192.168.1.227

NIC1 is Microsoft Loopback Adapter and cannot access internet (no gateway)

NIC2 is Realtek RTL8169/8110 Family Gigabit Ethernet NIC, and I access
internet thought NIC2.

Then I setup slave node in PC2 with NIC1 = 192.168.1.5 (only one nic)

On PC1, I use pgAdminIII connect to 192.168.1.226 and 192.168.1.5.

Next I configure slony-i for db_master and db_slave on master node and slave
node.
Create table "staff" in two databases.
And  they work well, no problem happen.

I restart PC1 many time and no problem happen too.

Next, I disable NIC2 = 192.168.1.227 and insert some record in db_master.
And wait for minute.

Next, I enable NIC2 = 192.168.1.227, using pgAdmin on PC1 to view table
"staff" on slave node,
but slave node cannot update data from master node.

Then I wait 1 hours later, and I has the same error.

Next I restart slony service but cannot fix this error.

Next I restart computer and cannot fix this error too.

Here is pg log files:
http://www.nabble.com/file/p25676840/postgresql-2009-09-30_000000.log
postgresql-2009-09-30_000000.log 
http://www.nabble.com/file/p25676840/postgresql-2009-09-30_103703.log
postgresql-2009-09-30_103703.log 
http://www.nabble.com/file/p25676840/postgresql-2009-09-30_105055.log
postgresql-2009-09-30_105055.log 
http://www.nabble.com/file/p25676840/postgresql-2009-09-30_135154.log
postgresql-2009-09-30_135154.log 
http://www.nabble.com/file/p25676840/postgresql-2009-09-30_135449.log
postgresql-2009-09-30_135449.log 

Please help me to fix this problem

Thank you very much


Bao Luc
baoluc@gmail.com 
-- 
View this message in context: http://www.nabble.com/Slony-I-and-network-failure.-tp25676840p25676840.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From michael at aers.ca  Wed Sep 30 12:00:17 2009
From: michael at aers.ca (michael@aers.ca)
Date: Wed Sep 30 12:00:36 2009
Subject: [Slony1-general] Slony init.d script help
In-Reply-To: <87d4598k7i.fsf@dba2.int.libertyrms.com>
References: <6B5AF6293A289F45826220B17ABE7937011A4991@BORON.aers.local>
	<87d4598k7i.fsf@dba2.int.libertyrms.com>
Message-ID: <6B5AF6293A289F45826220B17ABE7937011A4A22@BORON.aers.local>

Thanks, Christopher, for the feedback. This is indeed a linux system.

For some reason the pid file is getting the pid of the child process.
I'm generating the pid file with the command:

/sbin/pidof -s /usr/bin/slony1-cluster1

This doesn't seem to specifically get the pid of the parent process
though.

-----Original Message-----
From: Christopher Browne [mailto:cbbrowne@ca.afilias.info] 
Sent: Tuesday, September 29, 2009 3:23 PM
To: Michael Holt
Cc: slony1-general@lists.slony.info
Subject: Re: [Slony1-general] Slony init.d script help


I presume this is on Linux?

Linux reports each thread as a separate entry in the process table,
which is why you'd see more processes than you have slons.  Notice
that 16149 is a child of 13894, which is quite consistent with this.

The PID file gets generated by the main thread, and so killing that
should also kill off any child threads.  You might want to check the
contents of the PID file to verify that.

-- 
(reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
Christopher Browne
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From melvin6925 at yahoo.com  Wed Sep 30 12:17:02 2009
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Wed Sep 30 12:17:18 2009
Subject: [Slony1-general] Slony init.d script help
In-Reply-To: <6B5AF6293A289F45826220B17ABE7937011A4A22@BORON.aers.local>
References: <6B5AF6293A289F45826220B17ABE7937011A4991@BORON.aers.local>
	<87d4598k7i.fsf@dba2.int.libertyrms.com>
	<6B5AF6293A289F45826220B17ABE7937011A4A22@BORON.aers.local>
Message-ID: <820915.439.qm@web53007.mail.re2.yahoo.com>

>For some reason the pid file is getting the pid of the child process.
>I'm generating the pid file with the command:
>
>/sbin/pidof -s /usr/bin/slony1-cluster1

 
I always find it easier to just use the -p option when starting slony.
IE:

slon -d1 -pslon_node_1.pid $SLONY_SCHEMA_NAME "dbname=$MASTERDBNAME user=$REPLICATIONUSER host=$MASTERHOST port=$PGPORT" > slon1.log &

That way the pid is always correct. 


Melvin Davidson 


      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090930/ae1ae603/attachment.htm
From bnichols at ca.afilias.info  Wed Sep 30 12:37:15 2009
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Wed Sep 30 12:37:31 2009
Subject: [Slony1-general] Autovacuum vs. Slon triggered vacuum
Message-ID: <1254339435.5138.287.camel@bnicholson-desktop>

What is the current consensus on slon triggered vacuums vs. PG 8.3/8.4
triggered autovacuum?

Is it better to let the slon handle it's own vacuuming, or to disable
the slon's vacuum and let autovacuum sort itself?

-- 
Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.


From lawrenceg at globalitcreations.com  Wed Sep 30 22:44:25 2009
From: lawrenceg at globalitcreations.com (Lawrence Giam)
Date: Thu Oct  1 05:07:30 2009
Subject: [Slony1-general] Failover and Failback
Message-ID: <F42B651E7C095744BA93C7D0D087CCD301D9FD93@gitc-mail01.globalitcreations.com>

Skipped content of type multipart/alternative-------------- next part -----=
---------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 4414 bytes
Desc: image001.jpg
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20091001=
/10845d14/attachment-0001.jpeg
