From stephane.schildknecht at postgresqlfr.org  Wed Jul  1 07:33:49 2009
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-15?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Wed Jul  1 07:33:56 2009
Subject: [Slony1-general] Error settling a replication
Message-ID: <4A4B73CD.2090003@postgresqlfr.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi,

I'm facing a strange error, I never saw yet.

I'm trying to settle a replication between 2 nodes, each of them running PG8.3.7.

Slony is version 2.0.2.

<stdin>:59: PGRES_FATAL_ERROR select "_gvr".setAddTable(1, 1, 'public.mytable',
'mytable_pk', 'table public.mytable');  - ERROR:  function
pg_catalog.quote_literal(integer) is not unique
LINE 1: ... (' || pg_catalog.quote_literal('_gvr') || ',' || pg_catalog...
                                                             ^
HINT:  Could not choose a best candidate function. You might need to add
explicit type casts.
QUERY:  SELECT  'create trigger "_gvr_logtrigger"' || ' after insert or update
or delete on ' ||  $1  || ' for each row execute procedure "_gvr".logTrigger ('
|| pg_catalog.quote_literal('_gvr') || ',' || pg_catalog.quote_literal( $2 ) ||
',' || pg_catalog.quote_literal( $3 ) || ');'
CONTEXT:  PL/pgSQL function "altertableaddtriggers" line 53 at EXECUTE statement
SQL statement "SELECT  "_gvr".alterTableAddTriggers( $1 )"
PL/pgSQL function "setaddtable_int" line 109 at PERFORM
SQL statement "SELECT  "_gvr".setAddTable_int( $1 ,  $2 ,  $3 ,  $4 ,  $5 )"
PL/pgSQL function "setaddtable" line 37 at PERFORM

What could have caused such an issue?

I have two pg_catalog.quote_literal() functions:

 pg_catalog | quote_literal | text                        | anyelement
 pg_catalog | quote_literal | text                        | text

There's one thing I did, btw, it's adding implicit casts as described here:
http://wiki.postgresql.org/images/d/d1/Pg83-implicit-casts.sql

Cheers,
- --
St?phane Schildknecht
PostgreSQLFr - http://www.postgresql.fr
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFKS3PMA+REPKWGI0ERAlDvAKDJOrtidQUUj/4ulGLDMVNCZrokUQCgzgH+
c4MvB6IhxZV/GBw4ck4x4Yc=
=CHuK
-----END PGP SIGNATURE-----
From stephane.schildknecht at postgresqlfr.org  Wed Jul  1 08:04:50 2009
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-15?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Wed Jul  1 08:04:56 2009
Subject: [Slony1-general] Error settling a replication
In-Reply-To: <4A4B73CD.2090003@postgresqlfr.org>
References: <4A4B73CD.2090003@postgresqlfr.org>
Message-ID: <4A4B7B12.7010005@postgresqlfr.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

St?phane A. Schildknecht a ?crit :
(...)

> 
> There's one thing I did, btw, it's adding implicit casts as described here:
> http://wiki.postgresql.org/images/d/d1/Pg83-implicit-casts.sql

Indeed, problem is here. Seems like you can't initiate a replication when using
these implicit casts on an 8.3 PostgreSQL database.

After dropping these casts, I could initialize my replication.

Regards,
SAS
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFKS3sRA+REPKWGI0ERAgneAJ4yUPYFOJ5H3KvlkFvPnyeczm/8+gCfXM+g
f+MThuWpbAH2z1PVlYyAhVU=
=8lSC
-----END PGP SIGNATURE-----
From gordo169 at gmail.com  Wed Jul  1 08:39:43 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Wed Jul  1 08:40:43 2009
Subject: [Slony1-general] Can I have an 8.3 subscriber with an 8.4 provider?
Message-ID: <24292011.post@talk.nabble.com>


I know I can have an 8.4 Postgres subscriber node with an 8.3 master
(actually 8.3.6), as appears to be the optimal upgrade path from 8.3 to 8.4.  
But I wonder if the reverse is possible. 

Thanks,
Gordon
-- 
View this message in context: http://www.nabble.com/Can-I-have-an-8.3-subscriber-with-an-8.4-provider--tp24292011p24292011.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From wmoran at potentialtech.com  Wed Jul  1 09:01:45 2009
From: wmoran at potentialtech.com (Bill Moran)
Date: Wed Jul  1 09:01:55 2009
Subject: [Slony1-general] Can I have an 8.3 subscriber with an 8.4
	provider?
In-Reply-To: <24292011.post@talk.nabble.com>
References: <24292011.post@talk.nabble.com>
Message-ID: <20090701120145.2de7007a.wmoran@potentialtech.com>

In response to Gordon Shannon <gordo169@gmail.com>:

> 
> I know I can have an 8.4 Postgres subscriber node with an 8.3 master
> (actually 8.3.6), as appears to be the optimal upgrade path from 8.3 to 8.4.  
> But I wonder if the reverse is possible. 

As long as you don't use any 8.4-only features in the database, yes.

-- 
Bill Moran
http://www.potentialtech.com
http://people.collaborativefusion.com/~wmoran/
From stephane.schildknecht at postgresqlfr.org  Wed Jul  1 09:49:09 2009
From: stephane.schildknecht at postgresqlfr.org (=?ISO-8859-15?Q?=22St=E9phane_A=2E_Schildknecht=22?=)
Date: Wed Jul  1 09:49:18 2009
Subject: [Slony1-general] Error settling a replication
In-Reply-To: <4A4B7B12.7010005@postgresqlfr.org>
References: <4A4B73CD.2090003@postgresqlfr.org>
	<4A4B7B12.7010005@postgresqlfr.org>
Message-ID: <4A4B9385.5040004@postgresqlfr.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

St?phane A. Schildknecht a ?crit :
> St?phane A. Schildknecht a ?crit :
> (...)
> 
>> There's one thing I did, btw, it's adding implicit casts as described here:
>> http://wiki.postgresql.org/images/d/d1/Pg83-implicit-casts.sql
> 
> Indeed, problem is here. Seems like you can't initiate a replication when using
> these implicit casts on an 8.3 PostgreSQL database.
> 
> After dropping these casts, I could initialize my replication.
> 

The problem then is:
 - if I use implicit casts, I can't replicate
 - if I don't, the application is unusable with PG8.3.

Is there a way I could use these implicit casts *and* replicate data between
the servers ?

Thanks in advance.

Regards,
SAS
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFKS5OFA+REPKWGI0ERAiOKAJ4u+ksfM3nXLh0/iCuw1A4QYtK1WQCeOdC2
pXQa9b4N0GzMqYDgHdC/UCI=
=ELb/
-----END PGP SIGNATURE-----
From dba at richyen.com  Wed Jul  1 15:25:01 2009
From: dba at richyen.com (Richard Yen)
Date: Wed Jul  1 15:25:18 2009
Subject: [Slony1-general] log shipper missing a line in scan.l for version
	1.2.15
Message-ID: <08924397-DE1F-4125-98EA-8BB55A625574@richyen.com>

Hi guys,

I recently ran into an issue with double single-quotes in the  
logshipping .sql files, and I reviewed my patch from bug #49

Looks like the additional "*cp++ = c;" line is missing at lines 175,  
191, 244, and 260

--Richard
From cscetbon.ext at orange-ftgroup.com  Thu Jul  2 05:42:58 2009
From: cscetbon.ext at orange-ftgroup.com (Cyril Scetbon)
Date: Thu Jul  2 05:43:02 2009
Subject: [Slony1-general] Failover hangs in 2.0.2
In-Reply-To: <7108ABFD-9BC6-430F-9045-2A7839301046@richyen.com>
References: <7108ABFD-9BC6-430F-9045-2A7839301046@richyen.com>
Message-ID: <4A4CAB52.4040700@orange-ftgroup.com>

Hi,

nothing more about this issue ?

Richard Yen wrote:
> Hi,
>
> I've been trying to get failover to work in 2.0.2, but it seems to hang.
>
> I have a 3-node architecture, and have tried the instructions, per 
> http://www.slony.info/documentation/failover.html#COMPLEXFAILOVER
>
> Here's how I do it (node 1 is provider, and node 2 is failover node):
>    -- subscribe node 3 to node 2
>    -- execute FAILOVER
>    -- slonik hangs
>
> If I go into node 2 and to and look at sl_subscribe, there is only one 
> row with provider=2, subscriber=3 (which is correct and expected).  
> However, looking at sl_status, looks like everything is running just 
> fine (sl_event_lag and sl_time_lag go up and down, as if there's 
> activity).  HOWEVER, if I do an update on node 2, the update never 
> makes it to node 3.  (Node 1 still says provider=1, subscriber=2 AND 
> provider=2, subscriber=3)
>
> slonik is still running/hanging during all this.
>
> if I strace the slonik process, I find the following:
>
> ======BEGIN STRACE======
> rt_sigprocmask(SIG_BLOCK, [PIPE], [], 8) = 0
> sendto(3, "Q\0\0\0\30begin transaction; \0"..., 25, 0, NULL, 0) = 25
> rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
> poll([{fd=3, events=POLLIN|POLLERR}], 1, -1) = 1 ([{fd=3, 
> revents=POLLIN}])
> recvfrom(3, "C\0\0\0\nBEGIN\0Z\0\0\0\5T"..., 16384, 0, NULL, NULL) = 17
> rt_sigprocmask(SIG_BLOCK, [PIPE], [], 8) = 0
> sendto(3, "Q\0\0\0Wselect nl_backendpid from 
> \"_sltest\".sl_nodelock     where nl_backendpid <> 28927; \0"..., 88, 
> 0, NULL, 0) = 88
> rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
> poll([{fd=3, events=POLLIN|POLLERR}], 1, -1) = 1 ([{fd=3, 
> revents=POLLIN}])
> recvfrom(3, 
> "T\0\0\0&\0\1nl_backendpid\0\304\27Dn\0\3\0\0\0\27\0\4\377\377\377\377\0\0D\0\0\0\17\0\1\0\0\0\00529006D\0\0\0\17\0\1\0\0\0\00529011D\0\0\0\17\0\1\0\0\0\00529012C\0\0\0\vSELECT\0Z\0\0\0\5T"..., 
> 16384, 0, NULL, NULL) = 105
> rt_sigprocmask(SIG_BLOCK, [PIPE], [], 8) = 0
> sendto(3, "Q\0\0\0\32rollback transaction;\0"..., 27, 0, NULL, 0) = 27
> rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
> poll([{fd=3, events=POLLIN|POLLERR}], 1, -1) = 1 ([{fd=3, 
> revents=POLLIN}])
> recvfrom(3, "C\0\0\0\rROLLBACK\0Z\0\0\0\5I"..., 16384, 0, NULL, NULL) 
> = 20
> rt_sigprocmask(SIG_BLOCK, [PIPE], [], 8) = 0
> sendto(4, "Q\0\0\0\30begin transaction; \0"..., 25, 0, NULL, 0) = 25
> rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
> poll([{fd=4, events=POLLIN|POLLERR}], 1, -1) = 1 ([{fd=4, 
> revents=POLLIN}])
> recvfrom(4, "C\0\0\0\nBEGIN\0Z\0\0\0\5T"..., 16384, 0, NULL, NULL) = 17
> rt_sigprocmask(SIG_BLOCK, [PIPE], [], 8) = 0
> sendto(4, "Q\0\0\0Wselect nl_backendpid from 
> \"_sltest\".sl_nodelock     where nl_backendpid <> 16155; \0"..., 88, 
> 0, NULL, 0) = 88
> rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
> poll([{fd=4, events=POLLIN|POLLERR}], 1, -1) = 1 ([{fd=4, 
> revents=POLLIN}])
> recvfrom(4, 
> "T\0\0\0&\0\1nl_backendpid\0\0\1\"\203\0\3\0\0\0\27\0\4\377\377\377\377\0\0D\0\0\0\17\0\1\0\0\0\00517510D\0\0\0\17\0\1\0\0\0\00517511C\0\0\0\vSELECT\0Z\0\0\0\5T"..., 
> 16384, 0, NULL, NULL) = 89
> rt_sigprocmask(SIG_BLOCK, [PIPE], [], 8) = 0
> sendto(4, "Q\0\0\0\32rollback transaction;\0"..., 27, 0, NULL, 0) = 27
> rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
> poll([{fd=4, events=POLLIN|POLLERR}], 1, -1) = 1 ([{fd=4, 
> revents=POLLIN}])
> recvfrom(4, "C\0\0\0\rROLLBACK\0Z\0\0\0\5I"..., 16384, 0, NULL, NULL) 
> = 20
> rt_sigprocmask(SIG_BLOCK, [CHLD], [], 8) = 0
> rt_sigaction(SIGCHLD, NULL, {SIG_DFL, [], 0}, 8) = 0
> rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
> nanosleep({1, 0}, {1, 0})               = 0
> ======END STRACE======
>
> This repeats over and over again in the log (infinite loop?)
>
> I also tried a different time with the script provided by slony-ctl, 
> but no luck. (It DOES, however, work when there's only 2 nodes)
>
> Are there any know issues for 3+ node failover in 2.0.2?
>
> Would anyone be able to walk me through this, if perhaps I'm doing 
> something wrong?
>
> Thanks!
> --Richard
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

-- 
Cyril SCETBON - Ing?nieur bases de donn?es
Cellule bases de donn?es
AUSY pour France T?l?com - OPF/PORTAILS/DOP/HEBEX

T?l : +33 (0)4 97 12 87 60
Jabber : cscetbon@jabber.org
France Telecom - Orange
790 Avenue du Docteur Maurice Donat 
B?timent Marco Polo C1 - Bureau 202
06250 Mougins
France

***********************************
Ce message et toutes les pieces jointes (ci-apres le 'message') sont
confidentiels et etablis a l'intention exclusive de ses destinataires.
Toute utilisation ou diffusion non autorisee est interdite.
Tout message electronique est susceptible d'alteration. Le Groupe France
Telecom decline toute responsabilite au titre de ce message s'il a ete
altere, deforme ou falsifie.
Si vous n'etes pas destinataire de ce message, merci de le detruire
immediatement et d'avertir l'expediteur.
***********************************
This message and any attachments (the 'message') are confidential and
intended solely for the addressees.
Any unauthorised use or dissemination is prohibited.
Messages are susceptible to alteration. France Telecom Group shall not be
liable for the message if altered, changed or falsified.
If you are not recipient of this message, please cancel it immediately and
inform the sender.
************************************

From devrim at gunduz.org  Thu Jul  2 06:06:24 2009
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Thu Jul  2 06:15:13 2009
Subject: [Slony1-general] Setting up cascading slaves
Message-ID: <1246539984.2647.164.camel@hp-laptop2.gunduz.org>

SGksCgpXaGF0IGlzIHRoZSB0cmljayB0byBzZXR1cCBjYXNjYWRpbmcgc2xhdmVzIHdpdGggU2xv
bnktST8gSSdtIHByb2JhYmx5CmJsaW5kLCBzaW5jZSBJIGRpZCBub3Qgc2VlIGFueXRoaW5nIGlu
IHRoZSBkb2N1bWVudC4gV2hhdCBJIHdhbnQgaXM6CgpwZzEgd2lsbCByZXBsaWNhdGUgdG8gcGcy
IGFuZCBwZzIgd2lsbCByZXBsaWNhdGUgdG8gcGczLgogCkkgc2V0dXAgdHdvIGRpZmZlcmVudCBz
bG9uX3Rvb2xzLmNvbmYgZm9yIHRoZXNlIHB1cnBvc2VzLCBpbml0aWFsaXplZApzbG9ueSBjbHVz
dGVycywgc3RhcnRlZCBkYWVtb25zIGZvciBlYWNoIG9mIHRoZW0sIGFuZCBzdWJzY3JpYmVkIHNl
dHMuCgpJIGNvdWxkIHBlcmZvcm0gaW5pdGlhbCBzeW5jIC0tIGhvd2V2ZXIsIHdoZW4gSSBpbnNl
cnQgYSB0ZXN0IGRhdGEgZnJvbQpwZzEsIEkgY2FuIHNlZSBpdCBwZzIgLS0gYnV0IG5vdCBpbiBw
ZzMuIAoKV2hhdCBhbSBJIGRvaW5nIHdyb25nPwoKUmVnYXJkcywKLS0gCkRldnJpbSBHw5xORMOc
WiwgUkhDRQpDb21tYW5kIFByb21wdCAtIGh0dHA6Ly93d3cuQ29tbWFuZFByb21wdC5jb20gCmRl
dnJpbX5ndW5kdXoub3JnLCBkZXZyaW1+UG9zdGdyZVNRTC5vcmcsIGRldnJpbS5ndW5kdXp+bGlu
dXgub3JnLnRyCiAgICAgICAgICAgICAgICAgICBodHRwOi8vd3d3Lmd1bmR1ei5vcmcKLS0tLS0t
LS0tLS0tLS0gbmV4dCBwYXJ0IC0tLS0tLS0tLS0tLS0tCkEgbm9uLXRleHQgYXR0YWNobWVudCB3
YXMgc2NydWJiZWQuLi4KTmFtZTogbm90IGF2YWlsYWJsZQpUeXBlOiBhcHBsaWNhdGlvbi9wZ3At
c2lnbmF0dXJlClNpemU6IDE5NyBieXRlcwpEZXNjOiBUaGlzIGlzIGEgZGlnaXRhbGx5IHNpZ25l
ZCBtZXNzYWdlIHBhcnQKVXJsIDogaHR0cDovL2xpc3RzLnNsb255LmluZm8vcGlwZXJtYWlsL3Ns
b255MS1nZW5lcmFsL2F0dGFjaG1lbnRzLzIwMDkwNzAyLzUzNWVmOWQxL2F0dGFjaG1lbnQucGdw
Cg==
From wmoran at potentialtech.com  Thu Jul  2 06:33:27 2009
From: wmoran at potentialtech.com (Bill Moran)
Date: Thu Jul  2 06:33:33 2009
Subject: [Slony1-general] Setting up cascading slaves
In-Reply-To: <1246539984.2647.164.camel@hp-laptop2.gunduz.org>
References: <1246539984.2647.164.camel@hp-laptop2.gunduz.org>
Message-ID: <20090702093327.a61d78af.wmoran@potentialtech.com>

In response to Devrim G?ND?Z <devrim@gunduz.org>:
> 
> What is the trick to setup cascading slaves with Slony-I? I'm probably
> blind, since I did not see anything in the document. What I want is:
> 
> pg1 will replicate to pg2 and pg2 will replicate to pg3.
>  
> I setup two different slon_tools.conf for these purposes, initialized
> slony clusters, started daemons for each of them, and subscribed sets.
> 
> I could perform initial sync -- however, when I insert a test data from
> pg1, I can see it pg2 -- but not in pg3. 
> 
> What am I doing wrong?

Not providing detailed enough information for anyone to help?

Make sure you have a slon process starting for each of the nodes (usually,
they run _on_ the node, but that's not required).  I.e, you should have
3 slon.conf files and the scripts starting slons up.

2x check what's in [cluster_name].sl_path, [cluster_name].sl_subscribe, and
[cluster_name].sl_status ... those tables may give you hints as to what's
not right.

Check the contents of your slon logs.  They can be pretty cryptic, but
sometimes the output will outright tell you what's wrong.  (Nothing like
a nice "no pg_hba.conf entry for user slony" message to clue one in)

I have this working in 11 different (separate) configurations, so I know
it works and is reliable ;)

-- 
Bill Moran
http://www.potentialtech.com
http://people.collaborativefusion.com/~wmoran/
From devrim at gunduz.org  Thu Jul  2 07:24:35 2009
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Thu Jul  2 07:25:18 2009
Subject: [Slony1-general] Setting up cascading slaves
In-Reply-To: <20090702093327.a61d78af.wmoran@potentialtech.com>
References: <1246539984.2647.164.camel@hp-laptop2.gunduz.org>
	<20090702093327.a61d78af.wmoran@potentialtech.com>
Message-ID: <1246544675.2647.186.camel@hp-laptop2.gunduz.org>

T24gVGh1LCAyMDA5LTA3LTAyIGF0IDA5OjMzIC0wNDAwLCBCaWxsIE1vcmFuIHdyb3RlOgo+IE5v
dCBwcm92aWRpbmcgZGV0YWlsZWQgZW5vdWdoIGluZm9ybWF0aW9uIGZvciBhbnlvbmUgdG8gaGVs
cD8KCk9rLCBoZXJlIGlzIHdoYXQgSSBkaWQ6CgoqIEluaXRpYWxpemVkIDMgY2x1c3RlcnMgb24g
bXkgbGFwdG9wLCBydW5uaW5nIGF0IHBvcnRzIDU0NDEsIDU0NDIgYW5kCjU0NDMuCgoqIENyZWF0
ZWQgcGFnaWxhIGRhdGFiYXNlLCBhbmQgaW5zdGFsbGVkIGl0cyBzY2hlbWEgYW5kIGRhdGEgdG8g
bm9kZTEuCgoqIENvcGllZCBzY2hlbWEgdXNpbmcgcGdfZHVtcCAtcyB0byAybmQgbm9kZSBhbmQg
M3JkIG5vZGUuCgoqIFNldHVwIGZvbGxvd2luZyBjb25mIGZpbGVzOgoKaHR0cDovL3d3dy5ndW5k
dXoub3JnL3Bvc3RncmVzcWwvc2xvbl90b29scy5jb25mLTEtdG8tMgpodHRwOi8vd3d3Lmd1bmR1
ei5vcmcvcG9zdGdyZXNxbC9zbG9uX3Rvb2xzLmNvbmYtMi10by0zCgoqIGFuZCBoZXJlIGlzIHRo
ZSBmdWxsIGxvZyBvZiB3aGF0IEkgZGlkOgoKaHR0cDovL3d3dy5ndW5kdXoub3JnL3Bvc3RncmVz
cWwvc2xvbi1jcy5sb2cKCgpIZXJlIGlzIHRoZSBwcyBvdXRwdXQgZm9yIHNsb25zOgoKcG9zdGdy
ZXMgIDUwMTcgICAgIDEgIDAgMTY6NTkgcHRzLzQgICAgMDA6MDA6MDAgL3Vzci9iaW4vc2xvbiAt
cyAxMDAwIC1kMiByZXBsaWNhdGlvbmZyb21ub2RlMXRvbm9kZTIgaG9zdD1sb2NhbGhvc3QgZGJu
YW1lPXBhZ2lsYSB1c2VyPXBvc3RncmVzIHBvcnQ9NTQ0MQpwb3N0Z3JlcyAgNTAyMSAgNTAxNyAg
MCAxNjo1OSBwdHMvNCAgICAwMDowMDowMCAvdXNyL2Jpbi9zbG9uIC1zIDEwMDAgLWQyIHJlcGxp
Y2F0aW9uZnJvbW5vZGUxdG9ub2RlMiBob3N0PWxvY2FsaG9zdCBkYm5hbWU9cGFnaWxhIHVzZXI9
cG9zdGdyZXMgcG9ydD01NDQxCnBvc3RncmVzICA1MDMxICAgICAxICAwIDE2OjU5IHB0cy80ICAg
IDAwOjAwOjAwIC91c3IvYmluL3BlcmwgL3Vzci9iaW4vc2xvbl93YXRjaGRvZyAtLWNvbmZpZz1z
bG9uX3Rvb2xzLmNvbmYtMS10by0yIG5vZGUxIDMwCnBvc3RncmVzICA1MDYxICAgICAxICAwIDE2
OjU5IHB0cy80ICAgIDAwOjAwOjAwIC91c3IvYmluL3Nsb24gLXMgMTAwMCAtZDIgcmVwbGljYXRp
b25mcm9tbm9kZTF0b25vZGUyIGhvc3Q9bG9jYWxob3N0IGRibmFtZT1wYWdpbGEgdXNlcj1wb3N0
Z3JlcyBwb3J0PTU0NDIKcG9zdGdyZXMgIDUwNjcgIDUwNjEgIDAgMTY6NTkgcHRzLzQgICAgMDA6
MDA6MDAgL3Vzci9iaW4vc2xvbiAtcyAxMDAwIC1kMiByZXBsaWNhdGlvbmZyb21ub2RlMXRvbm9k
ZTIgaG9zdD1sb2NhbGhvc3QgZGJuYW1lPXBhZ2lsYSB1c2VyPXBvc3RncmVzIHBvcnQ9NTQ0Mgpw
b3N0Z3JlcyAgNTA3NSAgICAgMSAgMCAxNjo1OSBwdHMvNCAgICAwMDowMDowMCAvdXNyL2Jpbi9w
ZXJsIC91c3IvYmluL3Nsb25fd2F0Y2hkb2cgLS1jb25maWc9c2xvbl90b29scy5jb25mLTEtdG8t
MiBub2RlMiAzMApwb3N0Z3JlcyAgNTEwNCAgICAgMSAgMCAxNjo1OSBwdHMvNCAgICAwMDowMDow
MCAvdXNyL2Jpbi9zbG9uIC1zIDEwMDAgLWQyIHJlcGxpY2F0aW9uZnJvbW5vZGUxdG9ub2RlMiAK
cG9zdGdyZXMgIDUyNTYgICAgIDEgIDAgMTc6MDAgcHRzLzQgICAgMDA6MDA6MDAgL3Vzci9iaW4v
c2xvbiAtcyAxMDAwIC1kMiByZXBsaWNhdGlvbmZyb21ub2RlMnRvbm9kZTMgaG9zdD1sb2NhbGhv
c3QgZGJuYW1lPXBhZ2lsYSB1c2VyPXBvc3RncmVzIHBvcnQ9NTQ0Mgpwb3N0Z3JlcyAgNTI2MiAg
NTI1NiAgMCAxNzowMCBwdHMvNCAgICAwMDowMDowMCAvdXNyL2Jpbi9zbG9uIC1zIDEwMDAgLWQy
IHJlcGxpY2F0aW9uZnJvbW5vZGUydG9ub2RlMyBob3N0PWxvY2FsaG9zdCBkYm5hbWU9cGFnaWxh
IHVzZXI9cG9zdGdyZXMgcG9ydD01NDQyCnBvc3RncmVzICA1MjcwICAgICAxICAwIDE3OjAwIHB0
cy80ICAgIDAwOjAwOjAwIC91c3IvYmluL3BlcmwgL3Vzci9iaW4vc2xvbl93YXRjaGRvZyAtLWNv
bmZpZz1zbG9uX3Rvb2xzLmNvbmYtMi10by0zIG5vZGUyIDMwCnBvc3RncmVzICA1MzEwICAgICAx
ICAwIDE3OjAwIHB0cy80ICAgIDAwOjAwOjAwIC91c3IvYmluL3Nsb24gLXMgMTAwMCAtZDIgcmVw
bGljYXRpb25mcm9tbm9kZTJ0b25vZGUzIGhvc3Q9bG9jYWxob3N0IGRibmFtZT1wYWdpbGEgdXNl
cj1wb3N0Z3JlcyBwb3J0PTU0NDMKcG9zdGdyZXMgIDUzMTMgIDUzMTAgIDAgMTc6MDAgcHRzLzQg
ICAgMDA6MDA6MDAgL3Vzci9iaW4vc2xvbiAtcyAxMDAwIC1kMiByZXBsaWNhdGlvbmZyb21ub2Rl
MnRvbm9kZTMgaG9zdD1sb2NhbGhvc3QgZGJuYW1lPXBhZ2lsYSB1c2VyPXBvc3RncmVzIHBvcnQ9
NTQ0Mwpwb3N0Z3JlcyAgNTMyNCAgICAgMSAgMCAxNzowMCBwdHMvNCAgICAwMDowMDowMCAvdXNy
L2Jpbi9wZXJsIC91c3IvYmluL3Nsb25fd2F0Y2hkb2cgLS1jb25maWc9c2xvbl90b29scy5jb25m
LTItdG8tMyBub2RlMyAzMApwb3N0Z3JlcyAgNjMyMSAgNTEwNCAgMCAxNzowOSBwdHMvNCAgICAw
MDowMDowMCAvdXNyL2Jpbi9zbG9uIC1zIDEwMDAgLWQyIHJlcGxpY2F0aW9uZnJvbW5vZGUxdG9u
b2RlMiAKCgpBZnRlciBmaW5hbCBjb21tYW5kLCBpbml0aWFsIGRhdGEgd2FzIHJlcGxpY2F0ZWQg
dG8gM3JkIG5vZGUuCgoKTm93LCBJIGlzc3VlIGFuIElOU0VSVCBzdGF0ZW1lbnQ6CgpJTlNFUlQg
SU5UTyBsYW5ndWFnZSBWQUxVRVMgKERFRkFVTFQsJ1R1cmtpc2gnLG5vdygpKTsKCkl0IHdhcyBy
ZXBsaWNhdGVkIHRvIG5vZGUyLCBidXQgbm90IHRvIG5vZGUgMy4uLgoKVGhlIGZvbGxvd2luZyBh
cmUgZnJvbSBub2RlIDI6CgpwYWdpbGE9IyBTRUxFQ1QgKiBmcm9tIF9yZXBsaWNhdGlvbmZyb21u
b2RlMnRvbm9kZTMuc2xfcGF0aCA7CiBwYV9zZXJ2ZXIgfCBwYV9jbGllbnQgfCAgICAgICAgICAg
ICAgICAgICAgIHBhX2Nvbm5pbmZvICAgICAgICAgICAgICAgICAgICAgIHwgcGFfY29ubnJldHJ5
IAotLS0tLS0tLS0tLSstLS0tLS0tLS0tLSstLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0rLS0tLS0tLS0tLS0tLS0KICAgICAgICAgMyB8ICAgICAg
ICAgMiB8IGhvc3Q9bG9jYWxob3N0IGRibmFtZT1wYWdpbGEgdXNlcj1wb3N0Z3JlcyBwb3J0PTU0
NDMgfCAgICAgICAgICAgMTAKICAgICAgICAgMiB8ICAgICAgICAgMyB8IGhvc3Q9bG9jYWxob3N0
IGRibmFtZT1wYWdpbGEgdXNlcj1wb3N0Z3JlcyBwb3J0PTU0NDIgfCAgICAgICAgICAgMTAKKDIg
cm93cykKCnBhZ2lsYT0jIFNFTEVDVCAqIGZyb20gX3JlcGxpY2F0aW9uZnJvbW5vZGUydG9ub2Rl
My5zbF9zdWJzY3JpYmUgOwogc3ViX3NldCB8IHN1Yl9wcm92aWRlciB8IHN1Yl9yZWNlaXZlciB8
IHN1Yl9mb3J3YXJkIHwgc3ViX2FjdGl2ZSAKLS0tLS0tLS0tKy0tLS0tLS0tLS0tLS0tKy0tLS0t
LS0tLS0tLS0tKy0tLS0tLS0tLS0tLS0rLS0tLS0tLS0tLS0tCiAgICAgICAyIHwgICAgICAgICAg
ICAyIHwgICAgICAgICAgICAzIHwgdCAgICAgICAgICAgfCB0CgpwYWdpbGE9IyBTRUxFQ1QgKiBm
cm9tIF9yZXBsaWNhdGlvbmZyb21ub2RlMnRvbm9kZTMuc2xfc3RhdHVzIDsKIHN0X29yaWdpbiB8
IHN0X3JlY2VpdmVkIHwgc3RfbGFzdF9ldmVudCB8ICAgICAgc3RfbGFzdF9ldmVudF90cyAgICAg
IHwgc3RfbGFzdF9yZWNlaXZlZCB8ICAgIHN0X2xhc3RfcmVjZWl2ZWRfdHMgICAgIHwgc3RfbGFz
dF9yZWNlaXZlZF9ldmVudF90cyAgfCBzdF9sYWdfbnVtX2V2ZW50cyB8ICAgc3RfbGFnX3RpbWUg
ICAKLS0tLS0tLS0tLS0rLS0tLS0tLS0tLS0tLSstLS0tLS0tLS0tLS0tLS0rLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLSstLS0tLS0tLS0tLS0tLS0tLS0rLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLSstLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tKy0tLS0tLS0tLS0tLS0tLS0tLS0r
LS0tLS0tLS0tLS0tLS0tLS0KICAgICAgICAgMiB8ICAgICAgICAgICAzIHwgICAgICAgICAgICA1
MyB8IDIwMDktMDctMDIgMTc6MjA6MzMuMDIwOTk5IHwgICAgICAgICAgICAgICA1MyB8IDIwMDkt
MDctMDIgMTc6MjA6MzYuMjExODQxIHwgMjAwOS0wNy0wMiAxNzoyMDozMy4wMjA5OTkgfCAgICAg
ICAgICAgICAgICAgMCB8IDAwOjAwOjA0LjAwMjQzMgooMSByb3cpCgpUaGUgZm9sbG93aW5nIGlz
IGZyb20gM3JkIG5vZGU6CgpwYWdpbGE9IyBTRUxFQ1QgKiBmcm9tIF9yZXBsaWNhdGlvbmZyb21u
b2RlMnRvbm9kZTMuc2xfc3RhdHVzIDsKIHN0X29yaWdpbiB8IHN0X3JlY2VpdmVkIHwgc3RfbGFz
dF9ldmVudCB8ICAgICAgc3RfbGFzdF9ldmVudF90cyAgICAgIHwgc3RfbGFzdF9yZWNlaXZlZCB8
ICAgIHN0X2xhc3RfcmVjZWl2ZWRfdHMgICAgIHwgc3RfbGFzdF9yZWNlaXZlZF9ldmVudF90cyAg
fCBzdF9sYWdfbnVtX2V2ZW50cyB8ICAgc3RfbGFnX3RpbWUgICAKLS0tLS0tLS0tLS0rLS0tLS0t
LS0tLS0tLSstLS0tLS0tLS0tLS0tLS0rLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLSstLS0t
LS0tLS0tLS0tLS0tLS0rLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLSstLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tKy0tLS0tLS0tLS0tLS0tLS0tLS0rLS0tLS0tLS0tLS0tLS0tLS0KICAg
ICAgICAgMyB8ICAgICAgICAgICAyIHwgICAgICAgICAgICAgMiB8IDIwMDktMDctMDIgMTc6MTU6
MTQuODk3MDA2IHwgICAgICAgICAgICAgICAgMiB8IDIwMDktMDctMDIgMTc6MTU6MTUuNTEwNDQz
IHwgMjAwOS0wNy0wMiAxNzoxNToxNC44OTcwMDYgfCAgICAgICAgICAgICAgICAgMCB8IDAwOjA1
OjU5LjQ2OTM0NQooMSByb3cpCgoKRG8geW91IHdhbnQgbW9yZSBpbmZvPwoKUmVnYXJkcywKLS0g
CkRldnJpbSBHw5xORMOcWiwgUkhDRQpDb21tYW5kIFByb21wdCAtIGh0dHA6Ly93d3cuQ29tbWFu
ZFByb21wdC5jb20gCmRldnJpbX5ndW5kdXoub3JnLCBkZXZyaW1+UG9zdGdyZVNRTC5vcmcsIGRl
dnJpbS5ndW5kdXp+bGludXgub3JnLnRyCiAgICAgICAgICAgICAgICAgICBodHRwOi8vd3d3Lmd1
bmR1ei5vcmcKLS0tLS0tLS0tLS0tLS0gbmV4dCBwYXJ0IC0tLS0tLS0tLS0tLS0tCkEgbm9uLXRl
eHQgYXR0YWNobWVudCB3YXMgc2NydWJiZWQuLi4KTmFtZTogbm90IGF2YWlsYWJsZQpUeXBlOiBh
cHBsaWNhdGlvbi9wZ3Atc2lnbmF0dXJlClNpemU6IDE5NyBieXRlcwpEZXNjOiBUaGlzIGlzIGEg
ZGlnaXRhbGx5IHNpZ25lZCBtZXNzYWdlIHBhcnQKVXJsIDogaHR0cDovL2xpc3RzLnNsb255Lmlu
Zm8vcGlwZXJtYWlsL3Nsb255MS1nZW5lcmFsL2F0dGFjaG1lbnRzLzIwMDkwNzAyLzc3MzRkZDJj
L2F0dGFjaG1lbnQucGdwCg==
From jeff at frostconsultingllc.com  Thu Jul  2 09:57:09 2009
From: jeff at frostconsultingllc.com (Jeff Frost)
Date: Thu Jul  2 09:57:21 2009
Subject: [Slony1-general] Setting up cascading slaves
In-Reply-To: <1246544675.2647.186.camel@hp-laptop2.gunduz.org>
References: <1246539984.2647.164.camel@hp-laptop2.gunduz.org>	<20090702093327.a61d78af.wmoran@potentialtech.com>
	<1246544675.2647.186.camel@hp-laptop2.gunduz.org>
Message-ID: <4A4CE6E5.5010403@frostconsultingllc.com>
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==
From pdoria at netmadeira.com  Fri Jul  3 02:36:44 2009
From: pdoria at netmadeira.com (Pedro Doria Meunier)
Date: Fri Jul  3 02:37:14 2009
Subject: [Slony1-general] EXECUTE SCRIPT and DDL changes
Message-ID: <4A4DD12C.8060504@netmadeira.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi,

Contrary to what's stated in the documentation, execute script
**seems** to propagate DDL changes (new objects) throughout the
replication set.

Using Slony ver.2.0.2

What I did was creating a new table and doing so using the execute
script function.
The table, as well as its sequence, were propagated throughout the set.

Have I fallen into a dark pit :] or is the documentation in need of
revision?

I ask this because everything *seems* ok despite what's stated in the
docs...

TIA,

- --
Pedro Doria Meunier
GSM: +351 96 17 20 188
Skype: pdoriam
 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (GNU/Linux)
Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org

iD8DBQFKTdEj2FH5GXCfxAsRAot6AJ40oDWa3p2o7hm0y75MLJsLzIvokgCfQk/j
94j8kGuNHRfv+dfhrXMEx6E=
=EtGU
-----END PGP SIGNATURE-----

From ajs at crankycanuck.ca  Fri Jul  3 03:57:43 2009
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri Jul  3 03:58:17 2009
Subject: [Slony1-general] EXECUTE SCRIPT and DDL changes
In-Reply-To: <4A4DD12C.8060504@netmadeira.com>
References: <4A4DD12C.8060504@netmadeira.com>
Message-ID: <20090703105743.GA8620@shinkuro.com>

On Fri, Jul 03, 2009 at 10:36:44AM +0100, Pedro Doria Meunier wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> Hi,
> 
> Contrary to what's stated in the documentation, execute script
> **seems** to propagate DDL changes (new objects) throughout the
> replication set.

What part of the documentation gave you the impression that execute
script wouldn't send DDL through the system?  That's the way you put
DDL changes in, in fact.

A

-- 
Andrew Sullivan
ajs@crankycanuck.ca
From pdoria at netmadeira.com  Fri Jul  3 04:06:49 2009
From: pdoria at netmadeira.com (Pedro Doria Meunier)
Date: Fri Jul  3 04:07:19 2009
Subject: [Slony1-general] EXECUTE SCRIPT and DDL changes
In-Reply-To: <20090703105743.GA8620@shinkuro.com>
References: <4A4DD12C.8060504@netmadeira.com>
	<20090703105743.GA8620@shinkuro.com>
Message-ID: <4A4DE649.1@netmadeira.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi Andrew,

This bit: http://www.slony.info/documentation/stmtmergeset.html

BR,

Pedro Doria Meunier
GSM: +351 96 17 20 188
Skype: pdoriam
 



Andrew Sullivan wrote:
> On Fri, Jul 03, 2009 at 10:36:44AM +0100, Pedro Doria Meunier
> wrote:
>> -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA1
>>
>> Hi,
>>
>> Contrary to what's stated in the documentation, execute script
>> **seems** to propagate DDL changes (new objects) throughout the
>> replication set.
>
> What part of the documentation gave you the impression that execute
>  script wouldn't send DDL through the system?  That's the way you
> put DDL changes in, in fact.
>
> A
>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (GNU/Linux)
Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org

iD8DBQFKTeZA2FH5GXCfxAsRAnwrAJ9N4MbJtU9rVIUVEq4GZfZGZ8LXmQCgv9TA
fOaaJ8xTKwqt+3DhZ2TcnF4=
=VfzP
-----END PGP SIGNATURE-----

From ajs at crankycanuck.ca  Fri Jul  3 06:21:08 2009
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri Jul  3 06:21:14 2009
Subject: [Slony1-general] EXECUTE SCRIPT and DDL changes
In-Reply-To: <4A4DE649.1@netmadeira.com>
References: <4A4DD12C.8060504@netmadeira.com>
	<20090703105743.GA8620@shinkuro.com> <4A4DE649.1@netmadeira.com>
Message-ID: <20090703132107.GC8620@shinkuro.com>

On Fri, Jul 03, 2009 at 12:06:49PM +0100, Pedro Doria Meunier wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> Hi Andrew,
> 
> This bit: http://www.slony.info/documentation/stmtmergeset.html

Oh, I see what you mean: you used the _same set_ and added tables to
it.  Hrm.  I see no evidence in the release notes that it changed.
Are you sure it's actually working?  In particular, the reason this is
supposed to fail is because of a bootstrap problem in which you can't
be sure that the tables are available everywhere before data shows up
in the first table.  This can result in bizarre race-condition
failures.  So it's not supposed to be allowed.  But note that I'm out
of touch on the tip of Slony development, since I don't actively use
it anywhere.  Maybe this has been worked around.

A

-- 
Andrew Sullivan
ajs@crankycanuck.ca
From pdoria at netmadeira.com  Fri Jul  3 07:20:46 2009
From: pdoria at netmadeira.com (Pedro Doria Meunier)
Date: Fri Jul  3 07:20:53 2009
Subject: [Slony1-general] EXECUTE SCRIPT and DDL changes
In-Reply-To: <20090703132107.GC8620@shinkuro.com>
References: <4A4DD12C.8060504@netmadeira.com>
	<20090703105743.GA8620@shinkuro.com> <4A4DE649.1@netmadeira.com>
	<20090703132107.GC8620@shinkuro.com>
Message-ID: <4A4E13BE.4040801@netmadeira.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi Andrew,

I made sure that what I was adding to the set was an *empty* table not
being updated by whatever... (yet) :)

BR,

Pedro Doria Meunier
GSM: +351 96 17 20 188
Skype: pdoriam
 



Andrew Sullivan wrote:
> On Fri, Jul 03, 2009 at 12:06:49PM +0100, Pedro Doria Meunier
> wrote:
>> -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA1
>>
>> Hi Andrew,
>>
>> This bit: http://www.slony.info/documentation/stmtmergeset.html
>
> Oh, I see what you mean: you used the _same set_ and added tables
> to it.  Hrm.  I see no evidence in the release notes that it
> changed. Are you sure it's actually working?  In particular, the
> reason this is supposed to fail is because of a bootstrap problem
> in which you can't be sure that the tables are available everywhere
> before data shows up in the first table.  This can result in
> bizarre race-condition failures.  So it's not supposed to be
> allowed.  But note that I'm out of touch on the tip of Slony
> development, since I don't actively use it anywhere.  Maybe this
> has been worked around.
>
> A
>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (GNU/Linux)
Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org

iD8DBQFKThO22FH5GXCfxAsRAk3GAJwLHOknSOV3flY+zA6bnz8kPXLnDgCgjRTZ
3X3Vu6LGrDHAHb6oimqFpec=
=6BJV
-----END PGP SIGNATURE-----

From ajs at crankycanuck.ca  Fri Jul  3 10:24:08 2009
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Fri Jul  3 10:24:20 2009
Subject: [Slony1-general] EXECUTE SCRIPT and DDL changes
In-Reply-To: <4A4E13BE.4040801@netmadeira.com>
References: <4A4DD12C.8060504@netmadeira.com>
	<20090703105743.GA8620@shinkuro.com> <4A4DE649.1@netmadeira.com>
	<20090703132107.GC8620@shinkuro.com>
	<4A4E13BE.4040801@netmadeira.com>
Message-ID: <20090703172407.GM8620@shinkuro.com>

On Fri, Jul 03, 2009 at 03:20:46PM +0100, Pedro Doria Meunier wrote:

> I made sure that what I was adding to the set was an *empty* table not
> being updated by whatever... (yet) :)

Sure, but the point of Slony is to make sure that you can't be wrong,
not to be "pretty sure".  That's why I find this surprising -- it
shouldn't have worked at all.  Are you certain the table is actually
added?  What does the set memebership say?  What happens if you try to
put data in there.  (This _is_ in a test environment, right?)

A


-- 
Andrew Sullivan
ajs@crankycanuck.ca
From pdoria at netmadeira.com  Fri Jul  3 10:30:15 2009
From: pdoria at netmadeira.com (Pedro Doria Meunier)
Date: Fri Jul  3 10:30:25 2009
Subject: [Slony1-general] EXECUTE SCRIPT and DDL changes
In-Reply-To: <20090703172407.GM8620@shinkuro.com>
References: <4A4DD12C.8060504@netmadeira.com>
	<20090703105743.GA8620@shinkuro.com> <4A4DE649.1@netmadeira.com>
	<20090703132107.GC8620@shinkuro.com>
	<4A4E13BE.4040801@netmadeira.com>
	<20090703172407.GM8620@shinkuro.com>
Message-ID: <4A4E4027.1000903@netmadeira.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

The fact is that I've already tested it by inserting a test row.
It got replicated, alrighty ... :]
BR,

Pedro Doria Meunier
GSM: +351 96 17 20 188
Skype: pdoriam
 



Andrew Sullivan wrote:
> On Fri, Jul 03, 2009 at 03:20:46PM +0100, Pedro Doria Meunier
> wrote:
>
>> I made sure that what I was adding to the set was an *empty*
>> table not being updated by whatever... (yet) :)
>
> Sure, but the point of Slony is to make sure that you can't be
> wrong, not to be "pretty sure".  That's why I find this surprising
> -- it shouldn't have worked at all.  Are you certain the table is
> actually added?  What does the set memebership say?  What happens
> if you try to put data in there.  (This _is_ in a test environment,
> right?)
>
> A
>
>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (GNU/Linux)
Comment: Using GnuPG with Fedora - http://enigmail.mozdev.org

iD8DBQFKTkAe2FH5GXCfxAsRAnKKAKCloJAy8PXebZ67fu+LobV44n5e1wCfa1eu
Q2q8xRpoHasIWW0faNwlvUY=
=TkRQ
-----END PGP SIGNATURE-----

From stuart at stuartbishop.net  Sat Jul  4 17:55:47 2009
From: stuart at stuartbishop.net (Stuart Bishop)
Date: Sat Jul  4 17:56:06 2009
Subject: [Slony1-general] EXECUTE SCRIPT and DDL changes
In-Reply-To: <4A4DE649.1@netmadeira.com>
References: <4A4DD12C.8060504@netmadeira.com>
	<20090703105743.GA8620@shinkuro.com> <4A4DE649.1@netmadeira.com>
Message-ID: <6bc73d4c0907041755h7774349fqe8ccf02d805ac5d4@mail.gmail.com>

On Fri, Jul 3, 2009 at 6:06 PM, Pedro Doria
Meunier<pdoria@netmadeira.com> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hi Andrew,
>
> This bit: http://www.slony.info/documentation/stmtmergeset.html

Is this actually the documentation you wanted to cite? It makes no
reference to DDL or execute script at all.

The execute script documentation
(http://www.slony.info/documentation/stmtddlscript.html) description
starts with 'Executes a script containing arbitrary SQL statements on
all nodes' so the documentation seems clear enough.

To 'create a new table' (create a new table on all nodes and add it to
an existing replication set).

 - create the table using EXECUTE SCRIPT or execute manually on all nodes
 - create a new replication set with the same origin as the target
replication set.
 - add the table and any sequences to the new replication set
 - subscribe the new replication set to nodes so the subscription list
is identical to the target replication sets subscription list.
 - wait for sync
 - merge the new replication set into the target replication set using
MERGE SET.

(I wish Slony-I had ADD TABLE so we didn't all have to implement this
logic in our maintenance tools...)


>>> Contrary to what's stated in the documentation, execute script
>>> **seems** to propagate DDL changes (new objects) throughout the
>>> replication set.
>>
>> What part of the documentation gave you the impression that execute
>> ?script wouldn't send DDL through the system? ?That's the way you
>> put DDL changes in, in fact.


-- 
Stuart Bishop <stuart@stuartbishop.net>
http://www.stuartbishop.net/
From dmitry.koterov at gmail.com  Sun Jul  5 06:19:58 2009
From: dmitry.koterov at gmail.com (Dmitry Koterov)
Date: Sun Jul  5 06:20:03 2009
Subject: [Slony1-general] MOVE SET confusion
In-Reply-To: <4A0ADD5F.3070108@ca.afilias.info>
References: <87hbzrbsl9.fsf@dba2.int.libertyrms.com>
	<722298.71196.qm@web23608.mail.ird.yahoo.com>
	<8a1bfe660905130611j4db1157bqf7b0dbf1c6ba81ab@mail.gmail.com>
	<4A0ADD5F.3070108@ca.afilias.info>
Message-ID: <d7df81620907050619x662518d7yf1f51ff762ab1f19@mail.gmail.com>

We had the same problem today: switchover node 5 to node 1 and watch that
all other nodes are not resubscribed: they are still subscribed to 5. So, we
got a cascaded replication instead of the master switchover.

So, could you please update the documentation at
http://slony.info/documentation/failover.html adding the advice to execute
SUBSCRIBE SET after MOVE SET if an user just want to move his master to
another node?

lock set (id =3D 1, origin =3D 1);
wait for event (origin =3D 1, confirmed =3D 2);
move set (id =3D 1, old origin =3D 1, new origin =3D 2);
wait for event (origin =3D 1, confirmed =3D 2);
...
subscribe set (id =3D 1, provider =3D 2, receiver =3D 3, forward =3D yes);
subscribe set (id =3D 1, provider =3D 2, receiver =3D 4, forward =3D yes);
...



On Wed, May 13, 2009 at 6:46 PM, Christopher Browne <
cbbrowne@ca.afilias.info> wrote:

> Laurent Laborde wrote:
>
>> BIG PS : That was my biggest concern. i wasn't 100% that lock set will
>> or will not lock the slave too.
>> it never lock the slaves, so you can keep your slave in production
>> while moving the master :)
>> yay \o/
>>
>>
> The really essential thing about LOCK SET is that it eliminates the abili=
ty
> to enter *new* replication data.  It locks the origin down so that there's
> no extra replication data to "slip in sideways" that could get lost when =
you
> switch the origin.
>
> On subscriber nodes, the tables were already "locked" against updates (see
> the "denyaccess" triggers), so LOCK SET is effectively a no-op on them.
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090705/=
f7bf5097/attachment.html
From aleksander.kmetec at intera.si  Sun Jul  5 12:41:18 2009
From: aleksander.kmetec at intera.si (Aleksander Kmetec)
Date: Sun Jul  5 12:41:32 2009
Subject: [Slony1-general] Some changes don't make it to the subscriber
Message-ID: <4A5101DE.9060202@intera.si>

Hi, everyone.

We're running several Slony clusters with many sets inside each cluster. They were all set up using the exact same 
scripts. Out of all those sets only one is continuously experiencing problems.

Every couple of days some changes don't make it to the slave node. This includes inserts and deletes (possibly updates 
also, but I can't say for sure). There are currently around 1000 changes missing on the slave node. Recreating the set 
or rebuilding the cluster fixes things, but only for a couple of days.

The replication does not hang! New changes for that set are being replicated without any problems. It's just changes 
from a certain time in the past that are missing. Unfortunately I can't determine if they were all made during the same 
transaction or not.

We're using Slony 2.0.2 with PG 8.3.7 on Centos 5.3 64bit. Database encoding is UTF8 and cluster locale is set to C. 
There is nothing in sl_log_1/2 tables. Grepping PG and Slony logs (on both servers) for "ERROR" does not reveal anything.

Does anyone have any suggestions what else I could try to find out what is causing this?

Regards,
Aleksander
From cbbrowne at ca.afilias.info  Mon Jul  6 10:27:00 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Jul  6 10:27:17 2009
Subject: [Slony1-general] Setting up cascading slaves
In-Reply-To: <1246539984.2647.164.camel@hp-laptop2.gunduz.org> (Devrim
	=?iso-8859-1?Q?G=DCND=DCZ's?= message of "Thu,
	02 Jul 2009 16:06:24 +0300")
References: <1246539984.2647.164.camel@hp-laptop2.gunduz.org>
Message-ID: <87my7h3eor.fsf@dba2.int.libertyrms.com>

Devrim G?ND?Z <devrim@gunduz.org> writes:
> What is the trick to setup cascading slaves with Slony-I? I'm probably
> blind, since I did not see anything in the document. What I want is:
>
> pg1 will replicate to pg2 and pg2 will replicate to pg3.
>  
> I setup two different slon_tools.conf for these purposes, initialized
> slony clusters, started daemons for each of them, and subscribed sets.
>
> I could perform initial sync -- however, when I insert a test data from
> pg1, I can see it pg2 -- but not in pg3. 
>
> What am I doing wrong?

You don't need multiple clusters for this, indeed, multiple clusters
will not work, as the trigger handling intended to suppress triggers
on node 2 for the purposes of the first (node 1+2) cluster will
suppress collecting data for the second (node 2+3) cluster.

You set it up as one big cluster, with nodes (say) 1, 2, 3.

- Add all 3 nodes (INIT CLUSTER, STORE NODE)

- Add STORE PATH so the nodes can talk (this could involve 6 paths,
  1->2, 1->3, 2->1, 2->3, 3->1, 3->2)

- Then, for cascaded subscriptions:

   subscribe set (id = 1, provider = 1, receiver = 2, forward=yes);
      (wait for this to complete...)
   subscribe set (id = 1, provider = 2, receiver = 3, forward=no);
-- 
output = ("cbbrowne" "@" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Mon Jul  6 10:59:35 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Jul  6 10:59:47 2009
Subject: [Slony1-general] EXECUTE SCRIPT and DDL changes
In-Reply-To: <6bc73d4c0907041755h7774349fqe8ccf02d805ac5d4@mail.gmail.com>
	(Stuart Bishop's message of "Sun, 5 Jul 2009 07:55:47 +0700")
References: <4A4DD12C.8060504@netmadeira.com>
	<20090703105743.GA8620@shinkuro.com> <4A4DE649.1@netmadeira.com>
	<6bc73d4c0907041755h7774349fqe8ccf02d805ac5d4@mail.gmail.com>
Message-ID: <87iqi53d6g.fsf@dba2.int.libertyrms.com>

Stuart Bishop <stuart@stuartbishop.net> writes:
> (I wish Slony-I had ADD TABLE so we didn't all have to implement this
> logic in our maintenance tools...)

That would certainly be nice to have; the trouble is that there are
substantial complications in the subscription logic surrounding the
issue of copying the data consistently.

I'll note that there is a function, add_empty_table_to_replication(),
added a while back, which you could run as part of EXECUTE SCRIPT,
just after adding the empty table to the schema.

Not quite as good as an "ADD TABLE ... to existing set ...", but not
VASTLY distant :-).

http://www.slony.info/documentation/partitioning.html
-- 
let name="cbbrowne" and tld="ca.afilias.info" in name ^ "@" ^ tld;;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Mon Jul  6 11:37:18 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Jul  6 11:37:31 2009
Subject: [Slony1-general] log shipper missing a line in scan.l for version
	1.2.15
In-Reply-To: <08924397-DE1F-4125-98EA-8BB55A625574@richyen.com> (Richard Yen's
	message of "Wed, 1 Jul 2009 15:25:01 -0700")
References: <08924397-DE1F-4125-98EA-8BB55A625574@richyen.com>
Message-ID: <87d48d3bfl.fsf@dba2.int.libertyrms.com>

Richard Yen <dba@richyen.com> writes:
> Hi guys,
>
> I recently ran into an issue with double single-quotes in the
> logshipping .sql files, and I reviewed my patch from bug #49
>
> Looks like the additional "*cp++ = c;" line is missing at lines 175,
> 191, 244, and 260

The extra "cp++ = c;" lines seems to be there fine in 1.2, 2.0, and
HEAD, and it is in the tarball for 1.2.5.

-----------------------------------------------------------------------
RCS file: /home/cvsd/slony1/slony1-engine/src/slony_logshipper/scan.l,v
Working file: scan.l
head: 1.3
branch:
locks: strict
access list:
symbolic names:
	REL_2_0_2: 1.3
	REL_1_2_16: 1.1.2.2
	REL_2_0_2_RC: 1.3
	REL_1_2_16_RC: 1.1.2.2
	REL_2_0_1: 1.3
	REL_2_0_0: 1.3
	REL_2_0_STABLE: 1.3.0.2
	REL_2_0_0_RC2: 1.3
	REL_1_2_15: 1.1.2.2
	REL_2_0_0_RC1: 1.3
	REL_1_2_14: 1.1.2.2
	REL_1_2_14_RC: 1.1.2.1
	REL_1_2_13: 1.1.2.1
	REL_1_2_12: 1.1.2.1
	REL_1_2_STABLE: 1.1.0.2
keyword substitution: kv
total revisions: 5;	selected revisions: 5
description:
----------------------------
revision 1.3
date: 2008-05-13 17:41:59 -0400;  author: cbbrowne;  state: Exp;  lines: +5 -5
Per bug #49 - reported by Richard Yen <dba@richyen.com>

Seems like double single-quotes and slashes are begin converted to single
single-quotes and slashes when not desired:

ERROR 2008-04-24 16:17:32 > PGRES_FATAL_ERROR: ERROR:  syntax error at or near
"Cahier"
LINE 1: ..._count, compare_to_database) values ('1969048', ''Cahier d'u...
                                                            ^
Query was: insert into "public"."m_object_paper" (id, title, x_firstname,
x_lastname, char_length, word_count, grade, grade_note, overwriteflag,
is_indexed, folder, "assignment", "owner", node, page_count,
compare_to_database) values ('1969048', ''Cahier d'un retour au pays natal' is
prinicpally defined by violence. Discuss', 'Charlotte', 'Byrne', '10937',
'1689', NULL, NULL, 't', 'f', '0', '88981', '445800', '2', NULL,
'1000100000000000000000100000000000101');
WARN  2008-04-24 16:17:32 > waiting for resume

My fix for this was in scan.l:175
+                            *cp++ = c;
                             *cp++ = c;
+                           len += 2;
-                           len++;

Similar changes at lines 191, 244, and 260
----------------------------
revision 1.2
date: 2007-09-08 22:37:05 -0400;  author: wieck;  state: Exp;  lines: +582 -0
Add slony_logshipper to HEAD.

Jan
----------------------------
revision 1.1
date: 2007-09-08 10:21:40 -0400;  author: wieck;  state: dead;
branches:  1.1.2;
file scan.l was initially added on branch REL_1_2_STABLE.
----------------------------
revision 1.1.2.2
date: 2008-05-13 17:40:28 -0400;  author: cbbrowne;  state: Exp;  lines: +5 -5
Per bug #49 - reported by Richard Yen <dba@richyen.com>

Seems like double single-quotes and slashes are begin converted to single
single-quotes and slashes when not desired:

ERROR 2008-04-24 16:17:32 > PGRES_FATAL_ERROR: ERROR:  syntax error at or near
"Cahier"
LINE 1: ..._count, compare_to_database) values ('1969048', ''Cahier d'u...
                                                            ^
Query was: insert into "public"."m_object_paper" (id, title, x_firstname,
x_lastname, char_length, word_count, grade, grade_note, overwriteflag,
is_indexed, folder, "assignment", "owner", node, page_count,
compare_to_database) values ('1969048', ''Cahier d'un retour au pays natal' is
prinicpally defined by violence. Discuss', 'Charlotte', 'Byrne', '10937',
'1689', NULL, NULL, 't', 'f', '0', '88981', '445800', '2', NULL,
'1000100000000000000000100000000000101');
WARN  2008-04-24 16:17:32 > waiting for resume

My fix for this was in scan.l:175
+                            *cp++ = c;
			     *cp++ = c;
+                           len += 2;
-                           len++;

Similar changes at lines 191, 244, and 260
----------------------------
revision 1.1.2.1
date: 2007-09-08 10:21:40 -0400;  author: wieck;  state: Exp;  lines: +580 -0
Add the slony_logshipper.

This is a standalone utility that can be used with the slon -x option
to postprocess slony archive log files. Since it does not require any
changes in the existing slony or slonik functionality, I decided to
the current stable branch to be released with 1.2.12.

Jan
=============================================================================

-- 
(reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From gordo169 at gmail.com  Mon Jul  6 14:49:36 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Mon Jul  6 14:49:55 2009
Subject: [Slony1-general] How mature is 2.0.2 with Postgres 8.4?
Message-ID: <24363428.post@talk.nabble.com>


We are getting some new hardware and I am considering moving from 8.3.7 to
8.4, which requires Slony 2.0.2.  (Linux/Dell/64-bit).  We have a pretty
simple cluster with one origin and 2 receivers. We are somewhat
risk-tolerant and tend to be early adopters.  

However, if anyone has any input on whether upgrading a production system
now is a good or bad idea, I would like to hear it.  I don't have a good
sense as to how stable 2.0.2 with 8.4 is.

Thanks!
-- 
View this message in context: http://www.nabble.com/How-mature-is-2.0.2-with-Postgres-8.4--tp24363428p24363428.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From aleksander.kmetec at intera.si  Mon Jul  6 18:10:54 2009
From: aleksander.kmetec at intera.si (Aleksander Kmetec)
Date: Mon Jul  6 18:11:18 2009
Subject: [Slony1-general] Some changes don't make it to the subscriber
In-Reply-To: <4A5101DE.9060202@intera.si>
References: <4A5101DE.9060202@intera.si>
Message-ID: <4A52A09E.7070609@intera.si>

Hi,

Aleksander Kmetec wrote:
> in the past that are missing. Unfortunately I can't determine if they 
> were all made during the same transaction or not.

I have now performed some additional checks and it turns out that all the changes have indeed most likely been performed 
inside a single transaction by a script which performs daily imports from an external database.

This script can sometimes take hours to complete (it's heavily throttled in order to have minimum impact on performance) 
and it also deletes many rows only to replace them with exact same ones.

Could either of this have caused a whole transaction to disappear from replication?

Regards,
Aleksander
From dba at richyen.com  Mon Jul  6 18:59:38 2009
From: dba at richyen.com (Richard Yen)
Date: Mon Jul  6 19:00:03 2009
Subject: [Slony1-general] log shipper missing a line in scan.l for version
	1.2.15
In-Reply-To: <87d48d3bfl.fsf@dba2.int.libertyrms.com>
References: <08924397-DE1F-4125-98EA-8BB55A625574@richyen.com>
	<87d48d3bfl.fsf@dba2.int.libertyrms.com>
Message-ID: <BFAE1E85-FCE5-46BD-9A1C-A97B5AD44F99@richyen.com>

Odd, if I download and untar from http://www.slony.info/downloads/1.2/source/slony1-1.2.15.tar.bz2

Under the src/slony_logshipper/scan.l file, I find that the extra "*cp+ 
+ = c;" lines are missing.  There's only once occurrence of the  
command at each of the lines I listed, not two.  There should be two  
at lines lines 175, 191, 244, and 260

--Richard



On Jul 6, 2009, at 11:37 AM, Christopher Browne wrote:

> Richard Yen <dba@richyen.com> writes:
>> Hi guys,
>>
>> I recently ran into an issue with double single-quotes in the
>> logshipping .sql files, and I reviewed my patch from bug #49
>>
>> Looks like the additional "*cp++ = c;" line is missing at lines 175,
>> 191, 244, and 260
>
> The extra "cp++ = c;" lines seems to be there fine in 1.2, 2.0, and
> HEAD, and it is in the tarball for 1.2.5.
>
> -----------------------------------------------------------------------
> RCS file: /home/cvsd/slony1/slony1-engine/src/slony_logshipper/ 
> scan.l,v
> Working file: scan.l
> head: 1.3
> branch:
> locks: strict
> access list:
> symbolic names:
> 	REL_2_0_2: 1.3
> 	REL_1_2_16: 1.1.2.2
> 	REL_2_0_2_RC: 1.3
> 	REL_1_2_16_RC: 1.1.2.2
> 	REL_2_0_1: 1.3
> 	REL_2_0_0: 1.3
> 	REL_2_0_STABLE: 1.3.0.2
> 	REL_2_0_0_RC2: 1.3
> 	REL_1_2_15: 1.1.2.2
> 	REL_2_0_0_RC1: 1.3
> 	REL_1_2_14: 1.1.2.2
> 	REL_1_2_14_RC: 1.1.2.1
> 	REL_1_2_13: 1.1.2.1
> 	REL_1_2_12: 1.1.2.1
> 	REL_1_2_STABLE: 1.1.0.2
> keyword substitution: kv
> total revisions: 5;	selected revisions: 5
> description:
> ----------------------------
> revision 1.3
> date: 2008-05-13 17:41:59 -0400;  author: cbbrowne;  state: Exp;   
> lines: +5 -5
> Per bug #49 - reported by Richard Yen <dba@richyen.com>
>
> Seems like double single-quotes and slashes are begin converted to  
> single
> single-quotes and slashes when not desired:
>
> ERROR 2008-04-24 16:17:32 > PGRES_FATAL_ERROR: ERROR:  syntax error  
> at or near
> "Cahier"
> LINE 1: ..._count, compare_to_database) values ('1969048', ''Cahier  
> d'u...
>                                                            ^
> Query was: insert into "public"."m_object_paper" (id, title,  
> x_firstname,
> x_lastname, char_length, word_count, grade, grade_note, overwriteflag,
> is_indexed, folder, "assignment", "owner", node, page_count,
> compare_to_database) values ('1969048', ''Cahier d'un retour au pays  
> natal' is
> prinicpally defined by violence. Discuss', 'Charlotte', 'Byrne',  
> '10937',
> '1689', NULL, NULL, 't', 'f', '0', '88981', '445800', '2', NULL,
> '1000100000000000000000100000000000101');
> WARN  2008-04-24 16:17:32 > waiting for resume
>
> My fix for this was in scan.l:175
> +                            *cp++ = c;
>                             *cp++ = c;
> +                           len += 2;
> -                           len++;
>
> Similar changes at lines 191, 244, and 260
> ----------------------------
> revision 1.2
> date: 2007-09-08 22:37:05 -0400;  author: wieck;  state: Exp;   
> lines: +582 -0
> Add slony_logshipper to HEAD.
>
> Jan
> ----------------------------
> revision 1.1
> date: 2007-09-08 10:21:40 -0400;  author: wieck;  state: dead;
> branches:  1.1.2;
> file scan.l was initially added on branch REL_1_2_STABLE.
> ----------------------------
> revision 1.1.2.2
> date: 2008-05-13 17:40:28 -0400;  author: cbbrowne;  state: Exp;   
> lines: +5 -5
> Per bug #49 - reported by Richard Yen <dba@richyen.com>
>
> Seems like double single-quotes and slashes are begin converted to  
> single
> single-quotes and slashes when not desired:
>
> ERROR 2008-04-24 16:17:32 > PGRES_FATAL_ERROR: ERROR:  syntax error  
> at or near
> "Cahier"
> LINE 1: ..._count, compare_to_database) values ('1969048', ''Cahier  
> d'u...
>                                                            ^
> Query was: insert into "public"."m_object_paper" (id, title,  
> x_firstname,
> x_lastname, char_length, word_count, grade, grade_note, overwriteflag,
> is_indexed, folder, "assignment", "owner", node, page_count,
> compare_to_database) values ('1969048', ''Cahier d'un retour au pays  
> natal' is
> prinicpally defined by violence. Discuss', 'Charlotte', 'Byrne',  
> '10937',
> '1689', NULL, NULL, 't', 'f', '0', '88981', '445800', '2', NULL,
> '1000100000000000000000100000000000101');
> WARN  2008-04-24 16:17:32 > waiting for resume
>
> My fix for this was in scan.l:175
> +                            *cp++ = c;
> 			     *cp++ = c;
> +                           len += 2;
> -                           len++;
>
> Similar changes at lines 191, 244, and 260
> ----------------------------
> revision 1.1.2.1
> date: 2007-09-08 10:21:40 -0400;  author: wieck;  state: Exp;   
> lines: +580 -0
> Add the slony_logshipper.
>
> This is a standalone utility that can be used with the slon -x option
> to postprocess slony archive log files. Since it does not require any
> changes in the existing slony or slonik functionality, I decided to
> the current stable branch to be released with 1.2.12.
>
> Jan
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> ======================================================================
>
> -- 
> (reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
> <http://dba2.int.libertyrms.com/>
> Christopher Browne
> (416) 673-4124 (land)
> "Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
> phasers on the Heffalump, Piglet, meet me in transporter room three"

From bnichols at ca.afilias.info  Tue Jul  7 05:23:48 2009
From: bnichols at ca.afilias.info (Brad Nicholson)
Date: Tue Jul  7 05:23:51 2009
Subject: [Slony1-general] How mature is 2.0.2 with Postgres 8.4?
In-Reply-To: <24363428.post@talk.nabble.com>
References: <24363428.post@talk.nabble.com>
Message-ID: <1246969428.7582.19.camel@bnicholson-desktop>

On Mon, 2009-07-06 at 14:49 -0700, Gordon Shannon wrote:
> We are getting some new hardware and I am considering moving from 8.3.7 to
> 8.4, which requires Slony 2.0.2.  (Linux/Dell/64-bit).  We have a pretty
> simple cluster with one origin and 2 receivers. We are somewhat
> risk-tolerant and tend to be early adopters.  
> 
> However, if anyone has any input on whether upgrading a production system
> now is a good or bad idea, I would like to hear it.  I don't have a good
> sense as to how stable 2.0.2 with 8.4 is.

Slony aside - considering that 8.4 is less than a week old, it's too
early in the 8.4 release to be able to answer this.

-- 
Brad Nicholson  416-673-4106
Database Administrator, Afilias Canada Corp.


From cbbrowne at ca.afilias.info  Tue Jul  7 08:00:32 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul  7 08:00:38 2009
Subject: [Slony1-general] log shipper missing a line in scan.l for version
	1.2.15
In-Reply-To: <BFAE1E85-FCE5-46BD-9A1C-A97B5AD44F99@richyen.com> (Richard Yen's
	message of "Mon, 6 Jul 2009 18:59:38 -0700")
References: <08924397-DE1F-4125-98EA-8BB55A625574@richyen.com>
	<87d48d3bfl.fsf@dba2.int.libertyrms.com>
	<BFAE1E85-FCE5-46BD-9A1C-A97B5AD44F99@richyen.com>
Message-ID: <87tz1o1qsv.fsf@dba2.int.libertyrms.com>

Richard Yen <dba@richyen.com> writes:
> Odd, if I download and untar from http://www.slony.info/downloads/1.2/source/slony1-1.2.15.tar.bz2
>
> Under the src/slony_logshipper/scan.l file, I find that the extra "*cp+
> + = c;" lines are missing.  There's only once occurrence of the
> command at each of the lines I listed, not two.  There should be two
> at lines lines 175, 191, 244, and 260

Oh, *two* of them???

I completely misread that!

I shall resolve that...
-- 
"cbbrowne","@","ca.afilias.info"
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From dba at richyen.com  Wed Jul  8 12:23:50 2009
From: dba at richyen.com (Richard Yen)
Date: Wed Jul  8 12:24:07 2009
Subject: [Slony1-general] log shipper missing a line in scan.l for version
	1.2.15
In-Reply-To: <87tz1o1qsv.fsf@dba2.int.libertyrms.com>
References: <08924397-DE1F-4125-98EA-8BB55A625574@richyen.com>
	<87d48d3bfl.fsf@dba2.int.libertyrms.com>
	<BFAE1E85-FCE5-46BD-9A1C-A97B5AD44F99@richyen.com>
	<87tz1o1qsv.fsf@dba2.int.libertyrms.com>
Message-ID: <7A945686-E68D-4011-B4BB-2635BFE7618D@richyen.com>

Also, seems like the changes listed in the following page don't show  
up in slony_logshipper.c anymore

http://main.slony.info/viewcvs/viewvc.cgi/slony1-engine/src/slony_logshipper/slony_logshipper.c?r1=1.2&r2=1.3&diff_format=h

Could you verify?

--Richard



On Jul 7, 2009, at 8:00 AM, Christopher Browne wrote:

> Richard Yen <dba@richyen.com> writes:
>> Odd, if I download and untar from http://www.slony.info/downloads/1.2/source/slony1-1.2.15.tar.bz2
>>
>> Under the src/slony_logshipper/scan.l file, I find that the extra  
>> "*cp+
>> + = c;" lines are missing.  There's only once occurrence of the
>> command at each of the lines I listed, not two.  There should be two
>> at lines lines 175, 191, 244, and 260
>
> Oh, *two* of them???
>
> I completely misread that!
>
> I shall resolve that...
> -- 
> "cbbrowne","@","ca.afilias.info"
> <http://dba2.int.libertyrms.com/>
> Christopher Browne
> (416) 673-4124 (land)
> "Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
> phasers on the Heffalump, Piglet, meet me in transporter room three"

From cbbrowne at ca.afilias.info  Wed Jul  8 13:37:35 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul  8 13:37:51 2009
Subject: [Slony1-general] log shipper missing a line in scan.l for version
	1.2.15
In-Reply-To: <7A945686-E68D-4011-B4BB-2635BFE7618D@richyen.com> (Richard Yen's
	message of "Wed, 8 Jul 2009 12:23:50 -0700")
References: <08924397-DE1F-4125-98EA-8BB55A625574@richyen.com>
	<87d48d3bfl.fsf@dba2.int.libertyrms.com>
	<BFAE1E85-FCE5-46BD-9A1C-A97B5AD44F99@richyen.com>
	<87tz1o1qsv.fsf@dba2.int.libertyrms.com>
	<7A945686-E68D-4011-B4BB-2635BFE7618D@richyen.com>
Message-ID: <871voq29o0.fsf@dba2.int.libertyrms.com>

Richard Yen <dba@richyen.com> writes:
> Also, seems like the changes listed in the following page don't show
> up in slony_logshipper.c anymore
>
> http://main.slony.info/viewcvs/viewvc.cgi/slony1-engine/src/slony_logshipper/slony_logshipper.c?r1=1.2&r2=1.3&diff_format=h
>
> Could you verify?

Ah...

Version 1.3 (Added patch per Richard Yen to allow log shipper to rescan automatically)
was added only to 2.0/HEAD, not to the 1.2 branch.

Rectified...
-- 
let name="cbbrowne" and tld="ca.afilias.info" in name ^ "@" ^ tld;;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From lawrencew00 at hotmail.com  Wed Jul  8 20:16:57 2009
From: lawrencew00 at hotmail.com (Lawrence Wong)
Date: Wed Jul  8 20:17:25 2009
Subject: [Slony1-general] Slony-I will not replicate more than 1 table
Message-ID: <SNT101-W57D6203BA00F8065D5B39DBD260@phx.gbl>


Hi,

My set up is a simple 1 master and 1 slave set up using Postgres 8.3 with S=
lony-I 1.2.15 on 2 Windows 2003 Servers.  My slonik script looks like:

cluster name =3D slony_db1;
node 1 admin conninfo=3D'host=3D10.5.11.20 dbname=3DDB user=3Dpg password=
=3Dpg';
node 2 admin conninfo=3D'host=3D10.5.11.21 dbname=3DDB user=3Dpg password=
=3Dpg';

init cluster (id=3D1,comment=3D'node 1');

#CREATE SET
create set (id =3D 1, origin =3D 1, comment =3D 'tables');

#SET ADD TABLE
set add sequence (set id=3D1, origin=3D1, id=3D1, fully qualified name =3D =
'sample.seq_1', comment=3D'');
set add sequence (set id=3D1, origin=3D1, id=3D2, fully qualified name =3D =
'sample.seq_2', comment=3D'');
set add table (set id=3D1, origin=3D1, id=3D3, fully qualified name =3D 'sa=
mple.table1',comment=3D'');
set add table (set id=3D1, origin=3D1, id=3D4, fully qualified name =3D 'sa=
mple.table2',comment=3D'');

#STORE NODE
store node ( id =3D 2, comment =3D 'node 2' );

#STORE PATH
store path ( server =3D 1, client =3D 2,
    conninfo =3D 'dbname=3DDB host=3D10.5.11.20 user=3Dpg password=3Dpg');
store path ( server =3D 2, client =3D 1,
    conninfo =3D 'dbname=3DDB host=3D10.5.11.21 user=3Dpg password=3Dpg');

#STORE LISTEN
store listen ( origin =3D 1, provider =3D 1, receiver =3D 2 );
store listen ( origin =3D 2, provider =3D 2, receiver =3D 1 );

#SUBSCRIBE SET
subscribe set (id=3D1, provider=3D1, receiver=3D2, forward=3Dno);

I run the slonik script with no errors.  But it does not replicate.  The ot=
her thing I want to mention is that when I comment one of the lines which a=
dds a table, say 'set add table (set id=3D1, origin=3D1, id=3D4, fully qual=
ified name =3D 'sample.table2',comment=3D'');', the replication works for '=
sample.table1'.  When I uncomment the line with 'sample.table2' and comment=
 out the set add line for 'sample.table1' -- replication works for sample.t=
able2.  =


What I am saying is, when I try to do replication with more then 1 table, S=
lony-I will not replicate changes for me.  But if I have just 1 table, eith=
er table 1 or table 2.  Slony-I replication works.  Can anyone give me any =
hints on what to do?

LW . . . Lawrence

_________________________________________________________________
Stay on top of things, check email from other accounts!
http://go.microsoft.com/?linkid=3D9671355
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090709/=
5d518524/attachment.htm
From melvin6925 at yahoo.com  Wed Jul  8 20:33:57 2009
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Wed Jul  8 20:34:26 2009
Subject: [Slony1-general] Slony-I will not replicate more than 1 table
Message-ID: <866211.17149.qm@web53009.mail.re2.yahoo.com>

Lawrence,

You don't put the subscribe set into the init script. It should be a separa=
te script.

The way I usually do things is to =

1. run the init script (everything you have except subscribe) on the master
2. Start the slon process on the master and slave(s)
3. Run the subscribe script on the master.

Melvin Davidson =

 =

 Folk Alley - All Folk - 24 Hours a day =

www.folkalley.com



--- On Wed, 7/8/09, Lawrence Wong <lawrencew00@hotmail.com> wrote:

From: Lawrence Wong <lawrencew00@hotmail.com>
Subject: [Slony1-general] Slony-I will not replicate more than 1 table
To: slony1-general@lists.slony.info
Date: Wednesday, July 8, 2009, 10:16 PM




#yiv1543896473 .hmmessage P
{
margin:0px;padding:0px;}
#yiv1543896473 {
font-size:10pt;font-family:Verdana;}

Hi,

My set up is a simple 1 master and 1 slave set up using Postgres 8.3 with S=
lony-I 1.2.15 on 2 Windows 2003 Servers.=A0 My slonik script looks like:

cluster name =3D slony_db1;
node 1 admin conninfo=3D'host=3D10.5.11.20 dbname=3DDB user=3Dpg password=
=3Dpg';
node 2 admin conninfo=3D'host=3D10.5.11.21 dbname=3DDB user=3Dpg password=
=3Dpg';

init cluster (id=3D1,comment=3D'node 1');

#CREATE SET
create set (id =3D 1, origin =3D 1, comment =3D 'tables');

#SET ADD TABLE
set add sequence (set id=3D1, origin=3D1, id=3D1, fully qualified name =3D =
'sample.seq_1', comment=3D'');
set add sequence (set id=3D1, origin=3D1, id=3D2, fully qualified name =3D =
'sample.seq_2', comment=3D'');
set add table (set id=3D1, origin=3D1, id=3D3, fully qualified name =3D 'sa=
mple.table1',comment=3D'');
set add table (set id=3D1, origin=3D1, id=3D4, fully qualified name =3D 'sa=
mple.table2',comment=3D'');

#STORE NODE
store node ( id =3D 2, comment =3D 'node 2' );

#STORE PATH
store path ( server =3D 1, client =3D 2,
=A0=A0=A0 conninfo =3D 'dbname=3DDB host=3D10.5.11.20 user=3Dpg password=3D=
pg');
store path ( server =3D 2, client =3D 1,
=A0=A0=A0 conninfo =3D 'dbname=3DDB host=3D10.5.11.21 user=3Dpg password=3D=
pg');

#STORE LISTEN
store listen ( origin =3D 1, provider =3D 1, receiver =3D 2 );
store listen ( origin =3D 2, provider =3D 2, receiver =3D 1 );

#SUBSCRIBE SET
subscribe set (id=3D1, provider=3D1, receiver=3D2, forward=3Dno);

I run the slonik script with no errors.=A0 But it does not replicate.=A0 Th=
e other thing I want to mention is that when I comment one of the lines whi=
ch adds a table, say 'set add table (set id=3D1, origin=3D1, id=3D4, fully =
qualified name =3D 'sample.table2',comment=3D'');', the replication works f=
or 'sample.table1'.=A0 When I uncomment the line with 'sample.table2' and c=
omment out the set add line for 'sample.table1' -- replication works for sa=
mple.table2.=A0 =


What I am saying is, when I try to do replication with more then 1 table, S=
lony-I will not replicate changes for me.=A0 But if I have just 1 table, ei=
ther table 1 or table 2.=A0 Slony-I replication works.=A0 Can anyone give m=
e any hints on what to do?

LW . . . Lawrence

Stay on top of things, check email from other accounts! Check it out. =


-----Inline Attachment Follows-----

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general



      =

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090708/=
929942ba/attachment.htm
From nettreeinc at gmail.com  Thu Jul  9 00:03:59 2009
From: nettreeinc at gmail.com (Net Tree Inc.)
Date: Thu Jul  9 00:04:50 2009
Subject: [Slony1-general] Re: Welcome to the "Slony1-general" mailing list
In-Reply-To: <mailman.0.1247122122.30806.slony1-general@lists.slony.info>
References: <mailman.0.1247122122.30806.slony1-general@lists.slony.info>
Message-ID: <bd9689740907090003y3186549dpe0eee0da5979702c@mail.gmail.com>

Environment:
WinXP w/ service Pack 3
PostgreSQL 8.4
Slony-I 1.2.15

I am having a problem I believe that happen to most of the newbee out there.
I was trying to create a "new object" of Slony cluster using pgAdmin III but
I am able to. message says " Slony-I creation scripts not available"...

is it that something I have to configure first before creating cluster
object ? does anyone can give me a link on how to install and do
clustering for slony cluster on WINDOWS envrionment?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090709/=
05c11b30/attachment.htm
From devrim at gunduz.org  Thu Jul  9 06:46:57 2009
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Thu Jul  9 07:12:21 2009
Subject: [Slony1-general] Setting up cascading slaves
In-Reply-To: <87my7h3eor.fsf@dba2.int.libertyrms.com>
References: <1246539984.2647.164.camel@hp-laptop2.gunduz.org>
	<87my7h3eor.fsf@dba2.int.libertyrms.com>
Message-ID: <1247147217.5440.14.camel@hp-laptop2.gunduz.org>
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From aleksander.kmetec at intera.si  Sun Jul 12 00:39:49 2009
From: aleksander.kmetec at intera.si (Aleksander Kmetec)
Date: Sun Jul 12 00:40:10 2009
Subject: Data loss in cleanupEvent() (was: Re: [Slony1-general] Some changes
	don't make it to the subscriber)
In-Reply-To: <4A52A09E.7070609@intera.si>
References: <4A5101DE.9060202@intera.si> <4A52A09E.7070609@intera.si>
Message-ID: <4A599345.2000100@intera.si>

After trying to find the reason for missing transactions reported in my previous messages, I started examining Slony 
source code and came upon a comment about an unconfirmed suspicion of missing transactions in cleanupEvent(). After that 
I increased the cleanup_interval setting to 6 hours so that cleanupEvent() would keep a lot more data around. This did 
the trick and for a week or so we didn't see a single row go missing.

Today I reduced the cleanup_interval back down to 10 minutes and ran the usual scripts. Rows went missing again. This 
leads me to believe that cleanupEvent() does indeed remove rows that are still needed.

Just like previous times there were no crashes and there's nothing unusual in either Slony or PG logs.

Regards,
Aleksander


Aleksander Kmetec wrote:
> Hi,
> 
> Aleksander Kmetec wrote:
>> in the past that are missing. Unfortunately I can't determine if they 
>> were all made during the same transaction or not.
> 
> I have now performed some additional checks and it turns out that all 
> the changes have indeed most likely been performed inside a single 
> transaction by a script which performs daily imports from an external 
> database.
> 
> This script can sometimes take hours to complete (it's heavily throttled 
> in order to have minimum impact on performance) and it also deletes many 
> rows only to replace them with exact same ones.
> 
> Could either of this have caused a whole transaction to disappear from 
> replication?
> 
> Regards,
> Aleksander
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
From vivek at khera.org  Mon Jul 13 08:02:59 2009
From: vivek at khera.org (Vick Khera)
Date: Mon Jul 13 08:03:07 2009
Subject: Data loss in cleanupEvent() (was: Re: [Slony1-general] Some 
	changes don't make it to the subscriber)
In-Reply-To: <4A599345.2000100@intera.si>
References: <4A5101DE.9060202@intera.si> <4A52A09E.7070609@intera.si>
	<4A599345.2000100@intera.si>
Message-ID: <2968dfd60907130802x4d855928i38c17a6fcc741446@mail.gmail.com>

On Sun, Jul 12, 2009 at 3:39 AM, Aleksander
Kmetec<aleksander.kmetec@intera.si> wrote:
> Today I reduced the cleanup_interval back down to 10 minutes and ran the
> usual scripts. Rows went missing again. This leads me to believe that
> cleanupEvent() does indeed remove rows that are still needed.

How far behind does your replication lag when you observe the lost
rows?  I have alarms go off via nagios when our replication falls
behind by more than 180 seconds (because our customers start to
notice).  Our normal lag is not more than 5 seconds or so, but
occasionally we get up to 10+ minutes when doing maintenance cleanup
jobs.  I don't have evidence of ever losing any data.
From ktm at rice.edu  Mon Jul 13 13:59:19 2009
From: ktm at rice.edu (Kenneth Marshall)
Date: Mon Jul 13 13:59:36 2009
Subject: [Slony1-general] Does Slony version 1.2.x work with postgresql 8.4?
Message-ID: <20090713205919.GA26996@it.is.rice.edu>

Dear Slony community,

In particular, I would like to be able to upgrade an
8.3 databae to 8.4 with minimal downtime. 1.2.14 does
not build with a missing function definition for
SerializableSnapshot. If not, is there a scheme to
upgrade to 8.4 that does not entail the outage of
a full dump/restore cycle?

Regards,
Ken
From peter.geoghegan86 at gmail.com  Mon Jul 13 14:06:17 2009
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Mon Jul 13 14:06:34 2009
Subject: [Slony1-general] Getting Slony 2.0.2 working on windows vista with
	PostgreSQL 8.4.0 - "No admin conninfo provided for node -1"
Message-ID: <db471ace0907131406v58bf57fcl9f993c9533a2a790@mail.gmail.com>

Hello,

I'm attempting to get Slony 2.0.2 working on windows Vista with
PostgreSQL 8.4.0. I'm using Hiroshi Saito's binary package (
http://developer.pgadmin.org/~hiroshi/Slony-I/ ), because there
apparently isn't a package available from a mainstream source, such as
the application stack builder, as with previous releases. This
apparently forces me to write slonik scripts, because PgAdmin 1.10
refuses to acknowledge the presence of the Slony SQL scripts from
Hiroshi's package, and I got away with just using PgAdmin3 in the
past.

After creating the Slony-I service and initialising engines (though
not the service), I hacked away at a slonik script in an effort to
initialise simple replication between one master and one slave of one
set, using Hiroshi's notes and the Slony documentation's "Replicating
your first database" section as a guide. This is what I've come up
with:

CLUSTER NAME = my_cluster;

NODE 1 ADMIN CONNINFO = 'host=127.0.0.1 dbname=master_db user=postgres
password=passwd';
NODE 2 ADMIN CONNINFO = 'host=10.0.0.67 dbname=slave_db user=postgres
password=passwd'';
init cluster ( id=1, comment = 'Master Node');

create set (id=1, origin=1, comment='my set');

set add table (set id=1, origin=1, id=1, fully qualified name =
'public.table_one', comment=.table_one');
# ... snip ...
set add table (set id=1, origin=1, id=16, fully qualified name =
'public.table_sixteen', comment=table_sixteen');


store node (id=2, comment = 'Slave node');

store path (server = 1, client = 2, conninfo='dbname=master_db
host=127.0.0.1 user=postgres password=passwd');
store path (server = 2, client = 1, conninfo='dbname=slave_db
host=10.0.0.67 user=postgres password=passwd');


SUBSCRIBE SET ( ID=1, PROVIDER=1, RECEIVER=2, FORWARD=NO );

Here's the output:

c:\Program Files\PostgreSQL\8.4\bin>slonik ./init_slonik.txt
./init_slonik.txt:29: Error: require EVENT NODE
./init_slonik.txt:29: Error: require No admin conninfo provided for node -1

I've googled the problem, and someone posted the same problem to the
mailing list a few months back. Brad Nicholson responded:

"Several Slonik commands assume the there is a node 1 and and try to send
the event through that.  If you don't have a node 1, the command will
fail.  These commands give you the option to specify a different node to
send the event through (event node).  Specify a node that you do have in
the cluster and it should work."

Chris Browne added:

"Brad's comments describe things for versions 1.0 thru 1.2; in 2.0, for
commands that need an event node, it is no longer optional to specify it.
If you do not indicate an event node, the slonik command will complain as
shown above, and fail, as shown."

Don't I have a node one, because I called init cluster above with id
1? How can the problem be fixed?

Regards,
Peter Geoghegan
From drees76 at gmail.com  Mon Jul 13 14:23:05 2009
From: drees76 at gmail.com (David Rees)
Date: Mon Jul 13 14:23:41 2009
Subject: [Slony1-general] Does Slony version 1.2.x work with postgresql 
	8.4?
In-Reply-To: <20090713205919.GA26996@it.is.rice.edu>
References: <20090713205919.GA26996@it.is.rice.edu>
Message-ID: <72dbd3150907131423g2d7a60d6i1eaf189d686b5165@mail.gmail.com>

On Mon, Jul 13, 2009 at 1:59 PM, Kenneth Marshall<ktm@rice.edu> wrote:
> In particular, I would like to be able to upgrade an
> 8.3 databae to 8.4 with minimal downtime. 1.2.14 does
> not build with a missing function definition for
> SerializableSnapshot. If not, is there a scheme to
> upgrade to 8.4 that does not entail the outage of
> a full dump/restore cycle?

Did you try the latest version (1.2.16) of slony first?

-Dave
From ktm at rice.edu  Mon Jul 13 16:20:13 2009
From: ktm at rice.edu (Kenneth Marshall)
Date: Mon Jul 13 16:20:36 2009
Subject: [Slony1-general] Does Slony version 1.2.x work with postgresql
	8.4?
In-Reply-To: <72dbd3150907131423g2d7a60d6i1eaf189d686b5165@mail.gmail.com>
References: <20090713205919.GA26996@it.is.rice.edu>
	<72dbd3150907131423g2d7a60d6i1eaf189d686b5165@mail.gmail.com>
Message-ID: <20090713232013.GA3392@it.is.rice.edu>

On Mon, Jul 13, 2009 at 02:23:05PM -0700, David Rees wrote:
> On Mon, Jul 13, 2009 at 1:59 PM, Kenneth Marshall<ktm@rice.edu> wrote:
> > In particular, I would like to be able to upgrade an
> > 8.3 databae to 8.4 with minimal downtime. 1.2.14 does
> > not build with a missing function definition for
> > SerializableSnapshot. If not, is there a scheme to
> > upgrade to 8.4 that does not entail the outage of
> > a full dump/restore cycle?
> 
> Did you try the latest version (1.2.16) of slony first?
> 
> -Dave
> 
No, because our version of 8.3.x is compiled against 1.2.14
and if 8.4 will work with that then I do not have to re-build
and test 8.3.x with a newer version of Slony and then handle
the change control process for upgrading to the new version
of Slony on the 8.3 server. Then replicate to 8.4, followed
by change control to make it the master, followed by another
round of rebuilding 8.4 with Slony 2.x, and more change
control to manage the upgrade to 2.x. If 1.2.14 will work
I avoid a whole lot of testing and change management.

Regards,
Ken
From drees76 at gmail.com  Mon Jul 13 16:39:02 2009
From: drees76 at gmail.com (David Rees)
Date: Mon Jul 13 16:39:43 2009
Subject: [Slony1-general] Does Slony version 1.2.x work with postgresql 
	8.4?
In-Reply-To: <20090713232013.GA3392@it.is.rice.edu>
References: <20090713205919.GA26996@it.is.rice.edu>
	<72dbd3150907131423g2d7a60d6i1eaf189d686b5165@mail.gmail.com> 
	<20090713232013.GA3392@it.is.rice.edu>
Message-ID: <72dbd3150907131639t522fed2bp6a97baa6af7188a1@mail.gmail.com>

On Mon, Jul 13, 2009 at 4:20 PM, Kenneth Marshall<ktm@rice.edu> wrote:
> On Mon, Jul 13, 2009 at 02:23:05PM -0700, David Rees wrote:
>> On Mon, Jul 13, 2009 at 1:59 PM, Kenneth Marshall<ktm@rice.edu> wrote:
>> > In particular, I would like to be able to upgrade an
>> > 8.3 databae to 8.4 with minimal downtime. 1.2.14 does
>> > not build with a missing function definition for
>> > SerializableSnapshot. If not, is there a scheme to
>> > upgrade to 8.4 that does not entail the outage of
>> > a full dump/restore cycle?
>>
>> Did you try the latest version (1.2.16) of slony first?
>
> No, because our version of 8.3.x is compiled against 1.2.14
> and if 8.4 will work with that then I do not have to re-build
> and test 8.3.x with a newer version of Slony and then handle
> the change control process for upgrading to the new version
> of Slony on the 8.3 server. Then replicate to 8.4, followed
> by change control to make it the master, followed by another
> round of rebuilding 8.4 with Slony 2.x, and more change
> control to manage the upgrade to 2.x. If 1.2.14 will work
> I avoid a whole lot of testing and change management.

Well, it's quite clear that 1.2.14 won't work without at least
applying some patches to it.

I found this thread regarding 1.2.15 which has the same problem.  The
compilation problem isn't fixed until 1.2.16.

http://www.nabble.com/Compilation-of-Slony-1.2.15-with-PostgreSQL-8.4-beta-1---td23218178.html

-Dave
From peter.geoghegan86 at gmail.com  Tue Jul 14 02:45:50 2009
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Tue Jul 14 02:46:33 2009
Subject: [Slony1-general] Re: Getting Slony 2.0.2 working on windows vista
	with PostgreSQL 8.4.0 - "No admin conninfo provided for node -1"
In-Reply-To: <db471ace0907131406v58bf57fcl9f993c9533a2a790@mail.gmail.com>
References: <db471ace0907131406v58bf57fcl9f993c9533a2a790@mail.gmail.com>
Message-ID: <db471ace0907140245k12f50397x967b304ae9710f93@mail.gmail.com>

I've fixed the problem. I had to change this:

> store node (id=2, comment = 'Slave node');

To this:

store node (id=2, comment = 'Slave node', event node=1);

to explicitly specify an event node. I spotted that there was a notice
on slony.info that slonik scripts ought to explicitly specify an event
node because it is no longer implicit as of 2.0, but it wasn't
immediately clear how to do this.

I also found that I had to transfer the Slony SQL scripts to
C:\MinGW\local\pgsql\share, where Hiroshi Saito's Slony-I 2.0.2
package looked for them.

Replication is now working.

You should consider updating the "Replicating your first database"
page of the docs so others don't repeat my mistake,

Regards,
Peter Geoghegan
From aleksander.kmetec at intera.si  Tue Jul 14 03:20:39 2009
From: aleksander.kmetec at intera.si (Aleksander Kmetec)
Date: Tue Jul 14 03:21:00 2009
Subject: [Slony1-general] Re: Data loss in cleanupEvent()
In-Reply-To: <2968dfd60907130802x4d855928i38c17a6fcc741446@mail.gmail.com>
References: <4A5101DE.9060202@intera.si>
	<4A52A09E.7070609@intera.si>	<4A599345.2000100@intera.si>
	<2968dfd60907130802x4d855928i38c17a6fcc741446@mail.gmail.com>
Message-ID: <4A5C5BF7.2090201@intera.si>


Vick Khera wrote:
> How far behind does your replication lag when you observe the lost
> rows? 

There doesn't seem to be any significant lag. It's just the changes from this several hours long transaction that go 
missing; everything else appears to be replicated almost instantly.

Replication is working fine now with the 6 hour cleanup interval, but I'd still like to know if this is a slony bug or 
if there's something strange going on with our cluster.

Regards,
Aleksander
From testtest at live.dk  Tue Jul 14 05:30:03 2009
From: testtest at live.dk (Henrik HJ)
Date: Tue Jul 14 05:30:10 2009
Subject: [Slony1-general] pgAdmin
Message-ID: <BLU119-W54178700A3A277F0C5722CD230@phx.gbl>

DQogIA0KSGkNCg0KDQpJIHRyeWluZyB0byBtYWtlIGEgY2x1c3RlciB3aXRoIFNsb255MSBieSB1
c2luZyBwZ0FkbWluLg0KDQoNCldoZW4gSSB0cnkgdG8gbWFrZSBhIG5ldyBjbHVzdGVyLiBJIGdl
dCB0aGUgTWVzc2FnZToNClNsb255LUkgY3JlYXRpb24gc2NyaXB0cyBub3QgYXZhaWxhYmxlOyBv
bmx5IGpvaW5pbmcgcG9zc2libGUuDQoNCg0KT24gdGhlIHBnQWRtaW4gZmlsZSDihpIgb3B0aW9u
IG1lbnUgSSBoYXZlIGZpbmQgdGhlIFNsb255LUkgcGF0aCBhbmQgdHJ5IHRvIC91c3IvYmluIGJ1
dCBJIGNhbsK0dCBmaW5kIHRoZSBzY3JpcHRzLg0KDQoNClBsZWFzZSBoZWxwLg0KDQoNCkhlbnJp
aw0KX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19f
X19fX19fX19fX18NCk5laiwgZGV0IGVyIGlra2Ugc3bDpnJ0IGF0IHNhbWxlIGFsbGUgdmVubmVy
bmUgZnJhIEhvdG1haWwsIE15c3BhY2Ugb2cgRmFjZWJvb2sgcMOlIE1lc3Nlbmdlci4gTMOmcyBt
ZXJlIGhlcg0KaHR0cDovL3d3dy5taWNyb3NvZnQuY29tL2Rhbm1hcmsvd2luZG93cy93aW5kb3dz
bGl2ZS9pbXBvcnQtZnJpZW5kcy8KLS0tLS0tLS0tLS0tLS0gbmV4dCBwYXJ0IC0tLS0tLS0tLS0t
LS0tCkFuIEhUTUwgYXR0YWNobWVudCB3YXMgc2NydWJiZWQuLi4KVVJMOiBodHRwOi8vbGlzdHMu
c2xvbnkuaW5mby9waXBlcm1haWwvc2xvbnkxLWdlbmVyYWwvYXR0YWNobWVudHMvMjAwOTA3MTQv
YTc2NGRlNjEvYXR0YWNobWVudC5odG0K
From mark at summersault.com  Tue Jul 14 05:30:28 2009
From: mark at summersault.com (Mark Stosberg)
Date: Tue Jul 14 05:30:45 2009
Subject: [Slony1-general] PATCH: slonik_drop_sequence.pl
Message-ID: <20090714083028.13b9fc95@summersault.com>


I noticed that "slonik_drop_sequence.pl" was missing from the altperl suite, so
I have added it. (Attached). The code needs a reality check and peer review.

It could also be stand to have a brief mention in the documentation, 
as the "slonik_drop_table" command has:

http://www.slony.info/documentation/dropthings.html

    Mark

-- 
 . . . . . . . . . . . . . . . . . . . . . . . . . . . 
   Mark Stosberg            Principal Developer  
   mark@summersault.com     Summersault, LLC     
   765-939-9301 ext 202     database driven websites
 . . . . . http://www.summersault.com/ . . . . . . . .
-------------- next part --------------
A non-text attachment was scrubbed...
Name: slonik_drop_sequence.pl
Type: application/x-perl
Size: 1323 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090714/2a3b8f13/slonik_drop_sequence-0001.bin
From mark at summersault.com  Tue Jul 14 05:48:19 2009
From: mark at summersault.com (Mark Stosberg)
Date: Tue Jul 14 05:48:36 2009
Subject: [Slony1-general] Re: PATCH: slonik_drop_sequence.pl
References: <20090714083028.13b9fc95@summersault.com>
Message-ID: <20090714084819.0c5d727d@summersault.com>

On Tue, 14 Jul 2009 08:30:28 -0400
Mark Stosberg <mark@summersault.com> wrote:

> 
> I noticed that "slonik_drop_sequence.pl" was missing from the altperl suite, so
> I have added it. (Attached). The code needs a reality check and peer review.

Sorry, I was confused. Apparently I wrote and submitted this script 3
years ago, and it has already been released....

> It could also be stand to have a brief mention in the documentation, 
> as the "slonik_drop_table" command has:
> 
> http://www.slony.info/documentation/dropthings.html

...but it could still use some documentation.

    Mark
-- 
 . . . . . . . . . . . . . . . . . . . . . . . . . . . 
   Mark Stosberg            Principal Developer  
   mark@summersault.com     Summersault, LLC     
   765-939-9301 ext 202     database driven websites
 . . . . . http://www.summersault.com/ . . . . . . . .


From vivek at khera.org  Tue Jul 14 07:13:29 2009
From: vivek at khera.org (Vick Khera)
Date: Tue Jul 14 07:13:37 2009
Subject: [Slony1-general] Re: Data loss in cleanupEvent()
In-Reply-To: <4A5C5BF7.2090201@intera.si>
References: <4A5101DE.9060202@intera.si> <4A52A09E.7070609@intera.si>
	<4A599345.2000100@intera.si>
	<2968dfd60907130802x4d855928i38c17a6fcc741446@mail.gmail.com>
	<4A5C5BF7.2090201@intera.si>
Message-ID: <2968dfd60907140713y1912615co4496ad8f7c34a6b9@mail.gmail.com>

On Tue, Jul 14, 2009 at 6:20 AM, Aleksander
Kmetec<aleksander.kmetec@intera.si> wrote:
> Replication is working fine now with the 6 hour cleanup interval, but I'd
> still like to know if this is a slony bug or if there's something strange
> going on with our cluster.

I don't have any multi-hour transactions, but it seems to me that
given slony uses the transactions to its advantage to enter rows into
the change log, that any rows in the change log wouldn't be visible to
any other transaction until the commit, and thus the cleanup action
could not see them to clean them until that time.  So it would really
have to be all or nothing for a transaction.

When that big transaction commits, how long does it take to replicate
that to the other node?  What if you cut your cleanup interval to just
under that amount of time and/or just over that time. Would you still
lose rows?
From lawrencew00 at hotmail.com  Tue Jul 14 07:37:54 2009
From: lawrencew00 at hotmail.com (Lawrence Wong)
Date: Tue Jul 14 07:38:12 2009
Subject: [Slony1-general] =?windows-1256?q?cache_lookup_failed_for_functi?=
 =?windows-1256?q?on_72629=FE?=
Message-ID: <SNT101-W5063CCC09881E083E441A6BD230@phx.gbl>

CkhpLAoKSSBhc2tlZCB0aGlzIHF1ZXN0aW9uIG9uIHRoZSBwb3N0Z3Jlc3FsIGdlbmVyYWwgbWFp
bGluZyBsaXN0IGJ1dCBJIGd1ZXNzIGl0IHdvdWxkIGJlIGJldHRlciB0byBhc2sgaXQgaGVyZS4g
IAoKSSBoYWQgYmVlbiB1c2luZyBteSBkYXRhYmFzZSBmb3IgYSBwcmV0dHkgbG9uZyB0aW1lIG5v
dy4gIEl0CmlzIHBvc3RncmVzIDguMyBvbiBXaW5kb3dzIDIwMDMgU2VydmVyIHdoaWNoIGlzIHJl
cGxpY2F0aW5nIGFsb25nIHdpdGggYW5vdGhlciBzZXJ2ZXIgcnVubmluZyBvbiBwb3N0Z3JlcyA4
LjMgYW5kIFdpbmRvd3MgMjAwMyBTZXJ2ZXIuICBUaGV5IGFyZSBib3RoIHVzaW5nIFNsb255LUkg
MS4yLjE1LiAgVG9kYXkgSSB0cmllZCB0byBtYWtlIHVwZGF0ZXMgYW5kIGluc2VydHMKdG8gYSBm
ZXcgb2YgbXkgdGFibGVzIGFuZCBJIGdvdCB0aGlzIGVycm9yIG1lc3NhZ2U6CgonQW4gZXJyb3Ig
aGFzIG9jY3VyZWQ6IEVSUk9SOiBjYWNoZSBsb29rdXAgZmFpbGVkIGZvciBmdW5jdGlvbiA3MjYy
OScKClRoaXMgaGFwcGVuZWQgb24gYWxsIHRoZSB0YWJsZXMgSSB0cmllZCBpbiBteSBkYXRhYmFz
ZSBbYWJvdXQgNDAgdGFibGVzXS4gIEkKdHJpZWQgc3RvcHBpbmcgdGhlIHNlcnZpY2UgYW5kIHN0
YXJ0aW5nIGl0LiAgSSB0cmllZCByZXN0YXJ0aW5nIHRoZQpzZXJ2ZXIuICBOZWl0aGVyIHdvcmtl
ZC4gSGFzIGFueW9uZSBzZWVuIHNvbWV0aGluZyBsaWtlIHRoaXMgYW5kIGtub3dzCmEgc29sdXRp
b24/ICBJIHdvdWxkIHJlYWxseSBsaWtlIHRvIGF2b2lkIGhhdmluZyB0byByZWluc3RhbGwgUG9z
dGdyZXMKYXMgbXkgdGFibGVzIGFyZSByYXRoZXIgbGFyZ2UgYW5kIGJhY2tpbmcgdXAgYW5kIHJl
c3RvcmluZyB3b3VsZCBiZQp0aW1lIGNvbnN1bWluZy4gIAoKX19fX19fX19fX19fX19fX19fX19f
X19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX18KSW50ZXJuZXQgZXhw
bG9yZXIgOCBsZXRzIHlvdSBicm93c2UgdGhlIHdlYiBmYXN0ZXIuCmh0dHA6Ly9nby5taWNyb3Nv
ZnQuY29tLz9saW5raWQ9OTY1NTU4MgotLS0tLS0tLS0tLS0tLSBuZXh0IHBhcnQgLS0tLS0tLS0t
LS0tLS0KQW4gSFRNTCBhdHRhY2htZW50IHdhcyBzY3J1YmJlZC4uLgpVUkw6IGh0dHA6Ly9saXN0
cy5zbG9ueS5pbmZvL3BpcGVybWFpbC9zbG9ueTEtZ2VuZXJhbC9hdHRhY2htZW50cy8yMDA5MDcx
NC81N2MzOTdmMC9hdHRhY2htZW50Lmh0bQo=
From cbbrowne at ca.afilias.info  Tue Jul 14 10:07:49 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul 14 10:08:04 2009
Subject: [Slony1-general] Does Slony version 1.2.x work with postgresql
	8.4?
In-Reply-To: <20090713205919.GA26996@it.is.rice.edu> (Kenneth Marshall's
	message of "Mon, 13 Jul 2009 15:59:19 -0500")
References: <20090713205919.GA26996@it.is.rice.edu>
Message-ID: <87y6qrxkey.fsf@dba2.int.libertyrms.com>

Kenneth Marshall <ktm@rice.edu> writes:
> In particular, I would like to be able to upgrade an
> 8.3 databae to 8.4 with minimal downtime. 1.2.14 does
> not build with a missing function definition for
> SerializableSnapshot. If not, is there a scheme to
> upgrade to 8.4 that does not entail the outage of
> a full dump/restore cycle?

No, 1.2 does not work with 8.4, and barring having considerably more
time to play with, it doesn't seem like a straightforward backport.

There were substantial changes in 8.4 to the implementation of
pg_listener, to change from on-table storage to an in-memory handling
that would fairly much break the whole listener loop.

Version 2.0 revised that substantially; that doesn't seem like a
backport, though.
-- 
(reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Adding more frammishes to the manual commands seems like gilding the
buggy whip."  -- Tom Lane
From glynastill at yahoo.co.uk  Tue Jul 14 11:16:30 2009
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Tue Jul 14 11:16:44 2009
Subject: [Slony1-general] Does Slony version 1.2.x work with postgresql
	8.4?
Message-ID: <800018.30124.qm@web23603.mail.ird.yahoo.com>


At the moment then.  Am I correct in thinking that those of us on pg 8.3 and slony 1.2 with fairly large databases have the following options to upgrade;

a) dump restore to 8.4 then rereplicate with 2.02
b) use pg_migrator on our origin nodes the rereplicate with 2.02

I was atleast hoping 1.2 would work with 8.4, or better still an upgrade path from 1.2 to 2.0 would surface...


--- On Tue, 14/7/09, Christopher Browne <cbbrowne@ca.afilias.info> wrote:

> From: Christopher Browne <cbbrowne@ca.afilias.info>
> Subject: Re: [Slony1-general] Does Slony version 1.2.x work with postgresql 8.4?
> To: "Kenneth Marshall" <ktm@rice.edu>
> Cc: slony1-general@lists.slony.info
> Date: Tuesday, 14 July, 2009, 6:07 PM
> Kenneth Marshall <ktm@rice.edu>
> writes:
> > In particular, I would like to be able to upgrade an
> > 8.3 databae to 8.4 with minimal downtime. 1.2.14 does
> > not build with a missing function definition for
> > SerializableSnapshot. If not, is there a scheme to
> > upgrade to 8.4 that does not entail the outage of
> > a full dump/restore cycle?
> 
> No, 1.2 does not work with 8.4, and barring having
> considerably more
> time to play with, it doesn't seem like a straightforward
> backport.
> 
> There were substantial changes in 8.4 to the implementation
> of
> pg_listener, to change from on-table storage to an
> in-memory handling
> that would fairly much break the whole listener loop.
> 
> Version 2.0 revised that substantially; that doesn't seem
> like a
> backport, though.
> -- 
> (reverse (concatenate 'string "ofni.sailifa.ac" "@"
> "enworbbc"))
> <http://dba2.int.libertyrms.com/>
> Christopher Browne
> (416) 673-4124 (land)
> "Adding more frammishes to the manual commands seems like
> gilding the
> buggy whip."? -- Tom Lane
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 


      
From ktm at rice.edu  Tue Jul 14 11:24:21 2009
From: ktm at rice.edu (Kenneth Marshall)
Date: Tue Jul 14 11:24:39 2009
Subject: [Slony1-general] Does Slony version 1.2.x work with postgresql
	8.4?
In-Reply-To: <87y6qrxkey.fsf@dba2.int.libertyrms.com>
References: <20090713205919.GA26996@it.is.rice.edu>
	<87y6qrxkey.fsf@dba2.int.libertyrms.com>
Message-ID: <20090714182421.GA14384@it.is.rice.edu>

On Tue, Jul 14, 2009 at 01:07:49PM -0400, Christopher Browne wrote:
> Kenneth Marshall <ktm@rice.edu> writes:
> > In particular, I would like to be able to upgrade an
> > 8.3 databae to 8.4 with minimal downtime. 1.2.14 does
> > not build with a missing function definition for
> > SerializableSnapshot. If not, is there a scheme to
> > upgrade to 8.4 that does not entail the outage of
> > a full dump/restore cycle?
> 
> No, 1.2 does not work with 8.4, and barring having considerably more
> time to play with, it doesn't seem like a straightforward backport.
> 
> There were substantial changes in 8.4 to the implementation of
> pg_listener, to change from on-table storage to an in-memory handling
> that would fairly much break the whole listener loop.
> 
> Version 2.0 revised that substantially; that doesn't seem like a
> backport, though.
> -- 
> (reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
> <http://dba2.int.libertyrms.com/>
> Christopher Browne
> (416) 673-4124 (land)
> "Adding more frammishes to the manual commands seems like gilding the
> buggy whip."  -- Tom Lane
> 
Thank you Chris. I will build 8.4 with Slony1 2.0.2.

Regards,
Ken
From cbbrowne at ca.afilias.info  Tue Jul 14 11:40:04 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul 14 11:40:21 2009
Subject: [Slony1-general] Re: Getting Slony 2.0.2 working on windows vista
	with PostgreSQL 8.4.0 - "No admin conninfo provided for node -1"
In-Reply-To: <db471ace0907140245k12f50397x967b304ae9710f93@mail.gmail.com>
	(Peter Geoghegan's message of "Tue, 14 Jul 2009 10:45:50 +0100")
References: <db471ace0907131406v58bf57fcl9f993c9533a2a790@mail.gmail.com>
	<db471ace0907140245k12f50397x967b304ae9710f93@mail.gmail.com>
Message-ID: <87tz1fxg57.fsf@dba2.int.libertyrms.com>

Peter Geoghegan <peter.geoghegan86@gmail.com> writes:
> You should consider updating the "Replicating your first database"
> page of the docs so others don't repeat my mistake,

That was changed a while back in CVS; I'll see about deploying a copy
of the docs that include this revision.
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Tue Jul 14 12:12:40 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul 14 12:12:53 2009
Subject: [Slony1-general] Re: PATCH: slonik_drop_sequence.pl
In-Reply-To: <20090714084819.0c5d727d@summersault.com> (Mark Stosberg's
	message of "Tue, 14 Jul 2009 08:48:19 -0400")
References: <20090714083028.13b9fc95@summersault.com>
	<20090714084819.0c5d727d@summersault.com>
Message-ID: <87hbxfxemv.fsf@dba2.int.libertyrms.com>

Mark Stosberg <mark@summersault.com> writes:
> On Tue, 14 Jul 2009 08:30:28 -0400
> Mark Stosberg <mark@summersault.com> wrote:
>
>> 
>> I noticed that "slonik_drop_sequence.pl" was missing from the altperl suite, so
>> I have added it. (Attached). The code needs a reality check and peer review.
>
> Sorry, I was confused. Apparently I wrote and submitted this script 3
> years ago, and it has already been released....
>
>> It could also be stand to have a brief mention in the documentation, 
>> as the "slonik_drop_table" command has:
>> 
>> http://www.slony.info/documentation/dropthings.html
>
> ...but it could still use some documentation.

Done...  Soon to be committed...
-- 
output = ("cbbrowne" "@" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From greg at endpoint.com  Tue Jul 14 12:12:46 2009
From: greg at endpoint.com (Greg Sabino Mullane)
Date: Tue Jul 14 12:13:01 2009
Subject: [Slony1-general] Does Slony version 1.2.x work with postgresql
	8.4?
In-Reply-To: <87y6qrxkey.fsf@dba2.int.libertyrms.com>
References: <20090713205919.GA26996@it.is.rice.edu>
	<87y6qrxkey.fsf@dba2.int.libertyrms.com>
Message-ID: <4A5CD8AE.7000502@endpoint.com>

> There were substantial changes in 8.4 to the implementation of
> pg_listener, to change from on-table storage to an in-memory handling
> that would fairly much break the whole listener loop.

Eh? What futuristic branch are you looking at? :) The pg_listener
changes by Andrew did not make it into 8.4: it uses the pg_listener
table same as previous versions. A shame, as myself (and Bucardo) were
really looking forward to using payload messages and not having to cron
constant pg_listener vacuuming!

-- =

Greg Sabino Mullane greg@endpoint.com
End Point Corporation
PGP Key: 0x14964AC8

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 226 bytes
Desc: OpenPGP digital signature
Url : http://lists.slony.info/pipermail/slony1-general/attachments/20090714=
/0fb9982e/signature.pgp
From aramos at uci.cu  Tue Jul 14 12:37:12 2009
From: aramos at uci.cu (Alain Ramos Medina)
Date: Tue Jul 14 12:30:37 2009
Subject: [Slony1-general] Partial replication with Slony
Message-ID: <4A5CDE68.7040500@uci.cu>

HI, I just wanna know if I can replicate portions of tables using Slony 
to simulate something like horizontal and vertical table fragmentation.

thanks in advance
From cbbrowne at ca.afilias.info  Tue Jul 14 12:45:13 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul 14 12:45:31 2009
Subject: [Slony1-general] Does Slony version 1.2.x work with postgresql
	8.4?
In-Reply-To: <4A5CD8AE.7000502@endpoint.com> (Greg Sabino Mullane's message of
	"Tue, 14 Jul 2009 15:12:46 -0400")
References: <20090713205919.GA26996@it.is.rice.edu>
	<87y6qrxkey.fsf@dba2.int.libertyrms.com>
	<4A5CD8AE.7000502@endpoint.com>
Message-ID: <878wirxd4m.fsf@dba2.int.libertyrms.com>

Greg Sabino Mullane <greg@endpoint.com> writes:
>> There were substantial changes in 8.4 to the implementation of
>> pg_listener, to change from on-table storage to an in-memory handling
>> that would fairly much break the whole listener loop.
>
> Eh? What futuristic branch are you looking at? :) The pg_listener
> changes by Andrew did not make it into 8.4: it uses the pg_listener
> table same as previous versions. A shame, as myself (and Bucardo) were
> really looking forward to using payload messages and not having to cron
> constant pg_listener vacuuming!

Oh, that's something of a surprise.

I had taken "must remove use of pg_listener table" as a takeaway from
the 2008 PgCon, because that was to be going into 8.4.

OK, so maybe reworking 1.2.x for 8.4 isn't quite so scary a quantity
of work...
-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@" [name;tld];;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From ajs at crankycanuck.ca  Tue Jul 14 12:57:58 2009
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Tue Jul 14 12:58:15 2009
Subject: [Slony1-general] Partial replication with Slony
In-Reply-To: <4A5CDE68.7040500@uci.cu>
References: <4A5CDE68.7040500@uci.cu>
Message-ID: <20090714195757.GP4628@shinkuro.com>

On Tue, Jul 14, 2009 at 03:37:12PM -0400, Alain Ramos Medina wrote:
> HI, I just wanna know if I can replicate portions of tables using Slony  
> to simulate something like horizontal and vertical table fragmentation.

No.  But you could alter your schema to perform the partitioning and
then replicate that.

A

-- 
Andrew Sullivan
ajs@crankycanuck.ca
From drees76 at gmail.com  Tue Jul 14 14:48:59 2009
From: drees76 at gmail.com (David Rees)
Date: Tue Jul 14 14:49:36 2009
Subject: [Slony1-general] Does Slony version 1.2.x work with postgresql 
	8.4?
In-Reply-To: <87y6qrxkey.fsf@dba2.int.libertyrms.com>
References: <20090713205919.GA26996@it.is.rice.edu>
	<87y6qrxkey.fsf@dba2.int.libertyrms.com>
Message-ID: <72dbd3150907141448we50e233j5c10ea586c885328@mail.gmail.com>

On Tue, Jul 14, 2009 at 10:07 AM, Christopher
Browne<cbbrowne@ca.afilias.info> wrote:
> Kenneth Marshall <ktm@rice.edu> writes:
>> In particular, I would like to be able to upgrade an
>> 8.3 databae to 8.4 with minimal downtime. 1.2.14 does
>> not build with a missing function definition for
>> SerializableSnapshot. If not, is there a scheme to
>> upgrade to 8.4 that does not entail the outage of
>> a full dump/restore cycle?
>
> No, 1.2 does not work with 8.4, and barring having considerably more
> time to play with, it doesn't seem like a straightforward backport.

Chris - can we get the requirements documents for both 1.2.X and 2.0.X
updated to reflect the current status of PostgreSQL support?

>From your 2.0.0 announcement[1] you say that 2.0 only supports
PostgreSQL 8.3+, yet the documentation[2] refers to 7.3.3 as being the
minimum and has a lot of references to versions of PostgreSQL older
than 8.3.

Similarly, if Slony 1.2 does not support PostgreSQL 8.4, it's
documentation should be reflected to say that - I (mistakenly?)
assumed that 1.2.16 supported PostgreSQL 8.4 as the list archives[3]
mentioned that 1.2.16 had some compile time fixes related one of the
8.4 beta releases.

In the 2.0 branch, I would also suggest that RELEASE be renamed to
RELEASE-1.2 and RELEASE-2.0 be renamed to RELEASE.

-Dave

PS - Anyone else have really bad formatting issues on the Slony
website using Firefox with content all squished over to one side?

[1] http://lists.slony.info/pipermail/slony1-general/2008-November/009014.html
[2] http://slony.info/documentation/requirements.html
[3] http://lists.slony.info/pipermail/slony1-general/2009-May/009434.html
From peter.geoghegan86 at gmail.com  Wed Jul 15 01:45:18 2009
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Wed Jul 15 01:45:58 2009
Subject: [Slony1-general] pgAdmin
In-Reply-To: <BLU119-W21367CE2101A31B83B42CDCD200@phx.gbl>
References: <BLU119-W54178700A3A277F0C5722CD230@phx.gbl>
	<db471ace0907140604j8a5a889q16a20012f434ad63@mail.gmail.com>
	<BLU119-W21367CE2101A31B83B42CDCD200@phx.gbl>
Message-ID: <db471ace0907150145s26ceab9cv8e02b88589332204@mail.gmail.com>

Henrik,

You must specify where to find the Slony-I SQL files in File>Options.
If you're using Slony 2.0.2, I suggest you forget about PgAdmin3 and
follow what I've done in my recent mail to the list (basically, hand
writing a slonik script per the "Replicating your first database" page
of the docs),

Regards,
Peter Geoghegan

2009/7/14 Henrik HJ <testtest@live.dk>:
- Show quoted text -
>
>
> Hi
>
> I trying to make a cluster with Slony1 by using pgAdmin.
>
> When I try to make a new cluster. I get the Message:
>
> Slony-I creation scripts not available; only joining possible.
>
> On the pgAdmin file ? option menu I have find the Slony-I path and try to
> /usr/bin but I can?t find the scripts.
>
> Please help.
>
> Henrik
>
> ________________________________
> Har du set den nye Windows Live Messenger? Hent den her!
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
>
From vanderleeden at scoreloop.com  Wed Jul 15 02:44:48 2009
From: vanderleeden at scoreloop.com (vanderleeden@scoreloop.com)
Date: Wed Jul 15 02:45:27 2009
Subject: [Slony1-general] Please lower the min value of slon_log_level from
	0 to -1
Message-ID: <20090715094448.2B6719C3BC@ec01.scoreloop.com>

Hi,

I'd like to set the slon log_level to -1 to suppress SLON_INFO messages in the logfile.
Currently the min value is 0 (defined in confoptions.c / ConfigureNamesInt).

If I use    'slon -d -1'   I'm getting the warning:
WARN   -1 is outside the valid range for parameter "log_level" (0 .. 4)

Could you please lower the setting to -1 to allow smaller log_level settings.

Thank you.

Rudolf VanderLeeden
Scoreloop AG, Munich, Germany


From vanderleeden at logicunited.com  Wed Jul 15 02:38:27 2009
From: vanderleeden at logicunited.com (Rudolf van der Leeden)
Date: Wed Jul 15 07:47:50 2009
Subject: [Slony1-general] Please lower the min value of slon_log_level from
	0 to -1
Message-ID: <80B27CD0-A54E-4E17-A04D-DF036622A28D@logicunited.com>

Hi,

I'd like to set the slon log_level to -1 to suppress SLON_INFO  
messages in the logfile.
Currently the min value is 0 (defined in confoptions.c /  
ConfigureNamesInt).

If I use    'slon -d -1'   I'm getting the warning:
   WARN   -1 is outside the valid range for parameter  
"log_level" (0 .. 4)

Could you please lower the setting to -1 to allow smaller log_level  
settings.

Thank you.

Rudolf VanderLeeden
Scoreloop AG, Munich, Germany


From cbbrowne at ca.afilias.info  Wed Jul 15 10:08:03 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 15 10:08:17 2009
Subject: [Slony1-general] Please lower the min value of slon_log_level
	from 0 to -1
In-Reply-To: <80B27CD0-A54E-4E17-A04D-DF036622A28D@logicunited.com> (Rudolf
	van der Leeden's message of "Wed, 15 Jul 2009 11:38:27 +0200")
References: <80B27CD0-A54E-4E17-A04D-DF036622A28D@logicunited.com>
Message-ID: <87bpnlx4b0.fsf@dba2.int.libertyrms.com>

Rudolf van der Leeden <vanderleeden@logicunited.com> writes:
> I'd like to set the slon log_level to -1 to suppress SLON_INFO
> messages in the logfile.
> Currently the min value is 0 (defined in confoptions.c /
> ConfigureNamesInt).
>
> If I use    'slon -d -1'   I'm getting the warning:
>   WARN   -1 is outside the valid range for parameter "log_level" (0
> .. 4)
>
> Could you please lower the setting to -1 to allow smaller log_level
> settings.

Seems like a generally bad idea to me, but I don't object so
strenuously that I'd refuse to put the change in...

Watch CVS ;-).
-- 
let name="cbbrowne" and tld="ca.afilias.info" in name ^ "@" ^ tld;;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From kleptog at gmail.com  Thu Jul 16 00:43:19 2009
From: kleptog at gmail.com (Martijn van Oosterhout)
Date: Thu Jul 16 00:43:59 2009
Subject: [Slony1-general] Question about slonik connection strings and events
Message-ID: <2fc2c5f10907160043v5729602dq8999cbd33bf02977@mail.gmail.com>

Hoi,

I've got a question that I haven't quite been able to find the answer
for in the docs. When writing a slonik script it needs connection
strings so that it can connect to all the different nodes for
administration purposes. One of the network topologies we're looking
at involves some machines behind firewalls and I'm trying to workout
how necessary it is to talk to them for every change. The docs say you
can use SSH tunneling to setup the links. My specific questions are:

1. If you do a MOVE SET command, does slonik connect to every node, or
just the node that is the current origin, or any random node?

2. If you use the moveset() plpgsql function, does it matter which
node it is executed on?

3. Same questions but for FAILOVER and the the failednode() function.

4. Can you tell slonik to automatically run a command before
attempting to setup a connection to a database?

The question I think hinges on what an "event" is, which I haven't
found a good definition for in the documentation (I would have thought
it would be under "concepts"). I think that because MOVE SET is an
event it can be executed anywhere, but because the failover is not an
event it has to be executed on every node individually. Am I right?

Essentially what I'm wondering is, do I need to be able to contact
every node in the cluster directly to do a controlled switchover or
failover?

Thanks in advance,
-- 
Martijn van Oosterhout <kleptog@gmail.com> http://svana.org/kleptog/
From gordo169 at gmail.com  Thu Jul 16 06:10:19 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Thu Jul 16 06:10:25 2009
Subject: [Slony1-general] pg_autovacuum table gone in 8.4?
Message-ID: <24516041.post@talk.nabble.com>


I'm trying to run Slony 2.0.2 with Postgres 8.4.  I'm getting this error:

14 0716 03:41:58 ERROR  cleanupThread: "select nspname, relname from
"_slony_cluster".TablesToVacuum();" - ERROR:  relation
"pg_catalog.pg_autovacuum" does not exist
14 0716 03:41:58 LINE 1: select enabled from "pg_catalog".pg_autovacuum
where vacreli...
14 0716 03:41:58                             ^
14 0716 03:41:58 QUERY:  select enabled from "pg_catalog".pg_autovacuum
where vacrelid =  $1 
14 0716 03:41:58 CONTEXT:  PL/pgSQL function "shouldslonyvacuumtable" line
25 at SQL statement
14 0716 03:41:58 PL/pgSQL function "tablestovacuum" line 6 at IF

It appears they may have dropped the use of the pg_autovacuum table in 8.4
in favor of table creation storage parameters.
-- 
View this message in context: http://www.nabble.com/pg_autovacuum-table-gone-in-8.4--tp24516041p24516041.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From cbbrowne at ca.afilias.info  Thu Jul 16 07:27:50 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Jul 16 07:27:59 2009
Subject: [Slony1-general] pg_autovacuum table gone in 8.4?
In-Reply-To: <24516041.post@talk.nabble.com> (Gordon Shannon's message of
	"Thu, 16 Jul 2009 06:10:19 -0700 (PDT)")
References: <24516041.post@talk.nabble.com>
Message-ID: <87r5wgvh21.fsf@dba2.int.libertyrms.com>

Gordon Shannon <gordo169@gmail.com> writes:
> I'm trying to run Slony 2.0.2 with Postgres 8.4.  I'm getting this error:
>
> 14 0716 03:41:58 ERROR  cleanupThread: "select nspname, relname from
> "_slony_cluster".TablesToVacuum();" - ERROR:  relation
> "pg_catalog.pg_autovacuum" does not exist
> 14 0716 03:41:58 LINE 1: select enabled from "pg_catalog".pg_autovacuum
> where vacreli...
> 14 0716 03:41:58                             ^
> 14 0716 03:41:58 QUERY:  select enabled from "pg_catalog".pg_autovacuum
> where vacrelid =  $1 
> 14 0716 03:41:58 CONTEXT:  PL/pgSQL function "shouldslonyvacuumtable" line
> 25 at SQL statement
> 14 0716 03:41:58 PL/pgSQL function "tablestovacuum" line 6 at IF
>
> It appears they may have dropped the use of the pg_autovacuum table in 8.4
> in favor of table creation storage parameters.

Evidently so :-(.

I'll have to take a peek...
-- 
(reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From nettreeinc at gmail.com  Thu Jul 16 09:47:38 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Thu Jul 16 09:47:51 2009
Subject: [Slony1-general] fe_sendauth: no password supplied error
Message-ID: <24520233.post@talk.nabble.com>


Environment: 
OS: Red hat 5
DB: Enterprise DB standard Version

When I am trying to run the bash script that with .sh extension following
the instruction from a slony replication topic from other forum
(http://www.linuxjournal.com/article/7834#comment-254807), I get "
fe_sendauth: no password supplied error"
I am just trying to replicating two DBs (master and slave) within the same
server (localhost), following the instrustion from that slony replication
article. 


This is copied from command window. Run with "postgres" admin role. 
-bash-3.2$ ./cluster_setup.sh
<stdin>:7: fe_sendauth: no password supplied
-bash-3.2$

After did some research and from what I can see from the error message. It
must had something to do with authentication accessing rights, but not sure
what exactly. I know I had configured pg_hba to open connections on host,
localhost and couple of others and set it to "trust". ( please see
attachment) but still not work. Also, from the bash script file I was trying
to execute, it did no list anywhere to "supplying" the password nor prompt
me to enter one. therefore, what is "no password supplied" suppose to mean?
How do I supply it? 



http://www.nabble.com/file/p24520233/pg_hba.conf pg_hba.conf 
http://www.nabble.com/file/p24520233/postgresql.conf postgresql.conf 
http://www.nabble.com/file/p24520233/cluster_setup.sh cluster_setup.sh 
-- 
View this message in context: http://www.nabble.com/fe_sendauth%3A-no-password-supplied-error-tp24520233p24520233.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From gordo169 at gmail.com  Thu Jul 16 10:40:54 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Thu Jul 16 10:41:08 2009
Subject: [Slony1-general] "Could not lock table" errors from slony
Message-ID: <24521158.post@talk.nabble.com>


I am running Slony 2.0.2 replicating tables from an 8.3 cluster to an 8.4. 
The following scenarios has happened consistently:

1. Issue SUBSCRIBE SET
2. Slon log on receiver goes into loop due to "transactions earlier than XID
nnnnnnn are still in progress".
3. A few minutes later, the long query on the provider node is finished.
4. Slon on receiver now tries to copy the set, but gets 
   "ERROR remoteWorkerThread_7: Could not lock table "public"."abc" on
subscriber"  where abc is the 1st table in the set.

It will loop like this indefinitely, until I bounce slon, then it works.
Also, at the same time that error happens in the slon log, I see this in the
Postgres log on the receiver:
  LOCK TABLE can only be used in transaction blocks.
  Statement: lock table "public"."abc"

This is very consistent.  It's not a show stopper, just annoying.  I don't
remember this in 2.0.1.
-- 
View this message in context: http://www.nabble.com/%22Could-not-lock-table%22-errors-from-slony-tp24521158p24521158.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From brettelliott at gmail.com  Thu Jul 16 10:51:17 2009
From: brettelliott at gmail.com (Brett)
Date: Thu Jul 16 10:51:34 2009
Subject: [Slony1-general] How should I delete a node? Also,
	sl_log_2 is at 1 million rows
Message-ID: <1247766677.5054.11.camel@brett-laptop>

A machine housing a slony slave database went down and the machine is
being worked on, meanwhile what is the best way to handle this? I'd like
to for now delete the node in slony so I don't get errors such as:

failed - could not connect to server: No route to host
        Is the server running on host "10.2.1.30" and accepting
        TCP/IP connections on port 5432?

in the logs. SLONIK DROP NODE contacts the node to be dropped and issues
commands on that node and obviously that won't work since the node is
unavailable right now.


Also, sl_log_2 is up to 1M rows and I see that slony FETCH queries are
slow. Could this down node be causing the large numbers of log entries?

I'm using slony version Using 1 2.0.1.

Thanks.

From cbbrowne at ca.afilias.info  Thu Jul 16 13:16:58 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Jul 16 13:17:15 2009
Subject: [Slony1-general] How should I delete a node? Also,
	sl_log_2 is at 1 million rows
In-Reply-To: <1247766677.5054.11.camel@brett-laptop> (Brett's message of "Thu, 
	16 Jul 2009 10:51:17 -0700")
References: <1247766677.5054.11.camel@brett-laptop>
Message-ID: <87k528v0w5.fsf@dba2.int.libertyrms.com>

Brett <brettelliott@gmail.com> writes:
> A machine housing a slony slave database went down and the machine is
> being worked on, meanwhile what is the best way to handle this? I'd like
> to for now delete the node in slony so I don't get errors such as:
>
> failed - could not connect to server: No route to host
>         Is the server running on host "10.2.1.30" and accepting
>         TCP/IP connections on port 5432?
>
> in the logs. SLONIK DROP NODE contacts the node to be dropped and issues
> commands on that node and obviously that won't work since the node is
> unavailable right now.

Actually, "obviously that won't work" isn't quite so...

You need to specify a different node as the EVENT NODE, and that
should work fine.
<http://www.slony.info/documentation/stmtdropnode.html>
-- 
"cbbrowne","@","ca.afilias.info"
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Thu Jul 16 13:21:14 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Jul 16 13:21:41 2009
Subject: [Slony1-general] fe_sendauth: no password supplied error
In-Reply-To: <24520233.post@talk.nabble.com> (roctaiwan's message of "Thu, 16
	Jul 2009 09:47:38 -0700 (PDT)")
References: <24520233.post@talk.nabble.com>
Message-ID: <87fxcwv0p1.fsf@dba2.int.libertyrms.com>

roctaiwan <nettreeinc@gmail.com> writes:
> http://www.nabble.com/file/p24520233/pg_hba.conf pg_hba.conf 

>From the pg_hba.conf contents, *any* connection requested should not
bother asking for a password, which seems to contradict other things
you mentioned.

Is it possible that the postmaster is still using former contents of
pg_hba.conf that required passwords?

You might need to run:
   pg_ctl reload
or
   /etc/init.d/postgresql reload
or
  submit a HUP signal to the postmaster process
    kill -HUP ${PGPID}
?
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From dmk at mr-paradox.net  Thu Jul 16 13:51:34 2009
From: dmk at mr-paradox.net (David Kerr)
Date: Thu Jul 16 13:51:50 2009
Subject: [Slony1-general] Novice replication question
Message-ID: <20090716205134.GA87699@mr-paradox.net>

Hello!

I'm evaluating Slony as a solution for the system we're building.

I'd like to run what i'm planning to do by the group to see if what
I want to do is even possible. 

I've got two servers / databases. NodeA and NodeB

NodeA replicates to NodeB.

for my nightly processing, I have some jobs that will update a large number 
of records, and would basically result in the system unavailable (could be 
a few hours).

What I'd like to do, is to run those Jobs in NodeB and then at the end
of the process "swap" NodeB and NodeA.

>From what i've read of slony so far, I think i can do at least that much.


So the concern is - up until the "swap" I need to be able to access and even 
update records in NodeA and I want to continue replicating between NodeA and 
NodeB.

During that time, I can control the records that are inserted/updated in
NodeA so I know that they wouldn't impact anything that is currently
running into NodeB.

Is that possible? (is there a better/ or standard way that I'm missing?)

Thanks

Dave
From nettreeinc at gmail.com  Thu Jul 16 19:33:21 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Thu Jul 16 19:33:49 2009
Subject: [Slony1-general] fe_sendauth: no password supplied error
In-Reply-To: <87fxcwv0p1.fsf@dba2.int.libertyrms.com>
References: <24520233.post@talk.nabble.com>
	<87fxcwv0p1.fsf@dba2.int.libertyrms.com>
Message-ID: <24527612.post@talk.nabble.com>


Hi Christopher, Thanks, it seems working and not asking me for password when
i run the script! I would never know that I need a reload using pg_ctl....
but yeah, any modification done to the .conf file needs a reload. 

I related issue comes up I will post it up again. thanks for the answer




>From the pg_hba.conf contents, *any* connection requested should not
bother asking for a password, which seems to contradict other things
you mentioned.

Is it possible that the postmaster is still using former contents of
pg_hba.conf that required passwords?

You might need to run:
   pg_ctl reload
or
   /etc/init.d/postgresql reload
or
  submit a HUP signal to the postmaster process
    kill -HUP ${PGPID}

-- 
View this message in context: http://www.nabble.com/fe_sendauth%3A-no-password-supplied-error-tp24520233p24527612.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Thu Jul 16 20:32:48 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Thu Jul 16 20:33:19 2009
Subject: [Slony1-general] Need some commands to monitoring/managing the
	slony clustering status
Message-ID: <24528069.post@talk.nabble.com>


I had following instructions of a forum on how to start replicating my first
DBs. But it has not teach me how to do submit some general commands. I have
few that I would wants to know. 

Instruction says: 
% slon sql_cluster "dbname=contactdb user=postgres" &
% slon sql_cluster "dbname=contactdb_slave user=postgres" &

Questions: 
should I type this one by one and press ENTER after the "&"?

I start from the first cluster, once it started it looks like is showning
the status log of the slony in the window
http://www.nabble.com/file/p24528069/slony_Ma_start.jpg , it just keeps
rolling and rolling, I had wait for about a min but it don't seem will stop.
For me to start my second cluster engine, I had ignore it and continue type
my second command even it would moving my lines to different line, and press
enter. But I am not sure if I had started it or not. 

How can I put these status in the back ground so I can continue doing my
thing. Also how could I put it back to screen front if I want to see it
again?
I was trying to do ctrl+c, I am not sure if I did that will I stoped the
replication process?
what is the command to see the status of all my clusters (cluster1,
cluster2, cluster3) status if its "stoped" or "started" ? 
Also whats the command to show me the detail status of clustering on every
cluster to identify if there is any problems during replicating process ?
How do I stop, start and restart the slon replication engine ? 

If I would wants to know more useful commands like those I asked, could you
referral me some places? I know www.slony.info is one place, but the
information up there is just kinda hard to absorb. 


-- 
View this message in context: http://www.nabble.com/Need-some-commands-to-monitoring-managing-the-slony-clustering-status-tp24528069p24528069.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From melvin6925 at yahoo.com  Thu Jul 16 20:40:45 2009
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Thu Jul 16 20:41:15 2009
Subject: [Slony1-general] Need some commands to monitoring/managing the
	slony clustering status
Message-ID: <541445.67824.qm@web53012.mail.re2.yahoo.com>

roctaiwan,

What you want to do is place the commands in a script that sends the output to background.
make sure you have defined the environment variables referenced also.

eg: the following script starts slon for node 1 

script name: start_node_1_db.sh
contents:
------------------------------------------------------------
slon -d1 -pslon_node_1.pid $SLONY_REP_SCHEMA "dbname=$MASTERDBNAME user=$REPLICATIONUSER host=$MASTERHOST port=$PGPORT" > slon1.log &


Melvin Davidson 
Folk Alley - All Folk - 24 Hours a day 
www.folkalley.com



--- On Thu, 7/16/09, roctaiwan <nettreeinc@gmail.com> wrote:

From: roctaiwan <nettreeinc@gmail.com>
Subject: [Slony1-general] Need some commands to monitoring/managing the slony clustering status
To: slony1-general@lists.slony.info
Date: Thursday, July 16, 2009, 10:32 PM


I had following instructions of a forum on how to start replicating my first
DBs. But it has not teach me how to do submit some general commands. I have
few that I would wants to know. 

Instruction says: 
% slon sql_cluster "dbname=contactdb user=postgres" &
% slon sql_cluster "dbname=contactdb_slave user=postgres" &

Questions: 
should I type this one by one and press ENTER after the "&"?

I start from the first cluster, once it started it looks like is showning
the status log of the slony in the window
http://www.nabble.com/file/p24528069/slony_Ma_start.jpg , it just keeps
rolling and rolling, I had wait for about a min but it don't seem will stop.
For me to start my second cluster engine, I had ignore it and continue type
my second command even it would moving my lines to different line, and press
enter. But I am not sure if I had started it or not. 

How can I put these status in the back ground so I can continue doing my
thing. Also how could I put it back to screen front if I want to see it
again?
I was trying to do ctrl+c, I am not sure if I did that will I stoped the
replication process?
what is the command to see the status of all my clusters (cluster1,
cluster2, cluster3) status if its "stoped" or "started" ? 
Also whats the command to show me the detail status of clustering on every
cluster to identify if there is any problems during replicating process ?
How do I stop, start and restart the slon replication engine ? 

If I would wants to know more useful commands like those I asked, could you
referral me some places? I know www.slony.info is one place, but the
information up there is just kinda hard to absorb. 


-- 
View this message in context: http://www.nabble.com/Need-some-commands-to-monitoring-managing-the-slony-clustering-status-tp24528069p24528069.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090716/cc8c4b65/attachment.html
From gordo169 at gmail.com  Thu Jul 16 21:22:39 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Thu Jul 16 21:23:10 2009
Subject: [Slony1-general] Novice replication question
In-Reply-To: <20090716205134.GA87699@mr-paradox.net>
References: <20090716205134.GA87699@mr-paradox.net>
Message-ID: <24528369.post@talk.nabble.com>




David Kerr wrote:
> 
> NodeA replicates to NodeB.
> 
> for my nightly processing, I have some jobs that will update a large
> number 
> of records, and would basically result in the system unavailable (could be 
> a few hours).
> 
> What I'd like to do, is to run those Jobs in NodeB and then at the end
> of the process "swap" NodeB and NodeA.
> 
> From what i've read of slony so far, I think i can do at least that much.
> 
> So the concern is - up until the "swap" I need to be able to access and
> even 
> update records in NodeA and I want to continue replicating between NodeA
> and 
> NodeB.
> 
> During that time, I can control the records that are inserted/updated in
> NodeA so I know that they wouldn't impact anything that is currently
> running into NodeB.
> 
> Is that possible? (is there a better/ or standard way that I'm missing?)
> 

I don't think so.  You don't say this, but I'm guessing the writes on NodeB
are into the same table as being inserted/updated on NodeA, and is
replicated?  If so, that won't work.

Let's say you have 2 tables, T1 and T2 that you are syncing from the origin
NodeA to the receiver NodeB, as part of a replication set.  You can't write
to T2 on NodeB during your maintenance, or ever, because Slony will have
triggers on the table to prevent writes outside of Slony itself.  Otherwise,
how could Slony guarantee that the tables are in sync?

You can, of course, write to Table2 on NodeB if T2 is not replicated.  But
then when you do the swap, all that T2 data would now only be on NodeB, not
on NodeA.  I guess you could create a set, sync the table, then destroy the
set (which leaves the data) every day before the nightly job, but know that
while the set is syncing the table will be exclusively locked by Slony on
the receiver side.  If it's a large table that would likely be impractical.

Hope this helps a bit.
Gordon


-- 
View this message in context: http://www.nabble.com/Novice-replication-question-tp24524258p24528369.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From kerdezixe at gmail.com  Fri Jul 17 06:32:15 2009
From: kerdezixe at gmail.com (Laurent Laborde)
Date: Fri Jul 17 06:32:23 2009
Subject: [Slony1-general] Does Slony version 1.2.x work with postgresql 
	8.4?
In-Reply-To: <87y6qrxkey.fsf@dba2.int.libertyrms.com>
References: <20090713205919.GA26996@it.is.rice.edu>
	<87y6qrxkey.fsf@dba2.int.libertyrms.com>
Message-ID: <8a1bfe660907170632s2f221ed3v7800b947eff9f755@mail.gmail.com>

On Tue, Jul 14, 2009 at 7:07 PM, Christopher
Browne<cbbrowne@ca.afilias.info> wrote:
> Kenneth Marshall <ktm@rice.edu> writes:
>> In particular, I would like to be able to upgrade an
>> 8.3 databae to 8.4 with minimal downtime. 1.2.14 does
>> not build with a missing function definition for
>> SerializableSnapshot. If not, is there a scheme to
>> upgrade to 8.4 that does not entail the outage of
>> a full dump/restore cycle?
>
> No, 1.2 does not work with 8.4, and barring having considerably more
> time to play with, it doesn't seem like a straightforward backport.
>
> There were substantial changes in 8.4 to the implementation of
> pg_listener, to change from on-table storage to an in-memory handling
> that would fairly much break the whole listener loop.
>
> Version 2.0 revised that substantially; that doesn't seem like a
> backport, though.

This is really a major problem.
upgrade to slony-1 2.x require to rebuild the whole cluster.
upgrade to pgsql-8.4 require to rebuild the whole database.

I cannot find a way to upgrade to 8.4 without innaceptable downtime
unless we upgrade both psql and slony at once :(

-- 
F4FQM
Kerunix Flan
Laurent Laborde
From glynastill at yahoo.co.uk  Fri Jul 17 06:48:19 2009
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Fri Jul 17 06:48:31 2009
Subject: [Slony1-general] Does Slony version 1.2.x work with postgresql
	8.4?
Message-ID: <775003.58675.qm@web23606.mail.ird.yahoo.com>




--- On Fri, 17/7/09, Laurent Laborde <kerdezixe@gmail.com> wrote:

> From: Laurent Laborde <kerdezixe@gmail.com>
> Subject: Re: [Slony1-general] Does Slony version 1.2.x work with postgresql  8.4?
> To: slony1-general@lists.slony.info
> Date: Friday, 17 July, 2009, 2:32 PM
> On Tue, Jul 14, 2009 at 7:07 PM,
> Christopher
> Browne<cbbrowne@ca.afilias.info>
> wrote:
> > Kenneth Marshall <ktm@rice.edu>
> writes:
> >> In particular, I would like to be able to upgrade
> an
> >> 8.3 databae to 8.4 with minimal downtime. 1.2.14
> does
> >> not build with a missing function definition for
> >> SerializableSnapshot. If not, is there a scheme
> to
> >> upgrade to 8.4 that does not entail the outage of
> >> a full dump/restore cycle?
> >
> > No, 1.2 does not work with 8.4, and barring having
> considerably more
> > time to play with, it doesn't seem like a
> straightforward backport.
> >
> > There were substantial changes in 8.4 to the
> implementation of
> > pg_listener, to change from on-table storage to an
> in-memory handling
> > that would fairly much break the whole listener loop.
> >
> > Version 2.0 revised that substantially; that doesn't
> seem like a
> > backport, though.
> 
> This is really a major problem.
> upgrade to slony-1 2.x require to rebuild the whole
> cluster.
> upgrade to pgsql-8.4 require to rebuild the whole
> database.
> 
> I cannot find a way to upgrade to 8.4 without innaceptable
> downtime
> unless we upgrade both psql and slony at once :(
> 

We're all in the same boat here, hopefully the ingenious chaps at afilias will pull something out of the bag at some point.


      
From dmk at mr-paradox.net  Fri Jul 17 08:17:13 2009
From: dmk at mr-paradox.net (David Kerr)
Date: Fri Jul 17 08:17:21 2009
Subject: [Slony1-general] Novice replication question
In-Reply-To: <24528369.post@talk.nabble.com>
References: <20090716205134.GA87699@mr-paradox.net>
	<24528369.post@talk.nabble.com>
Message-ID: <20090717151713.GA82020@mr-paradox.net>

On Thu, Jul 16, 2009 at 09:22:39PM -0700, Gordon Shannon wrote:
- Let's say you have 2 tables, T1 and T2 that you are syncing from the origin
- NodeA to the receiver NodeB, as part of a replication set.  You can't write
- to T2 on NodeB during your maintenance, or ever, because Slony will have
- triggers on the table to prevent writes outside of Slony itself.  Otherwise,
- how could Slony guarantee that the tables are in sync?

Ok that's the key. I was hoping that even though the table was in the 
replication set, I'd be able to write to it on node 2, while still 
replicating other tables from NodeA (but specificially not the one i'm writing 
to on node 2).

I agree, seems like i could cobble something together by dropping and re-creating
/re-syncing sets. But it's probably not worth it.

Thanks

Dave
From sweta.mulgavker at gmail.com  Thu Jul 16 23:18:31 2009
From: sweta.mulgavker at gmail.com (Sweta Mulgavker)
Date: Fri Jul 17 09:45:33 2009
Subject: [Slony1-general] Error while... gmake all
Message-ID: <4077e7710907162318u2827ac37nfa30067c082327e@mail.gmail.com>

Hello All,

I am trying to install slony1-1.1.6 on a CentOS 5 machine with postgres
8.1.11.

While exectuting the "gmake" all command I got the followng error

[root@quirinus slony1-1.1.6]# gmake all
gmake[1]: Entering directory `/root/Desktop/slony_install/slony1-1.1.6/src'
gmake[2]: Entering directory
`/root/Desktop/slony_install/slony1-1.1.6/src/xxid'
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -fpic -I../..
-I/usr/include/ -I/usr/include/pgsql/server/  -c -o xxid.o xxid.c
gcc -shared -o xxid.so xxid.o
cp xxid.v73.sql xxid.v74.sql
cp xxid.v73.sql xxid.v80.sql
gmake[2]: Leaving directory
`/root/Desktop/slony_install/slony1-1.1.6/src/xxid'
gmake[2]: Entering directory
`/root/Desktop/slony_install/slony1-1.1.6/src/slon'
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
-I../.. -I/usr/include/ -I/usr/include/ -I/usr/include/pgsql/server/  -c -o
slon.o slon.c
In file included from slon.c:31:
confoptions.h:191: warning: missing braces around initializer
confoptions.h:191: warning: (near initialization for
=91ConfigureNamesInt[7].gen=92)
confoptions.h:217: warning: missing braces around initializer
confoptions.h:217: warning: (near initialization for
=91ConfigureNamesBool[2].gen=92)
confoptions.h:234: warning: missing braces around initializer
confoptions.h:234: warning: (near initialization for
=91ConfigureNamesReal[1].gen=92)
confoptions.h:328: warning: missing braces around initializer
confoptions.h:328: warning: (near initialization for
=91ConfigureNamesString[8].gen=92)
slon.c: In function =91main=92:
slon.c:737: warning: control reaches end of non-void function
slon.c: At top level:
confoptions.h:98: warning: =91ConfigureNamesInt=92 defined but not used
confoptions.h:194: warning: =91ConfigureNamesBool=92 defined but not used
confoptions.h:220: warning: =91ConfigureNamesReal=92 defined but not used
confoptions.h:237: warning: =91ConfigureNamesString=92 defined but not used
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
-I../.. -I/usr/include/ -I/usr/include/ -I/usr/include/pgsql/server/  -c -o
runtime_config.o runtime_config.c
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
-I../.. -I/usr/include/ -I/usr/include/ -I/usr/include/pgsql/server/  -c -o
local_listen.o local_listen.c
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
-I../.. -I/usr/include/ -I/usr/include/ -I/usr/include/pgsql/server/  -c -o
remote_listen.o remote_listen.c
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
-I../.. -I/usr/include/ -I/usr/include/ -I/usr/include/pgsql/server/  -c -o
remote_worker.o remote_worker.c
In file included from remote_worker.c:29:
confoptions.h:191: warning: missing braces around initializer
confoptions.h:191: warning: (near initialization for
=91ConfigureNamesInt[7].gen=92)
confoptions.h:217: warning: missing braces around initializer
confoptions.h:217: warning: (near initialization for
=91ConfigureNamesBool[2].gen=92)
confoptions.h:234: warning: missing braces around initializer
confoptions.h:234: warning: (near initialization for
=91ConfigureNamesReal[1].gen=92)
confoptions.h:328: warning: missing braces around initializer
confoptions.h:328: warning: (near initialization for
=91ConfigureNamesString[8].gen=92)
remote_worker.c: In function =91copy_set=92:
remote_worker.c:2160: warning: unused variable =91ntuples3=92
remote_worker.c: At top level:
confoptions.h:98: warning: =91ConfigureNamesInt=92 defined but not used
confoptions.h:194: warning: =91ConfigureNamesBool=92 defined but not used
confoptions.h:220: warning: =91ConfigureNamesReal=92 defined but not used
confoptions.h:237: warning: =91ConfigureNamesString=92 defined but not used
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
-I../.. -I/usr/include/ -I/usr/include/ -I/usr/include/pgsql/server/  -c -o
sync_thread.o sync_thread.c
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
-I../.. -I/usr/include/ -I/usr/include/ -I/usr/include/pgsql/server/  -c -o
cleanup_thread.o cleanup_thread.c
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
-I../.. -I/usr/include/ -I/usr/include/ -I/usr/include/pgsql/server/  -c -o
scheduler.o scheduler.c
scheduler.c:653: warning: =91sched_shutdown=92 defined but not used
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
-I../.. -I/usr/include/ -I/usr/include/ -I/usr/include/pgsql/server/  -c -o
dbutils.o dbutils.c
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
-I../.. -I/usr/include/ -I/usr/include/ -I/usr/include/pgsql/server/  -c -o
conf-file.o conf-file.c
In file included from src/slon/conf-file.l:16:
confoptions.h:191: warning: missing braces around initializer
confoptions.h:191: warning: (near initialization for
=91ConfigureNamesInt[7].gen=92)
confoptions.h:217: warning: missing braces around initializer
confoptions.h:217: warning: (near initialization for
=91ConfigureNamesBool[2].gen=92)
confoptions.h:234: warning: missing braces around initializer
confoptions.h:234: warning: (near initialization for
=91ConfigureNamesReal[1].gen=92)
confoptions.h:328: warning: missing braces around initializer
confoptions.h:328: warning: (near initialization for
=91ConfigureNamesString[8].gen=92)
<stdout>:1617: warning: no previous prototype for =91yyget_lineno=92
<stdout>:1626: warning: no previous prototype for =91yyget_in=92
<stdout>:1634: warning: no previous prototype for =91yyget_out=92
<stdout>:1642: warning: no previous prototype for =91yyget_leng=92
<stdout>:1651: warning: no previous prototype for =91yyget_text=92
<stdout>:1660: warning: no previous prototype for =91yyset_lineno=92
<stdout>:1672: warning: no previous prototype for =91yyset_in=92
<stdout>:1677: warning: no previous prototype for =91yyset_out=92
<stdout>:1682: warning: no previous prototype for =91yyget_debug=92
<stdout>:1687: warning: no previous prototype for =91yyset_debug=92
<stdout>:1721: warning: no previous prototype for =91yylex_destroy=92
src/slon/conf-file.l:116: warning: no previous prototype for
=91ProcessConfigFile=92
src/slon/conf-file.l: In function =91ProcessConfigFile=92:
src/slon/conf-file.l:156: warning: suggest parentheses around assignment
used as truth value
src/slon/conf-file.l: At top level:
confoptions.h:98: warning: =91ConfigureNamesInt=92 defined but not used
confoptions.h:194: warning: =91ConfigureNamesBool=92 defined but not used
confoptions.h:220: warning: =91ConfigureNamesReal=92 defined but not used
confoptions.h:237: warning: =91ConfigureNamesString=92 defined but not used
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
-I../.. -I/usr/include/ -I/usr/include/ -I/usr/include/pgsql/server/  -c -o
confoptions.o confoptions.c
In file included from confoptions.c:17:
confoptions.h:191: warning: missing braces around initializer
confoptions.h:191: warning: (near initialization for
=91ConfigureNamesInt[7].gen=92)
confoptions.h:217: warning: missing braces around initializer
confoptions.h:217: warning: (near initialization for
=91ConfigureNamesBool[2].gen=92)
confoptions.h:234: warning: missing braces around initializer
confoptions.h:234: warning: (near initialization for
=91ConfigureNamesReal[1].gen=92)
confoptions.h:328: warning: missing braces around initializer
confoptions.h:328: warning: (near initialization for
=91ConfigureNamesString[8].gen=92)
confoptions.c:40: warning: no previous prototype for =91build_conf_variable=
s=92
confoptions.c:121: warning: =91add_conf_variable=92 defined but not used
confoptions.c: In function =91set_config_option=92:
confoptions.c:451: warning: =91newval=92 may be used uninitialized in this
function
confoptions.c:473: warning: =91newval=92 may be used uninitialized in this
function
confoptions.c:499: warning: =91newval=92 may be used uninitialized in this
function
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
-I../.. -I/usr/include/ -I/usr/include/ -I/usr/include/pgsql/server/  -c -o
misc.o misc.c
In file included from misc.c:33:
confoptions.h:191: warning: missing braces around initializer
confoptions.h:191: warning: (near initialization for
=91ConfigureNamesInt[7].gen=92)
confoptions.h:217: warning: missing braces around initializer
confoptions.h:217: warning: (near initialization for
=91ConfigureNamesBool[2].gen=92)
confoptions.h:234: warning: missing braces around initializer
confoptions.h:234: warning: (near initialization for
=91ConfigureNamesReal[1].gen=92)
confoptions.h:328: warning: missing braces around initializer
confoptions.h:328: warning: (near initialization for
=91ConfigureNamesString[8].gen=92)
confoptions.h:98: warning: =91ConfigureNamesInt=92 defined but not used
confoptions.h:194: warning: =91ConfigureNamesBool=92 defined but not used
confoptions.h:220: warning: =91ConfigureNamesReal=92 defined but not used
confoptions.h:237: warning: =91ConfigureNamesString=92 defined but not used
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -pthread
-I../.. -I/usr/include/ -o slon slon.o runtime_config.o local_listen.o
remote_listen.o remote_worker.o sync_thread.o cleanup_thread.o scheduler.o
dbutils.o conf-file.o confoptions.o misc.o -pthread -L/usr/lib/
-L/usr/lib/pgsql/ -lpq -Wl,-rpath,/usr/lib/  -L/usr/lib/ -lpq
gmake[2]: Leaving directory
`/root/Desktop/slony_install/slony1-1.1.6/src/slon'
gmake[2]: Entering directory
`/root/Desktop/slony_install/slony1-1.1.6/src/slonik'
gcc -g -O2 -Wall -Wmissing-prototypes -Wmissing-declarations -I../..
-I/usr/include/ -I/usr/include/pgsql/server/
-DPGSHARE=3D"\"/usr/share/pgsql/\"" -DSED=3D"\"sed\"" -I/usr/include/
-I/usr/include/pgsql/server/  -c -o slonik.o slonik.c
In file included from slonik.c:26:
/usr/include/libpq-fe.h:33:25: error: openssl/ssl.h: No such file or
directory
In file included from slonik.c:26:
/usr/include/libpq-fe.h:276: error: expected =91=3D=92, =91,=92, =91;=92, =
=91asm=92 or
=91__attribute__=92 before =91*=92 token
gmake[2]: *** [slonik.o] Error 1
gmake[2]: Leaving directory
`/root/Desktop/slony_install/slony1-1.1.6/src/slonik'
gmake[1]: *** [all] Error 2
gmake[1]: Leaving directory `/root/Desktop/slony_install/slony1-1.1.6/src'
gmake: *** [all] Error 2


I downloaded the rpm from http://www.slony.info/downloads/1.1/rpm/

Please help me to slove this error....

Thankyou in advance.

Regards,
Sweta Mulgavker :)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090717/=
f1f85a0f/attachment-0001.htm
From gordo169 at gmail.com  Fri Jul 17 12:42:39 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Fri Jul 17 12:42:54 2009
Subject: [Slony1-general] How to downgrade from 2.0.2 to 2.0.1
Message-ID: <24540512.post@talk.nabble.com>


Can anyone suggest a good path to downgrade from 2.0.2 to 2.0.1 on Linux
(Centos)?  We have 1 provider, 2 receivers running 8.3.  I'm thinking
something like:

- stop all slon daemons
- dump Slony functions
- uninstall/reinstall Slony rpm package
- drop functions and reinstall functions from backup
- start back up

...but I know I'm playing with fire.  I don't know if the C libraries would
be expecting something different with meta data, e.g.

I'm having several serious problems running 2.0.2, and want to go back.  I'm
hoping there's a better or safer way to do it.

Thanks.
-- 
View this message in context: http://www.nabble.com/How-to-downgrade-from-2.0.2-to-2.0.1-tp24540512p24540512.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From devrim at gunduz.org  Fri Jul 17 12:59:42 2009
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Fri Jul 17 13:00:50 2009
Subject: [Slony1-general] How to downgrade from 2.0.2 to 2.0.1
In-Reply-To: <24540512.post@talk.nabble.com>
References: <24540512.post@talk.nabble.com>
Message-ID: <1247860782.2876.66.camel@hp-laptop2.gunduz.org>

T24gRnJpLCAyMDA5LTA3LTE3IGF0IDEyOjQyIC0wNzAwLCBHb3Jkb24gU2hhbm5vbiB3cm90ZToK
PiAKPiBJJ20gaGF2aW5nIHNldmVyYWwgc2VyaW91cyBwcm9ibGVtcyBydW5uaW5nIDIuMC4yLCBh
bmQgd2FudCB0byBnbwo+IGJhY2suICBJJ20gaG9waW5nIHRoZXJlJ3MgYSBiZXR0ZXIgb3Igc2Fm
ZXIgd2F5IHRvIGRvIGl0LgoKV2hhdCBraW5kIG9mIGlzc3Vlcz8gSSB0aGluayByZXBvcnRpbmcg
dGhlbSBhbmQgYXNraW5nIHRoZW0gdG8gYmUgZml4ZWQKY291bGQgYmUgYmV0dGVyIGZvciB5b3Ug
KGFuZCBldmVyeW9uZSBpbiB0aGUgcGxhbmV0KQoKUmVnYXJkcywKLS0gCkRldnJpbSBHw5xORMOc
WiwgUkhDRQpDb21tYW5kIFByb21wdCAtIGh0dHA6Ly93d3cuQ29tbWFuZFByb21wdC5jb20gCmRl
dnJpbX5ndW5kdXoub3JnLCBkZXZyaW1+UG9zdGdyZVNRTC5vcmcsIGRldnJpbS5ndW5kdXp+bGlu
dXgub3JnLnRyCiAgICAgICAgICAgICAgICAgICBodHRwOi8vd3d3Lmd1bmR1ei5vcmcKLS0tLS0t
LS0tLS0tLS0gbmV4dCBwYXJ0IC0tLS0tLS0tLS0tLS0tCkEgbm9uLXRleHQgYXR0YWNobWVudCB3
YXMgc2NydWJiZWQuLi4KTmFtZTogbm90IGF2YWlsYWJsZQpUeXBlOiBhcHBsaWNhdGlvbi9wZ3At
c2lnbmF0dXJlClNpemU6IDE5NyBieXRlcwpEZXNjOiBUaGlzIGlzIGEgZGlnaXRhbGx5IHNpZ25l
ZCBtZXNzYWdlIHBhcnQKVXJsIDogaHR0cDovL2xpc3RzLnNsb255LmluZm8vcGlwZXJtYWlsL3Ns
b255MS1nZW5lcmFsL2F0dGFjaG1lbnRzLzIwMDkwNzE3LzkzMmFhYzFhL2F0dGFjaG1lbnQucGdw
Cg==
From gordo169 at gmail.com  Fri Jul 17 13:18:25 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Fri Jul 17 13:18:41 2009
Subject: [Slony1-general] How to downgrade from 2.0.2 to 2.0.1
In-Reply-To: <1247860782.2876.66.camel@hp-laptop2.gunduz.org>
References: <24540512.post@talk.nabble.com>
	<1247860782.2876.66.camel@hp-laptop2.gunduz.org>
Message-ID: <24540853.post@talk.nabble.com>




Devrim G?ND?Z wrote:
> 
> What kind of issues? I think reporting them and asking them to be fixed
> could be better for you (and everyone in the planet)
> 

Issue 1:

After a successful initial copy of a set, it starts getting this error: 
ERROR:  duplicate key value violates unique constraint "text_metrics_pkey"
continuously when doing updates to a table in the set it just synced.  I
repeated this exact behavior on another node by unsubscribing the set and
resyncing it.  I had to unsubscribe the set everywhere.

Issue 2:

After a successful initial copy of a different set, it starts getting this
error:

14 0717 10:45:01 CONFIG slon: child terminated status: 11; pid: 28356,
current worker pid: 28356

..and then it restarts the worker process.  This repeats indefinitely every
10 sec until I unsubscribe the set from the node.  

Regards
-- 
View this message in context: http://www.nabble.com/How-to-downgrade-from-2.0.2-to-2.0.1-tp24540512p24540853.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From seklecki at noc.cfi.pgh.pa.us  Fri Jul 17 13:49:31 2009
From: seklecki at noc.cfi.pgh.pa.us (Brian A. Seklecki)
Date: Fri Jul 17 13:56:03 2009
Subject: [Slony1-general] How to downgrade from 2.0.2 to 2.0.1
In-Reply-To: <24540853.post@talk.nabble.com>
References: <24540512.post@talk.nabble.com>
	<1247860782.2876.66.camel@hp-laptop2.gunduz.org>
	<24540853.post@talk.nabble.com>
Message-ID: <1247863771.10382.14641.camel@soundwave.ws.pitbpa0.priv.collaborativefusion.com>


> 14 0717 10:45:01 CONFIG slon: child terminated status: 11; pid: 28356,
> current worker pid: 28356
> 
> ..and then it restarts the worker process.  This repeats indefinitely
> every
> 10 sec until I unsubscribe the set from the node.  


Ughhh...who's RPMs are are you using?  Sig11 is a segfault, which could
mean bad patches in the code by the RPM builder, or mismatch libraries,
etc.

Run your slon(8) process via strace(8) and get us the output.  Also,
build a debug binary (COPTS/CFLAGS+=-g)

  ~~BAS


From gordo169 at gmail.com  Fri Jul 17 14:09:02 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Fri Jul 17 14:09:19 2009
Subject: [Slony1-general] How to downgrade from 2.0.2 to 2.0.1
In-Reply-To: <1247863771.10382.14641.camel@soundwave.ws.pitbpa0.priv.collaborativefusion.com>
References: <24540512.post@talk.nabble.com>
	<1247860782.2876.66.camel@hp-laptop2.gunduz.org>
	<24540853.post@talk.nabble.com>
	<1247863771.10382.14641.camel@soundwave.ws.pitbpa0.priv.collaborativefusion.com>
Message-ID: <24541745.post@talk.nabble.com>




Brian A. Seklecki-2 wrote:
> 
> Ughhh...who's RPMs are are you using?  Sig11 is a segfault, which could
> mean bad patches in the code by the RPM builder, or mismatch libraries,
> etc.
> 
> Run your slon(8) process via strace(8) and get us the output.  Also,
> build a debug binary (COPTS/CFLAGS+=-g)
> 

Here is the srpm we started with:
http://yum.pgsqlrpms.org/srpms/8.4/fedora/fedora-11-i386/slony1-2.0.2-1.f11.src.rpm

We built it unmodified except for disable build of documentation. We
confirmed that the downloaded source md5sum matches that of the source at:
http://www.slony.info/downloads/2.0/source/

We are looking into the strace...  BTW, what do you mean by slon(8)?

Thx

-- 
View this message in context: http://www.nabble.com/How-to-downgrade-from-2.0.2-to-2.0.1-tp24540512p24541745.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From drees76 at gmail.com  Fri Jul 17 14:10:12 2009
From: drees76 at gmail.com (David Rees)
Date: Fri Jul 17 14:10:49 2009
Subject: [Slony1-general] Error while... gmake all
In-Reply-To: <4077e7710907162318u2827ac37nfa30067c082327e@mail.gmail.com>
References: <4077e7710907162318u2827ac37nfa30067c082327e@mail.gmail.com>
Message-ID: <72dbd3150907171410w63623ad5s7e2eb2419204c585@mail.gmail.com>

On Thu, Jul 16, 2009 at 11:18 PM, Sweta
Mulgavker<sweta.mulgavker@gmail.com> wrote:
> I am trying to install slony1-1.1.6 on a CentOS 5 machine with postgres
> 8.1.11.

Why such an old version of slon1 and postgres?

> While exectuting the "gmake" all command I got the followng error
>
> [root@quirinus slony1-1.1.6]# gmake all
>
<snip>
>
> I downloaded the rpm from http://www.slony.info/downloads/1.1/rpm/

I'm confused - are you building from source or from src.rpm?

Personally, I think you're better off avoiding the src.rpms and just
building from the source tarball.

-Dave
From devrim at gunduz.org  Fri Jul 17 21:08:38 2009
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Fri Jul 17 21:10:15 2009
Subject: [Slony1-general] Error while... gmake all
In-Reply-To: <4077e7710907162318u2827ac37nfa30067c082327e@mail.gmail.com>
References: <4077e7710907162318u2827ac37nfa30067c082327e@mail.gmail.com>
Message-ID: <1247890118.2876.71.camel@hp-laptop2.gunduz.org>

T24gRnJpLCAyMDA5LTA3LTE3IGF0IDExOjQ4ICswNTMwLCBTd2V0YSBNdWxnYXZrZXIgd3JvdGU6
Cj4gCj4gSSBhbSB0cnlpbmcgdG8gaW5zdGFsbCBzbG9ueTEtMS4xLjYgb24gYSBDZW50T1MgNSBt
YWNoaW5lIHdpdGgKPiBwb3N0Z3JlcyA4LjEuMTEuCgpodHRwOi8veXVtLnBnc3FscnBtcy5vcmcv
OC4xL3JlZGhhdC9yaGVsLTUteDg2XzY0LwpodHRwOi8veXVtLnBnc3FscnBtcy5vcmcvOC4xL3Jl
ZGhhdC9yaGVsLTUtaTM4NgoKaGFzIFNsb255LUkgMS4yLjE2IChsYXRlc3QpIHBhY2thZ2VzIGZv
ciBDZW50T1MgNSwgY29tcGlsZWQgYWdhaW5zdApQb3N0Z3JlU1FMIDguMS4gWW91IGNhbiB1c2Ug
dGhlbS4gSWYgeW91IHdhbnQgeXVtIHRvIGdyYWIgdGhvc2UKcGFja2FnZXMsIHRoZW4gcGxlYXNl
IGZvbGxvdyB0aGUgaW5zdHJ1Y3Rpb25zOgoKaHR0cDovL3l1bS5wZ3NxbHJwbXMub3JnL2hvd3Rv
eXVtLnBocAoKLS0gCkRldnJpbSBHw5xORMOcWiwgUkhDRQpDb21tYW5kIFByb21wdCAtIGh0dHA6
Ly93d3cuQ29tbWFuZFByb21wdC5jb20gCmRldnJpbX5ndW5kdXoub3JnLCBkZXZyaW1+UG9zdGdy
ZVNRTC5vcmcsIGRldnJpbS5ndW5kdXp+bGludXgub3JnLnRyCiAgICAgICAgICAgICAgICAgICBo
dHRwOi8vd3d3Lmd1bmR1ei5vcmcKLS0tLS0tLS0tLS0tLS0gbmV4dCBwYXJ0IC0tLS0tLS0tLS0t
LS0tCkEgbm9uLXRleHQgYXR0YWNobWVudCB3YXMgc2NydWJiZWQuLi4KTmFtZTogbm90IGF2YWls
YWJsZQpUeXBlOiBhcHBsaWNhdGlvbi9wZ3Atc2lnbmF0dXJlClNpemU6IDE5NyBieXRlcwpEZXNj
OiBUaGlzIGlzIGEgZGlnaXRhbGx5IHNpZ25lZCBtZXNzYWdlIHBhcnQKVXJsIDogaHR0cDovL2xp
c3RzLnNsb255LmluZm8vcGlwZXJtYWlsL3Nsb255MS1nZW5lcmFsL2F0dGFjaG1lbnRzLzIwMDkw
NzE4LzY0MmIyYzNmL2F0dGFjaG1lbnQucGdwCg==
From vivek at khera.org  Sat Jul 18 18:37:55 2009
From: vivek at khera.org (Vick Khera)
Date: Sat Jul 18 18:38:19 2009
Subject: [Slony1-general] Does Slony version 1.2.x work with postgresql 
	8.4?
In-Reply-To: <8a1bfe660907170632s2f221ed3v7800b947eff9f755@mail.gmail.com>
References: <20090713205919.GA26996@it.is.rice.edu>
	<87y6qrxkey.fsf@dba2.int.libertyrms.com>
	<8a1bfe660907170632s2f221ed3v7800b947eff9f755@mail.gmail.com>
Message-ID: <2968dfd60907181837i392c1127nabb4e3cf4564ca83@mail.gmail.com>

On Fri, Jul 17, 2009 at 9:32 AM, Laurent Laborde<kerdezixe@gmail.com> wrote:
> This is really a major problem.
> upgrade to slony-1 2.x require to rebuild the whole cluster.
> upgrade to pgsql-8.4 require to rebuild the whole database.
>

has anyone tried the pg_migrator to update an 8.3 to 8.4 on disk?
From gordo169 at gmail.com  Sun Jul 19 16:43:37 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Sun Jul 19 16:44:00 2009
Subject: [Slony1-general] How to downgrade from 2.0.2 to 2.0.1
In-Reply-To: <24541745.post@talk.nabble.com>
References: <24540512.post@talk.nabble.com>
	<1247860782.2876.66.camel@hp-laptop2.gunduz.org>
	<24540853.post@talk.nabble.com>
	<1247863771.10382.14641.camel@soundwave.ws.pitbpa0.priv.collaborativefusion.com>
	<24541745.post@talk.nabble.com>
Message-ID: <24561999.post@talk.nabble.com>




Gordon Shannon wrote:
> 
> 
> 
> Brian A. Seklecki-2 wrote:
>> 
>> Ughhh...who's RPMs are are you using?  Sig11 is a segfault, which could
>> mean bad patches in the code by the RPM builder, or mismatch libraries,
>> etc.
>> 
>> Run your slon(8) process via strace(8) and get us the output.  Also,
>> build a debug binary (COPTS/CFLAGS+=-g)
>> 
> 
> Here is the srpm we started with:
> http://yum.pgsqlrpms.org/srpms/8.4/fedora/fedora-11-i386/slony1-2.0.2-1.f11.src.rpm
> 
> We built it unmodified except for disable build of documentation. We
> confirmed that the downloaded source md5sum matches that of the source at:
> http://www.slony.info/downloads/2.0/source/
> 
> We are looking into the strace...  BTW, what do you mean by slon(8)?
> 
> Thx
> 
> 

I think I found the duplicate key problem.  The "log_actionseq" column in
the sl_log_1/2 tables is a bigint.  But the compress_actionseq() function in
remote_worker.c is working only with signed ints not longs.  So if a value
greater than 2,147,483,647 comes along, the value in curr_number will
overflow.

Here's the relevant debug info from the log:

14 0719 18:33:27 DEBUG4 compress_actionseq(list,subquery) Action list:
'4832430056','4832430057','4832430058','4832430059','4832430060','4832430061','4832430062','4832430063','4832430064','4832430065','4832430066','4832430067','4832430068','4832430069','4832430070','4832430071','4832430072','4832430073','4832430074','4832430075','4832430076','4832430077','4832430078','4832430079','4832430080','4832430081','4832430082','4832430083','4832430084','4832430085','4832430086','4832430087','4832430088','4832430089','4832430090','4832430091','4832430092','4832430093','4832430094','4832430095','4832430096','4832430097','4832430098','4832430099','4832430100','4832430101','4832430102','4832430103','4832430104','4832430105','4832430106','4832430107','4832430108','4832430109','4832430110','4832430111','4832430112','4832430113','4832430114','4832430115','4832430116','4832430117','4832430118','4832430119','4832430120','4832430121','4832430122','4832430123','4832430124','4832430125','4832430126','4832430127','4832430128','4832430129','4832430130','4832430131','4832430132','4832430133','4832430134','4832430135','4832430136','4832430137','4832430138','4832430139','4832430140','4832430141','4832430142','4832430143','4832430144','4832430145','4832430146','4832430147','4832430148','4832430149','4832430150','4832430151','4832430152','4832430153','4832430154','4832430155','4832430156','4832430157','4832430158','4832430159'
14 0719 18:33:27 DEBUG4 Finished number: 537462760
14 0719 18:33:27 DEBUG4 Finished number: 537462761
14 0719 18:33:27 DEBUG4 Finished number: 537462762
(...)
14 0719 18:33:27 DEBUG4 Finished number: 537462860
14 0719 18:33:27 DEBUG4 Finished number: 537462861
14 0719 18:33:27 DEBUG4 Finished number: 537462862
14 0719 18:33:27 DEBUG4 Finished number: 537462863
14 0719 18:33:27 DEBUG4 between entry - 537462760 537462863
14 0719 18:33:27 DEBUG4  compressed actionseq subquery...   log_actionseq
not between '537462760' and '537462863'

Note that 537462760 is what you get when truncate the 8-byte 4832430056 to a
4-byte integer. So essentially it trying to sync rows that already came over
in the subscription event.

Let me know if you need more details.

-Gordon


-- 
View this message in context: http://www.nabble.com/How-to-downgrade-from-2.0.2-to-2.0.1-tp24540512p24561999.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Sun Jul 19 21:29:09 2009
From: nettreeinc at gmail.com (Net Tree Inc.)
Date: Sun Jul 19 21:29:40 2009
Subject: [Slony1-general] Simple Question on Command HOW-TO: find out slony
	version I am using
Message-ID: <bd9689740907192129h1bc54b0dwf8ac967cf250e5b9@mail.gmail.com>

How do I find out the slony version I am currently using?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090720/=
0fa84bc6/attachment.htm
From nettreeinc at gmail.com  Sun Jul 19 21:30:59 2009
From: nettreeinc at gmail.com (Net Tree Inc.)
Date: Sun Jul 19 21:31:29 2009
Subject: [Slony1-general] General Features of Slony I 2.0.2
Message-ID: <bd9689740907192130v2f6f80fbrcafd2d296112a7c2@mail.gmail.com>

Could somebody send me general features (what Slony can do and can not do).
Or link me to where I can find such info.

Appreciated.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090720/=
a8363e5b/attachment.htm
From glynastill at yahoo.co.uk  Mon Jul 20 00:18:31 2009
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Mon Jul 20 00:19:05 2009
Subject: [Slony1-general] General Features of Slony I 2.0.2
Message-ID: <556883.6513.qm@web23606.mail.ird.yahoo.com>



> From: Net Tree Inc. <nettreeinc@gmail.com>
> Subject: [Slony1-general] General Features of Slony I 2.0.2
> To: slony1-general@lists.slony.info
> Date: Monday, 20 July, 2009, 5:30 AM
> Could somebody send me general
> features (what Slony can do and can not do).
> Or link me to where I can find such info. 
> ?

http://www.slony.info/documentation/slonyintro.html


      
From nettreeinc at gmail.com  Mon Jul 20 01:10:54 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Mon Jul 20 01:11:30 2009
Subject: [Slony1-general] Simple Question on Command HOW-TO: find out
	slony version I am using
In-Reply-To: <bd9689740907192129h1bc54b0dwf8ac967cf250e5b9@mail.gmail.com>
References: <bd9689740907192129h1bc54b0dwf8ac967cf250e5b9@mail.gmail.com>
Message-ID: <24565583.post@talk.nabble.com>


Also, once I started the slon engine. How do I stop it?

I was using pg_ctl=stop but message said : 

pg_ctl: PID file "/var/lib/pgsql/data/postmaster.pid" does not exist
Is postmaster running?

My other thought is, since I started the slon engine by giving command:
"slon sql_cluster "dbname=contactdb user=postgres"", I would assume I need
to stop it by using "slon something something" also. 

I am confusing environment between Slonik (slon script compiler?), slon (the
replciation engine) and postgresql itself.  Could anyone guide me on where I
can find more infomation about this? Also, commands to control the slon.

Thanks


roctaiwan wrote:
> 
> How do I find out the slony version I am currently using?
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/Simple-Question-on-Command-HOW-TO%3A-find-out-slony-version-I-am-using-tp24563764p24565583.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Mon Jul 20 01:25:23 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Mon Jul 20 01:25:59 2009
Subject: [Slony1-general] General Features of Slony I 2.0.2
In-Reply-To: <556883.6513.qm@web23606.mail.ird.yahoo.com>
References: <bd9689740907192130v2f6f80fbrcafd2d296112a7c2@mail.gmail.com>
	<556883.6513.qm@web23606.mail.ird.yahoo.com>
Message-ID: <24565726.post@talk.nabble.com>


Glyn, The link you provide, I had been there. But it doesn't list out
features on slony. I am writing evaluation paper on slony and needs to find
out what's so special on slony-I.  Any one know where to find it?





Glyn Astill wrote:
> 
> 
> 
>> From: Net Tree Inc. <nettreeinc@gmail.com>
>> Subject: [Slony1-general] General Features of Slony I 2.0.2
>> To: slony1-general@lists.slony.info
>> Date: Monday, 20 July, 2009, 5:30 AM
>> Could somebody send me general
>> features (what Slony can do and can not do).
>> Or link me to where I can find such info. 
>> ?
> 
> http://www.slony.info/documentation/slonyintro.html
> 
> 
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/General-Features-of-Slony-I-2.0.2-tp24563772p24565726.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Mon Jul 20 01:53:48 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Mon Jul 20 01:54:24 2009
Subject: [Slony1-general] fe_sendauth: no password supplied error
In-Reply-To: <87fxcwv0p1.fsf@dba2.int.libertyrms.com>
References: <24520233.post@talk.nabble.com>
	<87fxcwv0p1.fsf@dba2.int.libertyrms.com>
Message-ID: <24566079.post@talk.nabble.com>


Hi Christopher,

Thanks for the previous answer. It did work. 

but now I am trying to expanding this replication processing from two DB on
the same server to have two DBs on each different servers. Which I believe I
need to rewrite the cluster_setup.sh script. I assume I would need to stop
the clustering process on both DBs. But I am not sure how to. I tried pg_ctl
restart, and the result is : 
waiting for server to shut
down............................................................... failed
pg_ctl.bin: server does not shut down

I tried pg_ctl stop and same thing as last one. 

what would be the normal procedures to start the slon and to stop the slon,
oh and to restart the slon? I am new to Slony, parden me. 



Christopher Browne wrote:
> 
> roctaiwan <nettreeinc@gmail.com> writes:
>> http://www.nabble.com/file/p24520233/pg_hba.conf pg_hba.conf 
> 
>>From the pg_hba.conf contents, *any* connection requested should not
> bother asking for a password, which seems to contradict other things
> you mentioned.
> 
> Is it possible that the postmaster is still using former contents of
> pg_hba.conf that required passwords?
> 
> You might need to run:
>    pg_ctl reload
> or
>    /etc/init.d/postgresql reload
> or
>   submit a HUP signal to the postmaster process
>     kill -HUP ${PGPID}
> ?
> -- 
> output = reverse("ofni.sailifa.ac" "@" "enworbbc")
> <http://dba2.int.libertyrms.com/>
> Christopher Browne
> (416) 673-4124 (land)
> "Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
> phasers on the Heffalump, Piglet, meet me in transporter room three"
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/fe_sendauth%3A-no-password-supplied-error-tp24520233p24566079.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Mon Jul 20 02:03:49 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Mon Jul 20 02:04:26 2009
Subject: [Slony1-general] Simple Slony general commands HOW TO:
	start,restart, stop slon
Message-ID: <24566211.post@talk.nabble.com>


I am trying to expanding my experiment of testing the slony replication from
two DB on the same server to have two DBs on each different servers. Which I
believe I need to rewrite the cluster_setup.sh script. I assume I would need
to stop the clustering process on both DBs. But I am not sure how to. I
tried pg_ctl restart, and the result is : 
waiting for server to shut
down............................................................... failed
pg_ctl.bin: server does not shut down

I tried pg_ctl stop and same thing as last one. 

what would be the normal procedures to start the slon and to stop the slon,
oh and to restart the slon? I am new to Slony, parden me. 
-- 
View this message in context: http://www.nabble.com/Simple-Slony-general-commands-HOW-TO%3A-start%2Crestart%2C-stop-slon-tp24566211p24566211.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Mon Jul 20 02:08:59 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Mon Jul 20 02:09:36 2009
Subject: [Slony1-general] Could not start slon cluster
Message-ID: <24566272.post@talk.nabble.com>


I was trying to start my slon cluster and following messages says:


-bash-3.2$ slon sql_cluster "dbname=contactdb user=postgres"
2009-07-20 17:02:08 CST CONFIG main: slon version 1.2.15 starting up
2009-07-20 17:02:08 CST DEBUG2 slon: watchdog process started
2009-07-20 17:02:08 CST DEBUG2 slon: watchdog ready - pid = 12620
2009-07-20 17:02:08 CST FATAL  main: Cannot connect to local database -
FATAL:  the database system is shutting down
 - sleep 10s
2009-07-20 17:02:08 CST DEBUG2 slon: worker process created - pid = 12621
2009-07-20 17:02:18 CST DEBUG2 slon_retry() from pid=12621
2009-07-20 17:02:18 CST DEBUG1 slon: retry requested
2009-07-20 17:02:18 CST DEBUG2 slon: notify worker process to shutdown
2009-07-20 17:02:18 CST DEBUG2 slon: child terminated status: 0; pid: 12621,
current worker pid: 12621
2009-07-20 17:02:18 CST DEBUG1 slon: restart of worker
2009-07-20 17:02:18 CST CONFIG main: slon version 1.2.15 starting up
2009-07-20 17:02:18 CST DEBUG2 slon: watchdog process started
2009-07-20 17:02:18 CST DEBUG2 slon: watchdog ready - pid = 12620
2009-07-20 17:02:18 CST DEBUG2 slon: worker process created - pid = 12649
2009-07-20 17:02:18 CST FATAL  main: Cannot connect to local database -
FATAL:  the database system is shutting down
 - sleep 10s
2009-07-20 17:02:28 CST DEBUG2 slon_retry() from pid=12649
2009-07-20 17:02:28 CST DEBUG1 slon: retry requested
2009-07-20 17:02:28 CST DEBUG2 slon: notify worker process to shutdown
2009-07-20 17:02:28 CST DEBUG2 slon: child terminated status: 0; pid: 12649,
current worker pid: 12649
2009-07-20 17:02:28 CST DEBUG1 slon: restart of worker
2009-07-20 17:02:28 CST CONFIG main: slon version 1.2.15 starting up
2009-07-20 17:02:28 CST DEBUG2 slon: watchdog process started
2009-07-20 17:02:28 CST DEBUG2 slon: watchdog ready - pid = 12620
2009-07-20 17:02:28 CST DEBUG2 slon: worker process created - pid = 12651
2009-07-20 17:02:28 CST FATAL  main: Cannot connect to local database -
FATAL:  the database system is shutting down
 - sleep 10s
2009-07-20 17:02:33 CST DEBUG1 slon: shutdown requested
2009-07-20 17:02:33 CST DEBUG2 slon: notify worker process to shutdown
2009-07-20 17:02:33 CST DEBUG2 slon_retry() from pid=12651
2009-07-20 17:02:33 CST DEBUG1 slon: retry requested
2009-07-20 17:02:33 CST DEBUG2 slon: notify worker process to shutdown
2009-07-20 17:02:33 CST DEBUG1 slon: shutdown requested
2009-07-20 17:02:33 CST DEBUG2 slon: notify worker process to shutdown
2009-07-20 17:02:33 CST FATAL  main: write to worker pipe failed -(9) Bad
file descriptor
2009-07-20 17:02:33 CST DEBUG2 slon: exit(-1)
(I press ctrl+c to stop it)
-bash-3.2$


I seems like my server is shutting down? Does it mean my Postgresql or Slony
is shutting down? shutting down meaning it's not down yet and is still
trying to shut down? how can i fix it?



-- 
View this message in context: http://www.nabble.com/Could-not-start-slon-cluster-tp24566272p24566272.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From ian.lea at gmail.com  Mon Jul 20 02:16:22 2009
From: ian.lea at gmail.com (Ian Lea)
Date: Mon Jul 20 02:17:19 2009
Subject: [Slony1-general] Could not start slon cluster
In-Reply-To: <24566272.post@talk.nabble.com>
References: <24566272.post@talk.nabble.com>
Message-ID: <8c4e68610907200216k5ebac98bnfda5e6eb5894b484@mail.gmail.com>

You seem to be bombarding this list with similar messages!  Slow down ...

The message means that postgres is shutting down, not slony.

Fix that first, probably using pg_ctl.  $ pg_ctl --help shows the options.

When postgres is up and running, on both master and slave databases,
start your slony processes.

To stop slony, you can kill the slon processes.  Googling for "stop
slon" will find you more information.



--
Ian.


On Mon, Jul 20, 2009 at 10:08 AM, roctaiwan<nettreeinc@gmail.com> wrote:
>
> I was trying to start my slon cluster and following messages says:
>
>
> -bash-3.2$ slon sql_cluster "dbname=contactdb user=postgres"
> 2009-07-20 17:02:08 CST CONFIG main: slon version 1.2.15 starting up
> 2009-07-20 17:02:08 CST DEBUG2 slon: watchdog process started
> 2009-07-20 17:02:08 CST DEBUG2 slon: watchdog ready - pid = 12620
> 2009-07-20 17:02:08 CST FATAL ?main: Cannot connect to local database -
> FATAL: ?the database system is shutting down
> ?- sleep 10s
> 2009-07-20 17:02:08 CST DEBUG2 slon: worker process created - pid = 12621
> 2009-07-20 17:02:18 CST DEBUG2 slon_retry() from pid=12621
> 2009-07-20 17:02:18 CST DEBUG1 slon: retry requested
> 2009-07-20 17:02:18 CST DEBUG2 slon: notify worker process to shutdown
> 2009-07-20 17:02:18 CST DEBUG2 slon: child terminated status: 0; pid: 12621,
> current worker pid: 12621
> 2009-07-20 17:02:18 CST DEBUG1 slon: restart of worker
> 2009-07-20 17:02:18 CST CONFIG main: slon version 1.2.15 starting up
> 2009-07-20 17:02:18 CST DEBUG2 slon: watchdog process started
> 2009-07-20 17:02:18 CST DEBUG2 slon: watchdog ready - pid = 12620
> 2009-07-20 17:02:18 CST DEBUG2 slon: worker process created - pid = 12649
> 2009-07-20 17:02:18 CST FATAL ?main: Cannot connect to local database -
> FATAL: ?the database system is shutting down
> ?- sleep 10s
> 2009-07-20 17:02:28 CST DEBUG2 slon_retry() from pid=12649
> 2009-07-20 17:02:28 CST DEBUG1 slon: retry requested
> 2009-07-20 17:02:28 CST DEBUG2 slon: notify worker process to shutdown
> 2009-07-20 17:02:28 CST DEBUG2 slon: child terminated status: 0; pid: 12649,
> current worker pid: 12649
> 2009-07-20 17:02:28 CST DEBUG1 slon: restart of worker
> 2009-07-20 17:02:28 CST CONFIG main: slon version 1.2.15 starting up
> 2009-07-20 17:02:28 CST DEBUG2 slon: watchdog process started
> 2009-07-20 17:02:28 CST DEBUG2 slon: watchdog ready - pid = 12620
> 2009-07-20 17:02:28 CST DEBUG2 slon: worker process created - pid = 12651
> 2009-07-20 17:02:28 CST FATAL ?main: Cannot connect to local database -
> FATAL: ?the database system is shutting down
> ?- sleep 10s
> 2009-07-20 17:02:33 CST DEBUG1 slon: shutdown requested
> 2009-07-20 17:02:33 CST DEBUG2 slon: notify worker process to shutdown
> 2009-07-20 17:02:33 CST DEBUG2 slon_retry() from pid=12651
> 2009-07-20 17:02:33 CST DEBUG1 slon: retry requested
> 2009-07-20 17:02:33 CST DEBUG2 slon: notify worker process to shutdown
> 2009-07-20 17:02:33 CST DEBUG1 slon: shutdown requested
> 2009-07-20 17:02:33 CST DEBUG2 slon: notify worker process to shutdown
> 2009-07-20 17:02:33 CST FATAL ?main: write to worker pipe failed -(9) Bad
> file descriptor
> 2009-07-20 17:02:33 CST DEBUG2 slon: exit(-1)
> (I press ctrl+c to stop it)
> -bash-3.2$
>
>
> I seems like my server is shutting down? Does it mean my Postgresql or Slony
> is shutting down? shutting down meaning it's not down yet and is still
> trying to shut down? how can i fix it?
>
>
>
> --
> View this message in context: http://www.nabble.com/Could-not-start-slon-cluster-tp24566272p24566272.html
> Sent from the Slony-I -- General mailing list archive at Nabble.com.
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
From nettreeinc at gmail.com  Mon Jul 20 02:39:45 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Mon Jul 20 02:40:22 2009
Subject: [Slony1-general] Could not start slon cluster
In-Reply-To: <8c4e68610907200216k5ebac98bnfda5e6eb5894b484@mail.gmail.com>
References: <24566272.post@talk.nabble.com>
	<8c4e68610907200216k5ebac98bnfda5e6eb5894b484@mail.gmail.com>
Message-ID: <24566679.post@talk.nabble.com>


 sorry that it feels like I am bombarding the forum, I don't mean it. I just
trying to find a best way to get my question out. I was thinking to put
similar question under same topic that I had post, but I am afraid that
people won't see it if it's underneeth the parents topic. 

I think my problem are all very simple and very general, but I need someone
to help me where to find a site that has all the first hand information
without jumping around in different forums from different sites digging out
info. I did look though the slony docunment, but I don't find answers to my
questions... 




Ian Lea wrote:
> 
> You seem to be bombarding this list with similar messages!  Slow down ...
> 
> The message means that postgres is shutting down, not slony.
> 
> Fix that first, probably using pg_ctl.  $ pg_ctl --help shows the options.
> 
> When postgres is up and running, on both master and slave databases,
> start your slony processes.
> 
> To stop slony, you can kill the slon processes.  Googling for "stop
> slon" will find you more information.
> 
> 
> 
> --
> Ian.
> 
> 
> On Mon, Jul 20, 2009 at 10:08 AM, roctaiwan<nettreeinc@gmail.com> wrote:
>>
>> I was trying to start my slon cluster and following messages says:
>>
>>
>> -bash-3.2$ slon sql_cluster "dbname=contactdb user=postgres"
>> 2009-07-20 17:02:08 CST CONFIG main: slon version 1.2.15 starting up
>> 2009-07-20 17:02:08 CST DEBUG2 slon: watchdog process started
>> 2009-07-20 17:02:08 CST DEBUG2 slon: watchdog ready - pid = 12620
>> 2009-07-20 17:02:08 CST FATAL ?main: Cannot connect to local database -
>> FATAL: ?the database system is shutting down
>> ?- sleep 10s
>> 2009-07-20 17:02:08 CST DEBUG2 slon: worker process created - pid = 12621
>> 2009-07-20 17:02:18 CST DEBUG2 slon_retry() from pid=12621
>> 2009-07-20 17:02:18 CST DEBUG1 slon: retry requested
>> 2009-07-20 17:02:18 CST DEBUG2 slon: notify worker process to shutdown
>> 2009-07-20 17:02:18 CST DEBUG2 slon: child terminated status: 0; pid:
>> 12621,
>> current worker pid: 12621
>> 2009-07-20 17:02:18 CST DEBUG1 slon: restart of worker
>> 2009-07-20 17:02:18 CST CONFIG main: slon version 1.2.15 starting up
>> 2009-07-20 17:02:18 CST DEBUG2 slon: watchdog process started
>> 2009-07-20 17:02:18 CST DEBUG2 slon: watchdog ready - pid = 12620
>> 2009-07-20 17:02:18 CST DEBUG2 slon: worker process created - pid = 12649
>> 2009-07-20 17:02:18 CST FATAL ?main: Cannot connect to local database -
>> FATAL: ?the database system is shutting down
>> ?- sleep 10s
>> 2009-07-20 17:02:28 CST DEBUG2 slon_retry() from pid=12649
>> 2009-07-20 17:02:28 CST DEBUG1 slon: retry requested
>> 2009-07-20 17:02:28 CST DEBUG2 slon: notify worker process to shutdown
>> 2009-07-20 17:02:28 CST DEBUG2 slon: child terminated status: 0; pid:
>> 12649,
>> current worker pid: 12649
>> 2009-07-20 17:02:28 CST DEBUG1 slon: restart of worker
>> 2009-07-20 17:02:28 CST CONFIG main: slon version 1.2.15 starting up
>> 2009-07-20 17:02:28 CST DEBUG2 slon: watchdog process started
>> 2009-07-20 17:02:28 CST DEBUG2 slon: watchdog ready - pid = 12620
>> 2009-07-20 17:02:28 CST DEBUG2 slon: worker process created - pid = 12651
>> 2009-07-20 17:02:28 CST FATAL ?main: Cannot connect to local database -
>> FATAL: ?the database system is shutting down
>> ?- sleep 10s
>> 2009-07-20 17:02:33 CST DEBUG1 slon: shutdown requested
>> 2009-07-20 17:02:33 CST DEBUG2 slon: notify worker process to shutdown
>> 2009-07-20 17:02:33 CST DEBUG2 slon_retry() from pid=12651
>> 2009-07-20 17:02:33 CST DEBUG1 slon: retry requested
>> 2009-07-20 17:02:33 CST DEBUG2 slon: notify worker process to shutdown
>> 2009-07-20 17:02:33 CST DEBUG1 slon: shutdown requested
>> 2009-07-20 17:02:33 CST DEBUG2 slon: notify worker process to shutdown
>> 2009-07-20 17:02:33 CST FATAL ?main: write to worker pipe failed -(9) Bad
>> file descriptor
>> 2009-07-20 17:02:33 CST DEBUG2 slon: exit(-1)
>> (I press ctrl+c to stop it)
>> -bash-3.2$
>>
>>
>> I seems like my server is shutting down? Does it mean my Postgresql or
>> Slony
>> is shutting down? shutting down meaning it's not down yet and is still
>> trying to shut down? how can i fix it?
>>
>>
>>
>> --
>> View this message in context:
>> http://www.nabble.com/Could-not-start-slon-cluster-tp24566272p24566272.html
>> Sent from the Slony-I -- General mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/Could-not-start-slon-cluster-tp24566272p24566679.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From glynastill at yahoo.co.uk  Mon Jul 20 03:00:22 2009
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Mon Jul 20 03:01:01 2009
Subject: [Slony1-general] General Features of Slony I 2.0.2
Message-ID: <37156.78558.qm@web23601.mail.ird.yahoo.com>


Hi,

Basically slony is an asynchronous master-slave replication system, it can do cascading replication and also has a log-shipping mode.

This is implimented via triggers and a set of daemons that apply 
the changes in syncs.

I think you need to go and take a little time to read the docs on the slony homepage. It's all there, you just need to read it all ;)

Glyn

--- On Mon, 20/7/09, roctaiwan <nettreeinc@gmail.com> wrote:

> From: roctaiwan <nettreeinc@gmail.com>
> Subject: Re: [Slony1-general] General Features of Slony I 2.0.2
> To: slony1-general@lists.slony.info
> Date: Monday, 20 July, 2009, 9:25 AM
> 
> Glyn, The link you provide, I had been there. But it
> doesn't list out
> features on slony. I am writing evaluation paper on slony
> and needs to find
> out what's so special on slony-I.? Any one know where
> to find it?
> 
> 
> 
> 
> 
> Glyn Astill wrote:
> > 
> > 
> > 
> >> From: Net Tree Inc. <nettreeinc@gmail.com>
> >> Subject: [Slony1-general] General Features of
> Slony I 2.0.2
> >> To: slony1-general@lists.slony.info
> >> Date: Monday, 20 July, 2009, 5:30 AM
> >> Could somebody send me general
> >> features (what Slony can do and can not do).
> >> Or link me to where I can find such info. 
> >> ?
> > 
> > http://www.slony.info/documentation/slonyintro.html
> > 
> > 
> > 
> > _______________________________________________
> > Slony1-general mailing list
> > Slony1-general@lists.slony.info
> > http://lists.slony.info/mailman/listinfo/slony1-general
> > 
> > 
> 
> -- 
> View this message in context: http://www.nabble.com/General-Features-of-Slony-I-2.0.2-tp24563772p24565726.html
> Sent from the Slony-I -- General mailing list archive at
> Nabble.com.
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 


      
From devrim at gunduz.org  Mon Jul 20 05:36:33 2009
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Mon Jul 20 05:37:35 2009
Subject: [Slony1-general] Error while... gmake all
In-Reply-To: <4077e7710907200511l7d5b2654yc4e9206a6ca8be20@mail.gmail.com>
References: <4077e7710907162318u2827ac37nfa30067c082327e@mail.gmail.com>
	<1247890118.2876.71.camel@hp-laptop2.gunduz.org>
	<4077e7710907200511l7d5b2654yc4e9206a6ca8be20@mail.gmail.com>
Message-ID: <1248093393.14532.15.camel@hp-laptop2.gunduz.org>

T24gTW9uLCAyMDA5LTA3LTIwIGF0IDE3OjQxICswNTMwLCBTd2V0YSBNdWxnYXZrZXIgd3JvdGU6
Cj4gQnV0IEkgd2FudCB0byByZXBsaWNhdGUgZnJvbSBvbmUgbWFzdGVyIHRvIGFub3RoZXIuIElz
IGl0IHBvc3NpYmxlPz8KCk5vLCBpdCBpcyBub3QgcG9zc2libGUgd2l0aCBTbG9ueS1JLgotLSAK
RGV2cmltIEfDnE5Ew5xaLCBSSENFCkNvbW1hbmQgUHJvbXB0IC0gaHR0cDovL3d3dy5Db21tYW5k
UHJvbXB0LmNvbSAKZGV2cmltfmd1bmR1ei5vcmcsIGRldnJpbX5Qb3N0Z3JlU1FMLm9yZywgZGV2
cmltLmd1bmR1en5saW51eC5vcmcudHIKICAgICAgICAgICAgICAgICAgIGh0dHA6Ly93d3cuZ3Vu
ZHV6Lm9yZwotLS0tLS0tLS0tLS0tLSBuZXh0IHBhcnQgLS0tLS0tLS0tLS0tLS0KQSBub24tdGV4
dCBhdHRhY2htZW50IHdhcyBzY3J1YmJlZC4uLgpOYW1lOiBub3QgYXZhaWxhYmxlClR5cGU6IGFw
cGxpY2F0aW9uL3BncC1zaWduYXR1cmUKU2l6ZTogMTk3IGJ5dGVzCkRlc2M6IFRoaXMgaXMgYSBk
aWdpdGFsbHkgc2lnbmVkIG1lc3NhZ2UgcGFydApVcmwgOiBodHRwOi8vbGlzdHMuc2xvbnkuaW5m
by9waXBlcm1haWwvc2xvbnkxLWdlbmVyYWwvYXR0YWNobWVudHMvMjAwOTA3MjAvNzYxOTBmZjEv
YXR0YWNobWVudC5wZ3AK
From DSasikumar at intelligent-addressing.co.uk  Mon Jul 20 05:59:15 2009
From: DSasikumar at intelligent-addressing.co.uk (Don Sasikumar)
Date: Mon Jul 20 05:59:24 2009
Subject: [Slony1-general] Slon cluster of remote DB is not starting
In-Reply-To: <37156.78558.qm@web23601.mail.ird.yahoo.com>
References: <37156.78558.qm@web23601.mail.ird.yahoo.com>
Message-ID: <274BA771880F684D9CC7E88C5C29E9D9F40119@hermod.intelligent-addressing.co.uk>

Hi All,

When I run the following Slon command, I get this error

slon Master_Cluster "host=IP_address dbname=db_name user=postgres"

Error

2009-07-20 13:49:49 GMT Daylight Time CONFIG main: slon version 1.2.15
starting up
2009-07-20 13:49:50 GMT Daylight Time ERROR  Slony-I module version is
1.2.16
2009-07-20 13:49:50 GMT Daylight Time ERROR  please upgrade Slony-I
shared module to version 1.2.15
2009-07-20 13:49:50 GMT Daylight Time FATAL  main: Node has wrong
Slony-I schema or module version loaded

Could anyone please help me with this?

Thanks,
Don


Intelligent Addressing Ltd. Registered Office: 1 Adam Street, London WC2N 6DD Reg. in England No.3863861 VAT No.GB 802258946

This message has been scanned for viruses by BlackSpider MailControl - www.blackspider.com
From ian.lea at gmail.com  Mon Jul 20 06:18:49 2009
From: ian.lea at gmail.com (Ian Lea)
Date: Mon Jul 20 06:19:14 2009
Subject: [Slony1-general] Slon cluster of remote DB is not starting
In-Reply-To: <274BA771880F684D9CC7E88C5C29E9D9F40119@hermod.intelligent-addressing.co.uk>
References: <37156.78558.qm@web23601.mail.ird.yahoo.com>
	<274BA771880F684D9CC7E88C5C29E9D9F40119@hermod.intelligent-addressing.co.uk>
Message-ID: <8c4e68610907200618w1357cfag3297c97eb88f5c8@mail.gmail.com>

> ...
> 2009-07-20 13:49:49 GMT Daylight Time CONFIG main: slon version 1.2.15
> starting up
> 2009-07-20 13:49:50 GMT Daylight Time ERROR ?Slony-I module version is
> 1.2.16

Looks like a mismatch between the versions of Slony that you've got
installed, 1.2.15 vs 1.2.16. Make sure that you've got the same
version everywhere.


--
Ian.
From ian.lea at gmail.com  Mon Jul 20 06:38:39 2009
From: ian.lea at gmail.com (Ian Lea)
Date: Mon Jul 20 06:39:05 2009
Subject: [Slony1-general] Could not start slon cluster
In-Reply-To: <24566679.post@talk.nabble.com>
References: <24566272.post@talk.nabble.com>
	<8c4e68610907200216k5ebac98bnfda5e6eb5894b484@mail.gmail.com> 
	<24566679.post@talk.nabble.com>
Message-ID: <8c4e68610907200638m2113eec0w2355d878162eb42e@mail.gmail.com>

OK.  As someone else has said, there is loads of info on the Slony
site.  And elsewhere. Then if you still need to ask on this list,
please do.

The page at http://www.catb.org/~esr/faqs/smart-questions.html titled
"How To Ask Questions The Smart Way" is well worth a read.


Good luck.


--
Ian.


On Mon, Jul 20, 2009 at 10:39 AM, roctaiwan<nettreeinc@gmail.com> wrote:
>
> ?sorry that it feels like I am bombarding the forum, I don't mean it. I just
> trying to find a best way to get my question out. I was thinking to put
> similar question under same topic that I had post, but I am afraid that
> people won't see it if it's underneeth the parents topic.
>
> I think my problem are all very simple and very general, but I need someone
> to help me where to find a site that has all the first hand information
> without jumping around in different forums from different sites digging out
> info. I did look though the slony docunment, but I don't find answers to my
> questions...
>
>
>
>
> Ian Lea wrote:
>>
>> You seem to be bombarding this list with similar messages! ?Slow down ...
>>
>> The message means that postgres is shutting down, not slony.
>>
>> Fix that first, probably using pg_ctl. ?$ pg_ctl --help shows the options.
>>
>> When postgres is up and running, on both master and slave databases,
>> start your slony processes.
>>
>> To stop slony, you can kill the slon processes. ?Googling for "stop
>> slon" will find you more information.
>>
>>
>>
>> --
>> Ian.
>>
>>
>> On Mon, Jul 20, 2009 at 10:08 AM, roctaiwan<nettreeinc@gmail.com> wrote:
>>>
>>> I was trying to start my slon cluster and following messages says:
>>>
>>>
>>> -bash-3.2$ slon sql_cluster "dbname=contactdb user=postgres"
>>> 2009-07-20 17:02:08 CST CONFIG main: slon version 1.2.15 starting up
>>> 2009-07-20 17:02:08 CST DEBUG2 slon: watchdog process started
>>> 2009-07-20 17:02:08 CST DEBUG2 slon: watchdog ready - pid = 12620
>>> 2009-07-20 17:02:08 CST FATAL ?main: Cannot connect to local database -
>>> FATAL: ?the database system is shutting down
>>> ?- sleep 10s
>>> 2009-07-20 17:02:08 CST DEBUG2 slon: worker process created - pid = 12621
>>> 2009-07-20 17:02:18 CST DEBUG2 slon_retry() from pid=12621
>>> 2009-07-20 17:02:18 CST DEBUG1 slon: retry requested
>>> 2009-07-20 17:02:18 CST DEBUG2 slon: notify worker process to shutdown
>>> 2009-07-20 17:02:18 CST DEBUG2 slon: child terminated status: 0; pid:
>>> 12621,
>>> current worker pid: 12621
>>> 2009-07-20 17:02:18 CST DEBUG1 slon: restart of worker
>>> 2009-07-20 17:02:18 CST CONFIG main: slon version 1.2.15 starting up
>>> 2009-07-20 17:02:18 CST DEBUG2 slon: watchdog process started
>>> 2009-07-20 17:02:18 CST DEBUG2 slon: watchdog ready - pid = 12620
>>> 2009-07-20 17:02:18 CST DEBUG2 slon: worker process created - pid = 12649
>>> 2009-07-20 17:02:18 CST FATAL ?main: Cannot connect to local database -
>>> FATAL: ?the database system is shutting down
>>> ?- sleep 10s
>>> 2009-07-20 17:02:28 CST DEBUG2 slon_retry() from pid=12649
>>> 2009-07-20 17:02:28 CST DEBUG1 slon: retry requested
>>> 2009-07-20 17:02:28 CST DEBUG2 slon: notify worker process to shutdown
>>> 2009-07-20 17:02:28 CST DEBUG2 slon: child terminated status: 0; pid:
>>> 12649,
>>> current worker pid: 12649
>>> 2009-07-20 17:02:28 CST DEBUG1 slon: restart of worker
>>> 2009-07-20 17:02:28 CST CONFIG main: slon version 1.2.15 starting up
>>> 2009-07-20 17:02:28 CST DEBUG2 slon: watchdog process started
>>> 2009-07-20 17:02:28 CST DEBUG2 slon: watchdog ready - pid = 12620
>>> 2009-07-20 17:02:28 CST DEBUG2 slon: worker process created - pid = 12651
>>> 2009-07-20 17:02:28 CST FATAL ?main: Cannot connect to local database -
>>> FATAL: ?the database system is shutting down
>>> ?- sleep 10s
>>> 2009-07-20 17:02:33 CST DEBUG1 slon: shutdown requested
>>> 2009-07-20 17:02:33 CST DEBUG2 slon: notify worker process to shutdown
>>> 2009-07-20 17:02:33 CST DEBUG2 slon_retry() from pid=12651
>>> 2009-07-20 17:02:33 CST DEBUG1 slon: retry requested
>>> 2009-07-20 17:02:33 CST DEBUG2 slon: notify worker process to shutdown
>>> 2009-07-20 17:02:33 CST DEBUG1 slon: shutdown requested
>>> 2009-07-20 17:02:33 CST DEBUG2 slon: notify worker process to shutdown
>>> 2009-07-20 17:02:33 CST FATAL ?main: write to worker pipe failed -(9) Bad
>>> file descriptor
>>> 2009-07-20 17:02:33 CST DEBUG2 slon: exit(-1)
>>> (I press ctrl+c to stop it)
>>> -bash-3.2$
>>>
>>>
>>> I seems like my server is shutting down? Does it mean my Postgresql or
>>> Slony
>>> is shutting down? shutting down meaning it's not down yet and is still
>>> trying to shut down? how can i fix it?
>>>
>>>
>>>
>>> --
>>> View this message in context:
>>> http://www.nabble.com/Could-not-start-slon-cluster-tp24566272p24566272.html
>>> Sent from the Slony-I -- General mailing list archive at Nabble.com.
>>>
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general@lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>>
>
> --
> View this message in context: http://www.nabble.com/Could-not-start-slon-cluster-tp24566272p24566679.html
> Sent from the Slony-I -- General mailing list archive at Nabble.com.
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
>
From melvin6925 at yahoo.com  Mon Jul 20 06:49:00 2009
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Mon Jul 20 06:49:06 2009
Subject: [Slony1-general] Simple Question on Command HOW-TO: find out
	slony version I am using
Message-ID: <965050.48198.qm@web53001.mail.re2.yahoo.com>

To get the slony version, you would use SELECT slonyversion(); Perhaps if y=
ou checked the documentaion,=A0 http://slony.info/documentation/ you might =
find answers to more of your questions. =


To kill slony, try pkill slon on all servers running slony.

Melvin Davidson =



--- On Mon, 7/20/09, roctaiwan <nettreeinc@gmail.com> wrote:

From: roctaiwan <nettreeinc@gmail.com>
Subject: Re: [Slony1-general] Simple Question on Command HOW-TO: find out s=
lony version I am using
To: slony1-general@lists.slony.info
Date: Monday, July 20, 2009, 3:10 AM


Also, once I started the slon engine. How do I stop it?

I was using pg_ctl=3Dstop but message said : =


pg_ctl: PID file "/var/lib/pgsql/data/postmaster.pid" does not exist
Is postmaster running?

My other thought is, since I started the slon engine by giving command:
"slon sql_cluster "dbname=3Dcontactdb user=3Dpostgres"", I would assume I n=
eed
to stop it by using "slon something something" also. =


I am confusing environment between Slonik (slon script compiler?), slon (the
replciation engine) and postgresql itself.=A0 Could anyone guide me on wher=
e I
can find more infomation about this? Also, commands to control the slon.

Thanks


roctaiwan wrote:
> =

> How do I find out the slony version I am currently using?
> =

> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> =

> =


-- =

View this message in context: http://www.nabble.com/Simple-Question-on-Comm=
and-HOW-TO%3A-find-out-slony-version-I-am-using-tp24563764p24565583.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general



      =

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090720/=
7e22f9c6/attachment-0001.htm
From sweta.mulgavker at gmail.com  Mon Jul 20 05:11:27 2009
From: sweta.mulgavker at gmail.com (Sweta Mulgavker)
Date: Mon Jul 20 08:08:39 2009
Subject: [Slony1-general] Error while... gmake all
In-Reply-To: <1247890118.2876.71.camel@hp-laptop2.gunduz.org>
References: <4077e7710907162318u2827ac37nfa30067c082327e@mail.gmail.com>
	<1247890118.2876.71.camel@hp-laptop2.gunduz.org>
Message-ID: <4077e7710907200511l7d5b2654yc4e9206a6ca8be20@mail.gmail.com>

Hello all,

What changes in configuration are required so as to achieve master to master
replication using --> *slony1-1.2.16-1.rhel5.i386

*One way replication works fine :)

But I want to replicate from one master to another. Is it possible??

What changes should I make in the *replconfig.cnf*  file so that two way
replication can occur.

Thank you in advance.

Regards,
Sweta Mulgavker.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090720/=
da6dc755/attachment.htm
From DSasikumar at intelligent-addressing.co.uk  Mon Jul 20 08:09:48 2009
From: DSasikumar at intelligent-addressing.co.uk (Don Sasikumar)
Date: Mon Jul 20 08:10:34 2009
Subject: [Slony1-general] [Slony-I general] Node has wrong Slony-I schema or
	module version loaded
Message-ID: <274BA771880F684D9CC7E88C5C29E9D9F40182@hermod.intelligent-addressing.co.uk>

Hi All,

 

I am getting this error stating that my SlonyI schema version is
different from my Slon version and asking me to upgrade my Slony-I
schema from 1.2.15 to 1.2.16.

 

C:\Program Files\PostgreSQL\8.3\bin>slon Master_Cluster "host=IP
dbname=testdb user=postgres"

2009-07-20 16:03:19 GMT Daylight Time CONFIG main: slon version 1.2.16
starting up

2009-07-20 16:03:19 GMT Daylight Time ERROR  Slony-I schema version is
1.2.15

2009-07-20 16:03:19 GMT Daylight Time ERROR  please upgrade Slony-I
schema to version 1.2.16

2009-07-20 16:03:19 GMT Daylight Time FATAL  main: Node has wrong
Slony-I schema or module version loaded

 

Can you please let me the exact file names and their location for this
schema upgradation?

 

Thanks,

Don Sasikumar

 



Intelligent Addressing Ltd. Registered Office: 1 Adam Street, London WC2N 6DD Reg. in England No.3863861 VAT No.GB 802258946

This message has been scanned for viruses by BlackSpider MailControl - www.blackspider.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090720/9860f334/attachment.htm
From cbbrowne at ca.afilias.info  Mon Jul 20 08:25:53 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Jul 20 08:26:02 2009
Subject: [Slony1-general] Error while... gmake all
In-Reply-To: <4077e7710907200511l7d5b2654yc4e9206a6ca8be20@mail.gmail.com>
	(Sweta Mulgavker's message of "Mon, 20 Jul 2009 17:41:27 +0530")
References: <4077e7710907162318u2827ac37nfa30067c082327e@mail.gmail.com>
	<1247890118.2876.71.camel@hp-laptop2.gunduz.org>
	<4077e7710907200511l7d5b2654yc4e9206a6ca8be20@mail.gmail.com>
Message-ID: <87vdlntlz2.fsf@dba2.int.libertyrms.com>

Sweta Mulgavker <sweta.mulgavker@gmail.com> writes:
> 	What changes in configuration are required so as to achieve master to master replication using --> slony1-1.2.16-1.rhel5.i386
> 						      One way replication works fine :)
> 				     But I want to replicate from one master to another. Is it possible??
> 			What changes should I make in the replconfig.cnf? file so that two way replication can occur.

You'll need to design your own replication system.

Slony-I is not a multimaster replication system.
-- 
output = ("cbbrowne" "@" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Mon Jul 20 09:03:30 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Jul 20 09:03:38 2009
Subject: [Slony1-general] How to downgrade from 2.0.2 to 2.0.1
In-Reply-To: <24561999.post@talk.nabble.com> (Gordon Shannon's message of
	"Sun, 19 Jul 2009 16:43:37 -0700 (PDT)")
References: <24540512.post@talk.nabble.com>
	<1247860782.2876.66.camel@hp-laptop2.gunduz.org>
	<24540853.post@talk.nabble.com>
	<1247863771.10382.14641.camel@soundwave.ws.pitbpa0.priv.collaborativefusion.com>
	<24541745.post@talk.nabble.com> <24561999.post@talk.nabble.com>
Message-ID: <87r5wbtk8d.fsf@dba2.int.libertyrms.com>

Gordon Shannon <gordo169@gmail.com> writes:
> I think I found the duplicate key problem.  The "log_actionseq" column in
> the sl_log_1/2 tables is a bigint.  But the compress_actionseq() function in
> remote_worker.c is working only with signed ints not longs.  So if a value
> greater than 2,147,483,647 comes along, the value in curr_number will
> overflow.
>
> Here's the relevant debug info from the log:
>
> 14 0719 18:33:27 DEBUG4 compress_actionseq(list,subquery) Action list:
> '4832430056','4832430057','4832430058','4832430059','4832430060','4832430061','4832430062','4832430063','4832430064','4832430065','4832430066','4832430067','4832430068','4832430069','4832430070','4832430071','4832430072','4832430073','4832430074','4832430075','4832430076','4832430077','4832430078','4832430079','4832430080','4832430081','4832430082','4832430083','4832430084','4832430085','4832430086','4832430087','4832430088','4832430089','4832430090','4832430091','4832430092','4832430093','4832430094','4832430095','4832430096','4832430097','4832430098','4832430099','4832430100','4832430101','4832430102','4832430103','4832430104','4832430105','4832430106','4832430107','4832430108','4832430109','4832430110','4832430111','4832430112','4832430113','4832430114','4832430115','4832430116','4832430117','4832430118','4832430119','4832430120','4832430121','4832430122','4832430123','4832430124','4832430125','4832430126','4832430127','4832430128','4832430129','4832430130','4832430131','4832430132','4832430133','4832430134','4832430135','4832430136','4832430137','4832430138','4832430139','4832430140','4832430141','4832430142','4832430143','4832430144','4832430145','4832430146','4832430147','4832430148','4832430149','4832430150','4832430151','4832430152','4832430153','4832430154','4832430155','4832430156','4832430157','4832430158','4832430159'
> 14 0719 18:33:27 DEBUG4 Finished number: 537462760
> 14 0719 18:33:27 DEBUG4 Finished number: 537462761
> 14 0719 18:33:27 DEBUG4 Finished number: 537462762
> (...)
> 14 0719 18:33:27 DEBUG4 Finished number: 537462860
> 14 0719 18:33:27 DEBUG4 Finished number: 537462861
> 14 0719 18:33:27 DEBUG4 Finished number: 537462862
> 14 0719 18:33:27 DEBUG4 Finished number: 537462863
> 14 0719 18:33:27 DEBUG4 between entry - 537462760 537462863
> 14 0719 18:33:27 DEBUG4  compressed actionseq subquery...   log_actionseq
> not between '537462760' and '537462863'
>
> Note that 537462760 is what you get when truncate the 8-byte 4832430056 to a
> 4-byte integer. So essentially it trying to sync rows that already came over
> in the subscription event.
>
> Let me know if you need more details.

Good work, man!

I don't believe "long" is an acceptable data type here; on a 32 bit
platform, that's still just a 32 bit value.  We've got to go with
"long long" to guarantee a 64 bit type.

The following patch should resolve this, and I'll observe that this is
an issue for both 1.2 and 2.0 :-(.

Index: src/slon/remote_worker.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/remote_worker.c,v
retrieving revision 1.176.2.1
diff -c -u -r1.176.2.1 remote_worker.c
--- src/slon/remote_worker.c	17 Jun 2009 21:37:38 -0000	1.176.2.1
+++ src/slon/remote_worker.c	20 Jul 2009 16:01:01 -0000
@@ -3806,7 +3806,7 @@
 		res1 = PQexec(local_dbconn, dstring_data(&query));
 		monitor_subscriber_query(&pm);
 
-		slon_log(SLON_INFO, "about to monitor_subscriber_query - pulling big actionid list %d\n", provider);
+		slon_log(SLON_INFO, "about to monitor_subscriber_query - pulling big actionid list for %d\n", provider->no_id);
 
 		if (PQresultStatus(res1) != PGRES_TUPLES_OK)
 		{
@@ -5537,7 +5537,7 @@
 compress_actionseq(const char *ssy_actionlist, SlonDString *action_subquery)
 {
 	CompressState			state;
-	int			curr_number,
+	long long			curr_number,
 				curr_min,
 				curr_max;
 	int			curr_digit;
@@ -5685,7 +5685,7 @@
 				if (state == COLLECTING_DIGITS)
 				{
 					/* Finished another number... Fold it into the ranges... */
-					slon_log(SLON_DEBUG4, "Finished number: %d\n", curr_number);
+					slon_log(SLON_DEBUG4, "Finished number: %lld\n", curr_number);
 
 					/*
 					 * If we haven't a range, then the range is the current
@@ -5736,16 +5736,16 @@
 						}
 						if (curr_max == curr_min)
 						{
-							slon_log(SLON_DEBUG4, "simple entry - %d\n", curr_max);
+							slon_log(SLON_DEBUG4, "simple entry - %lld\n", curr_max);
 							slon_appendquery(action_subquery,
-										" log_actionseq <> '%d' ", curr_max);
+										" log_actionseq <> '%lld' ", curr_max);
 						}
 						else
 						{
-							slon_log(SLON_DEBUG4, "between entry - %d %d\n",
+							slon_log(SLON_DEBUG4, "between entry - %lld %lld\n",
 									 curr_min, curr_max);
 							slon_appendquery(action_subquery,
-								 " log_actionseq not between '%d' and '%d' ",
+								 " log_actionseq not between '%lld' and '%lld' ",
 											 curr_min, curr_max);
 						}
 						curr_min = curr_number;
@@ -5771,16 +5771,16 @@
 		}
 		if (curr_max == curr_min)
 		{
-			slon_log(SLON_DEBUG4, "simple entry - %d\n", curr_max);
+			slon_log(SLON_DEBUG4, "simple entry - %lld\n", curr_max);
 			slon_appendquery(action_subquery,
-							 " log_actionseq <> '%d' ", curr_max);
+							 " log_actionseq <> '%lld' ", curr_max);
 		}
 		else
 		{
-			slon_log(SLON_DEBUG4, "between entry - %d %d\n",
+			slon_log(SLON_DEBUG4, "between entry - %lld %lld\n",
 					 curr_min, curr_max);
 			slon_appendquery(action_subquery,
-							 " log_actionseq not between '%d' and '%d' ",
+							 " log_actionseq not between '%lld' and '%lld' ",
 							 curr_min, curr_max);
 		}
 

-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@" [name;tld];;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Mon Jul 20 09:08:46 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Jul 20 09:08:55 2009
Subject: [Slony1-general] Simple Question on Command HOW-TO: find out
	slony version I am using
In-Reply-To: <bd9689740907192129h1bc54b0dwf8ac967cf250e5b9@mail.gmail.com>
	(Net Tree's message of "Mon, 20 Jul 2009 12:29:09 +0800")
References: <bd9689740907192129h1bc54b0dwf8ac967cf250e5b9@mail.gmail.com>
Message-ID: <87my6ztjzl.fsf@dba2.int.libertyrms.com>

"Net Tree Inc." <nettreeinc@gmail.com> writes:
> How do I find out the slony version I am currently using?

chris@dba2:~/Slony-I/CMD/slony1-2.0> which slon
/home/chris/dbs/postgresql-8.3.3/bin/slon
chris@dba2:~/Slony-I/CMD/slony1-2.0> which slonik
/home/chris/dbs/postgresql-8.3.3/bin/slonik
chris@dba2:~/Slony-I/CMD/slony1-2.0> slon -v
slon version 2.0.3
chris@dba2:~/Slony-I/CMD/slony1-2.0> slonik -v
slonik version 2.0.3

Apparently I'm running 2.0.3 (not that it has necessarily been
released just yet ;-).
-- 
let name="cbbrowne" and tld="ca.afilias.info" in name ^ "@" ^ tld;;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From gordo169 at gmail.com  Mon Jul 20 10:06:25 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Mon Jul 20 10:06:36 2009
Subject: [Slony1-general] How to downgrade from 2.0.2 to 2.0.1
In-Reply-To: <87r5wbtk8d.fsf@dba2.int.libertyrms.com>
References: <24540512.post@talk.nabble.com>
	<1247860782.2876.66.camel@hp-laptop2.gunduz.org>
	<24540853.post@talk.nabble.com>
	<1247863771.10382.14641.camel@soundwave.ws.pitbpa0.priv.collaborativefusion.com>
	<24541745.post@talk.nabble.com> <24561999.post@talk.nabble.com>
	<87r5wbtk8d.fsf@dba2.int.libertyrms.com>
Message-ID: <24573745.post@talk.nabble.com>




Christopher Browne wrote:
> 
> The following patch should resolve this, and I'll observe that this is
> an issue for both 1.2 and 2.0 :-(.
> 

Thanks!  We're close but not quite there yet. I believe we'll also need to
patch dbutils.c, slon_appendquery_int() to handle the %lld. Otherwise we get
%lld's in the formatted output:

DEBUG4  compressed actionseq subquery...   log_actionseq not between '%lld'
and '%lld'

-gordon
-- 
View this message in context: http://www.nabble.com/How-to-downgrade-from-2.0.2-to-2.0.1-tp24540512p24573745.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From cbbrowne at ca.afilias.info  Mon Jul 20 12:32:44 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Mon Jul 20 12:32:58 2009
Subject: [Slony1-general] How to downgrade from 2.0.2 to 2.0.1
In-Reply-To: <24573745.post@talk.nabble.com> (Gordon Shannon's message of
	"Mon, 20 Jul 2009 10:06:25 -0700 (PDT)")
References: <24540512.post@talk.nabble.com>
	<1247860782.2876.66.camel@hp-laptop2.gunduz.org>
	<24540853.post@talk.nabble.com>
	<1247863771.10382.14641.camel@soundwave.ws.pitbpa0.priv.collaborativefusion.com>
	<24541745.post@talk.nabble.com> <24561999.post@talk.nabble.com>
	<87r5wbtk8d.fsf@dba2.int.libertyrms.com>
	<24573745.post@talk.nabble.com>
Message-ID: <87fxcrtajn.fsf@dba2.int.libertyrms.com>

Gordon Shannon <gordo169@gmail.com> writes:
> Christopher Browne wrote:
>> 
>> The following patch should resolve this, and I'll observe that this is
>> an issue for both 1.2 and 2.0 :-(.
>> 
>
> Thanks!  We're close but not quite there yet. I believe we'll also need to
> patch dbutils.c, slon_appendquery_int() to handle the %lld. Otherwise we get
> %lld's in the formatted output:
>
> DEBUG4  compressed actionseq subquery...   log_actionseq not between '%lld'
> and '%lld'

Good catch, again :-(.

Seems to me that in this case, it warrants us using a different
letter, say, "L".

Otherwise, slon_appendquery_int() is introduced some hairy, and
not-notably-useful-for-debugging, changes :-(.  (That is, making it
smart enough to recognize the bits of "%lld" doesn't actually make it
meaningfully intelligent...)
-- 
"cbbrowne","@","ca.afilias.info"
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From drees76 at gmail.com  Mon Jul 20 12:50:17 2009
From: drees76 at gmail.com (David Rees)
Date: Mon Jul 20 12:50:50 2009
Subject: [Slony1-general] Error while... gmake all
In-Reply-To: <1247890118.2876.71.camel@hp-laptop2.gunduz.org>
References: <4077e7710907162318u2827ac37nfa30067c082327e@mail.gmail.com> 
	<1247890118.2876.71.camel@hp-laptop2.gunduz.org>
Message-ID: <72dbd3150907201250g5bfd9befs711ef78eb9934a3f@mail.gmail.com>

2009/7/17 Devrim G?ND?Z <devrim@gunduz.org>:
> On Fri, 2009-07-17 at 11:48 +0530, Sweta Mulgavker wrote:
>>
>> I am trying to install slony1-1.1.6 on a CentOS 5 machine with
>> postgres 8.1.11.
>
> http://yum.pgsqlrpms.org/8.1/redhat/rhel-5-x86_64/
> http://yum.pgsqlrpms.org/8.1/redhat/rhel-5-i386
>
> has Slony-I 1.2.16 (latest) packages for CentOS 5, compiled against
> PostgreSQL 8.1. You can use them. If you want yum to grab those
> packages, then please follow the instructions:
>
> http://yum.pgsqlrpms.org/howtoyum.php

Perhaps the Slony download page [1] should be updated to point people
there for RPMs.  All the RPMs visible from the Slony site seem to be
very old.

-Dave

[1] http://slony.info/downloads/
From nettreeinc at gmail.com  Mon Jul 20 22:36:27 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Mon Jul 20 22:37:05 2009
Subject: [Slony1-general] Simple Question on Command HOW-TO: find out
	slony version I am using
In-Reply-To: <bd9689740907192129h1bc54b0dwf8ac967cf250e5b9@mail.gmail.com>
References: <bd9689740907192129h1bc54b0dwf8ac967cf250e5b9@mail.gmail.com>
Message-ID: <24581970.post@talk.nabble.com>


Thanks Melvin for this very helpful info. Every simple small command is a big
help to me.  ^_^


>How do I find out the status of my slon if it's currently on or off? 


To find out if slon is running:
ps -ef | grep slon
Also, check your slony log files.

You should be running slon in background, not foreground
Here is a sample startup command.

slon -d1 -pslon_node_1.pid $PTS_META_REP "dbname=$MASTERDBNAME
user=$REPLICATIONUSER host=$MASTERHOST port=$PGPORT" > slon1.log &

To find out if postgres is running:
ps -ef | grep postgres

Another simple test is:
psql postgres

If you can connect, it's running!

The questions you ask are actually basic Linux questions. I  highly
recommend you buy a Linux book and learn that first.


Melvin Davidson 

_______________________________________________
Slony1-general mailing list
Slony1-general@lists.slony.info
http://lists.slony.info/mailman/listinfo/slony1-general



-- 
View this message in context: http://www.nabble.com/Simple-Question-on-Command-HOW-TO%3A-find-out-slony-version-I-am-using-tp24563764p24581970.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Mon Jul 20 23:07:25 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Mon Jul 20 23:08:02 2009
Subject: [Slony1-general] Simple Question on Command HOW-TO: find out
	slony version I am using
In-Reply-To: <bd9689740907192129h1bc54b0dwf8ac967cf250e5b9@mail.gmail.com>
References: <bd9689740907192129h1bc54b0dwf8ac967cf250e5b9@mail.gmail.com>
Message-ID: <24582216.post@talk.nabble.com>


[root@Master-DB-Slony-I ~]# ps -ef | grep slon
postgres 19093     1  0 10:40 pts/2    00:00:00
/opt/PostgresPlus/8.3/bin/slon.bi        n -f master.slon
postgres 19094 19093  0 10:40 pts/2    00:00:00
/opt/PostgresPlus/8.3/bin/slon.bi        n -f master.slon
postgres 19138     1  0 10:41 pts/2    00:00:00
/opt/PostgresPlus/8.3/bin/slon.bi        n -f slave.slon
postgres 19139 19138  0 10:41 pts/2    00:00:01
/opt/PostgresPlus/8.3/bin/slon.bi        n -f slave.slon
root     26844 18122  0 14:00 pts/2    00:00:00 grep slon


could someone explain above log to me. Is above saying I started total 4
slon engine, two on master db and two on slave db? 
If I would like to kill the slon using kill slon or pkill slon and needs to
address which one to kill by including the PID. What are the PID numbers
should I kill to close all the slon process? 19093 and 19094 for master and
19138 and 19139 for slave?





roctaiwan wrote:
> 
> How do I find out the slony version I am currently using?
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/Simple-Question-on-Command-HOW-TO%3A-find-out-slony-version-I-am-using-tp24563764p24582216.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Mon Jul 20 23:54:24 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Mon Jul 20 23:55:04 2009
Subject: [Slony1-general] Could not start slon cluster
In-Reply-To: <8c4e68610907200216k5ebac98bnfda5e6eb5894b484@mail.gmail.com>
References: <24566272.post@talk.nabble.com>
	<8c4e68610907200216k5ebac98bnfda5e6eb5894b484@mail.gmail.com>
Message-ID: <24582647.post@talk.nabble.com>


Everytime I try to restart postgres it give me following messages:

-bash-3.2$ pg_ctl restar
pg_ctl: unrecognized operation mode "restar"
Try "pg_ctl --help" for more information.
-bash-3.2$ pg_ctl restart
pg_ctl: PID file "/var/lib/pgsql/data/postmaster.pid" does not exist
Is postmaster running?
starting postmaster anyway
pg_ctl: could not read file "/var/lib/pgsql/data/postmaster.opts"
-bash-3.2$


is that normal? Message says "starting postmaster anyway", does it still
restarting for me? 

Ian Lea wrote:
> 
> You seem to be bombarding this list with similar messages!  Slow down ...
> 
> The message means that postgres is shutting down, not slony.
> 
> Fix that first, probably using pg_ctl.  $ pg_ctl --help shows the options.
> 
> When postgres is up and running, on both master and slave databases,
> start your slony processes.
> 
> To stop slony, you can kill the slon processes.  Googling for "stop
> slon" will find you more information.
> 
> 
> 
> --
> Ian.
> 
> 
> On Mon, Jul 20, 2009 at 10:08 AM, roctaiwan<nettreeinc@gmail.com> wrote:
>>
>> I was trying to start my slon cluster and following messages says:
>>
>>
>> -bash-3.2$ slon sql_cluster "dbname=contactdb user=postgres"
>> 2009-07-20 17:02:08 CST CONFIG main: slon version 1.2.15 starting up
>> 2009-07-20 17:02:08 CST DEBUG2 slon: watchdog process started
>> 2009-07-20 17:02:08 CST DEBUG2 slon: watchdog ready - pid = 12620
>> 2009-07-20 17:02:08 CST FATAL ?main: Cannot connect to local database -
>> FATAL: ?the database system is shutting down
>> ?- sleep 10s
>> 2009-07-20 17:02:08 CST DEBUG2 slon: worker process created - pid = 12621
>> 2009-07-20 17:02:18 CST DEBUG2 slon_retry() from pid=12621
>> 2009-07-20 17:02:18 CST DEBUG1 slon: retry requested
>> 2009-07-20 17:02:18 CST DEBUG2 slon: notify worker process to shutdown
>> 2009-07-20 17:02:18 CST DEBUG2 slon: child terminated status: 0; pid:
>> 12621,
>> current worker pid: 12621
>> 2009-07-20 17:02:18 CST DEBUG1 slon: restart of worker
>> 2009-07-20 17:02:18 CST CONFIG main: slon version 1.2.15 starting up
>> 2009-07-20 17:02:18 CST DEBUG2 slon: watchdog process started
>> 2009-07-20 17:02:18 CST DEBUG2 slon: watchdog ready - pid = 12620
>> 2009-07-20 17:02:18 CST DEBUG2 slon: worker process created - pid = 12649
>> 2009-07-20 17:02:18 CST FATAL ?main: Cannot connect to local database -
>> FATAL: ?the database system is shutting down
>> ?- sleep 10s
>> 2009-07-20 17:02:28 CST DEBUG2 slon_retry() from pid=12649
>> 2009-07-20 17:02:28 CST DEBUG1 slon: retry requested
>> 2009-07-20 17:02:28 CST DEBUG2 slon: notify worker process to shutdown
>> 2009-07-20 17:02:28 CST DEBUG2 slon: child terminated status: 0; pid:
>> 12649,
>> current worker pid: 12649
>> 2009-07-20 17:02:28 CST DEBUG1 slon: restart of worker
>> 2009-07-20 17:02:28 CST CONFIG main: slon version 1.2.15 starting up
>> 2009-07-20 17:02:28 CST DEBUG2 slon: watchdog process started
>> 2009-07-20 17:02:28 CST DEBUG2 slon: watchdog ready - pid = 12620
>> 2009-07-20 17:02:28 CST DEBUG2 slon: worker process created - pid = 12651
>> 2009-07-20 17:02:28 CST FATAL ?main: Cannot connect to local database -
>> FATAL: ?the database system is shutting down
>> ?- sleep 10s
>> 2009-07-20 17:02:33 CST DEBUG1 slon: shutdown requested
>> 2009-07-20 17:02:33 CST DEBUG2 slon: notify worker process to shutdown
>> 2009-07-20 17:02:33 CST DEBUG2 slon_retry() from pid=12651
>> 2009-07-20 17:02:33 CST DEBUG1 slon: retry requested
>> 2009-07-20 17:02:33 CST DEBUG2 slon: notify worker process to shutdown
>> 2009-07-20 17:02:33 CST DEBUG1 slon: shutdown requested
>> 2009-07-20 17:02:33 CST DEBUG2 slon: notify worker process to shutdown
>> 2009-07-20 17:02:33 CST FATAL ?main: write to worker pipe failed -(9) Bad
>> file descriptor
>> 2009-07-20 17:02:33 CST DEBUG2 slon: exit(-1)
>> (I press ctrl+c to stop it)
>> -bash-3.2$
>>
>>
>> I seems like my server is shutting down? Does it mean my Postgresql or
>> Slony
>> is shutting down? shutting down meaning it's not down yet and is still
>> trying to shut down? how can i fix it?
>>
>>
>>
>> --
>> View this message in context:
>> http://www.nabble.com/Could-not-start-slon-cluster-tp24566272p24566272.html
>> Sent from the Slony-I -- General mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/Could-not-start-slon-cluster-tp24566272p24582647.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From jean-paul at postgresqlfr.org  Mon Jul 20 23:56:42 2009
From: jean-paul at postgresqlfr.org (Jean-Paul Argudo)
Date: Mon Jul 20 23:57:23 2009
Subject: [Slony1-general] Does Slony version 1.2.x work with postgresql
	8.4?
In-Reply-To: <8a1bfe660907170632s2f221ed3v7800b947eff9f755@mail.gmail.com>
References: <20090713205919.GA26996@it.is.rice.edu>	<87y6qrxkey.fsf@dba2.int.libertyrms.com>
	<8a1bfe660907170632s2f221ed3v7800b947eff9f755@mail.gmail.com>
Message-ID: <4A6566AA.5000204@postgresqlfr.org>

Hi there,

>> No, 1.2 does not work with 8.4, and barring having considerably more
>> time to play with, it doesn't seem like a straightforward backport.
>>
>> There were substantial changes in 8.4 to the implementation of
>> pg_listener, to change from on-table storage to an in-memory handling
>> that would fairly much break the whole listener loop.
>>
>> Version 2.0 revised that substantially; that doesn't seem like a
>> backport, though.
> 
> This is really a major problem.
> upgrade to slony-1 2.x require to rebuild the whole cluster.
> upgrade to pgsql-8.4 require to rebuild the whole database.

I agree with Laurent here.

> I cannot find a way to upgrade to 8.4 without innaceptable downtime
> unless we upgrade both psql and slony at once :(

Anyway, even upgrading both psql and slony at once will cause a big
downtime :-(

We'll have same problems, in many production environments for customers.

Cheers,

-- 
Jean-Paul Argudo
www.PostgreSQLFr.org
www.Dalibo.com
From ian.lea at gmail.com  Tue Jul 21 02:10:54 2009
From: ian.lea at gmail.com (Ian Lea)
Date: Tue Jul 21 02:11:59 2009
Subject: [Slony1-general] Could not start slon cluster
In-Reply-To: <24582647.post@talk.nabble.com>
References: <24566272.post@talk.nabble.com>
	<8c4e68610907200216k5ebac98bnfda5e6eb5894b484@mail.gmail.com> 
	<24582647.post@talk.nabble.com>
Message-ID: <8c4e68610907210210y15dc0bf5td54739506fbce7a3@mail.gmail.com>

No, it's not normal.  The "starting anyway" line is followed by an
error and it probably hasn't worked.  Run $ pg_ctl status and see what
it says.  If the server is running it will tell you.

It appears that something is wrong with your postgres installation.

This is a postgres problem, not Slony.  There are separate postgres
mailing lists.


--
Ian.


On Tue, Jul 21, 2009 at 7:54 AM, roctaiwan<nettreeinc@gmail.com> wrote:
>
> Everytime I try to restart postgres it give me following messages:
>
> -bash-3.2$ pg_ctl restar
> pg_ctl: unrecognized operation mode "restar"
> Try "pg_ctl --help" for more information.
> -bash-3.2$ pg_ctl restart
> pg_ctl: PID file "/var/lib/pgsql/data/postmaster.pid" does not exist
> Is postmaster running?
> starting postmaster anyway
> pg_ctl: could not read file "/var/lib/pgsql/data/postmaster.opts"
> -bash-3.2$
>
>
> is that normal? Message says "starting postmaster anyway", does it still
> restarting for me?
From melvin6925 at yahoo.com  Tue Jul 21 06:52:19 2009
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Tue Jul 21 06:52:28 2009
Subject: [Slony1-general] Could not start slon cluster
Message-ID: <101094.78071.qm@web53007.mail.re2.yahoo.com>

roctaiwan,

You really need to pay attention and look for yourself what the message is =
sayinig!
This is very simple.

> pg_ctl: unrecognized operation mode "restar"
The OPTION should be restart, not restar.=A0 So do =

pg_ctl restart=A0=A0=A0=A0=A0=A0=A0=A0=A0 and it will work.

Note that this had  nothing to do with slony. I highly suggest you subscrib=
e to =

http://archives.postgresql.org/pgsql-novice/ for these type of questions.

Melvin Davidson =

  =

From: roctaiwan <nettreeinc@gmail.com>
Subject: Re: [Slony1-general] Could not start slon cluster
To: slony1-general@lists.slony.info
Date: Tuesday, July 21, 2009, 1:54 AM


Everytime I try to restart postgres it give me following messages:

-bash-3.2$ pg_ctl restar
pg_ctl: unrecognized operation mode "restar"
Try "pg_ctl --help" for more information.
-bash-3.2$ pg_ctl restart
pg_ctl: PID file "/var/lib/pgsql/data/postmaster.pid" does not exist
Is postmaster running?
starting postmaster anyway
pg_ctl: could not read file "/var/lib/pgsql/data/postmaster.opts"
-bash-3.2$


is that normal? Message says "starting postmaster anyway", does it still
restarting for me? =


Ian Lea wrote:
> =

> You seem to be bombarding this list with similar messages!=A0 Slow down .=
..
> =

> The message means that postgres is shutting down, not slony.
> =

> Fix that first, probably using pg_ctl.=A0 $ pg_ctl --help shows the optio=
ns.
> =

> When postgres is up and running, on both master and slave databases,
> start your slony processes.
> =

> To stop slony, you can kill the slon processes.=A0 Googling for "stop
> slon" will find you more information.
> =

> =

> =

> --
> Ian.
> =





      =

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090721/=
d994926a/attachment-0001.htm
From cbbrowne at ca.afilias.info  Tue Jul 21 12:40:49 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul 21 12:41:12 2009
Subject: [Slony1-general] Simple Question on Command HOW-TO: find out
	slony version I am using
In-Reply-To: <24582216.post@talk.nabble.com> (roctaiwan's message of "Mon, 20
	Jul 2009 23:07:25 -0700 (PDT)")
References: <bd9689740907192129h1bc54b0dwf8ac967cf250e5b9@mail.gmail.com>
	<24582216.post@talk.nabble.com>
Message-ID: <87eis9su2l.fsf@dba2.int.libertyrms.com>

roctaiwan <nettreeinc@gmail.com> writes:
> [root@Master-DB-Slony-I ~]# ps -ef | grep slon
> postgres 19093     1  0 10:40 pts/2    00:00:00
> /opt/PostgresPlus/8.3/bin/slon.bi        n -f master.slon
> postgres 19094 19093  0 10:40 pts/2    00:00:00
> /opt/PostgresPlus/8.3/bin/slon.bi        n -f master.slon
> postgres 19138     1  0 10:41 pts/2    00:00:00
> /opt/PostgresPlus/8.3/bin/slon.bi        n -f slave.slon
> postgres 19139 19138  0 10:41 pts/2    00:00:01
> /opt/PostgresPlus/8.3/bin/slon.bi        n -f slave.slon
> root     26844 18122  0 14:00 pts/2    00:00:00 grep slon
>
>
> could someone explain above log to me. Is above saying I started total 4
> slon engine, two on master db and two on slave db? 

If this is Linux we're talking about, it is probable that this is in
fact just two processes, each with two threads.  

The slon process spawns a thread for each node with which it
communicates, and on Linux, threads are expressed as processes.

> If I would like to kill the slon using kill slon or pkill slon and needs to
> address which one to kill by including the PID. What are the PID numbers
> should I kill to close all the slon process? 19093 and 19094 for master and
> 19138 and 19139 for slave?

I suspect that processes 19094 and 19139 are "child threads"; if you
kill the parent processes, 19093 and 19138, that would also kill off
the child threads.  It is possible that killing the children would
also take out the parents; I'm not certain.
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Tue Jul 21 12:45:07 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul 21 12:45:28 2009
Subject: [Slony1-general] Could not start slon cluster
In-Reply-To: <8c4e68610907210210y15dc0bf5td54739506fbce7a3@mail.gmail.com>
	(Ian Lea's message of "Tue, 21 Jul 2009 10:10:54 +0100")
References: <24566272.post@talk.nabble.com>
	<8c4e68610907200216k5ebac98bnfda5e6eb5894b484@mail.gmail.com>
	<24582647.post@talk.nabble.com>
	<8c4e68610907210210y15dc0bf5td54739506fbce7a3@mail.gmail.com>
Message-ID: <87ab2xstvg.fsf@dba2.int.libertyrms.com>

Ian Lea <ian.lea@gmail.com> writes:
> No, it's not normal.  The "starting anyway" line is followed by an
> error and it probably hasn't worked.  Run $ pg_ctl status and see what
> it says.  If the server is running it will tell you.
>
> It appears that something is wrong with your postgres installation.
>
> This is a postgres problem, not Slony.  There are separate postgres
> mailing lists.

Actually, that seems a little like "driver error" to me...

If the postmaster was down, then asking it to "restart" is more than
was necessary.

chris@dba2:~/somewhere> pg_ctl status
pg_ctl: server is running (PID: 15270)
/home/chris/dbs/postgresql-8.4.0/bin/postgres
chris@dba2:~/somewhere> pg_ctl stop
waiting for server to shut down.... done
server stopped
chris@dba2:~/somewhere> pg_ctl restart
pg_ctl: PID file "/mnt/PostgreSQL/dbs/PG84/postgresql-8.4.0/postmaster.pid" does not exist
Is server running?
starting server anyway
server starting

The message doesn't need to indicate a problem with the PostgreSQL
installation; it nicely expresses the scenario characterized as "I
badly asked the postmaster to restart, and it politely told me what
kind of a bozo I was."

It's not a Slony-I issue, at any rate...
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Tue Jul 21 14:20:23 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul 21 14:20:48 2009
Subject: [Slony1-general] pg_autovacuum table gone in 8.4?
In-Reply-To: <24516041.post@talk.nabble.com> (Gordon Shannon's message of
	"Thu, 16 Jul 2009 06:10:19 -0700 (PDT)")
References: <24516041.post@talk.nabble.com>
Message-ID: <873a8pspgo.fsf@dba2.int.libertyrms.com>

Gordon Shannon <gordo169@gmail.com> writes:
> I'm trying to run Slony 2.0.2 with Postgres 8.4.  I'm getting this error:
>
> 14 0716 03:41:58 ERROR  cleanupThread: "select nspname, relname from
> "_slony_cluster".TablesToVacuum();" - ERROR:  relation
> "pg_catalog.pg_autovacuum" does not exist
> 14 0716 03:41:58 LINE 1: select enabled from "pg_catalog".pg_autovacuum
> where vacreli...
> 14 0716 03:41:58                             ^
> 14 0716 03:41:58 QUERY:  select enabled from "pg_catalog".pg_autovacuum
> where vacrelid =  $1 
> 14 0716 03:41:58 CONTEXT:  PL/pgSQL function "shouldslonyvacuumtable" line
> 25 at SQL statement
> 14 0716 03:41:58 PL/pgSQL function "tablestovacuum" line 6 at IF
>
> It appears they may have dropped the use of the pg_autovacuum table in 8.4
> in favor of table creation storage parameters.

Happily, when I "forked" the function into 8.3 and 8.4 versions, it
turns out that the API was perfectly fine for either; indeed, the
change to pull data from pg_class rather than pg_autovacuum is
relatively minor within the function.

I just committed that to 2.0+HEAD.
-- 
output = ("cbbrowne" "@" "linuxdatabases.info")
http://linuxfinances.info/info/linuxdistributions.html
"Why use Windows, since there is a door?"
-- <fachat@galileo.rhein-neckar.de> Andre Fachat
From gordo169 at gmail.com  Tue Jul 21 15:30:56 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Tue Jul 21 15:31:26 2009
Subject: [Slony1-general] How to downgrade from 2.0.2 to 2.0.1
In-Reply-To: <87fxcrtajn.fsf@dba2.int.libertyrms.com>
References: <24540512.post@talk.nabble.com>
	<1247860782.2876.66.camel@hp-laptop2.gunduz.org>
	<24540853.post@talk.nabble.com>
	<1247863771.10382.14641.camel@soundwave.ws.pitbpa0.priv.collaborativefusion.com>
	<24541745.post@talk.nabble.com> <24561999.post@talk.nabble.com>
	<87r5wbtk8d.fsf@dba2.int.libertyrms.com>
	<24573745.post@talk.nabble.com>
	<87fxcrtajn.fsf@dba2.int.libertyrms.com>
Message-ID: <24596980.post@talk.nabble.com>




Christopher Browne wrote:
> 
> Seems to me that in this case, it warrants us using a different
> letter, say, "L".
> 

I'm new to this, so I'm not sure of the protocol on these forums, or where
and if I should post this, but I humbly submit the patch I came up with that
combines your patch, and the suggested fix using the L character.  This is
working correctly for us now.  Please let me know the proper procedure as I
want to do the right thing here. :-)

--- src/slon/dbutils.c	2009-07-20 14:35:19.000000000 -0400
+++ src/slon/dbutils.c	2009-07-21 12:19:09.000000000 -0400
@@ -494,6 +494,12 @@
 						fmt++;
 						break;
 
+					case 'L':
+						sprintf(buf, "%lld", va_arg(ap, long long));
+						dstring_append(dsp, buf);
+						fmt++;
+						break;
+
 					default:
 						dstring_addchar(dsp, '%');
 						dstring_addchar(dsp, *fmt);
--- src/slon/remote_worker.c	2008-08-29 17:06:45.000000000 -0400
+++ src/slon/remote_worker.c	2009-07-21 12:19:00.000000000 -0400
@@ -3782,7 +3782,7 @@
 		res1 = PQexec(local_dbconn, dstring_data(&query));
 		monitor_subscriber_query(&pm);
 
-		slon_log(SLON_INFO, "about to monitor_subscriber_query - pulling big
actionid list %d\n", provider);
+	 slon_log(SLON_INFO, "about to monitor_subscriber_query - pulling big
actionid list for %d\n", provider->no_id); 
 
 		if (PQresultStatus(res1) != PGRES_TUPLES_OK)
 		{
@@ -5513,7 +5513,7 @@
 compress_actionseq(const char *ssy_actionlist, SlonDString
*action_subquery)
 {
 	CompressState			state;
-	int			curr_number,
+	long long	 curr_number, 
 				curr_min,
 				curr_max;
 	int			curr_digit;
@@ -5661,7 +5661,7 @@
 				if (state == COLLECTING_DIGITS)
 				{
 					/* Finished another number... Fold it into the ranges... */
-					slon_log(SLON_DEBUG4, "Finished number: %d\n", curr_number);
+	 slon_log(SLON_DEBUG4, "Finished number: %lld\n", curr_number); 
 
 					/*
 					 * If we haven't a range, then the range is the current
@@ -5712,16 +5712,16 @@
 						}
 						if (curr_max == curr_min)
 						{
-							slon_log(SLON_DEBUG4, "simple entry - %d\n", curr_max);
+	 slon_log(SLON_DEBUG4, "simple entry - %lld\n", curr_max); 
 							slon_appendquery(action_subquery,
-										" log_actionseq <> '%d' ", curr_max);
+	 " log_actionseq <> '%L' ", curr_max); 
 						}
 						else
 						{
-							slon_log(SLON_DEBUG4, "between entry - %d %d\n",
+	 slon_log(SLON_DEBUG4, "between entry - %lld %lld\n", 
 									 curr_min, curr_max);
 							slon_appendquery(action_subquery,
-								 " log_actionseq not between '%d' and '%d' ",
+	 " log_actionseq not between '%L' and '%L' ", 
 											 curr_min, curr_max);
 						}
 						curr_min = curr_number;
@@ -5747,17 +5747,15 @@
 		}
 		if (curr_max == curr_min)
 		{
-			slon_log(SLON_DEBUG4, "simple entry - %d\n", curr_max);
+	 slon_log(SLON_DEBUG4, "simple entry - %lld\n", curr_max); 
 			slon_appendquery(action_subquery,
-							 " log_actionseq <> '%d' ", curr_max);
+	 " log_actionseq <> '%L' ", curr_max); 
 		}
 		else
 		{
-			slon_log(SLON_DEBUG4, "between entry - %d %d\n",
+	 slon_log(SLON_DEBUG4, "between entry - %lld %lld\n", 
 					 curr_min, curr_max);
-			slon_appendquery(action_subquery,
-							 " log_actionseq not between '%d' and '%d' ",
-							 curr_min, curr_max);
+		slon_appendquery(action_subquery, " log_actionseq not between '%L' and
'%L' ", curr_min, curr_max);
 		}
 
 

-- 
View this message in context: http://www.nabble.com/How-to-downgrade-from-2.0.2-to-2.0.1-tp24540512p24596980.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From cbbrowne at ca.afilias.info  Tue Jul 21 15:46:02 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul 21 15:46:31 2009
Subject: [Slony1-general] Does Slony version 1.2.x work with postgresql
	8.4?
In-Reply-To: <8a1bfe660907170632s2f221ed3v7800b947eff9f755@mail.gmail.com>
	(Laurent Laborde's message of "Fri, 17 Jul 2009 15:32:15 +0200")
References: <20090713205919.GA26996@it.is.rice.edu>
	<87y6qrxkey.fsf@dba2.int.libertyrms.com>
	<8a1bfe660907170632s2f221ed3v7800b947eff9f755@mail.gmail.com>
Message-ID: <87vdllr6xh.fsf@dba2.int.libertyrms.com>

Laurent Laborde <kerdezixe@gmail.com> writes:
> On Tue, Jul 14, 2009 at 7:07 PM, Christopher
> Browne<cbbrowne@ca.afilias.info> wrote:
>> Kenneth Marshall <ktm@rice.edu> writes:
>>> In particular, I would like to be able to upgrade an
>>> 8.3 databae to 8.4 with minimal downtime. 1.2.14 does
>>> not build with a missing function definition for
>>> SerializableSnapshot. If not, is there a scheme to
>>> upgrade to 8.4 that does not entail the outage of
>>> a full dump/restore cycle?
>>
>> No, 1.2 does not work with 8.4, and barring having considerably more
>> time to play with, it doesn't seem like a straightforward backport.
>>
>> There were substantial changes in 8.4 to the implementation of
>> pg_listener, to change from on-table storage to an in-memory handling
>> that would fairly much break the whole listener loop.
>>
>> Version 2.0 revised that substantially; that doesn't seem like a
>> backport, though.
>
> This is really a major problem.
> upgrade to slony-1 2.x require to rebuild the whole cluster.
> upgrade to pgsql-8.4 require to rebuild the whole database.
>
> I cannot find a way to upgrade to 8.4 without innaceptable downtime
> unless we upgrade both psql and slony at once :(

In view of the "noise", I'm taking a look at how challenging it is to
get 8.4 support into the 1.2 branch...

In the initial "browse," I'm seeing two places that are affected by
SerializableSnapshot...

 - xxid.c has a couple of references.

          From the remedying that took place in the 2.0 branch, this
          looks not overly scary; if I augment autoconf to detect
          whether or not we have GetActiveSnapshot(), then the same
          strategy used in src/slony1_funcs.c looks like it should
          resolve this fairly easily.

 - slony1_funcs.c has rather more references.

          Some seem readily addressed as above.

          Unfortunately, the function _Slony_I_createEvent() seems to
          be quite substantially different between 1.2 and 2.0,
          relating to some things that do seem to refer to
          SerializableSnapshot...

    Notably, the following whole fragment is no longer in 2.0, and
    doesn't seem to "translate."

	/*
	 * Build the comma separated list of transactions in progress as Text
	 * datum.
	 */
	*(cp = buf) = '\0';
	for (xcnt = 0; xcnt < SerializableSnapshot->xcnt; xcnt++)
	{
		if ((cp + 30) >= (buf + buf_size))
		{
			buf_size *= 2;
			buf = repalloc(buf, buf_size);
			cp = buf + strlen(buf);
		}
		sprintf(cp, "%s'%u'", (xcnt > 0) ? "," : "",
				SerializableSnapshot->xip[xcnt]);
		cp += strlen(cp);
	}
	ev_xip = DatumGetTextP(DirectFunctionCall1(textin, PointerGetDatum(buf)));
	/*
	 * Call the saved INSERT plan
	 */
	argv[0] = TransactionIdGetDatum(SerializableSnapshot->xmin);
	argv[1] = TransactionIdGetDatum(SerializableSnapshot->xmax);
	argv[2] = PointerGetDatum(ev_xip);
	nulls[0] = ' ';
	nulls[1] = ' ';
	nulls[2] = ' ';

There's another pretty visible 8.4 issue...  src/slon/cleanup_thread.c
has a reference to pg_autovacuum.

I think that one will not be grossly difficult to work around; I
puzzled thru this today for the 2.0 branch, and it was not overly
troublesome.  It'll be uglier for 1.2, but it should not be horrible.

We now *do* have a strategy for getting a quick + clean upgrade from
1.2 to 2.0, once one is on 8.3 or 8.4.  But I fully agree with you
that there's an ugly amount of upgrading that the current version
upgrade choices forces onto you.

You can't have version 2.0 without being on 8.3 or 8.4; that's a
*given* :-).

But it seems like rather a bad thing to force someone who wants to
upgrade to 8.4 to *also* do a full cluster upgrade to 8.3 just because
Slony-I 1.2 doesn't support 8.4.  If it's not *grossly* difficult, 

Jan, can you comment particularly on the
slony1_funcs.c/SerializableSnapshot bits, please?
-- 
(reverse (concatenate 'string "ofni.secnanifxunil" "@" "enworbbc"))
http://linuxdatabases.info/info/lisp.html
"I support Microsoft's right to innovate.  I just wish they would make
use of that right." - Steve Shaw
From gordo169 at gmail.com  Tue Jul 21 15:49:44 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Tue Jul 21 15:50:11 2009
Subject: [Slony1-general] pg_autovacuum table gone in 8.4?
In-Reply-To: <873a8pspgo.fsf@dba2.int.libertyrms.com>
References: <24516041.post@talk.nabble.com>
	<873a8pspgo.fsf@dba2.int.libertyrms.com>
Message-ID: <24597190.post@talk.nabble.com>




Christopher Browne wrote:
> 
> Happily, when I "forked" the function into 8.3 and 8.4 versions, it
> turns out that the API was perfectly fine for either; indeed, the
> change to pull data from pg_class rather than pg_autovacuum is
> relatively minor within the function.
> 
> I just committed that to 2.0+HEAD.
> 

Cool.  My hope is that, since the slony package creates the slony tables,
that you will create them with no special autovacuum settings. That way, if
I have autovacuum turned on in my system, then autovacuum will vacuum all
Slony tables, rather than Slony doing it.  I just want to be sure that Slony
doesn't ever vacuum sl_log_1 or sl_log_2, since it's a waste of time as they
are truncated.  Please let me know if my assumptions is wrong.

Thanks!

-- 
View this message in context: http://www.nabble.com/pg_autovacuum-table-gone-in-8.4--tp24516041p24597190.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From aleksander.kmetec at intera.si  Tue Jul 21 16:43:52 2009
From: aleksander.kmetec at intera.si (Aleksander Kmetec)
Date: Tue Jul 21 16:44:01 2009
Subject: [Slony1-general] Re: Data loss in cleanupEvent() - with steps
	to reproduce!
In-Reply-To: <2968dfd60907140713y1912615co4496ad8f7c34a6b9@mail.gmail.com>
References: <4A5101DE.9060202@intera.si>
	<4A52A09E.7070609@intera.si>	<4A599345.2000100@intera.si>	<2968dfd60907130802x4d855928i38c17a6fcc741446@mail.gmail.com>	<4A5C5BF7.2090201@intera.si>
	<2968dfd60907140713y1912615co4496ad8f7c34a6b9@mail.gmail.com>
Message-ID: <4A6652B8.6040701@intera.si>

Hi,

I did some additional testing and I now have a way of reproducing data loss. It looks like the actual problem is in 
logswitch_finnish(); but telling cleanupEvent() to leave data around for a longer period prevents it form happening.


Follow these steps:

1. Open 2 psql terminals and connect to the master database with both.

2. Make sure there is no log switch in progress. When I did this both sl_log tables were empty.

3. In terminal 1 run:

BEGIN;
INSERT INTO sometable VALUES (...);

4. In terminal 2 run:

SELECT _clustername.logswitch_start();
SELECT _clustername.logswitch_finish();
-- (logswitch_finish() will just sit there waiting)

5. In terminal 1 run:

COMMIT;
-- (This will also cause logswitch_finish() in terminal 2 to complete)

6. Check both sl_log tables - they will be empty.
7. Check the table on slave node - the new row won't be there.

On my test setup I get data loss every time.

-----------------------------------------------------------------------
WARNING: I'm just blindly guessing here. Do things even work that way?

What I think may be happening :
- my transaction starts
- logswitch_finnish() is called
- there are no visible old rows around which logswitch_finnish() could detect and determine it should not truncate 
sl_log. My transaction is generating new rows at this time, but they are not visible to logswitch_finnish().
- logswitch_finnish() executes a TRUNCATE statement. This statement just sits there waiting for a lock on sl_log.
- my tranaction commits
- truncate gets a lock on sl_log and immediately destroys all the rows generated by my transaction.

I had cleanup_interval set to 1 minute during testing but it didn't seem to affect the results - transactions lasting 
only 20 seconds were also lost. The reason why setting cleanup_interval to 6 hours made this problem go away on our 
production cluster could be that this made some statements from up to 6 hours ago visible to logswitch_finnish() and it 
knew it shouldn't truncate the log tables.

/End blind guessing.
-----------------------------------------------------------------------

Does any of this make sense?

Regards,
Aleksander


From aleksander.kmetec at intera.si  Tue Jul 21 18:47:37 2009
From: aleksander.kmetec at intera.si (Aleksander Kmetec)
Date: Tue Jul 21 18:47:49 2009
Subject: [Slony1-general] Inserts into sl_log tables timing out (related to
	the dataloss bug)
Message-ID: <4A666FB9.7090205@intera.si>

Hi,

there's another bug I've been seeing from time to time, but was unable to reproduce it until today.

We have statement_timeout set to 10 seconds and we would sometimes get timeouts on extremely simple statements, like 
single-row inserts. I believe this is caused by a waiting TRUNCATE in logswitch_finish() blocking writes to sl_log.

To reproduce:

1. follow steps 1 to 4 as described here:
http://lists.slony.info/pipermail/slony1-general/2009-July/009663.html

2. Instead of committing in terminal 1, open a third instance of psql and run:

SET statement_timeout TO '3s';
INSERT ... -- (any table which has a logtrigger will do)

You should get the following error:
ERROR:  canceling statement due to statement timeout
CONTEXT:  SQL statement "INSERT INTO _clustername.sl_log_1 (log_origin, log_txid, log_tableid, log_actionseq, 
log_cmdtype, log_cmddata) VALUES (1, "pg_catalog".txid_current(), $1, nextval('_clustername.sl_action_seq'), $2, $3); "


Please let me know if there's something I can do to help fix these problems.

Regards,
Aleksander

From nettreeinc at gmail.com  Tue Jul 21 20:44:00 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Jul 21 20:44:36 2009
Subject: [Slony1-general] [Quick question] Slony configuration on
	postgresql.conf and pg_hba.conf
Message-ID: <24599592.post@talk.nabble.com>


I know we all required to configurate the postgresql.conf and pg_hba.conf for
linux to work properly with Slony. But I found that each of the conf files
located at three different places.... which conf should be configurated, all
three? is this same as any other system that has slony and postgresql?


[root@Slave-DB1-Slony-I ~]# find / -name postgresql.conf
/usr/local/pgsql/data/postgresql.conf
/var/lib/pgsql/data/postgresql.conf
/opt/PostgresPlus/8.3/data/postgresql.conf
[root@Slave-DB1-Slony-I ~]#
[root@Slave-DB1-Slony-I ~]# find / -name pg_hba.conf
/usr/local/pgsql/data/pg_hba.conf
/var/lib/pgsql/pg_hba.conf
/var/lib/pgsql/data/pg_hba.conf
/opt/PostgresPlus/8.3/data/pg_hba.conf
[root@Slave-DB1-Slony-I ~]#



-- 
View this message in context: http://www.nabble.com/-Quick-question--Slony-configuration-on-postgresql.conf-and-pg_hba.conf-tp24599592p24599592.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Tue Jul 21 20:48:09 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Jul 21 20:48:44 2009
Subject: [Slony1-general] [Quick question] Slony configuration on
	postgresql.conf and pg_hba.conf
In-Reply-To: <24599592.post@talk.nabble.com>
References: <24599592.post@talk.nabble.com>
Message-ID: <24599634.post@talk.nabble.com>


correction. it seems like my pg_hba.conf are located at 4 places. 
it must has somthing to do with the initialization steps I did following a
article on web. (usr/bin/initdb)

This makes me think. to do initdb on /data folder is this a MUST do steps or
it's just not necessary? because many articles on well know websites didn't
even mention this step. I wonder what this is for.



roctaiwan wrote:
> 
> I know we all required to configurate the postgresql.conf and pg_hba.conf
> for linux to work properly with Slony. But I found that each of the conf
> files located at three different places.... which conf should be
> configurated, all three? is this same as any other system that has slony
> and postgresql?
> 
> 
> [root@Slave-DB1-Slony-I ~]# find / -name postgresql.conf
> /usr/local/pgsql/data/postgresql.conf
> /var/lib/pgsql/data/postgresql.conf
> /opt/PostgresPlus/8.3/data/postgresql.conf
> [root@Slave-DB1-Slony-I ~]#
> [root@Slave-DB1-Slony-I ~]# find / -name pg_hba.conf
> /usr/local/pgsql/data/pg_hba.conf
> /var/lib/pgsql/pg_hba.conf
> /var/lib/pgsql/data/pg_hba.conf
> /opt/PostgresPlus/8.3/data/pg_hba.conf
> [root@Slave-DB1-Slony-I ~]#
> 
> 
> 
> 

-- 
View this message in context: http://www.nabble.com/-Quick-question--Slony-configuration-on-postgresql.conf-and-pg_hba.conf-tp24599592p24599634.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Tue Jul 21 20:51:48 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Jul 21 20:52:22 2009
Subject: [Slony1-general] Could not start slon cluster
In-Reply-To: <101094.78071.qm@web53007.mail.re2.yahoo.com>
References: <24566272.post@talk.nabble.com>
	<101094.78071.qm@web53007.mail.re2.yahoo.com>
Message-ID: <24599666.post@talk.nabble.com>


Ok, I will post this type of issue to different mail list. 


> pg_ctl: unrecognized operation mode "restar"
The OPTION should be restart, not restar.? So do 
pg_ctl restart????????? and it will work.

Note that this had  nothing to do with slony. I highly suggest you subscribe
to 
http://archives.postgresql.org/pgsql-novice/ for these type of questions.

Melvin Davidson 
 
-- 
View this message in context: http://www.nabble.com/Could-not-start-slon-cluster-tp24566272p24599666.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Tue Jul 21 21:18:14 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Jul 21 21:18:50 2009
Subject: [Slony1-general] Could not start slon cluster
In-Reply-To: <24566272.post@talk.nabble.com>
References: <24566272.post@talk.nabble.com>
Message-ID: <24599840.post@talk.nabble.com>


It's related with slon replication issue.

I follow steps of an article on  http://www.linuxjournal.com/article/7834 
http://www.linuxjournal.com/article/7834   to testing my first slony
replication, which I believe many new slony users would of been there. They
besically have two scripts that needs to run to setup the replication
envrionment and to subscribing to the set to second node ask it to
replicating. First script is called cluster_setup.sh and the other is called
subscribe.sh. My question is. The case of that article is replicating two
DBs on the same machine (localhost). If I would like to replicating two DBs
on two different machine, whats the steps for me to run these two scripts?
Do I run both scripts on master machine, should I run cluster_setup.sh on
master and subscribe.sh on slave or should I run both scripts on both
machine??

On top of that, Since I am not a coder, does anyone know where I can get
more sample scripts if I would like to run multiple slaves on different
machines, maybe 10 slaves, maybe 6 slaves? where are good locations? 

has anyone had experience run slony-I that has storage attached to it,
commnuicating through private IP addresses? how should I get scripts for
this type of envrionment? 

http://www.nabble.com/file/p24599840/slaves%2Bwith%2Bstorage.jpg 
-- 
View this message in context: http://www.nabble.com/Could-not-start-slon-cluster-tp24566272p24599840.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From guillaume at lelarge.info  Wed Jul 22 00:06:23 2009
From: guillaume at lelarge.info (Guillaume Lelarge)
Date: Wed Jul 22 00:07:07 2009
Subject: [Slony1-general] [Quick question] Slony configuration on
	postgresql.conf and pg_hba.conf
In-Reply-To: <24599634.post@talk.nabble.com>
References: <24599592.post@talk.nabble.com> <24599634.post@talk.nabble.com>
Message-ID: <200907220906.24071.guillaume@lelarge.info>

Le mercredi 22 juillet 2009 ? 05:48:09, roctaiwan a ?crit :
> correction. it seems like my pg_hba.conf are located at 4 places.
> it must has somthing to do with the initialization steps I did following a
> article on web. (usr/bin/initdb)
>
> This makes me think. to do initdb on /data folder is this a MUST do steps
> or it's just not necessary? because many articles on well know websites
> didn't even mention this step. I wonder what this is for.
>

The initdb step creates the PostgreSQL cluster, ie the directory where 
PostgreSQL puts all the data files, transactions logs files and configuration 
files.

I think you used an RPM package, which launch the initdb step with the 
directory /var/lib/pgsql/data when you use "/etc/init.d/postgresql initdb". 
You probably tried on your own to create the /usr/local/pgsql/data cluster 
directory. And you used the Enterprise DB one-click installer for the 
/opt/PostgresPlus/8.3/data directory.

You only need one data directory. I think it would be better to keep the one 
generated by the RPM package.

Regards.


-- 
Guillaume.
 http://www.postgresqlfr.org
 http://dalibo.com
From nettreeinc at gmail.com  Wed Jul 22 00:34:59 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Wed Jul 22 00:35:41 2009
Subject: [Slony1-general] slon engine pointing to the wrong directory
Message-ID: <24601412.post@talk.nabble.com>


I have 2 slaves and one master replication system. One of the slon on my
slave is running the "postgres" in /usr/bin/postgres. (see below). 

-bash-3.2$ pg_ctl status
pg_ctl: postmaster is running (PID: 15368)
/usr/bin/postgres

But my master db which I know is working find is using different postgres,
the one at /opt/PostgresPlus/8.3/bin/postgres.bin


[root@Master-DB-Slony-I data]# su - postgres
-bash-3.2$ pg_ctl status
pg_ctl.bin: server is running (PID: 31493)
/opt/PostgresPlus/8.3/bin/postgres.bin

But I don't think the slave turn on the one in /usr/bin/postgres is correct,
because when I giving commands such as create user, createdb etc it's always
saying it's already exist. But when I use pgAdmin III to access it, it's
nothing there... 

Know how to fix it? 

-- 
View this message in context: http://www.nabble.com/slon-engine-pointing-to-the-wrong-directory-tp24601412p24601412.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From cscetbon.ext at orange-ftgroup.com  Wed Jul 22 05:23:28 2009
From: cscetbon.ext at orange-ftgroup.com (Cyril Scetbon)
Date: Wed Jul 22 05:23:33 2009
Subject: [Slony1-general] Failover hangs in 2.0.2
In-Reply-To: <7108ABFD-9BC6-430F-9045-2A7839301046@richyen.com>
References: <7108ABFD-9BC6-430F-9045-2A7839301046@richyen.com>
Message-ID: <4A6704C0.8090206@orange-ftgroup.com>

Did you tried with slony 1.2 ? Are there any differences in the code of 
failover or drop node function ?

Richard Yen wrote:
> Hi,
>
> I've been trying to get failover to work in 2.0.2, but it seems to hang.
>
> I have a 3-node architecture, and have tried the instructions, per 
> http://www.slony.info/documentation/failover.html#COMPLEXFAILOVER
>
> Here's how I do it (node 1 is provider, and node 2 is failover node):
>    -- subscribe node 3 to node 2
>    -- execute FAILOVER
>    -- slonik hangs
>
> If I go into node 2 and to and look at sl_subscribe, there is only one 
> row with provider=2, subscriber=3 (which is correct and expected).  
> However, looking at sl_status, looks like everything is running just 
> fine (sl_event_lag and sl_time_lag go up and down, as if there's 
> activity).  HOWEVER, if I do an update on node 2, the update never 
> makes it to node 3.  (Node 1 still says provider=1, subscriber=2 AND 
> provider=2, subscriber=3)
>
> slonik is still running/hanging during all this.
>
> if I strace the slonik process, I find the following:
>
> ======BEGIN STRACE======
> rt_sigprocmask(SIG_BLOCK, [PIPE], [], 8) = 0
> sendto(3, "Q\0\0\0\30begin transaction; \0"..., 25, 0, NULL, 0) = 25
> rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
> poll([{fd=3, events=POLLIN|POLLERR}], 1, -1) = 1 ([{fd=3, 
> revents=POLLIN}])
> recvfrom(3, "C\0\0\0\nBEGIN\0Z\0\0\0\5T"..., 16384, 0, NULL, NULL) = 17
> rt_sigprocmask(SIG_BLOCK, [PIPE], [], 8) = 0
> sendto(3, "Q\0\0\0Wselect nl_backendpid from 
> \"_sltest\".sl_nodelock     where nl_backendpid <> 28927; \0"..., 88, 
> 0, NULL, 0) = 88
> rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
> poll([{fd=3, events=POLLIN|POLLERR}], 1, -1) = 1 ([{fd=3, 
> revents=POLLIN}])
> recvfrom(3, 
> "T\0\0\0&\0\1nl_backendpid\0\304\27Dn\0\3\0\0\0\27\0\4\377\377\377\377\0\0D\0\0\0\17\0\1\0\0\0\00529006D\0\0\0\17\0\1\0\0\0\00529011D\0\0\0\17\0\1\0\0\0\00529012C\0\0\0\vSELECT\0Z\0\0\0\5T"..., 
> 16384, 0, NULL, NULL) = 105
> rt_sigprocmask(SIG_BLOCK, [PIPE], [], 8) = 0
> sendto(3, "Q\0\0\0\32rollback transaction;\0"..., 27, 0, NULL, 0) = 27
> rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
> poll([{fd=3, events=POLLIN|POLLERR}], 1, -1) = 1 ([{fd=3, 
> revents=POLLIN}])
> recvfrom(3, "C\0\0\0\rROLLBACK\0Z\0\0\0\5I"..., 16384, 0, NULL, NULL) 
> = 20
> rt_sigprocmask(SIG_BLOCK, [PIPE], [], 8) = 0
> sendto(4, "Q\0\0\0\30begin transaction; \0"..., 25, 0, NULL, 0) = 25
> rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
> poll([{fd=4, events=POLLIN|POLLERR}], 1, -1) = 1 ([{fd=4, 
> revents=POLLIN}])
> recvfrom(4, "C\0\0\0\nBEGIN\0Z\0\0\0\5T"..., 16384, 0, NULL, NULL) = 17
> rt_sigprocmask(SIG_BLOCK, [PIPE], [], 8) = 0
> sendto(4, "Q\0\0\0Wselect nl_backendpid from 
> \"_sltest\".sl_nodelock     where nl_backendpid <> 16155; \0"..., 88, 
> 0, NULL, 0) = 88
> rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
> poll([{fd=4, events=POLLIN|POLLERR}], 1, -1) = 1 ([{fd=4, 
> revents=POLLIN}])
> recvfrom(4, 
> "T\0\0\0&\0\1nl_backendpid\0\0\1\"\203\0\3\0\0\0\27\0\4\377\377\377\377\0\0D\0\0\0\17\0\1\0\0\0\00517510D\0\0\0\17\0\1\0\0\0\00517511C\0\0\0\vSELECT\0Z\0\0\0\5T"..., 
> 16384, 0, NULL, NULL) = 89
> rt_sigprocmask(SIG_BLOCK, [PIPE], [], 8) = 0
> sendto(4, "Q\0\0\0\32rollback transaction;\0"..., 27, 0, NULL, 0) = 27
> rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
> poll([{fd=4, events=POLLIN|POLLERR}], 1, -1) = 1 ([{fd=4, 
> revents=POLLIN}])
> recvfrom(4, "C\0\0\0\rROLLBACK\0Z\0\0\0\5I"..., 16384, 0, NULL, NULL) 
> = 20
> rt_sigprocmask(SIG_BLOCK, [CHLD], [], 8) = 0
> rt_sigaction(SIGCHLD, NULL, {SIG_DFL, [], 0}, 8) = 0
> rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
> nanosleep({1, 0}, {1, 0})               = 0
> ======END STRACE======
>
> This repeats over and over again in the log (infinite loop?)
>
> I also tried a different time with the script provided by slony-ctl, 
> but no luck. (It DOES, however, work when there's only 2 nodes)
>
> Are there any know issues for 3+ node failover in 2.0.2?
>
> Would anyone be able to walk me through this, if perhaps I'm doing 
> something wrong?
>
> Thanks!
> --Richard
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

-- 
Cyril SCETBON - Ing?nieur bases de donn?es
Cellule bases de donn?es
AUSY pour France T?l?com - OPF/PORTAILS/DOP/HEBEX

T?l : +33 (0)4 97 12 87 60
Jabber : cscetbon@jabber.org
France Telecom - Orange
790 Avenue du Docteur Maurice Donat 
B?timent Marco Polo C1 - Bureau 202
06250 Mougins
France

***********************************
Ce message et toutes les pieces jointes (ci-apres le 'message') sont
confidentiels et etablis a l'intention exclusive de ses destinataires.
Toute utilisation ou diffusion non autorisee est interdite.
Tout message electronique est susceptible d'alteration. Le Groupe France
Telecom decline toute responsabilite au titre de ce message s'il a ete
altere, deforme ou falsifie.
Si vous n'etes pas destinataire de ce message, merci de le detruire
immediatement et d'avertir l'expediteur.
***********************************
This message and any attachments (the 'message') are confidential and
intended solely for the addressees.
Any unauthorised use or dissemination is prohibited.
Messages are susceptible to alteration. France Telecom Group shall not be
liable for the message if altered, changed or falsified.
If you are not recipient of this message, please cancel it immediately and
inform the sender.
************************************

From cbbrowne at ca.afilias.info  Wed Jul 22 07:55:53 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 22 07:56:01 2009
Subject: [Slony1-general] pg_autovacuum table gone in 8.4?
In-Reply-To: <24597190.post@talk.nabble.com> (Gordon Shannon's message of
	"Tue, 21 Jul 2009 15:49:44 -0700 (PDT)")
References: <24516041.post@talk.nabble.com>
	<873a8pspgo.fsf@dba2.int.libertyrms.com>
	<24597190.post@talk.nabble.com>
Message-ID: <87my6wrcli.fsf@dba2.int.libertyrms.com>

Gordon Shannon <gordo169@gmail.com> writes:
> Christopher Browne wrote:
>> 
>> Happily, when I "forked" the function into 8.3 and 8.4 versions, it
>> turns out that the API was perfectly fine for either; indeed, the
>> change to pull data from pg_class rather than pg_autovacuum is
>> relatively minor within the function.
>> 
>> I just committed that to 2.0+HEAD.
>> 
>
> Cool.  My hope is that, since the slony package creates the slony
> tables, that you will create them with no special autovacuum
> settings. That way, if I have autovacuum turned on in my system,
> then autovacuum will vacuum all Slony tables, rather than Slony
> doing it.  I just want to be sure that Slony doesn't ever vacuum
> sl_log_1 or sl_log_2, since it's a waste of time as they are
> truncated.  Please let me know if my assumptions is wrong.

Exactly so, at least as the default setting.

There are 3 distinguishable behaviours:

- By default, with 8.3/8.4, autovac is running, and the Slony-I tables
  will be caught by autovac, with the result that the Slony-I cleanup
  thread won't bother vacuuming them.

- If *you* set sl_log_1/sl_log_2 to be excluded by autovac, then the
  Slony-I cleanup process will vacuum them.

- On the other hand, if you shut off autovac, then the Slony-I cleanup
  process will notice, and vacuum *all* of its tables.

You can choose any of the three options...
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Wed Jul 22 08:00:18 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 22 08:00:25 2009
Subject: [Slony1-general] [Quick question] Slony configuration on
	postgresql.conf and pg_hba.conf
In-Reply-To: <24599592.post@talk.nabble.com> (roctaiwan's message of "Tue, 21
	Jul 2009 20:44:00 -0700 (PDT)")
References: <24599592.post@talk.nabble.com>
Message-ID: <87iqhkrce5.fsf@dba2.int.libertyrms.com>

roctaiwan <nettreeinc@gmail.com> writes:
> I know we all required to configurate the postgresql.conf and pg_hba.conf for
> linux to work properly with Slony. But I found that each of the conf files
> located at three different places.... which conf should be configurated, all
> three? is this same as any other system that has slony and postgresql?
>
>
> [root@Slave-DB1-Slony-I ~]# find / -name postgresql.conf
> /usr/local/pgsql/data/postgresql.conf
> /var/lib/pgsql/data/postgresql.conf
> /opt/PostgresPlus/8.3/data/postgresql.conf
> [root@Slave-DB1-Slony-I ~]#
> [root@Slave-DB1-Slony-I ~]# find / -name pg_hba.conf
> /usr/local/pgsql/data/pg_hba.conf
> /var/lib/pgsql/pg_hba.conf
> /var/lib/pgsql/data/pg_hba.conf
> /opt/PostgresPlus/8.3/data/pg_hba.conf
> [root@Slave-DB1-Slony-I ~]#

Evidently your Linux distribution does its own thing, several times!

I'd suggest checking with the running database; the following "show"
commands will tell you where *it* thinks its config files are.

mydb=# show hba_file;
                       hba_file
-------------------------------------------------------
 /mnt/PostgreSQL/dbs/PG84/postgresql-8.4.0/pg_hba.conf
(1 row)

mydb=# show config_file;
                        config_file
-----------------------------------------------------------
 /mnt/PostgreSQL/dbs/PG84/postgresql-8.4.0/postgresql.conf
(1 row)

That's what *I* would edit on my system :-).
-- 
output = ("cbbrowne" "@" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Wed Jul 22 09:03:50 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 22 09:04:00 2009
Subject: Cleanup thread changes - Re: [Slony1-general] Does Slony version
	1.2.x work with postgresql 8.4?
In-Reply-To: <87vdllr6xh.fsf@dba2.int.libertyrms.com> (Christopher Browne's
	message of "Tue, 21 Jul 2009 18:46:02 -0400")
References: <20090713205919.GA26996@it.is.rice.edu>
	<87y6qrxkey.fsf@dba2.int.libertyrms.com>
	<8a1bfe660907170632s2f221ed3v7800b947eff9f755@mail.gmail.com>
	<87vdllr6xh.fsf@dba2.int.libertyrms.com>
Message-ID: <87d47sr9g9.fsf_-_@dba2.int.libertyrms.com>

Christopher Browne <cbbrowne@ca.afilias.info> writes:
> In view of the "noise", I'm taking a look at how challenging it is to
> get 8.4 support into the 1.2 branch...

... Bits Jan's looking at elided ...

> There's another pretty visible 8.4 issue...  src/slon/cleanup_thread.c
> has a reference to pg_autovacuum.

The patch for this for 8.4 seems pretty small...  

This is decidedly not tested, as the issues with xxid.c/slony1_funcs.c
prevent installation at this point.

But this bit, at least, is hardly scary.

Index: cleanup_thread.c
===================================================================
RCS file: /home/cvsd/slony1/slony1-engine/src/slon/cleanup_thread.c,v
retrieving revision 1.33.2.4
diff -c -u -r1.33.2.4 cleanup_thread.c
--- cleanup_thread.c	22 Aug 2007 21:20:23 -0000	1.33.2.4
+++ cleanup_thread.c	22 Jul 2009 16:00:07 -0000
@@ -302,11 +302,22 @@
 				sprintf(tstring, table_list[t], rtcfg_namespace);
 				if (a_vac==1)
 				{
-					slon_mkquery(&query3,"select (case when pga.enabled ISNULL THEN true ELSE pga.enabled END) "
-						"from \"pg_catalog\".pg_namespace PGN, \"pg_catalog\".pg_class PGC LEFT OUTER JOIN "
-						"\"pg_catalog\".pg_autovacuum pga ON (PGC.oid = pga.vacrelid) where PGC.relnamespace = PGN.oid "
-						"and %s.slon_quote_input('%s')=%s.slon_quote_brute(PGN.nspname) || '.' || %s.slon_quote_brute(PGC.relname);",
-					 	rtcfg_namespace,tstring, rtcfg_namespace, rtcfg_namespace);
+					if (conn->pg_version < 80400) {
+						slon_mkquery(&query3,"select (case when pga.enabled ISNULL THEN true ELSE pga.enabled END) "
+									 "from \"pg_catalog\".pg_namespace PGN, \"pg_catalog\".pg_class PGC LEFT OUTER JOIN "
+									 "\"pg_catalog\".pg_autovacuum pga ON (PGC.oid = pga.vacrelid) where PGC.relnamespace = PGN.oid "
+									 "and %s.slon_quote_input('%s')=%s.slon_quote_brute(PGN.nspname) || '.' || %s.slon_quote_brute(PGC.relname);",
+									 rtcfg_namespace,tstring, rtcfg_namespace, rtcfg_namespace);
+
+					} else {
+						/* PostgreSQL 8.4 */
+						slon_mkquery (&query3, 
+									  "select coalesce ('autovacuum_enabled=on' = any(reloptions), 't'::boolean) "
+									  "from \"pg_catalog\".pg_class pgc, \"pg_catalog\".pg_namespace pgn "
+									  "where pgc.relnamespace = pgn.oid and %s.slon_quote_input('%s')= "
+									  " %s.slon_quote_brute(PGN.nspname) || '.' || %s.slon_quote_brute(PGC.relname);",
+									  rtcfg_namespace,tstring, rtcfg_namespace, rtcfg_namespace);
+					}
 
 					res = PQexec(dbconn, dstring_data(&query3));
 					if (PQresultStatus(res) != PGRES_TUPLES_OK)  /* query error */

-- 
"cbbrowne","@","ca.afilias.info"
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From tmblue at gmail.com  Wed Jul 22 11:34:37 2009
From: tmblue at gmail.com (Tory M Blue)
Date: Wed Jul 22 11:34:49 2009
Subject: [Slony1-general] Vacuum of sl_1 and 2 logs. (postgres)
Message-ID: <8a547c840907221134p29d896b0r1cd38a5cf3bd3080@mail.gmail.com>

So I've noticed recently that I'm vacuuming the sl_?.log files with
postgres and this doesn't appear right. The fact is slon has it's own
process for dealing with this and I believe it's a clean truncate.

I'm wondering how do I go about figuring out whether postgres is in
fact vacuuming these logs and how to stop it?  I would rather allow
slon to handle it and stop postgres from dealing with the slon tables
(as far as vacuuming them goes)

In slon 2.0.2 there is this query that doesn't seem to work in the
1.2.x versions.

select oid, relname from pg_class where relnamespace = (select oid
from pg_namespace where nspname = '_' || 'MyCluster') and relhasindex;



Thanks
Tory

Note: I'm one of the unhappy ones that feel the current postgres/slon
upgrade paths are painful. Multi TB db requires a dump restore?!?!?!?!
slon can't be ported a tad to be backward compatible, so you have to
completely build new and uninstall install, and rebuild your slon
cluster.
From cbbrowne at ca.afilias.info  Wed Jul 22 13:19:53 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 22 13:20:10 2009
Subject: [Slony1-general] Vacuum of sl_1 and 2 logs. (postgres)
In-Reply-To: <8a547c840907221134p29d896b0r1cd38a5cf3bd3080@mail.gmail.com>
	(Tory M. Blue's message of "Wed, 22 Jul 2009 11:34:37 -0700")
References: <8a547c840907221134p29d896b0r1cd38a5cf3bd3080@mail.gmail.com>
Message-ID: <871vo8qxli.fsf@dba2.int.libertyrms.com>

Tory M Blue <tmblue@gmail.com> writes:
> So I've noticed recently that I'm vacuuming the sl_?.log files with
> postgres and this doesn't appear right. The fact is slon has it's own
> process for dealing with this and I believe it's a clean truncate.

I would actually counsel taking the opposite approach, that it may be
preferable for autovacuum to handle vacuuming the Slony-I tables than
for Slony-I to do it itself.

Autovacuum should have a better capability to cope with the dual factors
of:
 a) Needing to vacuum some tables "even more often", as well as
 b) Needing to not vacuum some tables very often.

In principle, we could make the cleanup thread in Slony-I smarter, but
that would duplicate the good work that has gone into the PostgreSQL
built-in...
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From tmblue at gmail.com  Wed Jul 22 14:02:09 2009
From: tmblue at gmail.com (Tory M Blue)
Date: Wed Jul 22 14:02:24 2009
Subject: [Slony1-general] Vacuum of sl_1 and 2 logs. (postgres)
In-Reply-To: <871vo8qxli.fsf@dba2.int.libertyrms.com>
References: <8a547c840907221134p29d896b0r1cd38a5cf3bd3080@mail.gmail.com>
	<871vo8qxli.fsf@dba2.int.libertyrms.com>
Message-ID: <8a547c840907221402p4fe844edw642e687e6294acb2@mail.gmail.com>

On Wed, Jul 22, 2009 at 1:19 PM, Christopher
Browne<cbbrowne@ca.afilias.info> wrote:
> Tory M Blue <tmblue@gmail.com> writes:
>> So I've noticed recently that I'm vacuuming the sl_?.log files with
>> postgres and this doesn't appear right. The fact is slon has it's own
>> process for dealing with this and I believe it's a clean truncate.
>
> I would actually counsel taking the opposite approach, that it may be
> preferable for autovacuum to handle vacuuming the Slony-I tables than
> for Slony-I to do it itself.
>
> Autovacuum should have a better capability to cope with the dual factors
> of:
> ?a) Needing to vacuum some tables "even more often", as well as
> ?b) Needing to not vacuum some tables very often.
>
> In principle, we could make the cleanup thread in Slony-I smarter, but
> that would duplicate the good work that has gone into the PostgreSQL
> built-in...

Ahh good info, although I would think that a postgres vacuum, using
delete's would be worse than a slon truncate of said table once
everything was replicated?

I have major index bloat and looking for anything and everything that
could help with it.

Thanks for the insight

Tory
From cbbrowne at ca.afilias.info  Wed Jul 22 14:50:06 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 22 14:50:24 2009
Subject: [Slony1-general] Vacuum of sl_1 and 2 logs. (postgres)
In-Reply-To: <8a547c840907221402p4fe844edw642e687e6294acb2@mail.gmail.com>
	(Tory M. Blue's message of "Wed, 22 Jul 2009 14:02:09 -0700")
References: <8a547c840907221134p29d896b0r1cd38a5cf3bd3080@mail.gmail.com>
	<871vo8qxli.fsf@dba2.int.libertyrms.com>
	<8a547c840907221402p4fe844edw642e687e6294acb2@mail.gmail.com>
Message-ID: <87vdlkpeup.fsf@dba2.int.libertyrms.com>

Tory M Blue <tmblue@gmail.com> writes:
> On Wed, Jul 22, 2009 at 1:19 PM, Christopher
> Browne<cbbrowne@ca.afilias.info> wrote:
>> Tory M Blue <tmblue@gmail.com> writes:
>>> So I've noticed recently that I'm vacuuming the sl_?.log files with
>>> postgres and this doesn't appear right. The fact is slon has it's own
>>> process for dealing with this and I believe it's a clean truncate.
>>
>> I would actually counsel taking the opposite approach, that it may be
>> preferable for autovacuum to handle vacuuming the Slony-I tables than
>> for Slony-I to do it itself.
>>
>> Autovacuum should have a better capability to cope with the dual factors
>> of:
>> ?a) Needing to vacuum some tables "even more often", as well as
>> ?b) Needing to not vacuum some tables very often.
>>
>> In principle, we could make the cleanup thread in Slony-I smarter, but
>> that would duplicate the good work that has gone into the PostgreSQL
>> built-in...
>
> Ahh good info, although I would think that a postgres vacuum, using
> delete's would be worse than a slon truncate of said table once
> everything was replicated?
>
> I have major index bloat and looking for anything and everything that
> could help with it.

I'll illustrate with a couple examples...

Consider the case where we have Slony-I do the vacuuming itself...

Comparison #1: sl_log_1

 - Every 3 iterations of the cleanup thread, by default, every 30
   minutes, it would vacuum all of its tables, bloated or not.

   Supposing sl_log_1 has lots of junk in it (deleted tuples or not),
   lots of time will be spent vacuuming it every 30 minutes,
   needed/useful or not.

On the other hand, if the autovac thread handles this, then...

 - If you're running Slony-I 2.0, where only TRUNCATE is used, there
   should never be a material # of dead tuples in sl_log_1,
   so you can expect it to *NEVER* vacuum sl_log_1.

   Winner: autovacuum :-)

Comparison #2: sl_confirm

   This table gets trimmed fairly often.  But let's suppose it's
   getting pretty bloated...

   - If we use Slony-I cleanup thread to vacuum, then it'll vacuum it
     every 30 minutes, regardless of usefulness.

   - If we use autovac, then:

       a) autovac may vacuum it *more* often, if it's getting tuples
          trimmed frequently

       b) On the other hand, if tuples don't get trimmed out for 2
          hours due to something holding onto data, then there will
          be NO deletes/updates to sl_confirm, and autovac won't
          bother vacuuming it.

   In either case, I expect autovacuum to be preferable.

There are various pathological cases characterized by the above where
"have the Slony-I cleanup thread do it" is distinctly inferior to "use
autovacuum."

I'm *STRONGLY* disinclined to try to improve the cleanup thread's
logic in this regard; relevant development effort would be *WAY*
better spent improving autovacuum, as that would be helpful to more
than just Slony-I users.
-- 
(reverse (concatenate 'string "ofni.sailifa.ac" "@" "enworbbc"))
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From peter.geoghegan86 at gmail.com  Wed Jul 22 15:28:42 2009
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Wed Jul 22 15:29:01 2009
Subject: [Slony1-general] Safely stopping and starting replication using
	"bare metal" slony functions
Message-ID: <db471ace0907221528q53b6ce79ned4984fdd078202e@mail.gmail.com>

Hello,

I'm maintaining an application that uses Slony-I. Users may very
occasionally stop replication, to build a backlog of updates, and
later restart replication and propagate that backlog all at once. It
isn't likely to be used more than every once in a long while. At the
moment, I simply stop the Slony-I windows service (i.e. all slon
daemons), and later restart it through win32 API calls. This has the
considerable disadvantage of requiring that the user be present on the
master - if the user is connecting to the database remotely, the Slony
service cannot be restarted. It also requires that the user have OS
administrative privileges.

My questions are:

1. Is stopping Slony-I in this manner sensible? I've had acceptable
results so far, but I'm aware that "Extended periods of downtime will
require to remove or deactivate the node in question in the
configuration", and that Slony-I is not suitable for  replicating
"offline nodes that only become available sporadic [sic] for
synchronization".

2. If it is sensible, is it possible to achieve the same result
through calls to "bare metal" slony functions?

Thanks,
Peter Geoghegan
From nettreeinc at gmail.com  Wed Jul 22 22:41:29 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Wed Jul 22 22:42:00 2009
Subject: [Slony1-general] [Quick question] Slony configuration on
	postgresql.conf and pg_hba.conf
In-Reply-To: <87iqhkrce5.fsf@dba2.int.libertyrms.com>
References: <24599592.post@talk.nabble.com>
	<87iqhkrce5.fsf@dba2.int.libertyrms.com>
Message-ID: <24619567.post@talk.nabble.com>


Thanks Christopher. 

I have found out my postgresql configuration file does run in different
place. Since I am running Enterprise DB plus standard install using
one-click setup method, I don't have to configurate much of things, it's
pretty much all done for me. but on this machine I have trouble with, its
pointing configuration file that's under var/lib/pgsql/data/..... 

Does anybody know why? Is that something I modified that does it? 

How can I change back to where its supposed to be
(/opt/PostgresPlus/8.3/data) ?

contactdb=# show config_file;
             config_file
-------------------------------------
 /var/lib/pgsql/data/postgresql.conf
(1 row)

contactdb=# show hba_file;
            hba_file
---------------------------------
 /var/lib/pgsql/data/pg_hba.conf
(1 row)

contactdb=#


Christopher Browne wrote:
> 
> Evidently your Linux distribution does its own thing, several times!
> 
> I'd suggest checking with the running database; the following "show"
> commands will tell you where *it* thinks its config files are.
> 
> mydb=# show hba_file;
>                        hba_file
> -------------------------------------------------------
>  /mnt/PostgreSQL/dbs/PG84/postgresql-8.4.0/pg_hba.conf
> (1 row)
> 
> mydb=# show config_file;
>                         config_file
> -----------------------------------------------------------
>  /mnt/PostgreSQL/dbs/PG84/postgresql-8.4.0/postgresql.conf
> (1 row)
> 
> That's what *I* would edit on my system :-).
> -- 
> output = ("cbbrowne" "@" "ca.afilias.info")
> <http://dba2.int.libertyrms.com/>
> Christopher Browne
> (416) 673-4124 (land)
> "Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
> phasers on the Heffalump, Piglet, meet me in transporter room three"
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/-Quick-question--Slony-configuration-on-postgresql.conf-and-pg_hba.conf-tp24599592p24619567.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Wed Jul 22 22:52:57 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Wed Jul 22 22:53:29 2009
Subject: [Slony1-general] slon engine pointing to the wrong directory
In-Reply-To: <24601412.post@talk.nabble.com>
References: <24601412.post@talk.nabble.com>
Message-ID: <24619689.post@talk.nabble.com>


Please look at the thread topic:  [Quick question] Slony configuration on
postgresql.conf and pg_hba.conf  
as these two are closely related issues. 
-- 
View this message in context: http://www.nabble.com/slon-engine-pointing-to-the-wrong-directory-tp24601412p24619689.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Wed Jul 22 23:36:13 2009
From: nettreeinc at gmail.com (Net Tree Inc.)
Date: Wed Jul 22 23:36:46 2009
Subject: [Slony1-general] Question on Load balancing
Message-ID: <bd9689740907222336s7d29b9f5vcc41db20b143f10f@mail.gmail.com>

Hello,
I am having a concept question about how the Read/Write process transaction
works within the DB network structure.
Assume I have an environment of one master DB with three slaves running
Slony-I replication server.
As you all know Slony is a Single master to multiple slaves replication
system supporting cascading and slave promotion. Master DB can do both read
and write, but the slaves can only do read and for reporting purpose.

Here is the question of concept that I don't understand, if you have answer
or provide me a link to referral is appreciated.

Q: For application Developer write codes to accessing the DB server to
getting/modify datas. Since there is only one server that can handle both
Read/Write which is the Master and slave is only to retrive data.
How should the developer know which DB server should they connecting
to for either to read or to write? If developer is using the same fixed
connection strings to a fix IP to Master server for Read/Write and another
connection string to a fix IP to (for example) my SLAVE1 server to retrive
datas.
How and what would happen if my SLAVE1 failed or due to maintaince that I
need to replace it?

Also what would happen for Master DB if issues such as failover or switch
over that I need to replacing the master server due to failure or
for maintaince, or to promote one of the slave to master?

I just need answer that tells me how PostgreSQL and Slony would react if
such issues happens. Answer can simply just to provide me a web link for me
to read further. Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090723/=
e865cda0/attachment.htm
From mnagaraja at alcatel-lucent.com  Thu Jul 23 05:18:32 2009
From: mnagaraja at alcatel-lucent.com (Nagaraja, Madhukar (Madhukar))
Date: Thu Jul 23 05:20:24 2009
Subject: [Slony1-general] Slon daemons generating core
Message-ID: <E9F099885B445541A55D830FC62E175A0B665AAC11@INBANSXCHMBSA3.in.alcatel-lucent.com>

Hello,

We have automated the starting stopping of slon and also other slony configuaration. But we observed nowadays that the slon daemons generating core files mainly I guess when we do reinit of our system. At that time we kill slon and restart. We actually restart slon whenever our processes restart as the postgresql would also restart.

Have any observed this ? Bcos we observed we left it for a week or so there was some 100MB of core. But this is in our test environment.

Thanks.
From cbbrowne at ca.afilias.info  Thu Jul 23 08:44:13 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Thu Jul 23 08:44:23 2009
Subject: [Slony1-general] Question on Load balancing
In-Reply-To: <bd9689740907222336s7d29b9f5vcc41db20b143f10f@mail.gmail.com>
	(Net Tree's message of "Thu, 23 Jul 2009 14:36:13 +0800")
References: <bd9689740907222336s7d29b9f5vcc41db20b143f10f@mail.gmail.com>
Message-ID: <87iqhjpfoy.fsf@dba2.int.libertyrms.com>

"Net Tree Inc." <nettreeinc@gmail.com> writes:
> I just need answer that tells me how PostgreSQL and Slony would
> react if such issues happens. Answer can simply just to provide me a
> web link for me to read further. Thanks!

These aren't notably PostgreSQL or Slony-I questions; they are
questions about how you choose to manage the configuration for your
applications.

That doesn't make them dumb questions, just things for which I don't
believe we can provide any prescriptive answers.

Historically, I know that we ("Afilias folk") have faced exactly these
issues as questions of how to deploy already-designed applications
that weren't designed with these questions in mind.

It's something of a struggle: you need to design the ability to
configure these things into applications in order for it NOT to be a
fairly horrible process to do various forms of failover.

Web applications frequently are pretty "horrible" in this fashion,
particularly when they try to be "user friendly" by putting a GUI in
front of the configuration such that you pretty much have to log into
the application in order to reconfigure it.

The thing I'd counsel is to make sure that the configuration used to
control how your apps access databases is centralized in such a way
that you can change it without having to "jump through burning hoops."

Plausible options include:

 - DB connectivity might be defined in a file that can be easily
   rewritten by a script.  ("XML document containing 18 other
   things" is not an example of such a thing...)

 - App configuration never changes; it points to a proxy (e.g. -
   pgbouncer, pgpool) which redirects it to the "real" database

   You can reconfigure pgbouncer/pgpool without "burning hoops" :-)

 - Standardize on a host name and port number; app configuration
   never changes...  But this points to a CNAME so that you may
   readily repoint to an alternate server via changing the CNAME
   in your DNS configuration.

There are probably other options; these three are reasonable ones off
the top of my head.
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From ajs at crankycanuck.ca  Thu Jul 23 08:53:15 2009
From: ajs at crankycanuck.ca (Andrew Sullivan)
Date: Thu Jul 23 08:53:33 2009
Subject: [Slony1-general] Could not start slon cluster
In-Reply-To: <24599840.post@talk.nabble.com>
References: <24566272.post@talk.nabble.com> <24599840.post@talk.nabble.com>
Message-ID: <20090723155314.GA5410@shinkuro.com>

On Tue, Jul 21, 2009 at 09:18:14PM -0700, roctaiwan wrote:

> I follow steps of an article on  http://www.linuxjournal.com/article/7834 
> http://www.linuxjournal.com/article/7834   to testing my first slony
> replication, which I believe many new slony users would of been there. They

Whether many new users would use that is immaterial.  There's actually
a tutorial in the Slony dics that explains what to do, too.

> replicating. First script is called cluster_setup.sh and the other is called
> subscribe.sh. My question is. The case of that article is replicating two
> DBs on the same machine (localhost). If I would like to replicating two DBs
> on two different machine, whats the steps for me to run these two scripts?

Who knows?  I don't have those scripts.  

It is unfortunately true that successfully operating slony such that
you will be satisfied requires some understanding of its theory of
operation and, in most cases, some ability to read and understand some
of its config files.  If you're not willing to learn all of that, you
really need to consider something other than slony, because the user
interface is still complicated and by no means easy to use.

A
-- 
Andrew Sullivan
ajs@crankycanuck.ca
From melvin6925 at yahoo.com  Thu Jul 23 11:40:00 2009
From: melvin6925 at yahoo.com (Melvin Davidson)
Date: Thu Jul 23 11:40:19 2009
Subject: [Slony1-general] Could not start slon cluster
Message-ID: <111819.34338.qm@web53009.mail.re2.yahoo.com>

>If I would like to replicating two DBs on two different machine, whats the steps for me to 
>run these two scripts?

Most people would simply change the ip addresses and/or hostname variables to point to the machines in the replication. But that would involve common sense and thinking things through.

Melvin Davidson 




      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090723/fe3ce783/attachment.htm
From gordo169 at gmail.com  Thu Jul 23 12:18:11 2009
From: gordo169 at gmail.com (Gordon Shannon)
Date: Thu Jul 23 12:18:27 2009
Subject: [Slony1-general] "Could not lock table" errors from slony
In-Reply-To: <24521158.post@talk.nabble.com>
References: <24521158.post@talk.nabble.com>
Message-ID: <24632675.post@talk.nabble.com>




Gordon Shannon wrote:
> 
> I am running Slony 2.0.2 replicating tables from an 8.3 cluster to an 8.4. 
> The following scenarios has happened consistently:
> 
> 1. Issue SUBSCRIBE SET
> 2. Slon log on receiver goes into loop due to "transactions earlier than
> XID nnnnnnn are still in progress".
> 3. A few minutes later, the long query on the provider node is finished.
> 4. Slon on receiver now tries to copy the set, but gets 
>    "ERROR remoteWorkerThread_7: Could not lock table "public"."abc" on
> subscriber"  where abc is the 1st table in the set.
> 
> It will loop like this indefinitely, until I bounce slon, then it works.
> Also, at the same time that error happens in the slon log, I see this in
> the Postgres log on the receiver:
>   LOCK TABLE can only be used in transaction blocks.
>   Statement: lock table "public"."abc"
> 
> This is very consistent.  It's not a show stopper, just annoying.  I don't
> remember this in 2.0.1.
> 

Here's what I think is happening.  I think Slony is attempting to lock the
table outside a transaction. Here's why.

In Postgres 8.3, if you lock a table outside a transaction, it silently does
nothing. This explains the message I always see against 8.3 in this
situation: "NOTICE:  there is no transaction in progress", and it continues
on.

In 8.4, however, Postgres throws an error, as explained above.

All of the messages I'm seeing consistently would be explained if this
theory is correct. :-)



-- 
View this message in context: http://www.nabble.com/%22Could-not-lock-table%22-errors-from-slony-tp24521158p24632675.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Thu Jul 23 23:08:41 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Thu Jul 23 23:09:15 2009
Subject: [Slony1-general] postgres and slon is started or not!?? Slon
	command not known by system
Message-ID: <24639114.post@talk.nabble.com>


I installed my postgreSQL and slony using one-click installer download on
EnterpriseDB.com website. This should be straight forward during the
installation and just simple following the steps. I have successfully
installed on my Master machine but not other two slaves. I forgot how I make
my master work, but I believe I didn't do anything unique, since it's just a
one-click installer~ (.bin file)

First Q, are my server successfully started or not? someone on forum taught
me to find out I can do pg_ctl status, or ps -ef | gres postgres or ps -ef |
gres slon . But please look the logs at below, I couldn't tell which command
is telling me the truth...

(copied from my DB1 slave server)
login as: root
root@172.22.4.221's password:
Last login: Fri Jul 24 12:52:19 2009 from 172.20.80.47
[root@Slave-DB1-Slony-I ~]# su - postgres
-bash-3.2$ pg_ctl status
pg_ctl: neither postmaster nor postgres running -------------------->This
line showing there is nothing is running?
-bash-3.2$ ps -ef | grep postgres   ---------------------------> is This
showing my postgres is running?
postgres  4142     1  0 13:10 ?        00:00:00
/opt/PostgresPlus/8.3/bin/postgres.bin -D /opt/PostgresPlus/8.3/data
postgres  4174  4142  0 13:10 ?        00:00:00 postgres: writer process        
postgres  4175  4142  0 13:10 ?        00:00:00 postgres: wal writer process    
postgres  4176  4142  0 13:10 ?        00:00:00 postgres: autovacuum
launcher process
postgres  4177  4142  0 13:10 ?        00:00:00 postgres: stats collector
process
root      4362  4330  0 13:15 pts/0    00:00:00 su - postgres
postgres  4363  4362  0 13:15 pts/0    00:00:00 -bash
postgres  4518  4363  0 13:17 pts/0    00:00:00 ps -ef
postgres  4519  4363  0 13:17 pts/0    00:00:00 grep postgres
-bash-3.2$ ps -ef | grep postgresql
postgres  4523  4363  0 13:17 pts/0    00:00:00 grep postgresql
-bash-3.2$ ps -ef | grep slon
postgres  4604  4363  0 13:20 pts/0    00:00:00 grep slon
-bash-3.2$

(copied from my DB2 slave server)

login as: root
root@172.22.4.222's password:
Last login: Fri Jul 24 12:34:37 2009 from 172.20.80.47
[root@Slave-DB2-Slony-I ~]# su - postgres
-bash-3.2$ pg_ctl status
pg_ctl: no server running
-bash-3.2$ ps -ef | grep postgresql
postgres  5507  5224  0 13:22 pts/0    00:00:00 grep postgresql
-bash-3.2$ ps -ef | grep postgres
postgres  4220     1  0 12:50 ?        00:00:00
/opt/PostgresPlus/8.3/bin/postgres.bin -D /opt/PostgresPlus/8.3/data
postgres  4252  4220  0 12:50 ?        00:00:00 postgres: writer process        
postgres  4253  4220  0 12:50 ?        00:00:00 postgres: wal writer process    
postgres  4254  4220  0 12:50 ?        00:00:00 postgres: autovacuum
launcher process
postgres  4255  4220  0 12:50 ?        00:00:00 postgres: stats collector
process
root      5223  5164  0 13:15 pts/0    00:00:00 su - postgres
postgres  5224  5223  0 13:15 pts/0    00:00:00 -bash
postgres  5547  5224  0 13:22 pts/0    00:00:00 ps -ef
postgres  5548  5224  0 13:22 pts/0    00:00:00 grep postgres
-bash-3.2$ ps -ef | grep slon
postgres  5550  5224  0 13:22 pts/0    00:00:00 grep slon
-bash-3.2$

Q2, I have setup one succeed machine that run both slony and postgresql
correctly. But the above two are not. if I want to start slon on these two
slaves it will say:

-bash-3.2$ slon sql_cluster "dbname=contactdb user=postgres"
-bash: slon: command not found

This happen on both of above machines.  Why is that? Many other times error
has something to do with postmaster.pid that it can not find. and I have no
clude about it... isn't it just a one-click installer?
-- 
View this message in context: http://www.nabble.com/postgres-and-slon-is-started-or-not%21---Slon-command-not-known-by-system-tp24639114p24639114.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From glynastill at yahoo.co.uk  Fri Jul 24 01:53:42 2009
From: glynastill at yahoo.co.uk (Glyn Astill)
Date: Fri Jul 24 01:54:21 2009
Subject: [Slony1-general] postgres and slon is started or not!?? Slon
	command not known by system
Message-ID: <762500.66179.qm@web23601.mail.ird.yahoo.com>



> First Q, are my server successfully started or not?

Your process list's show that postgresql is running on both machines, however no slon daemons are running.

> Q2, I have setup one succeed machine that run both slony
> and postgresql
> correctly. But the above two are not. if I want to start
> slon on these two
> slaves it will say:
> 
> -bash-3.2$ slon sql_cluster "dbname=contactdb
> user=postgres"
> -bash: slon: command not found
> 
> This happen on both of above machines.? Why is that?

Either slony is not installed, or your path is probably not set correctly.

It would appear that most of the issues you're having are with the *nix environment in general, perhaps it'd be an idea to do a bit of reading or get some training there.


      
From aleksander.kmetec at intera.si  Fri Jul 24 19:39:10 2009
From: aleksander.kmetec at intera.si (Aleksander Kmetec)
Date: Fri Jul 24 19:39:35 2009
Subject: [Slony1-general] Safely stopping and starting replication using
	"bare metal" slony functions
In-Reply-To: <db471ace0907221528q53b6ce79ned4984fdd078202e@mail.gmail.com>
References: <db471ace0907221528q53b6ce79ned4984fdd078202e@mail.gmail.com>
Message-ID: <4A6A704E.3010902@intera.si>


Peter Geoghegan wrote:
> 2. If it is sensible, is it possible to achieve the same result
> through calls to "bare metal" slony functions?

I don't know any officially approved way of doing this, but you could disable the user account slony is using to connect 
to the database and terminate any existing connections, like this:

-- prevent user slony from logging in
ALTER USER slony NOLOGIN;

-- terminate all connections for user slony
SELECT pg_terminate_backend(procpid)
FROM pg_stat_activity
WHERE usename = 'slony';

-- your changes here
INSERT ....

-- allow user slony to log in again
ALTER USER slony LOGIN;

After a minute or so, slony should reconnect to your database and begin replicating the changes.

Regards,
Aleksander
From peter.geoghegan86 at gmail.com  Sat Jul 25 02:25:30 2009
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Sat Jul 25 02:26:09 2009
Subject: [Slony1-general] Safely stopping and starting replication using 
	"bare metal" slony functions
In-Reply-To: <4A6A704E.3010902@intera.si>
References: <db471ace0907221528q53b6ce79ned4984fdd078202e@mail.gmail.com>
	<4A6A704E.3010902@intera.si>
Message-ID: <db471ace0907250225i2c40e833s39316bd345d67ab5@mail.gmail.com>

Aleksander,

I appreciate the suggestion, but that seems very hacky. I just hoped
that there was a simple mechanism for stopping replication using bare
metal functions,

Regards,
Peter Geoghegan
From peter.geoghegan86 at gmail.com  Sun Jul 26 05:19:59 2009
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Sun Jul 26 05:20:02 2009
Subject: [Slony1-general] sl_status incorrectly reports long event lag
Message-ID: <db471ace0907260519l55b61b8jf4e91c953a30ea38@mail.gmail.com>

Hello,

I'm experiencing a curious issue with Slony 2.0.2 . If I shut down a
slave, and subsequently boot it back up, the sl_status view's event
lag grows continually, as if replication isn't working between the
master and that particular slave. However, replication is observably
working. I can make the sl_status view's lag for the slave return to
zero by restarting slon daemons.

Why might this be? I find the sl_status view very important for my
particular application.

Regards,
Peter Geoghegan
From nettreeinc at gmail.com  Mon Jul 27 02:53:18 2009
From: nettreeinc at gmail.com (Net Tree Inc.)
Date: Mon Jul 27 02:53:59 2009
Subject: [Slony1-general] Helpful admin Tools referral : scripts manager
Message-ID: <bd9689740907270253m27be30eycbe327ec56b02be8@mail.gmail.com>

Hi Everyone,

Does any of you know if there is any tools that can be use, has scripts
templates available on easy the job of setting up clusters, and subscribing
the sets?
I am trying to find template scripts to setup clusters for multi-slave
environment. I am not doing something special nor unique. My environment is
very simple and straight forward. I just need a scripts to run to setup my 3
slaves clusters, that's all.  I need something like the one in attachment
(please see attachment). in case if you don't see any attachments, since I
am not too sure how to attach it using my Gmail to send this mailing list, I
have post the script at below



Listing 1. cluster_setup.sh


#!/bin/sh

CLUSTER=3Dsql_cluster
DB1=3Dcontactdb
DB2=3Dcontactdb_slave
H1=3Dlocalhost
H2=3Dlocalhost
U=3Dpostgres

slonik <<_EOF_

cluster name =3D $CLUSTER;

node 1 admin conninfo =3D 'dbname=3D$DB1 host=3D$H1 user=3D$U';
node 2 admin conninfo =3D 'dbname=3D$DB2 host=3D$H2 user=3D$U';

init cluster (id =3D 1, comment =3D 'Node 1');

create set (id =3D 1, origin =3D 1,
       comment =3D 'contact table');

set add table (set id =3D 1, origin =3D 1, id =3D 1,
        full qualified name =3D 'public.contact',
        comment =3D 'Table contact');

set add sequence (set id =3D 1, origin =3D 1, id =3D 2,
        full qualified name =3D 'public.contact_seq',
        comment =3D 'Sequence contact_seq');

store node (id =3D 2, comment =3D 'Node 2');
store path (server =3D 1, client =3D 2,
      conninfo =3D 'dbname=3D$DB1 host=3D$H1 user=3D$U');

store path (server =3D 2, client =3D 1,
           conninfo =3D 'dbname=3D$DB2 host=3D$H2 user=3D$U');

store listen (origin =3D 1, provider =3D 1, receiver =3D 2);
store listen (origin =3D 2, provider =3D 2, receiver =3D 1);


--
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090727/=
4839520d/attachment.htm
From nettreeinc at gmail.com  Mon Jul 27 03:00:42 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Mon Jul 27 03:01:23 2009
Subject: [Slony1-general] postgres and slon is started or not!?? Slon
	command not known by system
In-Reply-To: <762500.66179.qm@web23601.mail.ird.yahoo.com>
References: <24639114.post@talk.nabble.com>
	<762500.66179.qm@web23601.mail.ird.yahoo.com>
Message-ID: <24677521.post@talk.nabble.com>


I  believe it has to do with my file path. But do you know where (which file)
can I re-config (redirect) my file path make it pointing to the correct
postgreSQL and slony?



Glyn Astill wrote:
> 
> 
> 
>> First Q, are my server successfully started or not?
> 
> Your process list's show that postgresql is running on both machines,
> however no slon daemons are running.
> 
>> Q2, I have setup one succeed machine that run both slony
>> and postgresql
>> correctly. But the above two are not. if I want to start
>> slon on these two
>> slaves it will say:
>> 
>> -bash-3.2$ slon sql_cluster "dbname=contactdb
>> user=postgres"
>> -bash: slon: command not found
>> 
>> This happen on both of above machines.? Why is that?
> 
> Either slony is not installed, or your path is probably not set correctly.
> 
> It would appear that most of the issues you're having are with the *nix
> environment in general, perhaps it'd be an idea to do a bit of reading or
> get some training there.
> 
> 
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/postgres-and-slon-is-started-or-not%21---Slon-command-not-known-by-system-tp24639114p24677521.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Mon Jul 27 03:01:57 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Mon Jul 27 03:02:38 2009
Subject: [Slony1-general] postgres and slon is started or not!?? Slon
	command not known by system
In-Reply-To: <24677521.post@talk.nabble.com>
References: <24639114.post@talk.nabble.com>
	<762500.66179.qm@web23601.mail.ird.yahoo.com>
	<24677521.post@talk.nabble.com>
Message-ID: <24677538.post@talk.nabble.com>


Glyn Astill answered:

Depends on the OS. Pehaps the most common place would be /etc/profile - add
your path in there before it's exported.




roctaiwan wrote:
> 
> I  believe it has to do with my file path. But do you know where (which
> file) can I re-config (redirect) my file path make it pointing to the
> correct postgreSQL and slony?
> 
> 
> 
> Glyn Astill wrote:
>> 
>> 
>> 
>>> First Q, are my server successfully started or not?
>> 
>> Your process list's show that postgresql is running on both machines,
>> however no slon daemons are running.
>> 
>>> Q2, I have setup one succeed machine that run both slony
>>> and postgresql
>>> correctly. But the above two are not. if I want to start
>>> slon on these two
>>> slaves it will say:
>>> 
>>> -bash-3.2$ slon sql_cluster "dbname=contactdb
>>> user=postgres"
>>> -bash: slon: command not found
>>> 
>>> This happen on both of above machines.? Why is that?
>> 
>> Either slony is not installed, or your path is probably not set
>> correctly.
>> 
>> It would appear that most of the issues you're having are with the *nix
>> environment in general, perhaps it'd be an idea to do a bit of reading or
>> get some training there.
>> 
>> 
>> 
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general@lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>> 
>> 
> 
> 

-- 
View this message in context: http://www.nabble.com/postgres-and-slon-is-started-or-not%21---Slon-command-not-known-by-system-tp24639114p24677538.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Mon Jul 27 03:05:26 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Mon Jul 27 03:06:07 2009
Subject: [Slony1-general] Helpful admin Tools referral : scripts manager?
Message-ID: <24677626.post@talk.nabble.com>


Hi Everyone,

Does any of you know if there is any tools that can be use, has scripts
templates available on easy the job of setting up clusters, and subscribing
the sets? Is there a such thing?

I am trying to find template scripts to setup clusters for multi-slave
environment. I am not doing something special nor unique. My environment is
very simple and straight forward. I just need a scripts to run to setup my 3
slaves clusters, that's all.  I need something like the one in attachment.
in case if you don't see any attachments I have also post the script at
below.



Listing 1. cluster_setup.sh


#!/bin/sh

CLUSTER=sql_cluster
DB1=contactdb
DB2=contactdb_slave
H1=localhost
H2=localhost
U=postgres

slonik <<_EOF_

cluster name = $CLUSTER;

node 1 admin conninfo = 'dbname=$DB1 host=$H1 user=$U';
node 2 admin conninfo = 'dbname=$DB2 host=$H2 user=$U';

init cluster (id = 1, comment = 'Node 1');

create set (id = 1, origin = 1,
       comment = 'contact table');

set add table (set id = 1, origin = 1, id = 1,
        full qualified name = 'public.contact',
        comment = 'Table contact');

set add sequence (set id = 1, origin = 1, id = 2,
        full qualified name = 'public.contact_seq',
        comment = 'Sequence contact_seq');

store node (id = 2, comment = 'Node 2');
store path (server = 1, client = 2,
      conninfo = 'dbname=$DB1 host=$H1 user=$U');

store path (server = 2, client = 1,
           conninfo = 'dbname=$DB2 host=$H2 user=$U');

store listen (origin = 1, provider = 1, receiver = 2);
store listen (origin = 2, provider = 2, receiver = 1);
http://www.nabble.com/file/p24677626/cluster_setup.sh.txt
cluster_setup.sh.txt 
-- 
View this message in context: http://www.nabble.com/Helpful-admin-Tools-referral-%3A-scripts-manager--tp24677626p24677626.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Mon Jul 27 03:24:59 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Mon Jul 27 03:25:40 2009
Subject: [Slony1-general] A place to get script templates?
Message-ID: <24678028.post@talk.nabble.com>


Is there a place that available to all public to download script templates ?
scripts to do general and basics things such as setting up database
clusters, subscribing sets , comparing databases etc.

There must be some templates that can be get and use it on my own
environment by simply just do some editing on scripts to fits for my own
need. 

I heard there is a tool on PG foundry site that it contain many helpful
scripts available to use, simply just config, edit and run to fit every
personal needs. But I must ask you all, is there a such thing like that?
what is it called? or anything that is similar to use? 

Thanks
-- 
View this message in context: http://www.nabble.com/A-place-to-get-script-templates--tp24678028p24678028.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From devrim at gunduz.org  Mon Jul 27 03:58:16 2009
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Mon Jul 27 03:59:23 2009
Subject: [Slony1-general] A place to get script templates?
In-Reply-To: <24678028.post@talk.nabble.com>
References: <24678028.post@talk.nabble.com>
Message-ID: <1248692296.2444.13.camel@hp-laptop2.gunduz.org>

T24gTW9uLCAyMDA5LTA3LTI3IGF0IDAzOjI0IC0wNzAwLCByb2N0YWl3YW4gd3JvdGU6Cj4gSXMg
dGhlcmUgYSBwbGFjZSB0aGF0IGF2YWlsYWJsZSB0byBhbGwgcHVibGljIHRvIGRvd25sb2FkIHNj
cmlwdAo+IHRlbXBsYXRlcyA/IHNjcmlwdHMgdG8gZG8gZ2VuZXJhbCBhbmQgYmFzaWNzIHRoaW5n
cyBzdWNoIGFzIHNldHRpbmcgdXAKPiBkYXRhYmFzZSBjbHVzdGVycywgc3Vic2NyaWJpbmcgc2V0
cyAsIGNvbXBhcmluZyBkYXRhYmFzZXMgZXRjLgoKaHR0cDovL3BnZm91bmRyeS5vcmcvcHJvamVj
dHMvc2xvbnkxLWN0bAotLSAKRGV2cmltIEfDnE5Ew5xaLCBSSENFCkNvbW1hbmQgUHJvbXB0IC0g
aHR0cDovL3d3dy5Db21tYW5kUHJvbXB0LmNvbSAKZGV2cmltfmd1bmR1ei5vcmcsIGRldnJpbX5Q
b3N0Z3JlU1FMLm9yZywgZGV2cmltLmd1bmR1en5saW51eC5vcmcudHIKICAgICAgICAgICAgICAg
ICAgIGh0dHA6Ly93d3cuZ3VuZHV6Lm9yZwotLS0tLS0tLS0tLS0tLSBuZXh0IHBhcnQgLS0tLS0t
LS0tLS0tLS0KQSBub24tdGV4dCBhdHRhY2htZW50IHdhcyBzY3J1YmJlZC4uLgpOYW1lOiBub3Qg
YXZhaWxhYmxlClR5cGU6IGFwcGxpY2F0aW9uL3BncC1zaWduYXR1cmUKU2l6ZTogMTk3IGJ5dGVz
CkRlc2M6IFRoaXMgaXMgYSBkaWdpdGFsbHkgc2lnbmVkIG1lc3NhZ2UgcGFydApVcmwgOiBodHRw
Oi8vbGlzdHMuc2xvbnkuaW5mby9waXBlcm1haWwvc2xvbnkxLWdlbmVyYWwvYXR0YWNobWVudHMv
MjAwOTA3MjcvNmIyMjE3OWQvYXR0YWNobWVudC5wZ3AK
From huang913 at gmail.com  Mon Jul 27 02:49:24 2009
From: huang913 at gmail.com (steven huang)
Date: Mon Jul 27 10:34:23 2009
Subject: [Slony1-general] Helpful admin Tools referral : scripts manager
Message-ID: <a4a28fbf0907270249g272ed04dne44512fca208ac7e@mail.gmail.com>

Skipped content of type multipart/alternative-------------- next part -----=
---------
Listing 1. cluster_setup.sh =



#!/bin/sh

CLUSTER=3Dsql_cluster
DB1=3Dcontactdb
DB2=3Dcontactdb_slave
H1=3D (IP)
H2=3D (IP)
U=3Dpostgres

slonik <<_EOF_

cluster name =3D $CLUSTER;

node 1 admin conninfo =3D 'dbname=3D$DB1 host=3D$H1 user=3D$U';
node 2 admin conninfo =3D 'dbname=3D$DB2 host=3D$H2 user=3D$U';

init cluster (id =3D 1, comment =3D 'Node 1');

create set (id =3D 1, origin =3D 1,
       comment =3D 'contact table');

set add table (set id =3D 1, origin =3D 1, id =3D 1,
        full qualified name =3D 'public.contact',
        comment =3D 'Table contact');

set add sequence (set id =3D 1, origin =3D 1, id =3D 2,
        full qualified name =3D 'public.contact_seq',
        comment =3D 'Sequence contact_seq');

store node (id =3D 2, comment =3D 'Node 2');
store path (server =3D 1, client =3D 2,
      conninfo =3D 'dbname=3D$DB1 host=3D$H1 user=3D$U');

store path (server =3D 2, client =3D 1,
           conninfo =3D 'dbname=3D$DB2 host=3D$H2 user=3D$U');

store listen (origin =3D 1, provider =3D 1, receiver =3D 2);
store listen (origin =3D 2, provider =3D 2, receiver =3D 1);
From sugunant at hotmail.com  Mon Jul 27 13:53:48 2009
From: sugunant at hotmail.com (Tham)
Date: Mon Jul 27 14:27:07 2009
Subject: [Slony1-general] slonyI  2.0.2 failover command failure
Message-ID: <24688067.post@talk.nabble.com>


The setup of 3 nodes (node 1 is master, node 2 is a slave that replicates the
master, and node 3 is another slave that replicates node 2). The
corresponding subscribe commands are:

subscribe set (id = 1, provider = 1, receiver = 2, forward = yes);
subscribe set (id = 1, provider = 2, receiver = 3, forward = yes);

At the point, everything is ok.

>From node 2 (slave) machine, issue the following command:

failover (id = 1, backup node = 2);

slonik seems to be stuck (don't get the OS prompt for more than 15 minutes.
after printing out the following message: 
<stdin>:8: NOTICE:  failedNode: set 1 has no other direct receivers - move
now
<stdin>:8: NOTICE:  failedNode: set 1 has no other direct receivers - move
now


Is this failover scenario in this configuration possible ? Any idea why the
slonik gets stuck ?

Thank you,
Tham
-- 
View this message in context: http://www.nabble.com/slonyI--2.0.2-failover-command-failure-tp24688067p24688067.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From dba at richyen.com  Mon Jul 27 15:33:14 2009
From: dba at richyen.com (Richard Yen)
Date: Mon Jul 27 15:33:34 2009
Subject: [Slony1-general] slonyI  2.0.2 failover command failure
In-Reply-To: <24688067.post@talk.nabble.com>
References: <24688067.post@talk.nabble.com>
Message-ID: <566BF2BA-41DB-4523-ACF5-44CED50529E7@richyen.com>

I've done a little research on this, but haven't the time to actually  
work out a fix.

It seems like slonik gets stuck because it is waiting for one of the  
slon daemons to quit or restart successfully.  This has always been an  
issue for me, and I haven't been able to successfully get FAILOVER to  
execute.

--Richard



On Jul 27, 2009, at 1:53 PM, Tham wrote:

>
> The setup of 3 nodes (node 1 is master, node 2 is a slave that  
> replicates the
> master, and node 3 is another slave that replicates node 2). The
> corresponding subscribe commands are:
>
> subscribe set (id = 1, provider = 1, receiver = 2, forward = yes);
> subscribe set (id = 1, provider = 2, receiver = 3, forward = yes);
>
> At the point, everything is ok.
>
>> From node 2 (slave) machine, issue the following command:
>
> failover (id = 1, backup node = 2);
>
> slonik seems to be stuck (don't get the OS prompt for more than 15  
> minutes.
> after printing out the following message:
> <stdin>:8: NOTICE:  failedNode: set 1 has no other direct receivers  
> - move
> now
> <stdin>:8: NOTICE:  failedNode: set 1 has no other direct receivers  
> - move
> now
>
>
> Is this failover scenario in this configuration possible ? Any idea  
> why the
> slonik gets stuck ?
>
> Thank you,
> Tham
> -- 
> View this message in context: http://www.nabble.com/slonyI--2.0.2-failover-command-failure-tp24688067p24688067.html
> Sent from the Slony-I -- General mailing list archive at Nabble.com.
>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

From dba at richyen.com  Mon Jul 27 15:34:23 2009
From: dba at richyen.com (Richard Yen)
Date: Mon Jul 27 15:34:43 2009
Subject: [Slony1-general] new scripts for slony1-ctl?
Message-ID: <7B0C8A37-D52C-4470-8D6C-BBB7C096A064@richyen.com>

Hi All,

I've created a couple new scripts that will allow for drop_node to be  
run via the slony1-ctl script set.  Where do I check them in or who do  
I email them to?  I suppose I can just email to the mailing list as an  
attachment, but I wanted to first go through the right channels.

Thanks!
--Richard
From nettreeinc at gmail.com  Mon Jul 27 21:41:23 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Mon Jul 27 21:41:53 2009
Subject: [Slony1-general] new scripts for slony1-ctl?
In-Reply-To: <7B0C8A37-D52C-4470-8D6C-BBB7C096A064@richyen.com>
References: <7B0C8A37-D52C-4470-8D6C-BBB7C096A064@richyen.com>
Message-ID: <24692407.post@talk.nabble.com>


Hi Richard,

I think share the script at Slony mailing list is a good way shares to most
of us. Or you can post it at the slony1-ctl main page on pgfoundry.

appreciate for sharing



Richard Yen-2 wrote:
> 
> Hi All,
> 
> I've created a couple new scripts that will allow for drop_node to be  
> run via the slony1-ctl script set.  Where do I check them in or who do  
> I email them to?  I suppose I can just email to the mailing list as an  
> attachment, but I wanted to first go through the right channels.
> 
> Thanks!
> --Richard
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/new-scripts-for-slony1-ctl--tp24689446p24692407.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Tue Jul 28 02:36:36 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Jul 28 02:37:15 2009
Subject: [Slony1-general] Administration Scripts
Message-ID: <24695848.post@talk.nabble.com>


There is something I don't understand in chapter 21 "Slony-I Administration
Scripts" from Slony 2.0.2 Documentation. I have listed my questions within
the content at below. 


21.1. altperl Scripts

There is a set of scripts to simplify administeration of set of Slony-I
instances. The scripts support having arbitrary numbers of nodes. They may
be installed as part of the installation process:
Where are the scripts that its talking about?
./configure --with-perltools

This will produce a number of scripts with the prefix slonik_. They
eliminate tedium by always referring to a central configuration file for the
details of your site configuration. A documented sample of this file is
provided in altperl/slon_tools.conf-sample. where is this "altperl" it is
talking about? and this slon_tools.conf file is used to set what? Most also
include some command line help with the "--help" option, making them easier
to learn and use.

Most generate Slonik scripts that are printed to STDOUT. At one time, the
commands were passed directly to slonik for execution. Unfortunately, this
turned out to be a pretty large calibre "foot gun", as minor typos on the
command line led, on a couple of occasions, to pretty calamitous actions.
The savvy administrator should review the script before piping it to slonik.
-- 
View this message in context: http://www.nabble.com/Administration-Scripts-tp24695848p24695848.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From cbbrowne at ca.afilias.info  Tue Jul 28 08:30:42 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul 28 08:30:51 2009
Subject: [Slony1-general] Administration Scripts
In-Reply-To: <24695848.post@talk.nabble.com> (roctaiwan's message of "Tue, 28
	Jul 2009 02:36:36 -0700 (PDT)")
References: <24695848.post@talk.nabble.com>
Message-ID: <873a8gome5.fsf@dba2.int.libertyrms.com>

roctaiwan <nettreeinc@gmail.com> writes:
> There is something I don't understand in chapter 21 "Slony-I Administration
> Scripts" from Slony 2.0.2 Documentation. I have listed my questions within
> the content at below. 
>
>
> 21.1. altperl Scripts
>
> There is a set of scripts to simplify administeration of set of Slony-I
> instances. The scripts support having arbitrary numbers of nodes. They may
> be installed as part of the installation process:
> Where are the scripts that its talking about?
> ./configure --with-perltools

In the source code tree, they reside in the directory "tools/altperl"

By default, they seem to get installed in /usr/local/bin.

If you install from an RPM package or such, then you can look for
where they are actually installed by listing the contents of the RPM
package.

> This will produce a number of scripts with the prefix slonik_. They
> eliminate tedium by always referring to a central configuration file for the
> details of your site configuration. A documented sample of this file is
> provided in altperl/slon_tools.conf-sample. where is this "altperl" it is
> talking about? and this slon_tools.conf file is used to set what? Most also
> include some command line help with the "--help" option, making them easier
> to learn and use.

Well, the tools require a fairly considerable amount of information
about your intended replication cluster, and that information needs to
reside in a file normally called slon_tools.conf.
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Tue Jul 28 08:34:08 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul 28 08:34:17 2009
Subject: [Slony1-general] sl_status incorrectly reports long event lag
In-Reply-To: <db471ace0907260519l55b61b8jf4e91c953a30ea38@mail.gmail.com>
	(Peter Geoghegan's message of "Sun, 26 Jul 2009 13:19:59 +0100")
References: <db471ace0907260519l55b61b8jf4e91c953a30ea38@mail.gmail.com>
Message-ID: <87y6q8n7nz.fsf@dba2.int.libertyrms.com>

Peter Geoghegan <peter.geoghegan86@gmail.com> writes:
> I'm experiencing a curious issue with Slony 2.0.2 . If I shut down a
> slave, and subsequently boot it back up, the sl_status view's event
> lag grows continually, as if replication isn't working between the
> master and that particular slave. However, replication is observably
> working. I can make the sl_status view's lag for the slave return to
> zero by restarting slon daemons.
>
> Why might this be? I find the sl_status view very important for my
> particular application.

Have you run test_slony_state to check the state of the cluster?

   http://slony.info/documentation/monitoring.html#TESTSLONYSTATE

It's likely that confirmations aren't getting back to the origin,
which tends to imply some form of misconfiguration or a network
problem.

That script should help give some idea as to where things are breaking
down.
-- 
let name="cbbrowne" and tld="ca.afilias.info" in name ^ "@" ^ tld;;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From hjoe at inode.at  Tue Jul 28 11:38:17 2009
From: hjoe at inode.at (Josef Huber)
Date: Tue Jul 28 11:39:25 2009
Subject: [Slony1-general] slony1-2.0.2 lag time
Message-ID: <4A6F4599.2090107@inode.at>

Hello,

we are using SLES 10 SP2,Postgres Version 8.3.7 and Slony 2.0.2.
1 Master and 5 Slaves also tried it with 1 Master 3 Slaves, no 
difference with the lag time.

My Problem is,that i've a lag time between 10-30 seconds, this is too 
much, can i shorten the lag time?
Tried Database Full Vaccum,Database run with autovaccum on, slony tables 
are deactivatet for autovaccum.
Anyone an idea, what we can do?
The new System is in testing phase, we write only 30% of the data, and 
the following produktiv system works fine.

We have another system running with postgres SLES10 SP2, Postgres 8.1 
and Slony 1.2,
with 1 Master and 3 Slaves, there we didn't have this problem.
This System runs under VM-Ware,the new System are all Native Servers.


From jcasanov at systemguards.com.ec  Tue Jul 28 12:14:23 2009
From: jcasanov at systemguards.com.ec (Jaime Casanova)
Date: Tue Jul 28 12:14:53 2009
Subject: [Slony1-general] sl_log_1 not used
Message-ID: <3073cc9b0907281214l1b7ceafesed9400da75ca4345@mail.gmail.com>

Hi,

A client has an installation of slony I 1.2.15 with pg 8.3.7 and they
are having troubles with the size of the database...
while checking i found that sl_log_2 has more than  102784620 dead
tuples and the table size grows to 22gb, vac_frecuency is 0 and
autovacuum is enabled but no vacuum has happened... i will execute a
manual vacuum later today when the impact is not so big

AFAIUI, slony should switch sl_log_* tables from time to time and then
truncate the other one but i don't see sl_log_1 being used (never), it
has 0 live tuples and 0 dead tuples...

ideas?

-- 
Atentamente,
Jaime Casanova
Soporte y capacitaci?n de PostgreSQL
Asesor?a y desarrollo de sistemas
Guayaquil - Ecuador
Cel. +59387171157
From cbbrowne at ca.afilias.info  Tue Jul 28 12:44:27 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul 28 12:44:44 2009
Subject: [Slony1-general] sl_log_1 not used
In-Reply-To: <3073cc9b0907281214l1b7ceafesed9400da75ca4345@mail.gmail.com>
	(Jaime Casanova's message of "Tue, 28 Jul 2009 14:14:23 -0500")
References: <3073cc9b0907281214l1b7ceafesed9400da75ca4345@mail.gmail.com>
Message-ID: <87y6q8lhic.fsf@dba2.int.libertyrms.com>

Jaime Casanova <jcasanov@systemguards.com.ec> writes:
> A client has an installation of slony I 1.2.15 with pg 8.3.7 and they
> are having troubles with the size of the database...
> while checking i found that sl_log_2 has more than  102784620 dead
> tuples and the table size grows to 22gb, vac_frecuency is 0 and
> autovacuum is enabled but no vacuum has happened... i will execute a
> manual vacuum later today when the impact is not so big
>
> AFAIUI, slony should switch sl_log_* tables from time to time and then
> truncate the other one but i don't see sl_log_1 being used (never), it
> has 0 live tuples and 0 dead tuples...
>
> ideas?

You may ask explicitly to initiate a switch between the log tables...

 - Via psql, run against the offending node:

      select _my_slony_schema.logswitch_start();

 - Via slonik

      [preamble elided...  assuming the origin is node #1...]

      switch log ( id = 1 );

Either are quite safe; all that the slonik command does is to call the
stored function logswitch_start().

It's conceivable that if slon processes are, for some reason, getting
killed and restarted frequently, that might [mumble, mumble, not
verifying details too much because I'm feeling lazy ;-)] suppress the
switch from ever getting requested.
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From guillaume at lelarge.info  Tue Jul 28 12:52:38 2009
From: guillaume at lelarge.info (Guillaume Lelarge)
Date: Tue Jul 28 12:52:56 2009
Subject: [Slony1-general] new scripts for slony1-ctl?
In-Reply-To: <24692407.post@talk.nabble.com>
References: <7B0C8A37-D52C-4470-8D6C-BBB7C096A064@richyen.com>
	<24692407.post@talk.nabble.com>
Message-ID: <200907282152.38348.guillaume@lelarge.info>

Le mardi 28 juillet 2009 ? 06:41:23, roctaiwan a ?crit :
> [...]
> I think share the script at Slony mailing list is a good way shares to most
> of us. Or you can post it at the slony1-ctl main page on pgfoundry.
>
> appreciate for sharing
>

You should also send them to St?phane Schildknecht 
(stephane.schildknecht@postgresqlfr.org). He's the main developer of slony1-
ctl and he'll really enjoy your help.

Regards.


-- 
Guillaume.
 http://www.postgresqlfr.org
 http://dalibo.com
From dba at richyen.com  Tue Jul 28 12:58:42 2009
From: dba at richyen.com (Richard Yen)
Date: Tue Jul 28 12:59:01 2009
Subject: [Slony1-general] new scripts for slony1-ctl?
In-Reply-To: <200907282152.38348.guillaume@lelarge.info>
References: <7B0C8A37-D52C-4470-8D6C-BBB7C096A064@richyen.com>
	<24692407.post@talk.nabble.com>
	<200907282152.38348.guillaume@lelarge.info>
Message-ID: <33905A78-90A5-43FF-A549-F4FC74CCED1B@richyen.com>

ok, that's the first thing I did, but haven't heard any  
acknowledgement from him in a week.  Guess he might be busy or  
something.

Thanks!
--Richard


On Jul 28, 2009, at 12:52 PM, Guillaume Lelarge wrote:

> Le mardi 28 juillet 2009 ? 06:41:23, roctaiwan a ?crit :
>> [...]
>> I think share the script at Slony mailing list is a good way shares  
>> to most
>> of us. Or you can post it at the slony1-ctl main page on pgfoundry.
>>
>> appreciate for sharing
>>
>
> You should also send them to St?phane Schildknecht
> (stephane.schildknecht@postgresqlfr.org). He's the main developer of  
> slony1-
> ctl and he'll really enjoy your help.
>
> Regards.
>
>
> -- 
> Guillaume.
> http://www.postgresqlfr.org
> http://dalibo.com
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

From jcasanov at systemguards.com.ec  Tue Jul 28 13:06:06 2009
From: jcasanov at systemguards.com.ec (Jaime Casanova)
Date: Tue Jul 28 13:06:42 2009
Subject: [Slony1-general] sl_log_1 not used
In-Reply-To: <87y6q8lhic.fsf@dba2.int.libertyrms.com>
References: <3073cc9b0907281214l1b7ceafesed9400da75ca4345@mail.gmail.com> 
	<87y6q8lhic.fsf@dba2.int.libertyrms.com>
Message-ID: <3073cc9b0907281306k68b3e46evc9777177aefe536e@mail.gmail.com>

On Tue, Jul 28, 2009 at 2:44 PM, Christopher
Browne<cbbrowne@ca.afilias.info> wrote:
>
> You may ask explicitly to initiate a switch between the log tables...
>
> ?- Via psql, run against the offending node:
>
> ? ? ?select _my_slony_schema.logswitch_start();
>

ok, this did the trick for now. thanks
i still have to figure out what causes the switch never fire.

-- 
Atentamente,
Jaime Casanova
Soporte y capacitaci?n de PostgreSQL
Asesor?a y desarrollo de sistemas
Guayaquil - Ecuador
Cel. +59387171157
From guillaume at lelarge.info  Tue Jul 28 13:23:50 2009
From: guillaume at lelarge.info (Guillaume Lelarge)
Date: Tue Jul 28 13:24:09 2009
Subject: [Slony1-general] new scripts for slony1-ctl?
In-Reply-To: <33905A78-90A5-43FF-A549-F4FC74CCED1B@richyen.com>
References: <7B0C8A37-D52C-4470-8D6C-BBB7C096A064@richyen.com>
	<200907282152.38348.guillaume@lelarge.info>
	<33905A78-90A5-43FF-A549-F4FC74CCED1B@richyen.com>
Message-ID: <200907282223.50927.guillaume@lelarge.info>

Le mardi 28 juillet 2009 ? 21:58:42, Richard Yen a ?crit :
> ok, that's the first thing I did, but haven't heard any
> acknowledgement from him in a week.  Guess he might be busy or
> something.
>

He was on holiday.


-- 
Guillaume.
 http://www.postgresqlfr.org
 http://dalibo.com
From jeff at frostconsultingllc.com  Tue Jul 28 14:49:55 2009
From: jeff at frostconsultingllc.com (Jeff Frost)
Date: Tue Jul 28 14:50:17 2009
Subject: [Slony1-general] 2.0.1 log volume
Message-ID: <4A6F7283.7050200@frostconsultingllc.com>

So, I'm running 2.0.1 with logging going to syslog and it's generating a
few gig of log per day on the subscriber nodes that are filled with this:

Jul 28 14:46:14 db4 slon[19244]: [609-1] 2009-07-28 14:46:14 PDT INFO  
remoteWorkerThread_1: sync_helper timing:  pqexec (s/count)- provider
0.004/3 - subscriber
Jul 28 14:46:14 db4 slon[19244]: [609-2]  0.000/3
Jul 28 14:46:14 db4 slon[19244]: [610-1] 2009-07-28 14:46:14 PDT INFO  
remoteWorkerThread_1: sync_helper timing:  large tuples 0.000/0
Jul 28 14:46:14 db4 slon[19244]: [611-1] 2009-07-28 14:46:14 PDT INFO  
remoteWorkerThread_1: SYNC 185177 done in 0.010 seconds
Jul 28 14:46:14 db4 slon[19244]: [612-1] 2009-07-28 14:46:14 PDT INFO  
remoteWorkerThread_1: SYNC 185177 sync_event timing:  pqexec (s/count)-
provider 0.001/1 -
Jul 28 14:46:14 db4 slon[19244]: [612-2]  subscriber 0.004/1 - IUD 0.000/0

Any way I can get it to quiet down?

Log settings:
log_level = 1
syslog = 2

-- 
Jeff Frost, Owner 	<jeff@frostconsultingllc.com>
Frost Consulting, LLC 	http://www.frostconsultingllc.com/
Phone: 916-647-6411	FAX: 916-405-4032

From devrim at gunduz.org  Tue Jul 28 14:55:37 2009
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Tue Jul 28 14:56:14 2009
Subject: [Slony1-general] 2.0.1 log volume
In-Reply-To: <4A6F7283.7050200@frostconsultingllc.com>
References: <4A6F7283.7050200@frostconsultingllc.com>
Message-ID: <1248818137.18987.20.camel@hp-laptop2.gunduz.org>

T24gVHVlLCAyMDA5LTA3LTI4IGF0IDE0OjQ5IC0wNzAwLCBKZWZmIEZyb3N0IHdyb3RlOgo+IAo+
IEFueSB3YXkgSSBjYW4gZ2V0IGl0IHRvIHF1aWV0IGRvd24/CgpJSVJDIHlvdSB3aWxsIG5lZWQg
dG8gbG93ZXIgREVCVUdMRVZFTCwgd2hpY2ggaXMgaW4gc2xvbl90b29scy5jb25mLgotLSAKRGV2
cmltIEfDnE5Ew5xaLCBSSENFCkNvbW1hbmQgUHJvbXB0IC0gaHR0cDovL3d3dy5Db21tYW5kUHJv
bXB0LmNvbSAKZGV2cmltfmd1bmR1ei5vcmcsIGRldnJpbX5Qb3N0Z3JlU1FMLm9yZywgZGV2cmlt
Lmd1bmR1en5saW51eC5vcmcudHIKICAgICAgICAgICAgICAgICAgIGh0dHA6Ly93d3cuZ3VuZHV6
Lm9yZwotLS0tLS0tLS0tLS0tLSBuZXh0IHBhcnQgLS0tLS0tLS0tLS0tLS0KQSBub24tdGV4dCBh
dHRhY2htZW50IHdhcyBzY3J1YmJlZC4uLgpOYW1lOiBub3QgYXZhaWxhYmxlClR5cGU6IGFwcGxp
Y2F0aW9uL3BncC1zaWduYXR1cmUKU2l6ZTogMTk3IGJ5dGVzCkRlc2M6IFRoaXMgaXMgYSBkaWdp
dGFsbHkgc2lnbmVkIG1lc3NhZ2UgcGFydApVcmwgOiBodHRwOi8vbGlzdHMuc2xvbnkuaW5mby9w
aXBlcm1haWwvc2xvbnkxLWdlbmVyYWwvYXR0YWNobWVudHMvMjAwOTA3MjkvMzU0YTAzMGQvYXR0
YWNobWVudC5wZ3AK
From jeff at frostconsultingllc.com  Tue Jul 28 15:26:33 2009
From: jeff at frostconsultingllc.com (Jeff Frost)
Date: Tue Jul 28 15:26:54 2009
Subject: [Slony1-general] 2.0.1 log volume
In-Reply-To: <1248818137.18987.20.camel@hp-laptop2.gunduz.org>
References: <4A6F7283.7050200@frostconsultingllc.com>
	<1248818137.18987.20.camel@hp-laptop2.gunduz.org>
Message-ID: <4A6F7B19.7000203@frostconsultingllc.com>

CgpEZXZyaW0gR8OcTkTDnFogd3JvdGU6Cj4gT24gVHVlLCAyMDA5LTA3LTI4IGF0IDE0OjQ5IC0w
NzAwLCBKZWZmIEZyb3N0IHdyb3RlOgo+ICAgCj4+IEFueSB3YXkgSSBjYW4gZ2V0IGl0IHRvIHF1
aWV0IGRvd24/Cj4+ICAgICAKPgo+IElJUkMgeW91IHdpbGwgbmVlZCB0byBsb3dlciBERUJVR0xF
VkVMLCB3aGljaCBpcyBpbiBzbG9uX3Rvb2xzLmNvbmYuCj4gICAKQW5kIHRoYXQncyBkaWZmZXJl
bnQgZnJvbSBsb2dfbGV2ZWwgaW4gc2xvbi5jb25mPyAgSGVyZSdzIHRoZSBjb21tZW50CmZyb20g
dGhlIHNhbXBsZToKCiMgRGVidWcgbG9nIGxldmVsIChoaWdoZXIgdmFsdWUgPT0+IG1vcmUgb3V0
cHV0KS4gIFJhbmdlOiBbMCw0XSwgZGVmYXVsdCA0CmxvZ19sZXZlbCA9IDEKClRoYXQgdXNlZCB0
byBkbyB0aGUgdHJpY2ssIGJ1dCBpbiAyLnggaXQgc2VlbXMgdG8gYmUgbW9yZSBjaGF0dHkuICBJ
CnRyaWVkIHNldHRpbmcgaXQgdG8gMCBidXQgaXQncyBzdGlsbCBqdXN0IGFzIGNoYXR0eS4KCi0t
IApKZWZmIEZyb3N0LCBPd25lciAJPGplZmZAZnJvc3Rjb25zdWx0aW5nbGxjLmNvbT4KRnJvc3Qg
Q29uc3VsdGluZywgTExDIAlodHRwOi8vd3d3LmZyb3N0Y29uc3VsdGluZ2xsYy5jb20vClBob25l
OiA5MTYtNjQ3LTY0MTEJRkFYOiA5MTYtNDA1LTQwMzIKCi0tLS0tLS0tLS0tLS0tIG5leHQgcGFy
dCAtLS0tLS0tLS0tLS0tLQpBbiBIVE1MIGF0dGFjaG1lbnQgd2FzIHNjcnViYmVkLi4uClVSTDog
aHR0cDovL2xpc3RzLnNsb255LmluZm8vcGlwZXJtYWlsL3Nsb255MS1nZW5lcmFsL2F0dGFjaG1l
bnRzLzIwMDkwNzI4L2EwNTRhYWU5L2F0dGFjaG1lbnQuaHRtCg==
From cbbrowne at ca.afilias.info  Tue Jul 28 15:37:22 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Tue Jul 28 15:37:45 2009
Subject: [Slony1-general] 2.0.1 log volume
In-Reply-To: <4A6F7B19.7000203@frostconsultingllc.com> (Jeff Frost's message
	of "Tue, 28 Jul 2009 15:26:33 -0700")
References: <4A6F7283.7050200@frostconsultingllc.com>
	<1248818137.18987.20.camel@hp-laptop2.gunduz.org>
	<4A6F7B19.7000203@frostconsultingllc.com>
Message-ID: <87iqhcl9i5.fsf@dba2.int.libertyrms.com>

Jeff Frost <jeff@frostconsultingllc.com> writes:
> 			   And that's different from log_level in slon.conf??? Here's the comment from the sample:
> 				 # Debug log level (higher value ==> more output).?? Range: [0,4], default 4
> 								log_level = 1
> 	  That used to do the trick, but in 2.x it seems to be more chatty.?? I tried setting it to 0 but it's still just as chatty.

I tried to make 2.x "more usefully chatty."

It used to be that to get *anything* out of the logs, you'd have to
tune them up to the "whoah, that's way too much!" level.

I tried pushing stuff I thought was "boring" down towards debugging
levels.

It's possible that there's too much stuff at SLON_INFO level.

It's pretty easy to grep for it all:

for type in FATAL ERROR WARN CONFIG INFO DEBUG1 DEBUG2 DEBUG3 DEBUG4; do
   find src/slon -name "*.c" | xargs grep -n "SLON_${type}" > /tmp/MSG_${type}
done

I'd certainly be open to the suggestion that some logging messages
ought to have their levels changed.

I point out how to get the "grand list" to encourage thinking about it
at least a little bit systematically.

It seems fairly clear that the messages you mentioned are ones that
warrant being moved to DEBUG1, so I'll do that, for sure.  If you have
other suggestions, then suggest away.
-- 
select 'cbbrowne' || '@' || 'ca.afilias.info';
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From peter.geoghegan86 at gmail.com  Tue Jul 28 16:22:05 2009
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Tue Jul 28 16:22:27 2009
Subject: [Slony1-general] sl_status incorrectly reports long event lag
In-Reply-To: <87y6q8n7nz.fsf@dba2.int.libertyrms.com>
References: <db471ace0907260519l55b61b8jf4e91c953a30ea38@mail.gmail.com>
	<87y6q8n7nz.fsf@dba2.int.libertyrms.com>
Message-ID: <db471ace0907281622j2f1927bbnb7e8a85e9ccfb43b@mail.gmail.com>

Chris,

> Have you run test_slony_state to check the state of the cluster?
>
> ? http://slony.info/documentation/monitoring.html#TESTSLONYSTATE
>
> It's likely that confirmations aren't getting back to the origin,
> which tends to imply some form of misconfiguration or a network
> problem.
>
> That script should help give some idea as to where things are breaking
> down.

I've been able to re-create the problem in isolation, away from my
live cluster, using two virtual machines (One MS Windows master with
all slons (the slony service), running Hiroshi Saito's Slony 2.0.2
package, the other a Linux machine that I've built 2.0.2 on ). This is
perhaps the simplest possible replication scenario: one master, one
slave, no failover or cascading, with a slonik script more or less
copied verbatim from the docs' "replicating your first database"
example. To my mind, this suggests that the problem is not in fact
down to misconfiguration.

I'll run those tests tomorrow in work and report back what they find.

Regards,
Peter Geoghegan
From jeff at frostconsultingllc.com  Tue Jul 28 19:26:32 2009
From: jeff at frostconsultingllc.com (Jeff Frost)
Date: Tue Jul 28 19:27:00 2009
Subject: [Slony1-general] 2.0.1 log volume
In-Reply-To: <87iqhcl9i5.fsf@dba2.int.libertyrms.com>
References: <4A6F7283.7050200@frostconsultingllc.com>	<1248818137.18987.20.camel@hp-laptop2.gunduz.org>	<4A6F7B19.7000203@frostconsultingllc.com>
	<87iqhcl9i5.fsf@dba2.int.libertyrms.com>
Message-ID: <4A6FB358.1030209@frostconsultingllc.com>

Christopher Browne wrote:
> Jeff Frost <jeff@frostconsultingllc.com> writes:
>   =

>> 			   And that's different from log_level in slon.conf?=C2  Here's the c=
omment from the sample:
>> 				 # Debug log level (higher value =3D=3D> more output).=C2  Range: [0=
,4], default 4
>> 								log_level =3D 1
>> 	  That used to do the trick, but in 2.x it seems to be more chatty.=C2 =
 I tried setting it to 0 but it's still just as chatty.
>>     =

>
> I tried to make 2.x "more usefully chatty."
>
> It used to be that to get *anything* out of the logs, you'd have to
> tune them up to the "whoah, that's way too much!" level.
>
>   =

I'd definitely agree with that!
> I tried pushing stuff I thought was "boring" down towards debugging
> levels.
>
> It's possible that there's too much stuff at SLON_INFO level.
>
> It's pretty easy to grep for it all:
>
> for type in FATAL ERROR WARN CONFIG INFO DEBUG1 DEBUG2 DEBUG3 DEBUG4; do
>    find src/slon -name "*.c" | xargs grep -n "SLON_${type}" > /tmp/MSG_${=
type}
> done
>
>   =

How do the various types above map to the 0-4 log_level?

I'm looking here: http://slony.info/documentation/slon.html but it's
still not obvious to me which ones are which. I'm guessing up to info is
0 and 1-4 maps to debug1-4?
> I'd certainly be open to the suggestion that some logging messages
> ought to have their levels changed.
>
> I point out how to get the "grand list" to encourage thinking about it
> at least a little bit systematically.
>
> It seems fairly clear that the messages you mentioned are ones that
> warrant being moved to DEBUG1, so I'll do that, for sure.  If you have
> other suggestions, then suggest away.
>   =


-- =

Jeff Frost, Owner 	<jeff@frostconsultingllc.com>
Frost Consulting, LLC 	http://www.frostconsultingllc.com/
Phone: 916-647-6411	FAX: 916-405-4032

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090728/=
ff758dbe/attachment.htm
From nettreeinc at gmail.com  Tue Jul 28 21:01:20 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Jul 28 21:01:49 2009
Subject: [Slony1-general] Administration Scripts
In-Reply-To: <873a8gome5.fsf@dba2.int.libertyrms.com>
References: <24695848.post@talk.nabble.com>
	<873a8gome5.fsf@dba2.int.libertyrms.com>
Message-ID: <24712053.post@talk.nabble.com>


Thanks Christopher.

Now, back even more backward a little bit. In this Chapter 21 what "package"
and installation it is talking about? is it including within the Enterprise
DB standard one-click installer package? is this talking about a "tool" that
I needs to download and install first? You also mentioned a RPM package,
there must be something I need to download especially so I would know what
chapter 21 is talking about.

I searched whole hard drive, the only /tools directories I have none arel
not related with postgresql. 
also, I can't find slon_tools.conf either. 



Christopher Browne wrote:
> 
>>
>> 21.1. altperl Scripts
>>
>> There is a set of scripts to simplify administeration of set of Slony-I
>> instances. The scripts support having arbitrary numbers of nodes. They
>> may
>> be installed as part of the installation process:
>> Where are the scripts that its talking about?
>> ./configure --with-perltools
> 
> In the source code tree, they reside in the directory "tools/altperl"
> 
> By default, they seem to get installed in /usr/local/bin.
> 
> If you install from an RPM package or such, then you can look for
> where they are actually installed by listing the contents of the RPM
> package.
> 
>> This will produce a number of scripts with the prefix slonik_. They
>> eliminate tedium by always referring to a central configuration file for
>> the
>> details of your site configuration. A documented sample of this file is
>> provided in altperl/slon_tools.conf-sample. where is this "altperl" it is
>> talking about? and this slon_tools.conf file is used to set what? Most
>> also
>> include some command line help with the "--help" option, making them
>> easier
>> to learn and use.
> 
> Well, the tools require a fairly considerable amount of information
> about your intended replication cluster, and that information needs to
> reside in a file normally called slon_tools.conf.
> 
> 
> -- 
> output = reverse("ofni.sailifa.ac" "@" "enworbbc")
> <http://dba2.int.libertyrms.com/>
> Christopher Browne
> (416) 673-4124 (land)
> "Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
> phasers on the Heffalump, Piglet, meet me in transporter room three"
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general
> 
> 

-- 
View this message in context: http://www.nabble.com/Administration-Scripts-tp24695848p24712053.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From nettreeinc at gmail.com  Tue Jul 28 21:47:46 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Tue Jul 28 21:48:16 2009
Subject: re[Slony1-general] plicate the whole DB and not just selecting by
	tables, how?
Message-ID: <24712383.post@talk.nabble.com>


the script template used to setup my cluster is attached in the attachment. 
http://www.nabble.com/file/p24712383/cluster_setup.sh.txt
cluster_setup.sh.txt 

This template is for scenario of one master with one slave environment and
choose tables within the DB which I would like to replicating. 

Q: How do I rewrite this script so I don't have to select tables, instead
selecting just the whole database to be replicate?

Q: how can I rewrite this script if I have more then one slaves? 

Answer for this two question is very important for me, please give me some
guide line and please make it easy to understand. 


-- 
View this message in context: http://www.nabble.com/replicate-the-whole-DB-and-not-just-selecting-by-tables%2C-how--tp24712383p24712383.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From sweta.mulgavker at gmail.com  Wed Jul 29 00:02:03 2009
From: sweta.mulgavker at gmail.com (Sweta Mulgavker)
Date: Wed Jul 29 00:02:38 2009
Subject: [Slony1-general] Replication between two clusters
Message-ID: <4077e7710907290002y4247c53eo3f1135b22d24cd4a@mail.gmail.com>

Hello All,

I have a system setup consisting of two machines say 'A' with 'IP-A' and 'B'
as 'IP-B'. Both these machines are running fake... Which is providing a
virtual IP for my machines say  'IP-C'
My current master is that machine which  holds 'IP-C' .

So if machine 'A' has its current IP as 'IP-C' then 'A' is the master and
'B' is the slave.

So current IP of 'A' --> 'IP-C' --> master  and  'B' --> 'IP-B' --> slave.

If machine 'A' goes down due to some reason  then machine 'B' takes up the
fake IP, 'IP-C' and it comes up as the master. Machine 'A' becomes the
slave. i.e.

current IP of 'A' --> 'IP-A' --> slave  and  'B' --> 'IP-C' --> master.

Postgres is running on both the machines.. and the data can be accessed /
modified  only from the current master.

Now, I wish to keep the data on both the machines in sync using slony...
Because if the databases are not in sync then my slave goes down and I have
to keep on taking dump from master and restore on the slave.

To do this how should I configure Slony???

Also does Slony replicate between two seprate clusters??? On two different
machines??

Is it possible?

Any help will be highly apperciated :)

Thankyou in advance.
Regards,
Sweta Mulgavker.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090729/=
a5531fdf/attachment.htm
From nettreeinc at gmail.com  Wed Jul 29 03:38:53 2009
From: nettreeinc at gmail.com (roctaiwan)
Date: Wed Jul 29 03:39:35 2009
Subject: [Slony1-general] Will Slony-I replicating Indexes also?
Message-ID: <24716658.post@talk.nabble.com>


Hi all,

I wants to know what will Slony replicating and what will not? 

will it replicating Schema? Index? functions? 

I am concerning about Indexes, since it would affect performance on user
from retrieving data from my slave nodes. 

If there a solution for this issue? or it has to done by hand manually doing
dumpall to slaves?

-- 
View this message in context: http://www.nabble.com/Will-Slony-I-replicating-Indexes-also--tp24716658p24716658.html
Sent from the Slony-I -- General mailing list archive at Nabble.com.

From peter.geoghegan86 at gmail.com  Wed Jul 29 07:08:36 2009
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Wed Jul 29 07:08:41 2009
Subject: [Slony1-general] sl_status incorrectly reports long event lag
In-Reply-To: <db471ace0907281622j2f1927bbnb7e8a85e9ccfb43b@mail.gmail.com>
References: <db471ace0907260519l55b61b8jf4e91c953a30ea38@mail.gmail.com>
	<87y6q8n7nz.fsf@dba2.int.libertyrms.com>
	<db471ace0907281622j2f1927bbnb7e8a85e9ccfb43b@mail.gmail.com>
Message-ID: <db471ace0907290708q3f224e73r279745611b91c3e0@mail.gmail.com>

Here are my results when running test_slony_state-dbi.pl from a remote
Linux machine:

peter@peter-development-machine:~/slony1-2.0.2/tools>
./test_slony_state-dbi.pl --user=postgres --password=my_password
--host=10.0.0.80 --database=lustre --cluster=lustre_cluster

DSN: dbi:Pg:dbname=lustre;host=10.0.0.80;user=postgres;password=my_password;
===========================
Rummage for DSNs
=============================
Query:

   select p.pa_server, p.pa_conninfo
   from "_lustre_cluster".sl_path p
--   where exists (select * from "_lustre_cluster".sl_subscribe s where
--                          (s.sub_provider = p.pa_server or
s.sub_receiver = p.pa_server) and
--                          sub_active = 't')
   group by pa_server, pa_conninfo;


Tests for node 1 - DSN = dbi:Pg:dbname=lustre host=127.0.0.1
user=postgres password=my_password
========================================
pg_listener info:
Pages: 0
Tuples: 0

Size Tests
================================================
       sl_log_1         0  0.000000
       sl_log_2         0  0.000000
      sl_seqlog         0  0.000000

Listen Path Analysis
===================================================
No problems found with sl_listen

--------------------------------------------------------------------------------
Summary of event info
 Origin  Min SYNC  Max SYNC Min SYNC Age Max SYNC Age
================================================================================
      1     28741     29914     00:00:00     02:58:00    0
      2        18        18 1 day 01:24:00 1 day 01:24:00    1
      3        26        26 1 day 01:19:00 1 day 01:19:00    1


---------------------------------------------------------------------------------
Summary of sl_confirm aging
   Origin   Receiver   Min SYNC   Max SYNC  Age of latest SYNC  Age of
eldest SYNC
=================================================================================
        1          2      28741      28741      03:03:00      03:03:00    1
        1          3      29800      29913      00:00:00      00:18:00    0
        2          1         18         18  1 day 01:19:00  1 day 01:19:00    1
        2          3         18         18  1 day 01:19:00  1 day 01:19:00    1
        3          1         26         26  1 day 01:19:00  1 day 01:19:00    1
        3          2         26         26  1 day 01:24:00  1 day 01:24:00    1


------------------------------------------------------------------------------

Listing of old open connections on node 1
       Database             PID            User    Query Age
     Query
================================================================================


Tests for node 3 - DSN = dbi:Pg:dbname=lustre_slave host=10.0.0.82
user=postgres password=my_password
========================================
DBI connect('dbname=lustre_slave host=10.0.0.82 user=postgres
password=my_password','',...) failed: FATAL:  database "lustre_slave"
does not exist at ./test_slony_state-dbi.pl line 79

Tests for node 2 - DSN = dbi:Pg:dbname=lustre_slave host=10.0.0.81
user=postgres password=my_password
========================================
DBI connect('dbname=lustre_slave host=10.0.0.81 user=postgres
password=my_password','',...) failed: FATAL:  database "lustre_slave"
does not exist at ./test_slony_state-dbi.pl line 79
sh: - : invalid option
Usage:	sh [GNU long option] [option] ...
	sh [GNU long option] [option] script-file ...
GNU long options:
	--debug
	--debugger
	--dump-po-strings
	--dump-strings
	--help
	--init-file
	--login
	--noediting
	--noprofile
	--norc
	--posix
	--protected
	--rcfile
	--restricted
	--verbose
	--version
	--wordexp
Shell options:
	-irsD or -c command or -O shopt_option		(invocation only)
	-abefhkmnptuvxBCHP or -o option


I have no idea why test_slony_state-dbi.pl cannot connect to the two
slave nodes's DBs - the databases are plainly present on the nodes - I
can connect directly to them from the machine that I'm running the
test on, and see everything is in order.

Why might this problem occur?

Thanks,
Peter Geoghegan
From cbbrowne at ca.afilias.info  Wed Jul 29 08:00:38 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 29 08:00:46 2009
Subject: [Slony1-general] sl_status incorrectly reports long event lag
In-Reply-To: <db471ace0907290708q3f224e73r279745611b91c3e0@mail.gmail.com>
	(Peter Geoghegan's message of "Wed, 29 Jul 2009 15:08:36 +0100")
References: <db471ace0907260519l55b61b8jf4e91c953a30ea38@mail.gmail.com>
	<87y6q8n7nz.fsf@dba2.int.libertyrms.com>
	<db471ace0907281622j2f1927bbnb7e8a85e9ccfb43b@mail.gmail.com>
	<db471ace0907290708q3f224e73r279745611b91c3e0@mail.gmail.com>
Message-ID: <87d47jlejt.fsf@dba2.int.libertyrms.com>

Peter Geoghegan <peter.geoghegan86@gmail.com> writes:
> --------------------------------------------------------------------------------
> Summary of event info
>  Origin  Min SYNC  Max SYNC Min SYNC Age Max SYNC Age
> ================================================================================
>       1     28741     29914     00:00:00     02:58:00    0
>       2        18        18 1 day 01:24:00 1 day 01:24:00    1
>       3        26        26 1 day 01:19:00 1 day 01:19:00    1
>
>
> ---------------------------------------------------------------------------------
> Summary of sl_confirm aging
>    Origin   Receiver   Min SYNC   Max SYNC  Age of latest SYNC  Age of
> eldest SYNC
> =================================================================================
>         1          2      28741      28741      03:03:00      03:03:00    1
>         1          3      29800      29913      00:00:00      00:18:00    0
>         2          1         18         18  1 day 01:19:00  1 day 01:19:00    1
>         2          3         18         18  1 day 01:19:00  1 day 01:19:00    1
>         3          1         26         26  1 day 01:19:00  1 day 01:19:00    1
>         3          2         26         26  1 day 01:24:00  1 day 01:24:00    1

This points us towards the problem...  Apparently node #1 isn't
getting events from nodes 2 and 3, and 2&3 aren't getting events from
each other.

> Tests for node 3 - DSN = dbi:Pg:dbname=lustre_slave host=10.0.0.82
> user=postgres password=my_password
> ========================================
> DBI connect('dbname=lustre_slave host=10.0.0.82 user=postgres
> password=my_password','',...) failed: FATAL:  database "lustre_slave"
> does not exist at ./test_slony_state-dbi.pl line 79
>
> Tests for node 2 - DSN = dbi:Pg:dbname=lustre_slave host=10.0.0.81
> user=postgres password=my_password
> ========================================
> DBI connect('dbname=lustre_slave host=10.0.0.81 user=postgres
> password=my_password','',...) failed: FATAL:  database "lustre_slave"
> does not exist at ./test_slony_state-dbi.pl line 79

... And this contributes towards telling us something about why ...

> I have no idea why test_slony_state-dbi.pl cannot connect to the two
> slave nodes's DBs - the databases are plainly present on the nodes -
> I can connect directly to them from the machine that I'm running the
> test on, and see everything is in order.
>
> Why might this problem occur?

This is the point at which it ceases to be analyzed as a "Slony-I
problem," and as a "what's busted with PostgreSQL connections?" 
problem.

Apparently the configuration that you've set up, e.g.

   dbname=lustre_slave host=10.0.0.81 user=postgres password=my_password
   dbname=lustre_slave host=10.0.0.82 user=postgres password=my_password

*isn't* working.

Figure out the difference between that and the legitimate
configuration and then you can run slonik "STORE PATH" requests with
the fixed connection info which should resolve the problem.
-- 
output = ("cbbrowne" "@" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From peter.geoghegan86 at gmail.com  Wed Jul 29 08:34:25 2009
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Wed Jul 29 08:34:34 2009
Subject: [Slony1-general] sl_status incorrectly reports long event lag
In-Reply-To: <87d47jlejt.fsf@dba2.int.libertyrms.com>
References: <db471ace0907260519l55b61b8jf4e91c953a30ea38@mail.gmail.com>
	<87y6q8n7nz.fsf@dba2.int.libertyrms.com>
	<db471ace0907281622j2f1927bbnb7e8a85e9ccfb43b@mail.gmail.com>
	<db471ace0907290708q3f224e73r279745611b91c3e0@mail.gmail.com>
	<87d47jlejt.fsf@dba2.int.libertyrms.com>
Message-ID: <db471ace0907290834g431a8ae7q23516f3db5e5cfb9@mail.gmail.com>

Chris,

> This is the point at which it ceases to be analyzed as a "Slony-I
> problem," and as a "what's busted with PostgreSQL connections?"
> problem.
>
> Apparently the configuration that you've set up, e.g.
>
> ? dbname=lustre_slave host=10.0.0.81 user=postgres password=my_password
> ? dbname=lustre_slave host=10.0.0.82 user=postgres password=my_password
>
> *isn't* working.

I've followed your instructions, and the problem is resolved.

Thanks a lot for your help,
Peter Geoghegan
From cbbrowne at ca.afilias.info  Wed Jul 29 09:06:34 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 29 09:06:43 2009
Subject: [Slony1-general] Administration Scripts
In-Reply-To: <24712053.post@talk.nabble.com> (roctaiwan's message of "Tue, 28
	Jul 2009 21:01:20 -0700 (PDT)")
References: <24695848.post@talk.nabble.com>
	<873a8gome5.fsf@dba2.int.libertyrms.com>
	<24712053.post@talk.nabble.com>
Message-ID: <878wi7lbhx.fsf@dba2.int.libertyrms.com>

roctaiwan <nettreeinc@gmail.com> writes:
> Thanks Christopher.
>
> Now, back even more backward a little bit. In this Chapter 21 what
> "package" and installation it is talking about? is it including
> within the Enterprise DB standard one-click installer package? is
> this talking about a "tool" that I needs to download and install
> first? You also mentioned a RPM package, there must be something I
> need to download especially so I would know what chapter 21 is
> talking about.
>
> I searched whole hard drive, the only /tools directories I have none arel
> not related with postgresql. 
> also, I can't find slon_tools.conf either. 

I'm sorry - I can't speak for what EnterpriseDB has done in their
installation process.

Calling it "deviant" would be going too far (and unkind to them!), but
you evidently have instances of both PostgreSQL and EnterpriseDB's
product installed, and the very fact of their concurrent presence on
your system is contributing to your confusion.

If you want EnterpriseDB support, then you need to contact them;
that's what they're paid for.
-- 
output = reverse("ofni.sailifa.ac" "@" "enworbbc")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Wed Jul 29 09:16:04 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 29 09:16:14 2009
Subject: re[Slony1-general] plicate the whole DB and not just selecting by
	tables, how?
In-Reply-To: <24712383.post@talk.nabble.com> (roctaiwan's message of "Tue, 28
	Jul 2009 21:47:46 -0700 (PDT)")
References: <24712383.post@talk.nabble.com>
Message-ID: <874osvlb22.fsf@dba2.int.libertyrms.com>

roctaiwan <nettreeinc@gmail.com> writes:
> Q: How do I rewrite this script so I don't have to select tables, instead
> selecting just the whole database to be replicate?

You can't.  Slony-I doesn't support that kind of usage.

If you truly need to replicate an entire database cluster, then you
should look into PITR, aka "warm standby."
<http://www.postgresql.org/docs/8.3/static/warm-standby.html>

> Q: how can I rewrite this script if I have more then one slaves? 

The documentation describes how to do this in some detail:
    <http://slony.info/documentation/addthings.html>

If that isn't clear enough, then it would be most useful to try to
make the documentation better, as that will help many users.
-- 
(format nil "~S@~S" "cbbrowne" "ca.afilias.info")
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From cbbrowne at ca.afilias.info  Wed Jul 29 09:21:28 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 29 09:21:37 2009
Subject: [Slony1-general] Will Slony-I replicating Indexes also?
In-Reply-To: <24716658.post@talk.nabble.com> (roctaiwan's message of "Wed, 29
	Jul 2009 03:38:53 -0700 (PDT)")
References: <24716658.post@talk.nabble.com>
Message-ID: <87zlanjw8n.fsf@dba2.int.libertyrms.com>

roctaiwan <nettreeinc@gmail.com> writes:

> Hi all,
>
> I wants to know what will Slony replicating and what will not? 
>
> will it replicating Schema? Index? functions? 

<http://slony.info/documentation/slonyintro.html#INTRODUCTION>
Section 1.5 describe what *is* replicated.

> I am concerning about Indexes, since it would affect performance on user
> from retrieving data from my slave nodes. 

Indexes are derived from the contents of the underlying table, so if
you have the schema defined on the subscribers, complete with indices,
then the indices will exist and be used as would be expected.

> If there a solution for this issue? or it has to done by hand manually doing
> dumpall to slaves?

You need to have the proper schema on all nodes.

There is a script called <slony1_dump.sh> which can pull this from the
origin if it is not already available.
-- 
let name="cbbrowne" and tld="ca.afilias.info" in name ^ "@" ^ tld;;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From JanWieck at Yahoo.com  Wed Jul 29 10:38:40 2009
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed Jul 29 10:35:33 2009
Subject: [Slony1-general] Does Slony version 1.2.x work with postgresql
	8.4?
In-Reply-To: <2968dfd60907181837i392c1127nabb4e3cf4564ca83@mail.gmail.com>
References: <20090713205919.GA26996@it.is.rice.edu>	<87y6qrxkey.fsf@dba2.int.libertyrms.com>	<8a1bfe660907170632s2f221ed3v7800b947eff9f755@mail.gmail.com>
	<2968dfd60907181837i392c1127nabb4e3cf4564ca83@mail.gmail.com>
Message-ID: <4A708920.8060108@Yahoo.com>

On 7/18/2009 9:37 PM, Vick Khera wrote:
> On Fri, Jul 17, 2009 at 9:32 AM, Laurent Laborde<kerdezixe@gmail.com> wrote:
>> This is really a major problem.
>> upgrade to slony-1 2.x require to rebuild the whole cluster.
>> upgrade to pgsql-8.4 require to rebuild the whole database.

Last week I committed some changes to 1.2, which eventually will be 
released soon. 1.2.17 will support 8.4.

Additionally there is functionality in 2.0 that will allow to upgrade 
the cluster to 2.0 during an outage without rebuilding all subscribers.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From jason at merchantcircle.com  Wed Jul 29 12:13:28 2009
From: jason at merchantcircle.com (Jason Culverhouse)
Date: Wed Jul 29 12:13:41 2009
Subject: [Slony1-general] Log Switch not happening version 1.2.15
Message-ID: <9824434E-78F4-4CDD-BBDA-1CFC8EF5A93D@merchantcircle.com>

Hi,
I am using Slony 1.2.15 and  I have one slave that is ~2 hours behind  
or about 5000 events.

I see this in the log on node 20 where node 40 is the master and 20  
and 30 are slaves
I rearranged the servers ~October of 2008 from 30 master to 40 master  
and I ma fairly that I I have restarted everything since then.

2009-07-29 11:45:55 PDT DEBUG2 remoteWorkerThread_40: current local  
log_status is 2
2009-07-29 11:45:55 PDT DEBUG2 remoteWorkerThread_40_30: current  
remote log_status = 3
2009-07-29 11:45:55 PDT DEBUG2 remoteWorkerThread_40_40: current  
remote log_status = 3

I see these notices on node 20
NOTICE:  Slony-I: log switch to sl_log_1 still in progress - sl_log_2  
not truncated
NOTICE:  Slony-I: log switch to sl_log_1 still in progress - sl_log_2  
not truncated
NOTICE:  Slony-I: log switch to sl_log_1 still in progress - sl_log_2  
not truncated

I see FETCH statements taking up 100% cpu on node 30

Any Idea on what to do here?

The only reference I could find was:

  4.6. I upgraded my cluster to Slony-I version 1.2. I'm now getting  
the following notice in the logs:
NOTICE: Slony-I: log switch to sl_log_2 still in progress - sl_log_1  
not truncated
Both sl_log_1 and sl_log_2 are continuing to grow, and sl_log_1 is  
never getting truncated. What's wrong?
This is symptomatic of the same issue as above with dropping  
replication: if there are still old connections lingering that are  
using old query plans that reference the old stored functions,  
resulting in the inserts to sl_log_1
Closing those connections and opening new ones will resolve the issue.
In the longer term, there is an item on the PostgreSQL TODO list to  
implement dependancy checking that would flush cached query plans when  
dependent objects change.

From cbbrowne at ca.afilias.info  Wed Jul 29 13:12:55 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 29 13:13:11 2009
Subject: [Slony1-general] Log Switch not happening version 1.2.15
In-Reply-To: <9824434E-78F4-4CDD-BBDA-1CFC8EF5A93D@merchantcircle.com>
References: <9824434E-78F4-4CDD-BBDA-1CFC8EF5A93D@merchantcircle.com>
Message-ID: <20090729161255.127443h8kpbt7b5z@webmail.afilias.info>

Quoting Jason Culverhouse <jason@merchantcircle.com>:
> I see these notices on node 20
> NOTICE:  Slony-I: log switch to sl_log_1 still in progress -  
> sl_log_2 not truncated
> NOTICE:  Slony-I: log switch to sl_log_1 still in progress -  
> sl_log_2 not truncated
> NOTICE:  Slony-I: log switch to sl_log_1 still in progress -  
> sl_log_2 not truncated
>
> I see FETCH statements taking up 100% cpu on node 30
>
> Any Idea on what to do here?

This happens when it's continuing to notice tuples lingering in sl_log_2.

You should be able to check on node 20 and see that "select * from  
_myschema.sl_log_2 limit 1;" returns a tuple.

I'd suggest running test_slony_state{-dbi}.pl, to check the general  
condition of things.  It'll doubtless gripe about there being a lot of  
tuples in sl_log_2.

I'd *suspect* that a confirmation isn't getting back to node 20, so it  
doesn't know that those tuples can be deleted, and hence never gets  
around to truncating the log table.

The "failure of event communications" is the thing I'd expect to be  
most likely to be the *real* cause.

I could be wrong, but it's pretty easy to check (see the other email  
thread today where I pointed someone at running the state test tool),  
and it is hopefully easy to rectify if that's the case.

If communications are fine, then the question arises, why aren't those  
old tuples in sl_log_2 getting trimmed?  That should help direct the  
questions, irrespective of cause.

FYI, if there's some misconfiguration that has caused this, and the  
truncate then comes thru (~10-15 minutes later), then you may want to  
induce another log switch pretty soon since sl_log_1 probably has a  
lot of data crudding it up.  Again, someone asked how to induce this  
just recently, so take a peek at recent archives for how to do that.


From cbbrowne at ca.afilias.info  Wed Jul 29 13:58:23 2009
From: cbbrowne at ca.afilias.info (Christopher Browne)
Date: Wed Jul 29 13:58:40 2009
Subject: [Slony1-general] Release candidates for 2.0.3, 1.2.17
Message-ID: <87r5vzjjf4.fsf@dba2.int.libertyrms.com>

I have put out release candidates for both of the above versions...

See slony.info for a listing of the respective changes that are in
these releases.

Both have some fairly significant changes that resolve important bugs.
I'll be doing some testing on various versions (including 8.4; one of
the important changes is for 1.2 and 2.0 to support PostgreSQL
8.4...), with a view to releasing these next week.
-- 
let name="cbbrowne" and tld="ca.afilias.info" in String.concat "@" [name;tld];;
<http://dba2.int.libertyrms.com/>
Christopher Browne
(416) 673-4124 (land)
"Bother,"  said Pooh,  "Eeyore, ready  two photon  torpedoes  and lock
phasers on the Heffalump, Piglet, meet me in transporter room three"
From jason at merchantcircle.com  Wed Jul 29 14:09:57 2009
From: jason at merchantcircle.com (Jason Culverhouse)
Date: Wed Jul 29 14:10:14 2009
Subject: [Slony1-general] Log Switch not happening version 1.2.15
Message-ID: <1E61645F-26C2-4244-A0E1-E70FA0707993@merchantcircle.com>

> Quoting Jason Culverhouse <jason at merchantcircle.com>:
> > I see these notices on node 20
> > NOTICE:  Slony-I: log switch to sl_log_1 still in progress -
> > sl_log_2 not truncated
> > NOTICE:  Slony-I: log switch to sl_log_1 still in progress -
> > sl_log_2 not truncated
> > NOTICE:  Slony-I: log switch to sl_log_1 still in progress -
> > sl_log_2 not truncated
> >
> > I see FETCH statements taking up 100% cpu on node 30
> >
> > Any Idea on what to do here?
>
> This happens when it's continuing to notice tuples lingering in  
> sl_log_2.
>
> You should be able to check on node 20 and see that "select * from
> _myschema.sl_log_2 limit 1;" returns a tuple.
>

Returns a tuple

on Node 40 and  30 I get this from sl_log_status
  sequence_name | last_value | increment_by | max_value | min_value |  
cache_value | log_cnt | is_cycled | is_called
---------------+------------+--------------+-----------+----------- 
+-------------+---------+-----------+-----------
  sl_log_status |          3 |            1 |         3 |         0  
|           1 |       0 | f         | t

On node 20
sequence_name | last_value | increment_by | max_value | min_value |  
cache_value | log_cnt | is_cycled | is_called
---------------+------------+--------------+-----------+----------- 
+-------------+---------+-----------+-----------
  sl_log_status |          2 |            1 |         3 |         0  
|           1 |       0 | f         | t


Node 20 only falls behind node 40 and is always a very similar number  
of log entries behind
  st_origin | st_received | st_last_event |      st_last_event_ts       
| st_last_received |    st_last_received_ts     |  
st_last_received_event_ts  | st_lag_num_events |   st_lag_time
-----------+-------------+---------------+---------------------------- 
+------------------+---------------------------- 
+----------------------------+-------------------+-----------------
         40 |          20 |      15469518 | 2009-07-29 14:03:51.453982  
|         15463732 | 2009-07-29 14:03:09.683189 | 2009-07-29  
12:05:51.162247 |              5786 | 01:58:00.426609
         40 |          30 |      15469518 | 2009-07-29 14:03:51.453982  
|         15469495 | 2009-07-29 14:04:21.481599 | 2009-07-29  
14:03:23.781414 |                23 | 00:00:27.807442

This time has been pretty consistent all day

> I'd suggest running test_slony_state{-dbi}.pl, to check the general
> condition of things.  It'll doubtless gripe about there being a lot of
> tuples in sl_log_2.

Tests for node 40 - DSN = host=node40 dbname=my_database user=postgres  
port=5432
========================================
pg_listener info:
Pages: 12
Tuples: 2

Size Tests
================================================
        sl_log_1    137219 3208410.000000
        sl_log_2       570 7030.000000
       sl_seqlog        16 258.000000

Listen Path Analysis
===================================================
No problems found with sl_listen

--------------------------------------------------------------------------------
Summary of event info
  Origin  Min SYNC  Max SYNC Min SYNC Age Max SYNC Age
= 
= 
= 
= 
= 
= 
= 
= 
========================================================================
      30  28037660  28037665     00:00:00     00:10:00    0
      40  15462272  15462752     01:56:00     02:05:00    1
      20   8725074   8725144     00:00:00     00:11:00    0


---------------------------------------------------------------------------------
Summary of sl_confirm aging
    Origin   Receiver   Min SYNC   Max SYNC  Age of latest SYNC  Age  
of eldest SYNC
= 
= 
= 
= 
= 
= 
= 
= 
= 
========================================================================
        20         30    8725074    8725139      00:00:00       
00:11:00    0
        20         40    8725080    8725139      00:00:00       
00:10:00    0
        30         20   28037660   28037665      00:00:00       
00:10:00    0
        30         40   28037660   28037665      00:00:00       
00:10:00    0
        40         20   15462272   15462752      00:00:00       
00:11:00    0
        40         30   15467769   15468362      00:00:00       
00:10:00    0


------------------------------------------------------------------------------

Listing of old open connections
        Database             PID            User    Query  
Age                Query
= 
= 
= 
= 
= 
= 
= 
= 
========================================================================


Tests for node 30 - DSN = host=node30 dbname=my_database user=postgres  
port=5432
========================================
pg_listener info:
Pages: 12
Tuples: 2

Size Tests
================================================
        sl_log_1    137219 3208410.000000
        sl_log_2       570 7030.000000
       sl_seqlog        16 258.000000

Listen Path Analysis
===================================================
No problems found with sl_listen

--------------------------------------------------------------------------------
Summary of event info
  Origin  Min SYNC  Max SYNC Min SYNC Age Max SYNC Age
= 
= 
= 
= 
= 
= 
= 
= 
========================================================================
      30  28037660  28037665     00:00:00     00:10:00    0
      40  15462272  15462752     01:56:00     02:05:00    1
      20   8725074   8725144     00:00:00     00:11:00    0


---------------------------------------------------------------------------------
Summary of sl_confirm aging
    Origin   Receiver   Min SYNC   Max SYNC  Age of latest SYNC  Age  
of eldest SYNC
= 
= 
= 
= 
= 
= 
= 
= 
= 
========================================================================
        20         30    8725074    8725139      00:00:00       
00:11:00    0
        20         40    8725080    8725139      00:00:00       
00:10:00    0
        30         20   28037660   28037665      00:00:00       
00:10:00    0
        30         40   28037660   28037665      00:00:00       
00:10:00    0
        40         20   15462272   15462752      00:00:00       
00:11:00    0
        40         30   15467769   15468362      00:00:00       
00:10:00    0


------------------------------------------------------------------------------

Listing of old open connections
        Database             PID            User    Query  
Age                Query
= 
= 
= 
= 
= 
= 
= 
= 
========================================================================


Tests for node 20 - DSN = host=node20 dbname=my_database user=postgres  
port=5432
========================================
pg_listener info:
Pages: 12
Tuples: 2

Size Tests
================================================
        sl_log_1    137219 3208410.000000
        sl_log_2       570 7030.000000
       sl_seqlog        16 258.000000

Listen Path Analysis
===================================================
No problems found with sl_listen

--------------------------------------------------------------------------------
Summary of event info
  Origin  Min SYNC  Max SYNC Min SYNC Age Max SYNC Age
= 
= 
= 
= 
= 
= 
= 
= 
========================================================================
      30  28037660  28037665     00:00:00     00:10:00    0
      40  15462272  15462752     01:56:00     02:05:00    1
      20   8725074   8725144     00:00:00     00:11:00    0


---------------------------------------------------------------------------------
Summary of sl_confirm aging
    Origin   Receiver   Min SYNC   Max SYNC  Age of latest SYNC  Age  
of eldest SYNC
= 
= 
= 
= 
= 
= 
= 
= 
= 
========================================================================
        20         30    8725074    8725139      00:00:00       
00:11:00    0
        20         40    8725080    8725139      00:00:00       
00:10:00    0
        30         20   28037660   28037665      00:00:00       
00:10:00    0
        30         40   28037660   28037665      00:00:00       
00:10:00    0
        40         20   15462272   15462752      00:00:00       
00:11:00    0
        40         30   15467769   15468362      00:00:00       
00:10:00    0


------------------------------------------------------------------------------

Listing of old open connections
        Database             PID            User    Query  
Age                Query
= 
= 
= 
= 
= 
= 
= 
= 
========================================================================

Sending message thus - | -s "Slony State Test Warning - Cluster  
my_cluster"
Message:


Node: 20 sl_log_1 tuples = 3.20841e+06 > 200000
================================================
Number of tuples in Slony-I table sl_log_1 is 3.20841e+06 which
exceeds 200000.

You may wish to investigate whether or not a node is down, or perhaps
if sl_confirm entries have not been propagating properly.


Node: 30 sl_log_1 tuples = 3.20841e+06 > 200000
================================================
Number of tuples in Slony-I table sl_log_1 is 3.20841e+06 which
exceeds 200000.

You may wish to investigate whether or not a node is down, or perhaps
if sl_confirm entries have not been propagating properly.


Node: 40 sl_log_1 tuples = 3.20841e+06 > 200000
================================================
Number of tuples in Slony-I table sl_log_1 is 3.20841e+06 which
exceeds 200000.

You may wish to investigate whether or not a node is down, or perhaps
if sl_confirm entries have not been propagating properly.




>
> I'd *suspect* that a confirmation isn't getting back to node 20, so it
> doesn't know that those tuples can be deleted, and hence never gets
> around to truncating the log table.
>
> The "failure of event communications" is the thing I'd expect to be
> most likely to be the *real* cause.
>
> I could be wrong, but it's pretty easy to check (see the other email
> thread today where I pointed someone at running the state test tool),
> and it is hopefully easy to rectify if that's the case.
>
> If communications are fine, then the question arises, why aren't those
> old tuples in sl_log_2 getting trimmed?  That should help direct the
> questions, irrespective of cause.
>
> FYI, if there's some misconfiguration that has caused this, and the
> truncate then comes thru (~10-15 minutes later), then you may want to
> induce another log switch pretty soon since sl_log_1 probably has a
> lot of data crudding it up.  Again, someone asked how to induce this
> just recently, so take a peek at recent archives for how to do that.
>
>
From jason at merchantcircle.com  Wed Jul 29 14:32:15 2009
From: jason at merchantcircle.com (Jason Culverhouse)
Date: Wed Jul 29 14:32:32 2009
Subject: [Slony1-general] Log Switch not happening version 1.2.15
In-Reply-To: <1E61645F-26C2-4244-A0E1-E70FA0707993@merchantcircle.com>
References: <1E61645F-26C2-4244-A0E1-E70FA0707993@merchantcircle.com>
Message-ID: <5CE74B7E-2897-4517-BC80-63FD1164E6C8@merchantcircle.com>


On Jul 29, 2009, at 2:09 PM, Jason Culverhouse wrote:
>
>
> You may wish to investigate whether or not a node is down, or perhaps
> if sl_confirm entries have not been propagating properly.
>
>>
>> I'd *suspect* that a confirmation isn't getting back to node 20, so  
>> it
>> doesn't know that those tuples can be deleted, and hence never gets
>> around to truncating the log table.

>> If communications are fine, then the question arises, why aren't  
>> those
>> old tuples in sl_log_2 getting trimmed?  That should help direct the
>> questions, irrespective of cause.

from http://www.slony.info/documentation/stmtmergeset.html
Is it possible that a merge set caused this?

I know I did a merge set and got a "subscription in progress" warning
I haven't reattempted the merge set since, so right now I have 2 sets,
could this be related to my problem?

>>
>> FYI, if there's some misconfiguration that has caused this, and the
>> truncate then comes thru (~10-15 minutes later), then you may want to
>> induce another log switch pretty soon since sl_log_1 probably has a
>> lot of data crudding it up.  Again, someone asked how to induce this
>> just recently, so take a peek at recent archives for how to do that.
>>
>>
> _______________________________________________
> Slony1-general mailing list
> Slony1-general@lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general

From testtest at live.dk  Thu Jul 30 00:57:45 2009
From: testtest at live.dk (Henrik HJ)
Date: Thu Jul 30 00:58:15 2009
Subject: [Slony1-general] Slony scripts
Message-ID: <BLU119-W27816F55859D39D8DCE783CD130@phx.gbl>


 =

Hi =


Can you pleas help me. =


I try to get pgAdmin III to work with slony, but I can not find the slony m=
odules xxid and slony1_funcs scripts.


Where can I get it?


Henrik
_________________________________________________________________
Dit GRATIS USB-stik p=E5 nettet er vokset til 25 GB! F=E5 Sky Drive nu!
http://www.microsoft.com/danmark/windows/windowslive/products/skydrive.aspx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090730/=
bce56a6e/attachment.htm
From peter.geoghegan86 at gmail.com  Thu Jul 30 04:26:56 2009
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Thu Jul 30 04:27:33 2009
Subject: [Slony1-general] sl_status incorrectly reports long event lag
In-Reply-To: <db471ace0907290834g431a8ae7q23516f3db5e5cfb9@mail.gmail.com>
References: <db471ace0907260519l55b61b8jf4e91c953a30ea38@mail.gmail.com>
	<87y6q8n7nz.fsf@dba2.int.libertyrms.com>
	<db471ace0907281622j2f1927bbnb7e8a85e9ccfb43b@mail.gmail.com>
	<db471ace0907290708q3f224e73r279745611b91c3e0@mail.gmail.com>
	<87d47jlejt.fsf@dba2.int.libertyrms.com>
	<db471ace0907290834g431a8ae7q23516f3db5e5cfb9@mail.gmail.com>
Message-ID: <db471ace0907300426n12271ea3h1e5722c0f1ec0a66@mail.gmail.com>

Sorry, but it seems I was wrong when I said that the problem was
fixed. I assumed that since the event lag returned to zero when new
paths where stored per Christopher's direction, the problem was
corrected. However, the problem persists.

When I restart a slave database (in the following example node 2),
replication works fine (at least as far as can be immediately
observed), but sl_status shows:

1;3;38689;"2009-07-30 12:11:51.796";38688;"2009-07-30
12:12:02.428316";"2009-07-30 12:11:41.859";1;"00:00:14.015"
1;2;38689;"2009-07-30 12:11:51.796";38605;"2009-07-30
11:52:35.119048";"2009-07-30 11:58:05.734";84;"00:13:50.14"

Node 2's event lag grows and grows (until the slon service is
restarted, at which time it returns to zero, just as before).

When I run test_slony_state-dbi.pl while the event lag continues to
grow, it outputs the following:

peter@peter-development-machine:~/slony1-2.0.2/tools>
./test_slony_state-dbi.pl --host=10.0.0.80 --database=lustre
--cluster=lustre_cluster --user=postgres --password=my_password
DSN: dbi:Pg:dbname=lustre;host=10.0.0.80;user=postgres;password=my_password;
===========================
Rummage for DSNs
=============================
Query:

   select p.pa_server, p.pa_conninfo
   from "_lustre_cluster".sl_path p
--   where exists (select * from "_lustre_cluster".sl_subscribe s where
--                          (s.sub_provider = p.pa_server or
s.sub_receiver = p.pa_server) and
--                          sub_active = 't')
   group by pa_server, pa_conninfo;


Tests for node 1 - DSN = dbi:Pg:dbname=lustre host=10.0.0.80
user=postgres password=my_password
========================================
pg_listener info:
Pages: 0
Tuples: 0

Size Tests
================================================
       sl_log_1         0  0.000000
       sl_log_2         0  0.000000
      sl_seqlog         0  0.000000

Listen Path Analysis
===================================================
No problems found with sl_listen

--------------------------------------------------------------------------------
Summary of event info
 Origin  Min SYNC  Max SYNC Min SYNC Age Max SYNC Age
================================================================================
      1     38605     38699     00:00:00     00:15:00    0
      2        20        20     01:08:00     01:08:00    1
      3        30        30     01:02:00     01:02:00    1


---------------------------------------------------------------------------------
Summary of sl_confirm aging
   Origin   Receiver   Min SYNC   Max SYNC  Age of latest SYNC  Age of
eldest SYNC
=================================================================================
        1          2      38605      38605      00:20:00      00:20:00    0
        1          3      38627      38698      00:00:00      00:11:00    0
        2          1         20         20      01:03:00      01:03:00    1
        2          3         20         20      01:02:00      01:02:00    1
        3          1         30         30      01:02:00      01:02:00    1
        3          2         30         30      01:08:00      01:08:00    1


------------------------------------------------------------------------------

Listing of old open connections on node 1
       Database             PID            User    Query Age
     Query
================================================================================


Tests for node 3 - DSN = dbi:Pg:dbname=lustre_slave host=10.0.0.82
user=postgres password=my_password
========================================
pg_listener info:
Pages: 0
Tuples: 0

Size Tests
================================================
       sl_log_1         0  0.000000
       sl_log_2         0  0.000000
      sl_seqlog         0  0.000000

Listen Path Analysis
===================================================
No problems found with sl_listen

--------------------------------------------------------------------------------
Summary of event info
 Origin  Min SYNC  Max SYNC Min SYNC Age Max SYNC Age
================================================================================
      1     38605     38699     00:00:00     00:15:00    0
      2        20        20     01:08:00     01:08:00    1
      3        30        30     01:02:00     01:02:00    1


---------------------------------------------------------------------------------
Summary of sl_confirm aging
   Origin   Receiver   Min SYNC   Max SYNC  Age of latest SYNC  Age of
eldest SYNC
=================================================================================
        1          2      38605      38605      00:21:00      00:21:00    0
        1          3      38629      38699      00:00:00      00:11:00    0
        2          1         20         20      01:03:00      01:03:00    1
        2          3         20         20      01:03:00      01:03:00    1
        3          1         30         30      01:03:00      01:03:00    1
        3          2         30         30      01:08:00      01:08:00    1


------------------------------------------------------------------------------

Listing of old open connections on node 3
       Database             PID            User    Query Age
     Query
================================================================================


Tests for node 2 - DSN = dbi:Pg:dbname=lustre_slave host=10.0.0.81
user=postgres password=my_password
========================================
pg_listener info:
Pages: 0
Tuples: 0

Size Tests
================================================
       sl_log_1         0  0.000000
       sl_log_2         0  0.000000
      sl_seqlog         0  0.000000

Listen Path Analysis
===================================================
No problems found with sl_listen

--------------------------------------------------------------------------------
Summary of event info
 Origin  Min SYNC  Max SYNC Min SYNC Age Max SYNC Age
================================================================================
      1     38573     38699    -00:05:00     00:15:00    0
      2        20        21     00:15:00     01:03:00    0
      3        30        30     00:57:00     00:57:00    1


---------------------------------------------------------------------------------
Summary of sl_confirm aging
   Origin   Receiver   Min SYNC   Max SYNC  Age of latest SYNC  Age of
eldest SYNC
=================================================================================
        1          2      38607      38699      00:00:00      00:15:00    0
        1          3      38573      38698     -00:05:00      00:15:00    0
        2          1         20         20      00:57:00      00:57:00    1
        2          3         20         20      00:57:00      00:57:00    1
        3          1         30         30      00:57:00      00:57:00    1
        3          2         30         30      01:02:00      01:02:00    1


------------------------------------------------------------------------------

Listing of old open connections on node 2
       Database             PID            User    Query Age
     Query
================================================================================

peter@peter-development-machine:~/slony1-2.0.2/tools>

Any further help you could offer is greatly appreciated,

Regards,
Peter Geoghegan
From peter.geoghegan86 at gmail.com  Thu Jul 30 04:51:57 2009
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Thu Jul 30 04:52:34 2009
Subject: [Slony1-general] Slony scripts
In-Reply-To: <BLU119-W27816F55859D39D8DCE783CD130@phx.gbl>
References: <BLU119-W27816F55859D39D8DCE783CD130@phx.gbl>
Message-ID: <db471ace0907300451h72e2a75aj8b6feef39dde13b1@mail.gmail.com>

> Hi
>
> Can you pleas help me.
>
> I try to get pgAdmin III to work with slony, but I can not find the slony
> modules xxid and slony1_funcs scripts.
>
> Where can I get it?
>
> Henrik
>

What platform are you doing this on?

Regards,
Peter Geoghegan
From peter.geoghegan86 at gmail.com  Thu Jul 30 07:02:59 2009
From: peter.geoghegan86 at gmail.com (Peter Geoghegan)
Date: Thu Jul 30 07:03:04 2009
Subject: [Slony1-general] Slony scripts
In-Reply-To: <274BA771880F684D9CC7E88C5C29E9D9FCFD60@hermod.intelligent-addressing.co.uk>
References: <BLU119-W27816F55859D39D8DCE783CD130@phx.gbl>
	<db471ace0907300451h72e2a75aj8b6feef39dde13b1@mail.gmail.com>
	<274BA771880F684D9CC7E88C5C29E9D9FCFD60@hermod.intelligent-addressing.co.uk>
Message-ID: <db471ace0907300702m57396f3bh8b0cd16feeea008e@mail.gmail.com>

What operating system? Windows? What PostgreSQL version? What Slony-I
version? These are the sorts of details you ought to have included in
your first e-mail to have any hope of getting a useful response.

Regards,
Peter Geoghegan
From testtest at live.dk  Fri Jul 31 04:28:23 2009
From: testtest at live.dk (Henrik HJ)
Date: Fri Jul 31 04:29:13 2009
Subject: [Slony1-general] Slony scripts
Message-ID: <BLU119-W22690712816E38317A94DECD100@phx.gbl>


Hi
 =

I'm trying to get pgAdmin to work together with SlonyI. But I can't find th=
e scripts the Slony-I creation scripts, with I link to point to in pgAdmin.


I'm using this version.
Slony1 vers. 2.0.2 on Fedora 10
pgAdmin vers. 1.10.0 on windows =

postgreSQL vers. 8.4.0 on Fedora 10


Thanks in advance and have a nice weekend.


Henrik
_________________________________________________________________
NY Windows Live Messenger med nye fede funktioner. Hent den her!
http://download.live.com/messenger
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-general/attachments/20090731/=
5491f0f6/attachment.html
