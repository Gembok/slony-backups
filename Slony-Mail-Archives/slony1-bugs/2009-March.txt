From slony1-bugs at lists.slony.info  Tue Mar  3 12:36:37 2009
From: slony1-bugs at lists.slony.info (slony1-bugs@lists.slony.info)
Date: Tue Mar  3 12:36:38 2009
Subject: [Slony1-bugs] [Bug 74] New: path to sh
Message-ID: <bug-74-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=74

           Summary: path to sh
           Product: Slony-I
           Version: devel
          Platform: PC
        OS/Version: Mac OS
            Status: NEW
          Severity: normal
          Priority: medium
         Component: core scripts
        AssignedTo: slony1-bugs@lists.slony.info
        ReportedBy: rod.taylor@gmail.com
                CC: slony1-bugs@lists.slony.info
   Estimated Hours: 0.0


Created an attachment (id=28)
 --> (http://www.slony.info/bugzilla/attachment.cgi?id=28)
Same patch as is in the report

Most scripts have the path as /bin/sh but a few point to /usr/bin/sh, which
does not exist on many systems.

The below patches to point to /bin/sh. If that is not acceptable, then all
should be configured via "configure" to a discovered shell.


*** ./tools/slony-cluster-analysis-mass.sh.orig Fri May 19 16:43:48 2006
--- ./tools/slony-cluster-analysis-mass.sh      Tue Mar  3 15:28:46 2009
***************
*** 1,4 ****
! #!/usr/bin/sh
  # $Id: slony-cluster-analysis-mass.sh,v 1.1 2006-05-19 20:43:48 cbbrowne Exp
$
  # Do cluster analyses

--- 1,4 ----
! #!/bin/sh
  # $Id: slony-cluster-analysis-mass.sh,v 1.1 2006-05-19 20:43:48 cbbrowne Exp
$
  # Do cluster analyses

*** ./tools/slony-cluster-analysis.sh.orig      Fri May 19 16:43:48 2006
--- ./tools/slony-cluster-analysis.sh   Tue Mar  3 15:28:40 2009
***************
*** 1,4 ****
! #!/usr/bin/sh
  # $Id: slony-cluster-analysis.sh,v 1.1 2006-05-19 20:43:48 cbbrowne Exp $
  # Analyze Slony-I Configuration

--- 1,4 ----
! #!/bin/sh
  # $Id: slony-cluster-analysis.sh,v 1.1 2006-05-19 20:43:48 cbbrowne Exp $
  # Analyze Slony-I Configuration


-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.
From slony1-bugs at lists.slony.info  Tue Mar  3 14:21:17 2009
From: slony1-bugs at lists.slony.info (slony1-bugs@lists.slony.info)
Date: Tue Mar  3 14:21:19 2009
Subject: [Slony1-bugs] [Bug 74] path to sh
In-Reply-To: <bug-74-4@http.www.slony.info/bugzilla/>
Message-ID: <20090303222117.43EA02903AA@main.slony.info>

http://www.slony.info/bugzilla/show_bug.cgi?id=74


Christopher Browne <cbbrowne@ca.afilias.info> changed:

           What    |Removed                     |Added
----------------------------------------------------------------------------
         Resolution|                            |FIXED
             Status|NEW                         |RESOLVED




--- Comment #1 from Christopher Browne <cbbrowne@ca.afilias.info>  2009-03-03 14:21:17 ---
Done, in 2.0 & HEAD.

Not an issue in 1.2


-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.
From tam.mclaughlin at gmail.com  Fri Mar  6 01:42:34 2009
From: tam.mclaughlin at gmail.com (Tam McLaughlin)
Date: Fri Mar  6 01:43:08 2009
Subject: [Slony1-bugs] Re: [Bug 67]
Message-ID: <fa4c18050903060142w2352a48dn19cbd0c21e719380@mail.gmail.com>

It looks like I still have this bug after updating to slony 2.0.1.
Can anyone confirm that this is the case or  additional info that you
require?
Thanks.
Tam

Details:
 ERROR,XX000,"Slony-I: old key column entities.cat_2_validation IS NULL on
UPDATE",,,,,,"update UK4628.entities set status_val_2 =3D 'STANDBY'
,status_val_3 =3D 'STANDBY'  where entity =3D 'SIR-1'",,
ERROR,XX000,"Slony-I: old key column lots_history_data.hold_name IS NULL on
UPDATE",,,,,,"update UK4628.lots_history_data set next_txid =3D 65298 where
lot =3D 'K009AX02' and next_txid is null",,

=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
$ psql --version
psql (PostgreSQL) 8.3.3
contains support for command-line editing
$ slon -v
slon version 2.0.1


SET default_with_oids =3D false;

CREATE TABLE lots_history_data (
    txid bigint NOT NULL,
    next_txid bigint,
    lot character varying(20) NOT NULL,
    product character varying(30),
    route character varying(20),
    operation integer,
    next_std_route character varying(20),
    next_std_oper integer,
    lot_is_parent character(1) DEFAULT 'N'::bpchar,
    qty real,
    owner character varying(20),
    due_date date,
    terminated character(1) DEFAULT 'N'::bpchar,
    hot_lot character(1) DEFAULT 'N'::bpchar,
    restricted character(1) DEFAULT 'N'::bpchar,
    in_rework character(1) DEFAULT 'N'::bpchar,
    kitt_comp character(1) DEFAULT 'N'::bpchar,
    in_transit character(1) DEFAULT 'N'::bpchar,
    in_non_seq_mode character(1) DEFAULT 'N'::bpchar,
    on_hold character(1) DEFAULT 'N'::bpchar,
    in_process character(1) DEFAULT 'Y'::bpchar,
    unit_trace character(1) DEFAULT 'N'::bpchar,
    sub_products character(1) DEFAULT 'N'::bpchar,
    allo_comp character(1) DEFAULT 'N'::bpchar,
    movein_reqd character(1) DEFAULT 'N'::bpchar,
    script_id character varying(20),
    script_ver numeric,
    script_step integer,
    next_script_id character varying(20),
    next_script_ver numeric,
    next_script_step integer,
    child_of character varying(20),
    hold_cat character varying(20),
    hold_name character varying(20),
    hold_note character varying(80),
    lotname character varying(20),
    eng_owner character varying(40),
    stores character(1) DEFAULT 'N'::bpchar,
    own_entity character varying(20),
    password character varying(20),
 created timestamp without time zone,
    updated timestamp without time zone,
    main_route character varying(20),
    main_seqno integer,
    seqno integer
);

ALTER TABLE uk4628.lots_history_data OWNER TO mes;

ALTER TABLE ONLY lots_history_data
    ADD CONSTRAINT lots_history_data_pk PRIMARY KEY (txid, lot);
CREATE INDEX lot_history_data_lot_next_txid_idx ON lots_history_data USING
btree (lot, next_txid);

CREATE INDEX lots_history_data_next_idx ON lots_history_data USING btree
(next_txid);

CREATE TRIGGER _mesrep_denyaccess
    BEFORE INSERT OR DELETE OR UPDATE ON lots_history_data
    FOR EACH ROW
    EXECUTE PROCEDURE _mesrep.denyaccess('_mesrep');

ALTER TABLE lots_history_data DISABLE TRIGGER _mesrep_denyaccess;

CREATE TRIGGER _mesrep_logtrigger
    AFTER INSERT OR DELETE OR UPDATE ON lots_history_data
    FOR EACH ROW
    EXECUTE PROCEDURE _mesrep.logtrigger('_mesrep', '83', 'kvk');

ALTER TABLE ONLY lots_history_data
    ADD CONSTRAINT lots_history_data_lot_fkey FOREIGN KEY (lot) REFERENCES
lots(lot);

REVOKE ALL ON TABLE lots_history_data FROM PUBLIC;
REVOKE ALL ON TABLE lots_history_data FROM mes;
GRANT ALL ON TABLE lots_history_data TO mes;
GRANT SELECT ON TABLE lots_history_data TO mesview;

-------------------------------------------------------
CREATE TABLE entities (
    entity character varying(20) NOT NULL,
    entity_type character varying(20),
    rtc_model character varying(20),
    description character varying(40),
    status_cat_0 character varying(20),
    status_val_0 character varying(20),
    status_cat_1 character varying(20),
    status_val_1 character varying(20),
    status_cat_2 character varying(20),
    status_val_2 character varying(20),
    status_cat_3 character varying(20),
    status_val_3 character varying(20),
    status_cat_4 character varying(20),
    status_val_4 character varying(20),
    status_cat_5 character varying(20),
    status_val_5 character varying(20),
    status_cat_6 character varying(20),
    status_val_6 character varying(20),
    status_cat_7 character varying(20),
    status_val_7 character varying(20),
    status_cat_8 character varying(20),
    status_val_8 character varying(20),
    status_cat_9 character varying(20),
    status_val_9 character varying(20),
    deleted character(1),
    location character varying(20),
    entity_group_1 character varying(20),
    entity_group_2 character varying(20),
    entity_group_3 character varying(20),
    entity_group_4 character varying(20),
    entity_group_5 character varying(20),
    cat_0_validation character varying(20),
    cat_1_validation character varying(20),
    cat_2_validation character varying(20),
    cat_3_validation character varying(20),
    cat_4_validation character varying(20),
    cat_5_validation character varying(20),
    cat_6_validation character varying(20),
    cat_7_validation character varying(20),
    cat_8_validation character varying(20),
    cat_9_validation character varying(20)
);

ALTER TABLE uk4628.entities OWNER TO mes;
ALTER TABLE ONLY entities
    ADD CONSTRAINT entities_pk PRIMARY KEY (entity);
CREATE TRIGGER _mesrep_denyaccess
    BEFORE INSERT OR DELETE OR UPDATE ON entities
    FOR EACH ROW
    EXECUTE PROCEDURE _mesrep.denyaccess('_mesrep');
ALTER TABLE entities DISABLE TRIGGER _mesrep_denyaccess;

CREATE TRIGGER _mesrep_logtrigger
    AFTER INSERT OR DELETE OR UPDATE ON entities
    FOR EACH ROW
    EXECUTE PROCEDURE _mesrep.logtrigger('_mesrep', '59', 'k');
REVOKE ALL ON TABLE entities FROM PUBLIC;
REVOKE ALL ON TABLE entities FROM mes;
GRANT ALL ON TABLE entities TO mes;
GRANT SELECT ON TABLE entities TO mesview;
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-bugs/attachments/20090306/c59=
4f3ef/attachment.htm
From slony1-bugs at lists.slony.info  Sun Mar  8 15:54:14 2009
From: slony1-bugs at lists.slony.info (slony1-bugs@lists.slony.info)
Date: Sun Mar  8 15:54:16 2009
Subject: [Slony1-bugs] [Bug 75] New: Bugs: (1) Initialize forgotten
 cs->plan_active_log. (2)
 Session_replication_role and "SET datestyle TO 'ISO'"
 in slonik at connection start.
Message-ID: <bug-75-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=75

           Summary: Bugs: (1) Initialize forgotten cs->plan_active_log. (2)
                    Session_replication_role and "SET datestyle TO 'ISO'" in
                    slonik at connection start.
           Product: Slony-I
           Version: devel
          Platform: PC
        OS/Version: Windows
            Status: NEW
          Severity: critical
          Priority: medium
         Component: slonik
        AssignedTo: slony1-bugs@lists.slony.info
        ReportedBy: dmitry.koterov@gmail.com
                CC: slony1-bugs@lists.slony.info
   Estimated Hours: 0.0


Created an attachment (id=29)
 --> (http://www.slony.info/bugzilla/attachment.cgi?id=29)
Patches for these 2 issues

First, a little patch (1)
-------------------------

--- slony1-2.0.1-orig/src/backend/slony1_funcs.c        2009-02-23
22:09:40.000000000 +0300
+++ slony1-2.0.1/src/backend/slony1_funcs.c     2009-03-08 21:58:38.000000000
+0300
@@ -332,5 +332,5 @@
         * Do the following only once per transaction.
         */
-       if (!TransactionIdEquals(cs->currentXid, newXid))
+       if (!TransactionIdEquals(cs->currentXid, newXid) ||
!cs->plan_active_log)
        {
                int32           log_status;

The patch just adds logical consistence: the code inside this "if" initializes
both cs->currentXid and plan_active_log once per transaction, but checks only
cs->currentXid. Additional check for cs->currentXid is safe and seems to be
reasonable anyway.

(Note that this issue correlate with the second one, so it will break slony if
you will not apply the second patch too. The effect of the second bug is
destroyed by the first bug.)


Second, bug with new session_replication_role and "SET datestyle TO 'ISO'" (2)
------------------------------------------------------------------------------

I detected that logTrigger is NOT turned off during the EXECUTE QUERY in
Slony-I v2.0.1.
Take a look at the following query log produced by slonik:

[postgres@slonopotam_m 1623] LOG:  ???????: begin transaction;
[postgres@slonopotam_m 1623] LOG:  ???????: SET datestyle TO 'ISO'; SET
session_replication_role TO local;
[postgres@slonopotam_m 1623] LOG:  ???????: select version();
[postgres@slonopotam_m 1623] LOG:  ???????: rollback transaction;
...
[postgres@slonopotam_m 1623] LOG:  ???????: begin transaction;
[postgres@slonopotam_m 1623] LOG:  ???????: select
"_sloncluster".ddlScript_prepare(4725, 2736);
[postgres@slonopotam_m 1623] LOG:  ???????: UPDATE a SET id_copy = id;
[postgres@slonopotam_m 1623] LOG:  execute <unnamed>: select
"_sloncluster".ddlScript_complete(4725, $1::text, 2736);
[postgres@slonopotam_m 1623] LOG:  ???????: commit transaction;
...

You see, the effect of "SET session_replication_role TO local" is NOT visible
within a next transaction, because it is rolled back. This is because
db_connect() executes it on a connection start (correct), but via
db_exec_command (wrong), and db_exec_command begins a transaction at its first
run.

(Frankly, thanks to (1) it is not visible, because cs->plan_active_log is
surprisely null within a logTrigger call. So, logTrigger IS CALLED, but
executes a NULL query which is ignored by Postgres.)

The solution is to perform connection initialization outside a transaction, via
direct PQexec call, not via db_begin_xact().
Here is the patch:

diff -U2 slony1-2.0.1-orig/src/slonik/dbutil.c slony1-2.0.1/src/slonik/dbutil.c
--- slony1-2.0.1-orig/src/slonik/dbutil.c       2008-04-24 00:34:21.000000000
+0400
+++ slony1-2.0.1/src/slonik/dbutil.c    2009-03-08 23:26:18.000000000 +0300
@@ -113,4 +113,5 @@
        PGconn     *dbconn;
        SlonDString     query;
+       PGresult   *res;

        db_notice_stmt = stmt;
@@ -144,10 +145,20 @@

        adminfo->dbconn = dbconn;
-       if (db_exec_command(stmt, adminfo, &query) < 0)
+
+       res = PQexec(adminfo->dbconn, dstring_data(&query));
+       if (PQresultStatus(res) != PGRES_COMMAND_OK &&
+               PQresultStatus(res) != PGRES_TUPLES_OK &&
+               PQresultStatus(res) != PGRES_EMPTY_QUERY)
        {
+               fprintf(stderr, "%s:%d: %s %s - %s",
+                               stmt->stmt_filename, stmt->stmt_lno,
+                               PQresStatus(PQresultStatus(res)),
+                               dstring_data(&query),
PQresultErrorMessage(res));
                printf("Unable to set datestyle\n");
+               PQclear(res);
                dstring_free(&query);
                return -1;
        }
+       PQclear(res);
        dstring_free(&query);
        return 0;




P.S.
Why postgres rolls back session variable assigned via "SET
session_replication_role TO local"? Because it is the standard behaviour, and
here is an illustration:

postgres=# show session_replication_role;
--> origin
postgres=# begin;
--> BEGIN
postgres=# set session_replication_role to replica;
--> SET
postgres=# show session_replication_role;
--> replica
postgres=# rollback;
--> ROLLBACK
postgres=# show se ssion_replication_role;
--> origin


-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.
From slony1-bugs at lists.slony.info  Mon Mar  9 11:26:05 2009
From: slony1-bugs at lists.slony.info (slony1-bugs@lists.slony.info)
Date: Mon Mar  9 11:26:07 2009
Subject: [Slony1-bugs] [Bug 75] Bugs: (1) Initialize forgotten
 cs->plan_active_log. (2)
 Session_replication_role and "SET datestyle TO 'ISO'"
 in slonik at connection start.
In-Reply-To: <bug-75-4@http.www.slony.info/bugzilla/>
Message-ID: <20090309182605.5184D2903DA@main.slony.info>

http://www.slony.info/bugzilla/show_bug.cgi?id=75


Dmitry Koterov <dmitry.koterov@gmail.com> changed:

           What    |Removed                     |Added
----------------------------------------------------------------------------
         OS/Version|Windows                     |Linux




-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.
From slony1-bugs at lists.slony.info  Tue Mar 10 23:52:52 2009
From: slony1-bugs at lists.slony.info (slony1-bugs@lists.slony.info)
Date: Tue Mar 10 23:52:54 2009
Subject: [Slony1-bugs] [Bug 76] New: repair config can fail
Message-ID: <bug-76-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=76

           Summary: repair config can fail
           Product: Slony-I
           Version: 1.2
          Platform: PC
        OS/Version: Linux
            Status: NEW
          Severity: normal
          Priority: medium
         Component: stored procedures
        AssignedTo: slony1-bugs@lists.slony.info
        ReportedBy: stuart@stuartbishop.net
                CC: slony1-bugs@lists.slony.info
   Estimated Hours: 0.0


I managed to trigger a failure in repair config, running repair config against
a database freshly restored from a pg_dump of a master node.

$ slonik << EOM
> cluster name = sl;
> node 1 admin conninfo = 'dbname=lpmain_staging_new user=slony';
> repair config (set id=1, event node=1, execute only on=1);
> EOM
<stdin>:3: PGRES_FATAL_ERROR select "_sl".updateReloid(1, 1);  -
ERROR:  duplicate key value violates unique constraint
"sl_table_tab_reloid_key"
CONTEXT:  SQL statement "update "_sl".sl_table set tab_reloid =
PGC.oid from pg_catalog.pg_class PGC, pg_catalog.pg_namespace PGN
where "_sl".slon_quote_brute("_sl".sl_table.tab_relname) =
"_sl".slon_quote_brute(PGC.relname) and PGC.relnamespace = PGN.oid and
"_sl".slon_quote_brute(PGN.nspname) =
"_sl".slon_quote_brute("_sl".sl_table.tab_nspname)"
PL/pgSQL function "updatereloid" line 39 at SQL statement


This is easy to replicate by swapping to tab_reloids:

dev=# select tab_id,tab_reloid from sl_table where tab_id in (200,201);
 tab_id | tab_reloid 
--------+------------
    200 |    6672479
    201 |    6672488
(2 rows)

dev=# update sl_table set tab_reloid=42 where tab_id=200;
UPDATE 1
dev=# update sl_table set tab_reloid=6672479 where tab_id=201;
UPDATE 1
dev=# update sl_table set tab_reloid=6672488 where tab_id=200;
UPDATE 1
dev=# \q

$ slonik << EOM
> cluster name = sl;
> node 1 admin conninfo = 'dbname=dev user=slony';
> repair config (set id=1, event node=1, execute only on=1);
> EOM
<stdin>:3: PGRES_FATAL_ERROR select "_sl".updateReloid(1, 1);  - ERROR: 
duplicate key value violates unique constraint "sl_table_tab_reloid_key"
CONTEXT:  SQL statement "update "_sl".sl_table set tab_reloid = PGC.oid from
pg_catalog.pg_class PGC, pg_catalog.pg_namespace PGN where
"_sl".slon_quote_brute("_sl".sl_table.tab_relname) =
"_sl".slon_quote_brute(PGC.relname) and PGC.relnamespace = PGN.oid and
"_sl".slon_quote_brute(PGN.nspname) =
"_sl".slon_quote_brute("_sl".sl_table.tab_nspname)"
PL/pgSQL function "updatereloid" line 39 at SQL statement


Populating the tab_reloid with (guaranteed) gibberish at the start of the
repair config would solve the problem.

A similar issue probably occurs with sequences.


-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.
From slony1-bugs at lists.slony.info  Fri Mar 13 05:28:20 2009
From: slony1-bugs at lists.slony.info (slony1-bugs@lists.slony.info)
Date: Fri Mar 13 05:28:22 2009
Subject: [Slony1-bugs] [Bug 77] New: Missing confirm records for events in
	sl_confirm table
Message-ID: <bug-77-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=77

           Summary: Missing confirm records for events in sl_confirm table
           Product: Slony-I
           Version: devel
          Platform: PC
        OS/Version: Linux
            Status: NEW
          Severity: normal
          Priority: low
         Component: stored procedures
        AssignedTo: slony1-bugs@lists.slony.info
        ReportedBy: orlin@tetracom.com
                CC: slony1-bugs@lists.slony.info
   Estimated Hours: 0.0


I have Postgres 8.3 and Slony 2.0.1 installed on two virtual machines with CENT
OS and cluster with two nodes(master and slave). The cluster name is
'kms_cluster', the ID of the master is 1, the ID of the slave is 2, the set ID
is 3. My goal is to create tables and add them to replication dynamical(i know
that Slony is not developed for such case, but i don't have a choice at the
moment) during the application execution. I use the stored functions provided
by Slony and my simple 'add table' algorithm is:

CREATE TABLE kms_gen.t_two
(
  dname character varying(55) NOT NULL,
  CONSTRAINT pk_t_two PRIMARY KEY (dname)
);--to all nodes

SELECT _kms_cluster.storeset(999, 'tmp');--master[newSetId,setName]

SELECT _kms_cluster.setaddtable(999, 700, 'kms_gen.t_two', 'pk_t_two',
'testmerge');--master[setId,tableid,tableName,tablePk,comments]

SELECT _kms_cluster.subscribeset(999, 1, 2,
FALSE);--master[set_id,provider_node_id,reciver_node_id]

-- optional SYNC event
-- SELECT _kms_cluster.createevent('_kms_cluster', 'SYNC', 'test sync event')
AS event_seqno;--master[cluster_name,ev_type,optional_descr]

SELECT _kms_cluster.mergeset(3, 999); --master[setid,tmpSetId] possible error
if the subscription is not complete

Of course the problem is that the merge must wait for the subscription process
end.I decide to check the sl_confirm table for the subscription event
confirmation, i know that the SUBSCRIBE_SET event confirm is not the end and i
decide to wait until the ENABLE_SUBSCRIPTION event is confirmed.Moreover, i
have another variant(i read how the WAIT FOR EVENT command is used by
subscription), i create optional SYNC event after the subscribe and wait for
it. 
The problem is that the confirm record for the ENABLE_SUBSCRIPTION or my SYNC
event sometimes does not appear in the master node sl_confirm table, i check
the sl_event table and get the events seqnumbers, then check the sl_confirm and
see that some events don't have confirm record.
In addition, i make all of this trough the postgresql-8.2-506.jdbc3.jar
driver(if this is relevant), when i execute "SELECT
_kms_cluster.createevent('_kms_cluster', 'SYNC', 'test sync event') AS
event_seqno;--master[cluster_name,ev_type,optional_descr]" trough pgAdmin
editor
then the generated SYNC event always has a confirm record in the sl_confirm
table.
Finally, my only question is: Is it by design so that not all events originate
on the master node have a confirm record in the master sl_confirm table or this
is just a strange behavior.


-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.
From slony1-bugs at lists.slony.info  Fri Mar 13 15:51:04 2009
From: slony1-bugs at lists.slony.info (slony1-bugs@lists.slony.info)
Date: Fri Mar 13 15:51:06 2009
Subject: [Slony1-bugs] [Bug 71] OpenSolaris segmentation fault slony1-2.0.0
In-Reply-To: <bug-71-4@http.www.slony.info/bugzilla/>
Message-ID: <20090313225104.D393A29026D@main.slony.info>

http://www.slony.info/bugzilla/show_bug.cgi?id=71


C?sar Acu?a <cjacuna@gmail.com> changed:

           What    |Removed                     |Added
----------------------------------------------------------------------------
                 CC|                            |cjacuna@gmail.com




--- Comment #3 from C?sar Acu?a <cjacuna@gmail.com>  2009-03-13 15:51:04 ---
Chris... sorry for the late response. 

I managed to narrow the problem to the function void dump_configuration(void)
in src/slon/confoptions.c


void
dump_configuration(void)
{
        int                     i;

        for (i = 0; ConfigureNamesInt[i].gen.name; i++)
        {
                slon_log(SLON_CONFIG, "main: Integer option %s = %d\n",
                        ConfigureNamesInt[i].gen.name,
*(ConfigureNamesInt[i].variable));
        }
        for (i = 0; ConfigureNamesBool[i].gen.name; i++)
        {
                slon_log(SLON_CONFIG, "main: Boolean option %s = %d\n",
                  ConfigureNamesBool[i].gen.name,
*(ConfigureNamesBool[i].variable));
        }

        for (i = 0; ConfigureNamesReal[i].gen.name; i++)
        {
                slon_log(SLON_CONFIG, "main: Real option %s = %f\n",
                  ConfigureNamesReal[i].gen.name,
*(ConfigureNamesReal[i].variable));
        }

        for (i = 0; ConfigureNamesString[i].gen.name; i++)
        {
--> Here           //slon_log(SLON_CONFIG, "main: String option %s = %s\n",
                                 ConfigureNamesString[i].gen.name,
*(ConfigureNamesString[i].variable));
        }
}


I commented out the line

slon_log(SLON_CONFIG, "main: String option %s = %s\n",
         ConfigureNamesString[i].gen.name,
*(ConfigureNamesString[i].variable));

and everything works fine.

Doing a little debugging, I found out that the problem arises when it tries to
dump to the log the content of the following configuration variables:

pid_file
archive_dir
sql_on_connection
lag_interval
command_on_logarchive


I appreciate the time you devoted to this problem.

Greetings,

C?sar Acu?a


(In reply to comment #2)
> Do you get any output before the crash, or is it buffered, and therefore lost?
> 
> That crash takes place while it's logging the set of configuration parameters
> to the slon log (file, or standard output).
> 
> If you have a way to capture the output in an unbuffered fashion, that would be
> pretty useful, as that would let us get closer to finding where it fails.
> 
> *Apparently*, as it's trying to dump the config parameters, one of them is
> failing to be accessed properly, hence the seg fault.  Perhaps one of them
> isn't properly enough initialized, and it would seem a mighty good thing if we
> can figure out at what point the failure takes place.
> 
> I'm not so familiar with Solaris; can you submit output to an unbuffered
> destination so that you capture as much log as possible?  If you can do that,
> that should make it pretty easy to track down where the problem would take
> place.
> 


-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.
From slony1-bugs at lists.slony.info  Wed Mar 18 08:14:31 2009
From: slony1-bugs at lists.slony.info (slony1-bugs@lists.slony.info)
Date: Wed Mar 18 08:14:32 2009
Subject: [Slony1-bugs] [Bug 78] New: Vacuum of sl_log_2 with 42M rows is
 blocking replication
Message-ID: <bug-78-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=78

           Summary: Vacuum of sl_log_2 with 42M rows is blocking replication
           Product: Slony-I
           Version: devel
          Platform: Other
        OS/Version: Linux
            Status: NEW
          Severity: critical
          Priority: high
         Component: stored procedures
        AssignedTo: slony1-bugs@lists.slony.info
        ReportedBy: gordon@collectiveintellect.com
                CC: slony1-bugs@lists.slony.info
   Estimated Hours: 0.0


We have a large transactional system running PostgreSQL 8.3.6 with Slony-I
2.0.1 on Linux Centos server with 64G memory and 2 quad-core cpus. For over 1.5
hours, the origin node's slon log has repeatedly output this every 11 or 12
minutes:

0318 11:03:34 INFO   cleanupThread:    0.258 seconds for cleanupEvent()
0318 11:04:15 ERROR  cleanupThread: "vacuum  analyze
"_slony_cluster".sl_event;" - ERROR  cleanupThread: "vacuum  analyze
"_slony_cluster".sl_confirm;" - ERROR  cleanupThread: "vacuum  analyze
"_slony_cluster".sl_setsync;" - ERROR  cleanupThread: "vacuum  analyze
"_slony_cluster".sl_log_1;" - ERROR  cleanupThread: "vacuum  analyze
"_slony_cluster".sl_log_2;" - ERROR  cleanupThread: "vacuum  analyze
"_slony_cluster".sl_seqlog;" - INFO   cleanupThread:   40.928 seconds for
vacuuming

pg_stat_activity shows a vacuum of sl_log_2 has been running for over 9 hours. 
It has 41.9 million rows in it, probably explaining why the vacuum is taking so
long. Meanwhile, sl_status shows st_lag_num_events is now over 2,500 on each
receiver node, and growing steadily.

This vacuum is coming from slony, not pg_autovacuum, as I have all slony tables
disabled from autovacuum via the pg_autovacuum table.

One thing I wonder is why doesn't cleanupEvent just truncate the table instead
of vacuum it?  (I'm new to Postgres so maybe that's a naive question.)

My receiver nodes are falling way behind.


-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.
From slony1-bugs at lists.slony.info  Wed Mar 18 10:23:14 2009
From: slony1-bugs at lists.slony.info (slony1-bugs@lists.slony.info)
Date: Wed Mar 18 10:23:16 2009
Subject: [Slony1-bugs] [Bug 78] Vacuum of sl_log_2 with 42M rows is blocking
	replication
In-Reply-To: <bug-78-4@http.www.slony.info/bugzilla/>
Message-ID: <20090318172314.C4E50290017@main.slony.info>

http://www.slony.info/bugzilla/show_bug.cgi?id=78





--- Comment #1 from Gordon Shannon <gordon@collectiveintellect.com>  2009-03-18 10:23:14 ---
Correction-- the slon log I have quoted was one of the receiver nodes, not the
origin node.


-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.
From slony1-bugs at lists.slony.info  Mon Mar 30 21:55:43 2009
From: slony1-bugs at lists.slony.info (slony1-bugs@lists.slony.info)
Date: Mon Mar 30 21:55:45 2009
Subject: [Slony1-bugs] [Bug 62] Initial subscription blocks with "sequence
 ID <xxx> has already been assigned"
In-Reply-To: <bug-62-4@http.www.slony.info/bugzilla/>
Message-ID: <20090331045543.3724729039A@main.slony.info>

http://www.slony.info/bugzilla/show_bug.cgi?id=62





--- Comment #4 from Stuart Bishop <stuart@stuartbishop.net>  2009-03-30 21:55:42 ---
A simple way to reproduce this issue is to attempt to build a new replica with
insufficient disk space. Rather than fail as it should, Slony-I gets thrown
into this loop.


-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.
