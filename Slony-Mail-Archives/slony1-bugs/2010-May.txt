From eross at locatrix.com  Mon May  3 18:12:37 2010
From: eross at locatrix.com (Andrew Eross)
Date: Tue, 4 May 2010 11:12:37 +1000
Subject: [Slony1-bugs] UTF-8 encoding error...problems with 2.0.3
Message-ID: <q2wb83c96a71005031812p7d0bdb0ay1eb987d9de4d23@mail.gmail.com>

Hi folks,

My company (and I) run Slony for 4 of our production databases, and have
been using it for years.

All 3 of our database servers are consistently running Postgres 8.3.10 on
Ubuntu, all databases created with  ENCODING = 'UTF8'.

I recently attempted to upgrade from 2.0.2 to 2.0.3, which went (seemingly)
quite smoothly, the upgrade caused itself went fine, following the standard
instructions.

However, afterwards our replication completely broke, and I started seeing
lots of UTF8 encoding errors reported in the slony log files for 2 of our
databases that had been previously replicating just fine.

Lots of these messages: "ERROR:  invalid byte sequence for *encoding*
 "UTF8":"

I've seen some posts out there in the world with the same problem, probably
from a different cause. e.g., this guy:
http://old.nabble.com/encoding-problems-td28383260.html

Same thing happened to me, and it was not a Postgres versioning issue, this
started occurring only after I upgraded to 2.0.3.

I eventually wrote a tool to manually traverse the entire database in
question and select/update every row after running it through iconv to be
sure there was no encoding issues. I'll open source the tool later, it was a
handy thing to make anyways.

After thatI tried dropping replication and starting from scratch with the
refreshed data, still resulted in the same error.

Anyways, how I've ended up fixing it is I've concluded the problem only
occurs with Slony 2.0.3 ... I have now re-installed 2.0.2 and re-created all
the replication from scratch...and poof no more UTF8 encoding errors.

Sorry, I wish I had the log files, but they got wiped out. I just wanted to
ask if this was a known issue? If not, I will endeavour to go re-create the
bug again and get the logs this time.

PS
Just a side note -- anecdotally -- separately I saw some other errors in the
log files not related to UTF8 encoding, just a general INSERT error because
it looked like Slony was trying to run an SQL query either with a missing
double-quote or with-out properly escaping the string first --- but, sorry,
again, someone deleted our logs, so I'll try to dig that out again too.

Cheers!

Andrew Eross
CTO
Locatrix Communications
+61.405.761.614
eross at locatrix.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-bugs/attachments/20100504/97136447/attachment.htm 

From ssinger at ca.afilias.info  Tue May  4 06:04:11 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 04 May 2010 09:04:11 -0400
Subject: [Slony1-bugs] UTF-8 encoding error...problems with 2.0.3
In-Reply-To: <q2wb83c96a71005031812p7d0bdb0ay1eb987d9de4d23@mail.gmail.com>
References: <q2wb83c96a71005031812p7d0bdb0ay1eb987d9de4d23@mail.gmail.com>
Message-ID: <4BE01B4B.3090808@ca.afilias.info>

Andrew Eross wrote:
> Hi folks,
> 
> My company (and I) run Slony for 4 of our production databases, and have 
> been using it for years.
> 
> All 3 of our database servers are consistently running Postgres 8.3.10 
> on Ubuntu, all databases created with  ENCODING = 'UTF8'.
> 
> I recently attempted to upgrade from 2.0.2 to 2.0.3, which went 
> (seemingly) quite smoothly, the upgrade caused itself went fine, 
> following the standard instructions.

Your problems are caused by the upgrade. 2.0.3 has a bug. See this 
thread 
http://lists.slony.info/pipermail/slony1-general/2010-April/010596.html


A fix has already been committed to cvs


> 
> However, afterwards our replication completely broke, and I started 
> seeing lots of UTF8 encoding errors reported in the slony log files for 
> 2 of our databases that had been previously replicating just fine.
> 
> Lots of these messages: "ERROR:  invalid byte sequence 
> for *encoding* "UTF8":"
> 
> I've seen some posts out there in the world with the same problem, 
> probably from a different cause. e.g., this 
> guy: http://old.nabble.com/encoding-problems-td28383260.html
> 
> Same thing happened to me, and it was not a Postgres versioning issue, 
> this started occurring only after I upgraded to 2.0.3.
> 
> I eventually wrote a tool to manually traverse the entire database in 
> question and select/update every row after running it through iconv to 
> be sure there was no encoding issues. I'll open source the tool later, 
> it was a handy thing to make anyways.
> 
> After thatI tried dropping replication and starting from scratch with 
> the refreshed data, still resulted in the same error.
> 
> Anyways, how I've ended up fixing it is I've concluded the problem only 
> occurs with Slony 2.0.3 ... I have now re-installed 2.0.2 and re-created 
> all the replication from scratch...and poof no more UTF8 encoding errors.
> 
> Sorry, I wish I had the log files, but they got wiped out. I just wanted 
> to ask if this was a known issue? If not, I will endeavour to go 
> re-create the bug again and get the logs this time.
> 
> PS
> Just a side note -- anecdotally -- separately I saw some other errors in 
> the log files not related to UTF8 encoding, just a general INSERT error 
> because it looked like Slony was trying to run an SQL query either with 
> a missing double-quote or with-out properly escaping the string first 
> --- but, sorry, again, someone deleted our logs, so I'll try to dig that 
> out again too.
> 
> Cheers!
> 
> Andrew Eross
> CTO
> Locatrix Communications
> +61.405.761.614
> eross at locatrix.com <mailto:eross at locatrix.com>
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Slony1-bugs mailing list
> Slony1-bugs at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-bugs


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From singh.gurjeet at gmail.com  Wed May 12 07:31:06 2010
From: singh.gurjeet at gmail.com (Gurjeet Singh)
Date: Wed, 12 May 2010 10:31:06 -0400
Subject: [Slony1-bugs] An old event not confirmed: A possible bug?
Message-ID: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com>

Hi All,

    I have two Slony test beds which show the exact same symptoms!

select * from sl_event order by ev_seqno;

 ev_origin |  ev_seqno  |        ev_timestamp        |
ev_snapshot         | ev_type |
-----------+------------+----------------------------+----------------------------+---------+-
         2 | 5000000002 | 2010-04-30 08:32:38.622928 |
458:458:                   | SYNC    |
         1 | 5000525721 | 2010-05-12 13:30:22.79626  |
72685915:72685915:         | SYNC    |
         1 | 5000525722 | 2010-05-12 13:30:24.800943 |
72686139:72686139:         | SYNC    |
         1 | 5000525723 | 2010-05-12 13:30:26.804862 |
72686224:72686224:         | SYNC    |
...

The reason I think this _might_ be a bug is that on both clusters, slave
node's sl_event has the exact same record for ev_seqno=5000000002 except for
the timestamp; same origin, and same snapshot!

The head of sl_confirm has:

 select * from sl_confirm order by con_seqno;

 con_origin | con_received | con_seqno  |       con_timestamp
------------+--------------+------------+----------------------------
          2 |            1 | 5000000002 | 2010-04-30 08:32:53.974021
          1 |            2 | 5000527075 | 2010-05-12 14:15:41.192279
          1 |            2 | 5000527076 | 2010-05-12 14:15:43.193607
          1 |            2 | 5000527077 | 2010-05-12 14:15:45.196291
          1 |            2 | 5000527078 | 2010-05-12 14:15:47.197005
...

Can someone comment on the health of the cluster? All events, except for
that on, are being confirmed and purged from the system regularly, so my
assumption is that the cluster is healthy and that the slave is in sync with
the master.

Thanks in advance.
-- 
gurjeet.singh
@ EnterpriseDB - The Enterprise Postgres Company
http://www.enterprisedb.com

singh.gurjeet@{ gmail | yahoo }.com
Twitter/Skype: singh_gurjeet

Mail sent from my BlackLaptop device
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-bugs/attachments/20100512/38abcc81/attachment.htm 

From JanWieck at Yahoo.com  Wed May 12 07:56:04 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed, 12 May 2010 10:56:04 -0400
Subject: [Slony1-bugs] [Slony1-general] An old event not confirmed: A
	possible bug?
In-Reply-To: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com>
References: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com>
Message-ID: <4BEAC184.1000702@Yahoo.com>

On 5/12/2010 10:31 AM, Gurjeet Singh wrote:
> Hi All,
> 
>     I have two Slony test beds which show the exact same symptoms!
> 
> select * from sl_event order by ev_seqno;
> 
>  ev_origin |  ev_seqno  |        ev_timestamp        |        
> ev_snapshot         | ev_type |
> -----------+------------+----------------------------+----------------------------+---------+-
>          2 | 5000000002 | 2010-04-30 08:32:38.622928 | 
> 458:458:                   | SYNC    |
>          1 | 5000525721 | 2010-05-12 13:30:22.79626  | 
> 72685915:72685915:         | SYNC    |
>          1 | 5000525722 | 2010-05-12 13:30:24.800943 | 
> 72686139:72686139:         | SYNC    |
>          1 | 5000525723 | 2010-05-12 13:30:26.804862 | 
> 72686224:72686224:         | SYNC    |
> ...
> 

Slony always keeps at least the last event per origin around. Otherwise 
the view sl_status would not work.

What should worry you is that there are no newer SYNC events from node 2 
available. Slony does create a sporadic SYNC every now and then even if 
there is no activity or the node isn't an origin anyway.

Is it possible that node 2's clock is way off?


Jan

> The reason I think this _might_ be a bug is that on both clusters, slave 
> node's sl_event has the exact same record for ev_seqno=5000000002 except 
> for the timestamp; same origin, and same snapshot!
> 
> The head of sl_confirm has:
> 
>  select * from sl_confirm order by con_seqno;
> 
>  con_origin | con_received | con_seqno  |       con_timestamp
> ------------+--------------+------------+----------------------------
>           2 |            1 | 5000000002 | 2010-04-30 08:32:53.974021
>           1 |            2 | 5000527075 | 2010-05-12 14:15:41.192279
>           1 |            2 | 5000527076 | 2010-05-12 14:15:43.193607
>           1 |            2 | 5000527077 | 2010-05-12 14:15:45.196291
>           1 |            2 | 5000527078 | 2010-05-12 14:15:47.197005
> ...
> 
> Can someone comment on the health of the cluster? All events, except for 
> that on, are being confirmed and purged from the system regularly, so my 
> assumption is that the cluster is healthy and that the slave is in sync 
> with the master.
> 
> Thanks in advance.
> -- 
> gurjeet.singh
> @ EnterpriseDB - The Enterprise Postgres Company
> http://www.enterprisedb.com
> 
> singh.gurjeet@{ gmail | yahoo }.com
> Twitter/Skype: singh_gurjeet
> 
> Mail sent from my BlackLaptop device
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From bugzilla-daemon at main.slony.info  Wed May 12 08:58:38 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Wed, 12 May 2010 08:58:38 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 119] New: cloneNodeFinish() fails updating
	sequences
Message-ID: <bug-119-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=119

           Summary: cloneNodeFinish() fails updating sequences
           Product: Slony-I
           Version: 2.0
          Platform: PC
        OS/Version: Linux
            Status: NEW
          Severity: critical
          Priority: low
         Component: stored procedures
        AssignedTo: slony1-bugs at lists.slony.info
        ReportedBy: ssinger at ca.afilias.info
                CC: slony1-bugs at lists.slony.info
   Estimated Hours: 0.0


In 2.0.3,

The CLONE FINISH slonik command will invoke the plpgsql function
cloneNodeFinish() that will in turn invoke updateReloid()

If your replication sets have sequences in them then this will fail because
sl_sequence does not have tab_id it has seq_id.

bug report + patch from Chirag Dave (cdave at ca.afilias.info):

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.

From singh.gurjeet at gmail.com  Wed May 12 09:44:52 2010
From: singh.gurjeet at gmail.com (Gurjeet Singh)
Date: Wed, 12 May 2010 12:44:52 -0400
Subject: [Slony1-bugs] [Slony1-general] An old event not confirmed: A
	possible bug?
In-Reply-To: <4BEAC184.1000702@Yahoo.com>
References: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com> 
	<4BEAC184.1000702@Yahoo.com>
Message-ID: <AANLkTinH3vK-4jAEWMaXvROXkJ0ThTQCBxmRzmxvfjco@mail.gmail.com>

On Wed, May 12, 2010 at 10:56 AM, Jan Wieck <JanWieck at yahoo.com> wrote:

> On 5/12/2010 10:31 AM, Gurjeet Singh wrote:
>
>> Hi All,
>>
>>    I have two Slony test beds which show the exact same symptoms!
>>
>> select * from sl_event order by ev_seqno;
>>
>>  ev_origin |  ev_seqno  |        ev_timestamp        |        ev_snapshot
>>         | ev_type |
>>
>> -----------+------------+----------------------------+----------------------------+---------+-
>>         2 | 5000000002 | 2010-04-30 08:32:38.622928 | 458:458:
>>       | SYNC    |
>>         1 | 5000525721 | 2010-05-12 13:30:22.79626  | 72685915:72685915:
>>       | SYNC    |
>>         1 | 5000525722 | 2010-05-12 13:30:24.800943 | 72686139:72686139:
>>       | SYNC    |
>>         1 | 5000525723 | 2010-05-12 13:30:26.804862 | 72686224:72686224:
>>       | SYNC    |
>> ...
>>
>>
> Slony always keeps at least the last event per origin around. Otherwise the
> view sl_status would not work.
>
> What should worry you is that there are no newer SYNC events from node 2
> available. Slony does create a sporadic SYNC every now and then even if
> there is no activity or the node isn't an origin anyway.
>
> Is it possible that node 2's clock is way off?
>

# ssh root at 10.32.169.215 date; ssh root at 10.32.169.216 date
Wed May 12 16:38:20 UTC 2010
Wed May 12 16:38:20 UTC 2010

Above the difference of times on the two nodes; 215 has the origin and 216
has the subscriber. They seem to be perfectly in sync.

I think I forgot to paste the test_slony_state.pl output before. This is
waht raised the concern
<snip>
Node: 2 Confirmations not propagating from 2 to 1
================================================
Confirmations not propagating quickly in sl_confirm -

For origin node 2, receiver node 1, earliest propagated
confirmation has age 12 days > 00:30:00

Are slons running for both nodes?

Could listen paths be missing so that confirmations are not propagating?


Node: 2 Events not propagating to node 2
================================================
Events not propagating quickly in sl_event -
For origin node 2, earliest propagated event of age 12 days 00:01:00 >
00:30:00

Are slons running for both nodes?

Could listen paths be missing so that events are not propagating?
</snip>

And the path and listen configs:

system.db=# select * from sl_path;
 pa_server | pa_client |                    pa_conninfo                    |
pa_connretry
-----------+-----------+---------------------------------------------------+--------------
         2 |         1 | dbname=system.db host=10.32.169.216 user=postgres
|           10
         1 |         2 | dbname=system.db host=10.32.169.215 user=postgres
|           10
(2 rows)
system.db=# select * from sl_listen ;
 li_origin | li_provider | li_receiver
-----------+-------------+-------------
         2 |           2 |           1
         1 |           1 |           2
(2 rows)


Thanks and best regards,


>
>
> Jan
>
>  The reason I think this _might_ be a bug is that on both clusters, slave
>> node's sl_event has the exact same record for ev_seqno=5000000002 except for
>> the timestamp; same origin, and same snapshot!
>>
>> The head of sl_confirm has:
>>
>>  select * from sl_confirm order by con_seqno;
>>
>>  con_origin | con_received | con_seqno  |       con_timestamp
>> ------------+--------------+------------+----------------------------
>>          2 |            1 | 5000000002 | 2010-04-30 08:32:53.974021
>>          1 |            2 | 5000527075 | 2010-05-12 14:15:41.192279
>>          1 |            2 | 5000527076 | 2010-05-12 14:15:43.193607
>>          1 |            2 | 5000527077 | 2010-05-12 14:15:45.196291
>>          1 |            2 | 5000527078 | 2010-05-12 14:15:47.197005
>> ...
>>
>> Can someone comment on the health of the cluster? All events, except for
>> that on, are being confirmed and purged from the system regularly, so my
>> assumption is that the cluster is healthy and that the slave is in sync with
>> the master.
>>
>> Thanks in advance.
>> --
>> gurjeet.singh
>> @ EnterpriseDB - The Enterprise Postgres Company
>> http://www.enterprisedb.com
>>
>> singh.gurjeet@{ gmail | yahoo }.com
>> Twitter/Skype: singh_gurjeet
>>
>> Mail sent from my BlackLaptop device
>>
>>
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>
>
>
> --
> Anyone who trades liberty for security deserves neither
> liberty nor security. -- Benjamin Franklin
>



-- 
gurjeet.singh
@ EnterpriseDB - The Enterprise Postgres Company
http://www.enterprisedb.com

singh.gurjeet@{ gmail | yahoo }.com
Twitter/Skype: singh_gurjeet

Mail sent from my BlackLaptop device
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-bugs/attachments/20100512/ba6812db/attachment-0001.htm 

From bugzilla-daemon at main.slony.info  Wed May 12 12:27:28 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Wed, 12 May 2010 12:27:28 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 120] New: store path - after slon has started
 does not take effect
Message-ID: <bug-120-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=120

           Summary: store path - after slon has started does not take
                    effect
           Product: Slony-I
           Version: 2.0
          Platform: All
        OS/Version: All
            Status: NEW
          Severity: minor
          Priority: low
         Component: slon
        AssignedTo: slony1-bugs at lists.slony.info
        ReportedBy: ssinger at ca.afilias.info
                CC: slony1-bugs at lists.slony.info
   Estimated Hours: 0.0


Create a cluster and start up the slon processes for each node, do not define
any paths.

Now add some paths with 'store path(....)' via slonik.

Do NOT restart the slon processes, but do things that require using the paths
(ie subscribe a set).   Replication does not seem to happen, it seems that the
slon process won't see the path until it restarts.

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.

From bugzilla-daemon at main.slony.info  Wed May 12 13:33:04 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Wed, 12 May 2010 13:33:04 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 121] New: 'provider node -1 for set' when
 subscribing from a forwarding node
Message-ID: <bug-121-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=121

           Summary: 'provider node -1 for set' when subscribing from a
                    forwarding node
           Product: Slony-I
           Version: 2.0
          Platform: All
        OS/Version: All
            Status: NEW
          Severity: normal
          Priority: low
         Component: slon
        AssignedTo: slony1-bugs at lists.slony.info
        ReportedBy: ssinger at ca.afilias.info
                CC: slony1-bugs at lists.slony.info
   Estimated Hours: 0.0


This has been observed with Slony 2.0.3

I have a 5 node cluster

(1)---->(2)
  \\
   \\
    (3)-----(4)
     \\
      \\
       (5)

With 2 replication sets (origin is node 1), node 3 is a forwarder to nodes 4
and 5.  Node 1 does not communicate directly with 4 or 5.

16:02:55 EDTDEBUG2 remoteWorkerThread_1: Received event #1 from 5000000039
type:ENABLE_SUBSCRIPTION
16:02:55 EDTINFO   copy_set 2 - omit=f - bool=0
16:02:55 EDTINFO   omit is FALSE
16:02:55 EDTERROR  remoteWorkerThread_1: provider node -1 for set 2not found in
runtime configuration
16:02:55 EDTINFO   slon: notify worker process to shutdown
16:02:55 EDTWARN   remoteWorkerThread_1: data copy for set 2 failed 1 times -
sleep 15 seconds
sched_wait_conn: write(): Bad file descriptor
16:02:55 EDTCONFIG slon: child terminated status: 65280; pid: 18487, current
worker pid: 18487
termination of slon2010-05-12 16:02:55 EDTCONFIG slon: child terminated status:
65280; pid: 18487, current worker pid: 18487



The slonik script that generated this set of actions was

cluster name=disorder_replica;
node 1 admin conninfo='dbname=test1 host=localhost user=slony password=test';
define CONNINFO1 'dbname=test1 host=localhost user=slony password=test';
node 2 admin conninfo='dbname=test2 host=localhost user=slony password=test';
define CONNINFO2 'dbname=test2 host=localhost user=slony password=test';
node 3 admin conninfo='dbname=test3 host=localhost user=slony password=test';
define CONNINFO3 'dbname=test3 host=localhost user=slony password=test';
node 4 admin conninfo='dbname=test4 host=localhost user=slony password=test';
define CONNINFO4 'dbname=test4 host=localhost user=slony password=test';
node 5 admin conninfo='dbname=test5 host=localhost user=slony password=test';
define CONNINFO5 'dbname=test5 host=localhost user=slony password=test';
subscribe set(id=2,provider=1,receiver=2,forward=yes);
wait for event(origin=1,confirmed=2,wait on=1);
sync(id=1);
wait for event(origin=1,confirmed=2,wait on=1);
subscribe set(id=2,provider=1,receiver=3,forward=yes);
wait for event(origin=1,confirmed=3,wait on=1);
sync(id=1);
wait for event(origin=1,confirmed=3,wait on=1);
subscribe set(id=2,provider=3,receiver=4);
wait for event(origin=3,confirmed=4,wait on=3);
sync(id=1);
wait for event(origin=3,confirmed=4,wait on=3);
subscribe set(id=2,provider=3,receiver=5);
wait for event(origin=3,confirmed=5,wait on=3);
sync(id=1);
wait for event(origin=3,confirmed=5,wait on=3);


I have seen this error on both nodes 4 and 5 a number of times but I am unable
to reproduce it consistently.  I am not issuing a UNSUBSCRIBE SET, so I think
this is different than bug #111

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.

From bugzilla-daemon at main.slony.info  Thu May 13 12:35:21 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Thu, 13 May 2010 12:35:21 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 119] cloneNodeFinish() fails updating sequences
In-Reply-To: <bug-119-4@http.www.slony.info/bugzilla/>
References: <bug-119-4@http.www.slony.info/bugzilla/>
Message-ID: <20100513193521.48B5A29033F@main.slony.info>

http://www.slony.info/bugzilla/show_bug.cgi?id=119

Steve Singer <ssinger at ca.afilias.info> changed:

           What    |Removed                     |Added
----------------------------------------------------------------------------
         Resolution|                            |FIXED
             Status|NEW                         |RESOLVED

--- Comment #1 from Steve Singer <ssinger at ca.afilias.info> 2010-05-13 12:35:20 PDT ---
Fixed in REL_2_0_STABLE

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.

From bugzilla-daemon at main.slony.info  Fri May 14 12:44:29 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Fri, 14 May 2010 12:44:29 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 122] New: bad error message: subscribe a set
 with no paths to a dropped node
Message-ID: <bug-122-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=122

           Summary: bad error message: subscribe a set with no paths to a
                    dropped node
           Product: Slony-I
           Version: 2.0
          Platform: All
        OS/Version: All
            Status: NEW
          Severity: minor
          Priority: low
         Component: slonik
        AssignedTo: slony1-bugs at lists.slony.info
        ReportedBy: ssinger at ca.afilias.info
                CC: slony1-bugs at lists.slony.info
   Estimated Hours: 0.0


1. Subscribe a node to a replication set
2. Drop the node
3. Recreate the node with the same node id
4. Subscribe the node to the replication set, do not explicitly run 'store
path' 


Slonik will give the following error

<stdin>:13: PGRES_FATAL_ERROR select "_disorder_replica".subscribeSet(1, 3, 4,
't', 'f');  - ERROR:  insert or update on table "sl_path" violates foreign key
constraint "pa_client-no_id-ref"


This error message does not leave the user with a good sense of what the actual
problem is

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.

From bugzilla-daemon at main.slony.info  Fri May 14 13:10:37 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Fri, 14 May 2010 13:10:37 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 122] bad error message: subscribe a set with no
 paths to a dropped node
In-Reply-To: <bug-122-4@http.www.slony.info/bugzilla/>
References: <bug-122-4@http.www.slony.info/bugzilla/>
Message-ID: <20100514201037.7CE60290330@main.slony.info>

http://www.slony.info/bugzilla/show_bug.cgi?id=122

--- Comment #1 from Steve Singer <ssinger at ca.afilias.info> 2010-05-14 13:10:37 PDT ---
Actually my slonik call for 'store node' was failing which was putting the
cluster into the state where this could happen.   But the issue still stands
(that this error message is not very helpful)

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.

From singh.gurjeet at gmail.com  Sun May 16 05:22:38 2010
From: singh.gurjeet at gmail.com (Gurjeet Singh)
Date: Sun, 16 May 2010 08:22:38 -0400
Subject: [Slony1-bugs] Slony triggers included in pg_dump
Message-ID: <AANLkTik3kuhrTf7xiy5ylZi-N4AprDvtBP831Il6oxhS@mail.gmail.com>

Hi All,

    The docs here http://www.slony.info/documentation/triggers.html says
that post Postgres 8.3 version

<snip>

   - If you take a pg_dump of a Slony-I node, and drop out the
Slony-Inamespace, this now cleanly removes
   *all* Slony-I components, leaving the database, *including its schema,*in a
   "pristine", consistent fashion, ready for whatever use may be desired.

</snip>

But I see that when dumping a schema containing tables monitored by Slony,
the dump still shows the log triggers on those tables.

postgres=# select version();

version
----------------------------------------------------------------------------------------------------
 PostgreSQL 8.3.10 on i686-pc-linux-gnu, compiled by GCC gcc (GCC) 4.3.2
20081105 (Red Hat 4.3.2-7)

# pg_dump --schema-only --schema pgbench -p 5433 -U postgres | grep "CREATE
TRIGGER"
CREATE TRIGGER _pgbench_logtrigger_1
CREATE TRIGGER _pgbench_logtrigger_2
CREATE TRIGGER _pgbench_logtrigger_3

So is that statement in the docs incorrect, or is this a regression?

Best regards,
-- 
gurjeet.singh
@ EnterpriseDB - The Enterprise Postgres Company
http://www.enterprisedb.com

singh.gurjeet@{ gmail | yahoo }.com
Twitter/Skype: singh_gurjeet

Mail sent from my BlackLaptop device
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-bugs/attachments/20100516/fabd4530/attachment.htm 

From ssinger at ca.afilias.info  Sun May 16 07:36:06 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Sun, 16 May 2010 10:36:06 -0400
Subject: [Slony1-bugs] [Slony1-general] Slony triggers included in
	pg_dump
In-Reply-To: <AANLkTik3kuhrTf7xiy5ylZi-N4AprDvtBP831Il6oxhS@mail.gmail.com>
References: <AANLkTik3kuhrTf7xiy5ylZi-N4AprDvtBP831Il6oxhS@mail.gmail.com>
Message-ID: <4BF002D6.3030805@ca.afilias.info>

Gurjeet Singh wrote:
> Hi All,
> 
>     The docs here http://www.slony.info/documentation/triggers.html says 
> that post Postgres 8.3 version
> 
> <snip>
> 
>     * If you take a pg_dump of a Slony-I node, and drop out the Slony-I
>       namespace, this now cleanly removes /all/ Slony-I components,
>       leaving the database, /including its schema,/ in a "pristine",
>       consistent fashion, ready for whatever use may be desired. 
> 
> </snip>
> 
> But I see that when dumping a schema containing tables monitored by 
> Slony, the dump still shows the log triggers on those tables.

When it says 'drop out' it means use the 'DROP SCHEMA' postgresql 
command to drop out the slony schema this will remove your triggers. 
This only works with Slony1 2.0.0 or above (You were probably reading 
the 2.0.3 documentation on the website).    This is the same procedure 
you need to use against a master with 1.2 (but with 2.0 it works against 
both slaves and masters).

In 1.2.x slony munges the catalog on the slaves and dropping the slony 
schema from a slave will leave your leave in a bad state.

If your planning on using Slony 2.0.x you are better off with the tip of 
REL_2_0_STABLE from cvs versus 2.0.3 due to the 'invalid utf8' bug fixed 
a few weeks ago.

Does the attached documentation patch make that paragraph more clear?



> -- 
> gurjeet.singh
> @ EnterpriseDB - The Enterprise Postgres Company
> http://www.enterprisedb.com
> 
> singh.gurjeet@{ gmail | yahoo }.com
> Twitter/Skype: singh_gurjeet
> 
> Mail sent from my BlackLaptop device

-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142
-------------- next part --------------
A non-text attachment was scrubbed...
Name: trigger.diff
Type: text/x-patch
Size: 827 bytes
Desc: not available
Url : http://lists.slony.info/pipermail/slony1-bugs/attachments/20100516/f791713a/attachment.bin 

From bugzilla-daemon at main.slony.info  Mon May 17 07:07:43 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Mon, 17 May 2010 07:07:43 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 123] New: Adding a node shortly after deleting
 it causes 'node -1 not found'
Message-ID: <bug-123-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=123

           Summary: Adding a node shortly after deleting it causes 'node
                    -1 not found'
           Product: Slony-I
           Version: 2.0
          Platform: All
        OS/Version: All
            Status: NEW
          Severity: major
          Priority: low
         Component: slon
        AssignedTo: slony1-bugs at lists.slony.info
        ReportedBy: ssinger at ca.afilias.info
                CC: slony1-bugs at lists.slony.info
   Estimated Hours: 0.0


This test was performed on a 5 node cluster  with 2.0.x (CVS)

1=====>2
\\
 \\
  3=====>5
   \\
    4

A subscription set was subscribed to all nodes.

Then 
-The slon for node 4 was killed
-Node 4 was dropped from replication, with node 1 being the event node
-A 'store node(id=4,event node=3) was executed  + add paths
-The slon for node 4 was started
-Node 4 is subscribed to the set coming from node 3.


The slon for node 4 encounters

Received event #1 from 5000000049 type:ENABLE_SUBSCRIPTION
2010-05-17 09:41:51 EDTINFO   copy_set 1 - omit=f - bool=0
2010-05-17 09:41:51 EDTINFO   omit is FALSE
2010-05-17 09:41:51 EDTERROR  remoteWorkerThread_1: provider node -1 for set
1not found in runtime configuration


The sl_event table on node 4 contains


4 5000000001  STORE_PATH
1 5000000048  SYNC
5 5000000004  SYNC
3 5000000011  STORE_NODE
3 5000000012  ENABLE_NODE
3 5000000013  STORE_PATH
3 5000000014  SUBSCRIBE_SET (3,4)
3 5000000015  SYNC
4 5000000002  SYNC
1 5000000049  ENABLE_SUBSCRIPTION  (3,4)

Slon seems to be processing the ENABLE_SUBSCRIPTION that is marked as having an
ev_origin of node 1 (the set origin) before the SUBSCRIBE_SET (having an
ev_origin of node 3, the provider)

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.

From singh.gurjeet at gmail.com  Mon May 17 10:58:14 2010
From: singh.gurjeet at gmail.com (Gurjeet Singh)
Date: Mon, 17 May 2010 13:58:14 -0400
Subject: [Slony1-bugs] [Slony1-general] Slony triggers included in
	pg_dump
In-Reply-To: <4BF002D6.3030805@ca.afilias.info>
References: <AANLkTik3kuhrTf7xiy5ylZi-N4AprDvtBP831Il6oxhS@mail.gmail.com> 
	<4BF002D6.3030805@ca.afilias.info>
Message-ID: <AANLkTilq9QncnLfnHKwS3YEr3sELyzEHHTMFZrHqdN6b@mail.gmail.com>

On Sun, May 16, 2010 at 10:36 AM, Steve Singer <ssinger at ca.afilias.info>wrote:

> Gurjeet Singh wrote:
>
>> Hi All,
>>
>>    The docs here http://www.slony.info/documentation/triggers.html says
>> that post Postgres 8.3 version
>>
>> <snip>
>>
>>    * If you take a pg_dump of a Slony-I node, and drop out the Slony-I
>>      namespace, this now cleanly removes /all/ Slony-I components,
>>      leaving the database, /including its schema,/ in a "pristine",
>>      consistent fashion, ready for whatever use may be desired.
>> </snip>
>>
>> But I see that when dumping a schema containing tables monitored by Slony,
>> the dump still shows the log triggers on those tables.
>>
>
> When it says 'drop out' it means use the 'DROP SCHEMA' postgresql command
> to drop out the slony schema this will remove your triggers. This only works
> with Slony1 2.0.0 or above (You were probably reading the 2.0.3
> documentation on the website).    This is the same procedure you need to use
> against a master with 1.2 (but with 2.0 it works against both slaves and
> masters).
>
> In 1.2.x slony munges the catalog on the slaves and dropping the slony
> schema from a slave will leave your leave in a bad state.
>
> If your planning on using Slony 2.0.x you are better off with the tip of
> REL_2_0_STABLE from cvs versus 2.0.3 due to the 'invalid utf8' bug fixed a
> few weeks ago.
>
> Does the attached documentation patch make that paragraph more clear?
>
>
The doc-patch does make it more clear, but I don't think DROP SCHEMA
_slony_cluster_name CASCADE is something I'd like to do with my running
Slony replication. A cleaner way would be more desirable.

Regards,
-- 
gurjeet.singh
@ EnterpriseDB - The Enterprise Postgres Company
http://www.enterprisedb.com

singh.gurjeet@{ gmail | yahoo }.com
Twitter/Skype: singh_gurjeet

Mail sent from my BlackLaptop device
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-bugs/attachments/20100517/755f3721/attachment.htm 

From ssinger at ca.afilias.info  Mon May 17 11:58:01 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Mon, 17 May 2010 14:58:01 -0400
Subject: [Slony1-bugs] [Slony1-general] Slony triggers included in
	pg_dump
In-Reply-To: <AANLkTilq9QncnLfnHKwS3YEr3sELyzEHHTMFZrHqdN6b@mail.gmail.com>
References: <AANLkTik3kuhrTf7xiy5ylZi-N4AprDvtBP831Il6oxhS@mail.gmail.com>
	<4BF002D6.3030805@ca.afilias.info>
	<AANLkTilq9QncnLfnHKwS3YEr3sELyzEHHTMFZrHqdN6b@mail.gmail.com>
Message-ID: <4BF191B9.5000607@ca.afilias.info>

Gurjeet Singh wrote:

> The doc-patch does make it more clear, but I don't think DROP SCHEMA 
> _slony_cluster_name CASCADE is something I'd like to do with my running 
> Slony replication. A cleaner way would be more desirable.

Another option you have is to restore the dump that you create of your 
pgbench schema to another database(a new one).  When you restore the 
CREATE TRIGGER statements will fail because the slony stored procedures 
won't exist in your new database.   This should leave you with your 
database - any slony stuff.  (I agree having to put up with 'errors' 
during the restore or make a second copy of the database just to do 
'DROP SCHEMA' are both less than ideal solutions)




> 
> Regards,
> -- 
> gurjeet.singh
> @ EnterpriseDB - The Enterprise Postgres Company
> http://www.enterprisedb.com
> 
> singh.gurjeet@{ gmail | yahoo }.com
> Twitter/Skype: singh_gurjeet
> 
> Mail sent from my BlackLaptop device


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From bugzilla-daemon at main.slony.info  Tue May 18 07:41:24 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Tue, 18 May 2010 07:41:24 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 124] New: EXECUTE SCRIPT,
	ONLY ON != EVENT NODE is wrong
Message-ID: <bug-124-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=124

           Summary: EXECUTE SCRIPT, ONLY ON != EVENT NODE is wrong
           Product: Slony-I
           Version: 2.0
          Platform: All
        OS/Version: All
            Status: NEW
          Severity: normal
          Priority: low
         Component: slonik
        AssignedTo: slony1-bugs at lists.slony.info
        ReportedBy: ssinger at ca.afilias.info
                CC: slony1-bugs at lists.slony.info
   Estimated Hours: 0.0


A slonik command like

EXECUTE SCRIPT( SET ID=1, FILENAME='/tmp/executeScript5718113975686807087.sql'
, EVENT NODE=1, EXECUTE ONLY ON=5);

Where the EVENT NODE != the EXECUTE ONLY ON node tries to execute the DDL
against node 1 (the event node).  This essentially ignores the node id
specified in the ONLY ON syntax.

We could either:

1) Submit the ddl script into the replication queue on the event node but only
execute it on the ONLY ON node.

2) Change EXECUTE ONLY ON to be a boolean so when it is true we only execute on
the EVENT NODE

3) Generate an error if event node != ONLY ON and let the user fix their
script.

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.

From bugzilla-daemon at main.slony.info  Tue May 18 10:43:30 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Tue, 18 May 2010 10:43:30 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 125] New: New init script
Message-ID: <bug-125-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=125

           Summary: New init script
           Product: Slony-I
           Version: devel
          Platform: PC
        OS/Version: Linux
            Status: NEW
          Severity: enhancement
          Priority: low
         Component: rpm
        AssignedTo: slony1-bugs at lists.slony.info
        ReportedBy: jose.arthur at gmail.com
                CC: devrim at gunduz.org
   Estimated Hours: 0.0


Created an attachment (id=41)
 --> (http://www.slony.info/bugzilla/attachment.cgi?id=41)
New init file

I modified the init script to support multiple daemons on the same machine.

For me, it is necessary because I have 2 databases on the same cluster, each
one using a different set, replicating.

How to use:

* copy or link /etc/init.d/slon to /etc/init.d/slon-new (choose your name).
* create a file /etc/sysconfig/slon-new (match above name) with at least this
variables:

SLONCONF=/etc/slon-new.conf
SLONPID=/var/run/slon-new.pid
SLONLOG=/var/log/slony-new

That's it.

I tested it on CentOS.

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are the assignee for the bug.

From bugzilla-daemon at main.slony.info  Tue May 18 11:02:03 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Tue, 18 May 2010 11:02:03 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 125] New init script
In-Reply-To: <bug-125-4@http.www.slony.info/bugzilla/>
References: <bug-125-4@http.www.slony.info/bugzilla/>
Message-ID: <20100518180203.11886290CA2@main.slony.info>

http://www.slony.info/bugzilla/show_bug.cgi?id=125

--- Comment #1 from Steve Singer <ssinger at ca.afilias.info> 2010-05-18 11:02:03 PDT ---
Can you include the output of 'diff' comparing your version against the version
of the slony script that you based your changes on (ideally the head of
REL_2_0_STABLE from cvs?)

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are the assignee for the bug.

From bugzilla-daemon at main.slony.info  Tue May 18 11:10:27 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Tue, 18 May 2010 11:10:27 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 125] New init script
In-Reply-To: <bug-125-4@http.www.slony.info/bugzilla/>
References: <bug-125-4@http.www.slony.info/bugzilla/>
Message-ID: <20100518181027.44B9B290CA2@main.slony.info>

http://www.slony.info/bugzilla/show_bug.cgi?id=125

--- Comment #2 from Jose Arthur Benetasso Villanova <jose.arthur at gmail.com> 2010-05-18 11:10:27 PDT ---
Created an attachment (id=42)
 --> (http://www.slony.info/bugzilla/attachment.cgi?id=42)
diff -Nur file

Here.

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are the assignee for the bug.

From bugzilla-daemon at main.slony.info  Tue May 18 12:44:40 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Tue, 18 May 2010 12:44:40 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 126] New: slon sometimes does not recover from a
	network outage
Message-ID: <bug-126-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=126

           Summary: slon sometimes does not recover from a network outage
           Product: Slony-I
           Version: 1.2
          Platform: Other
        OS/Version: Other
            Status: NEW
          Severity: normal
          Priority: low
         Component: slon
        AssignedTo: slony1-bugs at lists.slony.info
        ReportedBy: ssinger at ca.afilias.info
                CC: slony1-bugs at lists.slony.info
   Estimated Hours: 0.0


We've received a report of slon not recovering properly from a network outage.

It appears that the remote listener thread  (8431) encountered a network error
while the network was done.  No network error for the remote worker threads
where observed.    After the error the remote listener for 8431 apparently
continued to queue events (but no logs are available).  Replication started to
fall behind and did not proceed after the network was restored.

Restarting slon made replication work again.


--


My theory is that we were waiting on a socket read() inside of libpq and the
network died.  Since we were not trying to send an event no packets where
generated to notify libpq that the network connection died.

Setting KEEPALIVE on the connections to postgres should address this.  We don't
appear to be doing that currently.



--


2010-05-16 20:59:35 UTC DEBUG2 remoteListenThread_8344: LISTEN
2010-05-16 20:59:35 UTC DEBUG2 remoteListenThread_8346: LISTEN
2010-05-16 20:59:35 UTC DEBUG2 remoteWorkerThread_8344: forward confirm
8394,112865 received by 8344
2010-05-16 20:59:35 UTC DEBUG2 remoteWorkerThread_8344: forward confirm
8394,112865 received by 8346
2010-05-16 20:59:38 UTC DEBUG2 remoteListenThread_8346: queue event
8346,157585 SYNC
2010-05-16 20:59:38 UTC DEBUG2 remoteListenThread_8346: UNLISTEN
2010-05-16 20:59:38 UTC DEBUG2 remoteWorkerThread_8346: Received event
8346,157585 SYNC
2010-05-16 20:59:38 UTC DEBUG2 calc sync size - last time: 1 last
length: 10001 ideal: 5 proposed size: 3
2010-05-16 20:59:38 UTC DEBUG2 remoteWorkerThread_8346: SYNC 157585
processing
2010-05-16 20:59:38 UTC DEBUG2 remoteWorkerThread_8346: no sets need
syncing for this event
2010-05-16 20:59:38 UTC DEBUG2 remoteListenThread_8344: queue event
8344,148724 SYNC
2010-05-16 20:59:38 UTC DEBUG2 remoteListenThread_8344: queue event
8346,157585 SYNC
2010-05-16 20:59:38 UTC DEBUG2 remoteWorker_event: event 8346,157585
ignored - duplicate
2010-05-16 20:59:38 UTC DEBUG2 remoteListenThread_8344: UNLISTEN
2010-05-16 20:59:38 UTC DEBUG2 remoteWorkerThread_8344: Received event
8344,148724 SYNC
2010-05-16 20:59:38 UTC DEBUG2 remoteWorkerThread_8344: SYNC 148724
processing
2010-05-16 20:59:38 UTC DEBUG2 remoteWorkerThread_8344: no sets need
syncing for this event
2010-05-16 20:59:38 UTC ERROR  remoteListenThread_8341: "select
con_origin, con_received,     max(con_seqno) as con_seqno,
  max(con_timestamp) as con_timestamp from "_oxrsin".sl_confirm where
con_received <> 8394 group by con_origin, con_received"
 could not receive data from server: Connection timed out
2010-05-16 20:59:38 UTC DEBUG2 remoteWorkerThread_8344: forward confirm
8346,157585 received by 8344
2010-05-16 20:59:42 UTC DEBUG2 syncThread: new sl_action_seq 1 - SYNC 112866
2010-05-16 20:59:45 UTC DEBUG2 localListenThread: Received event
8394,112866 SYNC
2010-05-16 20:59:45 UTC DEBUG2 remoteListenThread_8344: LISTEN
2010-05-16 20:59:45 UTC DEBUG2 remoteListenThread_8346: LISTEN
2010-05-16 20:59:45 UTC DEBUG2 remoteListenThread_8344: LISTEN
2010-05-16 20:59:45 UTC DEBUG2 remoteListenThread_8346: LISTEN
2010-05-16 20:59:45 UTC DEBUG2 remoteWorkerThread_8346: forward confirm
8394,112866 received by 8344
2010-05-16 20:59:45 UTC DEBUG2 remoteWorkerThread_8346: forward confirm
8344,148724 received by 8346
2010-05-16 20:59:45 UTC DEBUG2 remoteWorkerThread_8344: forward confirm
8394,112866 received by 8346
2010-05-16 20:59:48 UTC DEBUG2 remoteListenThread_8346: queue event
8346,157586 SYNC
2010-05-16 20:59:48 UTC DEBUG2 remoteListenThread_8346: UNLISTEN
2010-05-16 20:59:48 UTC DEBUG2 remoteWorkerThread_8346: Received event
8346,157586 SYNC
2010-05-16 20:59:48 UTC DEBUG2 calc sync size - last time: 1 last
length: 10002 ideal: 5 proposed size: 3
2010-05-16 20:59:48 UTC DEBUG2 remoteWorkerThread_8346: SYNC 157586
processing
2010-05-16 20:59:48 UTC DEBUG2 remoteWorkerThread_8346: no sets need
syncing for this event
2010-05-16 20:59:48 UTC DEBUG2 remoteListenThread_8344: queue event
8344,148725 SYNC
2010-05-16 20:59:48 UTC DEBUG2 remoteListenThread_8344: queue event
8346,157586 SYNC
2010-05-16 20:59:48 UTC DEBUG2 remoteWorker_event: event 8346,157586
ignored - duplicate
2010-05-16 20:59:48 UTC DEBUG2 remoteListenThread_8344: UNLISTEN
2010-05-16 20:59:48 UTC DEBUG2 remoteWorkerThread_8344: Received event
8344,148725 SYNC
2010-05-16 20:59:48 UTC DEBUG2 remoteWorkerThread_8344: SYNC 148725
processing
2010-05-16 20:59:48 UTC DEBUG2 remoteWorkerThread_8344: no sets need
syncing for this event
2010-05-16 20:59:48 UTC DEBUG2 remoteWorkerThread_8344: forward confirm
8346,157586 received by 8344
2010-05-16 20:59:49 UTC DEBUG2 remoteWorkerThread_8346: forward confirm
8344,148725 received by 8346
2010-05-16 20:59:52 UTC DEBUG2 syncThread: new sl_action_seq 1 - SYNC 112867
2010-05-16 20:59:55 UTC DEBUG2 remoteListenThread_8344: LISTEN
2010-05-16 20:59:55 UTC DEBUG2 remoteListenThread_8346: LISTEN
2010-05-16 20:59:55 UTC DEBUG2 remoteWorkerThread_8344: forward confirm
8394,112867 received by 8344
2010-05-16 20:59:55 UTC DEBUG2 remoteWorkerThread_8346: forward confirm
8394,112867 received by 8346

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.

From bugzilla-daemon at main.slony.info  Tue May 18 14:06:11 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Tue, 18 May 2010 14:06:11 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 125] New init script
In-Reply-To: <bug-125-4@http.www.slony.info/bugzilla/>
References: <bug-125-4@http.www.slony.info/bugzilla/>
Message-ID: <20100518210611.E36802903B6@main.slony.info>

http://www.slony.info/bugzilla/show_bug.cgi?id=125

Devrim GUNDUZ <devrim at gunduz.org> changed:

           What    |Removed                     |Added
----------------------------------------------------------------------------
             Status|NEW                         |ASSIGNED

--- Comment #3 from Devrim GUNDUZ <devrim at gunduz.org> 2010-05-18 14:06:12 PDT ---
This did not work for me.

pkill kills slon daemons, however slon_watchdog restarts slons in a few
seconds.

Why don't you call slon_kill there?

Regards, Devrim

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are the assignee for the bug.

From bugzilla-daemon at main.slony.info  Tue May 18 15:02:08 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Tue, 18 May 2010 15:02:08 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 125] New init script
In-Reply-To: <bug-125-4@http.www.slony.info/bugzilla/>
References: <bug-125-4@http.www.slony.info/bugzilla/>
Message-ID: <20100518220208.3156F2903C0@main.slony.info>

http://www.slony.info/bugzilla/show_bug.cgi?id=125

--- Comment #4 from Jose Arthur Benetasso Villanova <jose.arthur at gmail.com> 2010-05-18 15:02:08 PDT ---
Hi Devrim.

I've compiled against a CentOS 5.5 and Postgresql 8.4.3 and using the specfile
When I run 'pgrep -f "slon -f /etc/slon.conf"' the command returns 2 processes,
but with the same name.

For me its the opposite: slon_kill didn't work.

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are the assignee for the bug.

From JanWieck at Yahoo.com  Tue May 18 19:55:49 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Tue, 18 May 2010 22:55:49 -0400
Subject: [Slony1-bugs] Slony triggers included in pg_dump
In-Reply-To: <AANLkTik3kuhrTf7xiy5ylZi-N4AprDvtBP831Il6oxhS@mail.gmail.com>
References: <AANLkTik3kuhrTf7xiy5ylZi-N4AprDvtBP831Il6oxhS@mail.gmail.com>
Message-ID: <4BF35335.7090502@Yahoo.com>

On 5/16/2010 8:22 AM, Gurjeet Singh wrote:
> Hi All,
> 
>     The docs here http://www.slony.info/documentation/triggers.html says 
> that post Postgres 8.3 version
> 
> <snip>
> 
>     * If you take a pg_dump of a Slony-I node, and drop out the Slony-I
>       namespace, this now cleanly removes /all/ Slony-I components,
>       leaving the database, /including its schema,/ in a "pristine",
>       consistent fashion, ready for whatever use may be desired. 
> 
> </snip>
> 
> But I see that when dumping a schema containing tables monitored by 
> Slony, the dump still shows the log triggers on those tables.

I think you misinterpret the statement.

If you take a complete dump of all schemas, restore that and THEN drop 
the slony schema, you have a pristine stand alone system as the result.

pg_dump does not exclude cross-schema definitions like those triggers. 
You will get the same for example with foreign keys. For a table in the 
schema you are dumping, that references a table in a non-dumped schema, 
pg_dump will still emit the ALTER TABLE attempting to create the constraint.


Jan

> 
> postgres=# select version();
>                                               
> version                                              
> ----------------------------------------------------------------------------------------------------
>  PostgreSQL 8.3.10 on i686-pc-linux-gnu, compiled by GCC gcc (GCC) 4.3.2 
> 20081105 (Red Hat 4.3.2-7)
> 
> # pg_dump --schema-only --schema pgbench -p 5433 -U postgres | grep 
> "CREATE TRIGGER"
> CREATE TRIGGER _pgbench_logtrigger_1
> CREATE TRIGGER _pgbench_logtrigger_2
> CREATE TRIGGER _pgbench_logtrigger_3
> 
> So is that statement in the docs incorrect, or is this a regression?
> 
> Best regards,
> -- 
> gurjeet.singh
> @ EnterpriseDB - The Enterprise Postgres Company
> http://www.enterprisedb.com
> 
> singh.gurjeet@{ gmail | yahoo }.com
> Twitter/Skype: singh_gurjeet
> 
> Mail sent from my BlackLaptop device
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Slony1-bugs mailing list
> Slony1-bugs at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-bugs


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From JanWieck at Yahoo.com  Tue May 18 20:04:51 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Tue, 18 May 2010 23:04:51 -0400
Subject: [Slony1-bugs] [Bug 123] New: Adding a node shortly after
 deleting it causes 'node -1 not found'
In-Reply-To: <bug-123-4@http.www.slony.info/bugzilla/>
References: <bug-123-4@http.www.slony.info/bugzilla/>
Message-ID: <4BF35553.4090402@Yahoo.com>

On 5/17/2010 10:07 AM, bugzilla-daemon at main.slony.info wrote:
> http://www.slony.info/bugzilla/show_bug.cgi?id=123
> 
>            Summary: Adding a node shortly after deleting it causes 'node
>                     -1 not found'
>            Product: Slony-I
>            Version: 2.0
>           Platform: All
>         OS/Version: All
>             Status: NEW
>           Severity: major
>           Priority: low
>          Component: slon
>         AssignedTo: slony1-bugs at lists.slony.info
>         ReportedBy: ssinger at ca.afilias.info
>                 CC: slony1-bugs at lists.slony.info
>    Estimated Hours: 0.0
> 
> 
> This test was performed on a 5 node cluster  with 2.0.x (CVS)
> 
> 1=====>2
> \\
>  \\
>   3=====>5
>    \\
>     4
> 
> A subscription set was subscribed to all nodes.
> 
> Then 
> -The slon for node 4 was killed
> -Node 4 was dropped from replication, with node 1 being the event node

Was this event waited on to be confirmed by all nodes?


Jan


> -A 'store node(id=4,event node=3) was executed  + add paths
> -The slon for node 4 was started
> -Node 4 is subscribed to the set coming from node 3.
> 
> 
> The slon for node 4 encounters
> 
> Received event #1 from 5000000049 type:ENABLE_SUBSCRIPTION
> 2010-05-17 09:41:51 EDTINFO   copy_set 1 - omit=f - bool=0
> 2010-05-17 09:41:51 EDTINFO   omit is FALSE
> 2010-05-17 09:41:51 EDTERROR  remoteWorkerThread_1: provider node -1 for set
> 1not found in runtime configuration
> 
> 
> The sl_event table on node 4 contains
> 
> 
> 4 5000000001  STORE_PATH
> 1 5000000048  SYNC
> 5 5000000004  SYNC
> 3 5000000011  STORE_NODE
> 3 5000000012  ENABLE_NODE
> 3 5000000013  STORE_PATH
> 3 5000000014  SUBSCRIBE_SET (3,4)
> 3 5000000015  SYNC
> 4 5000000002  SYNC
> 1 5000000049  ENABLE_SUBSCRIPTION  (3,4)
> 
> Slon seems to be processing the ENABLE_SUBSCRIPTION that is marked as having an
> ev_origin of node 1 (the set origin) before the SUBSCRIBE_SET (having an
> ev_origin of node 3, the provider)
> 


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From bugzilla-daemon at main.slony.info  Wed May 19 08:44:12 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Wed, 19 May 2010 08:44:12 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 125] New init script
In-Reply-To: <bug-125-4@http.www.slony.info/bugzilla/>
References: <bug-125-4@http.www.slony.info/bugzilla/>
Message-ID: <20100519154412.80924290CDC@main.slony.info>

http://www.slony.info/bugzilla/show_bug.cgi?id=125

--- Comment #5 from Devrim GUNDUZ <devrim at gunduz.org> 2010-05-19 08:44:12 PDT ---
(In reply to comment #4)

> For me its the opposite: slon_kill didn't work.

Why?

Regards, Devrim

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are the assignee for the bug.

From ssinger at ca.afilias.info  Wed May 19 08:54:02 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Wed, 19 May 2010 11:54:02 -0400
Subject: [Slony1-bugs] [Bug 123] New: Adding a node shortly after
 deleting it causes 'node -1 not found'
In-Reply-To: <4BF35553.4090402@Yahoo.com>
References: <bug-123-4@http.www.slony.info/bugzilla/>
	<4BF35553.4090402@Yahoo.com>
Message-ID: <4BF4099A.2020205@ca.afilias.info>

Jan Wieck wrote:
> On 5/17/2010 10:07 AM, bugzilla-daemon at main.slony.info wrote:
>> http://www.slony.info/bugzilla/show_bug.cgi?id=123

>>
>> Then 
>> -The slon for node 4 was killed
>> -Node 4 was dropped from replication, with node 1 being the event node
> 
> Was this event waited on to be confirmed by all nodes?


If I do

DROP NODE(id=4,event node=1);
sync(id=1);
wait for event(origin=1, confirmed=all, wait on=1);

still gives me the -1 error,

DROP NODE(id=4,event node=1);
wait for event(origin=1, confirmed=all, wait on=1);

'seems' to not cause the error.


> 
> 
> Jan
> 
> 
>> -A 'store node(id=4,event node=3) was executed  + add paths
>> -The slon for node 4 was started
>> -Node 4 is subscribed to the set coming from node 3.
>>
>>
>> The slon for node 4 encounters
>>
>> Received event #1 from 5000000049 type:ENABLE_SUBSCRIPTION
>> 2010-05-17 09:41:51 EDTINFO   copy_set 1 - omit=f - bool=0
>> 2010-05-17 09:41:51 EDTINFO   omit is FALSE
>> 2010-05-17 09:41:51 EDTERROR  remoteWorkerThread_1: provider node -1 for set
>> 1not found in runtime configuration
>>
>>
>> The sl_event table on node 4 contains
>>
>>
>> 4 5000000001  STORE_PATH
>> 1 5000000048  SYNC
>> 5 5000000004  SYNC
>> 3 5000000011  STORE_NODE
>> 3 5000000012  ENABLE_NODE
>> 3 5000000013  STORE_PATH
>> 3 5000000014  SUBSCRIBE_SET (3,4)
>> 3 5000000015  SYNC
>> 4 5000000002  SYNC
>> 1 5000000049  ENABLE_SUBSCRIPTION  (3,4)
>>
>> Slon seems to be processing the ENABLE_SUBSCRIPTION that is marked as having an
>> ev_origin of node 1 (the set origin) before the SUBSCRIBE_SET (having an
>> ev_origin of node 3, the provider)
>>
> 
> 


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From bugzilla-daemon at main.slony.info  Wed May 19 10:03:35 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Wed, 19 May 2010 10:03:35 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 125] New init script
In-Reply-To: <bug-125-4@http.www.slony.info/bugzilla/>
References: <bug-125-4@http.www.slony.info/bugzilla/>
Message-ID: <20100519170335.734FE2903A2@main.slony.info>

http://www.slony.info/bugzilla/show_bug.cgi?id=125

--- Comment #6 from Jose Arthur Benetasso Villanova <jose.arthur at gmail.com> 2010-05-19 10:03:34 PDT ---
Hi Devrim.

The output is:
---
slon_kill.pl...   Killing all slon and slon_watchdog instances for the cluster
replication
1.  Kill slon watchdogs
No watchdogs found

2. Kill slon processes
No slon processes found
---

Inside the script:
line 44: egrep found nothing, since I don't have any process called
slon_watchdog
line 56: egrep found nothing, since I don't have any process that match '[s]lon
.*$CLUSTER_NAME'

I've tested using the cvs HEAD and 2.0.STABLE

Again, both processes have the same name:

[root at Centosx64 ~]# ps -fu postgres
UID        PID  PPID  C STIME TTY          TIME CMD
postgres 19747     1  0 14:01 ?        00:00:00 /usr/bin/slon -f /etc/slon.conf
postgres 19749 19747  0 14:01 ?        00:00:00 /usr/bin/slon -f /etc/slon.conf

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are the assignee for the bug.

From cscetbon.ext at orange-ftgroup.com  Thu May 20 06:03:08 2010
From: cscetbon.ext at orange-ftgroup.com (Cyril Scetbon)
Date: Thu, 20 May 2010 15:03:08 +0200
Subject: [Slony1-bugs] Events confirmed not removed
Message-ID: <4BF5330C.3010902@orange-ftgroup.com>

Hi,

We noticed that when we restart slony there's an event created on the 
local node (receiver) which is confirmed by others :

select * from _OURCLUSTER.sl_event where 
ev_origin=102;                                
 ev_origin | ev_seqno |        ev_timestamp        |     
ev_snapshot      | ev_type | ev_data1 | ev_data2 | ev_data3 | ev_data4 | 
ev_data5 | ev_data6 | ev_data7 | ev_data8
-----------+----------+----------------------------+----------------------+---------+----------+----------+----------+----------+----------+----------+----------+----------
       102 |       51 | 2010-05-20 12:27:00.099562 | 
338318875:338318875: | SYNC    |          |          |          
|          |          |          |          |
(1 row)

select * from _pns_slony_voila_archi_0.sl_confirm where con_origin=102;
 con_origin | con_received | con_seqno |       con_timestamp       
------------+--------------+-----------+----------------------------
        102 |          101 |        51 | 2010-05-20 12:27:02.78581
        102 |          103 |        51 | 2010-05-20 12:27:00.118815
        102 |          104 |        51 | 2010-05-20 12:27:00.253975
(3 rows)

However, this event row and the confirms rows are not deleted for a long 
time (>>30 mn). That's why test_slony_state.pl warns us about it. I know 
that we can modify the variables $WANTCONFIRM and $WANTAGE in this 
script, but I noticed that you use these SQL request in cleanupEvent 
function :

        delete from @NAMESPACE at .sl_confirm
                where con_origin = v_max_row.con_origin
                and con_received = v_max_row.con_received
                and con_seqno < v_max_row.con_seqno;

            delete from @NAMESPACE at .sl_event
                    where ev_origin = v_min_row.con_origin
                    and ev_seqno < v_max_sync;

The reason why our rows are not deleted is that the sequence value is 
the maximum value. I think you should use the operator <= instead of < 
to correct this behaviour.

What's your opinion ?
-- 
Cyril SCETBON

From cscetbon.ext at orange-ftgroup.com  Thu May 20 06:35:23 2010
From: cscetbon.ext at orange-ftgroup.com (Cyril Scetbon)
Date: Thu, 20 May 2010 15:35:23 +0200
Subject: [Slony1-bugs] [Slony1-general] An old event not confirmed: A
	possible bug?
In-Reply-To: <4BEAC184.1000702@Yahoo.com>
References: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com>
	<4BEAC184.1000702@Yahoo.com>
Message-ID: <4BF53A9B.7020103@orange-ftgroup.com>



Jan Wieck a ?crit :
> On 5/12/2010 10:31 AM, Gurjeet Singh wrote:
>   
>> Hi All,
>>
>>     I have two Slony test beds which show the exact same symptoms!
>>
>> select * from sl_event order by ev_seqno;
>>
>>  ev_origin |  ev_seqno  |        ev_timestamp        |        
>> ev_snapshot         | ev_type |
>> -----------+------------+----------------------------+----------------------------+---------+-
>>          2 | 5000000002 | 2010-04-30 08:32:38.622928 | 
>> 458:458:                   | SYNC    |
>>          1 | 5000525721 | 2010-05-12 13:30:22.79626  | 
>> 72685915:72685915:         | SYNC    |
>>          1 | 5000525722 | 2010-05-12 13:30:24.800943 | 
>> 72686139:72686139:         | SYNC    |
>>          1 | 5000525723 | 2010-05-12 13:30:26.804862 | 
>> 72686224:72686224:         | SYNC    |
>> ...
>>
>>     
>
> Slony always keeps at least the last event per origin around. Otherwise 
> the view sl_status would not work.
>   
Hi Jan, Can you talk more about it ? I've posted a mail today to 
slony1-bugs cause test_slony_state.pl is warning us about old events 
(that's exactly the eldest ones). That's a matter for events generated 
from the local node. I see events from the local node only when I 
restart it :

select * from _OURCLUSTER.sl_event where 
ev_origin=102;                                  
 ev_origin | ev_seqno |        ev_timestamp        |     
ev_snapshot      | ev_type | ev_data1 | ev_data2 | ev_data3 | ev_data4 | 
ev_data5 | ev_data6 | ev_data7 | ev_data8
-----------+----------+----------------------------+----------------------+---------+----------+----------+----------+----------+----------+----------+----------+----------
       102 |       51 | 2010-05-20 12:27:00.099562 | 
338318875:338318875: | SYNC    |          |          |          
|          |          |          |          |
(1 row)

select * from _OURCLUSTER.sl_confirm where con_origin=102;
 con_origin | con_received | con_seqno |       con_timestamp       
------------+--------------+-----------+----------------------------
        102 |          101 |        51 | 2010-05-20 12:27:02.78581
        102 |          103 |        51 | 2010-05-20 12:27:00.118815
        102 |          104 |        51 | 2010-05-20 12:27:00.253975

the SYNC appears in slony logs as "new sl_action_seq 1 - SYNC %d"

> What should worry you is that there are no newer SYNC events from node 2 
> available. Slony does create a sporadic SYNC every now and then even if 
> there is no activity or the node isn't an origin anyway.
>
> Is it possible that node 2's clock is way off?
>
>
> Jan
>
>   
>> The reason I think this _might_ be a bug is that on both clusters, slave 
>> node's sl_event has the exact same record for ev_seqno=5000000002 except 
>> for the timestamp; same origin, and same snapshot!
>>
>> The head of sl_confirm has:
>>
>>  select * from sl_confirm order by con_seqno;
>>
>>  con_origin | con_received | con_seqno  |       con_timestamp
>> ------------+--------------+------------+----------------------------
>>           2 |            1 | 5000000002 | 2010-04-30 08:32:53.974021
>>           1 |            2 | 5000527075 | 2010-05-12 14:15:41.192279
>>           1 |            2 | 5000527076 | 2010-05-12 14:15:43.193607
>>           1 |            2 | 5000527077 | 2010-05-12 14:15:45.196291
>>           1 |            2 | 5000527078 | 2010-05-12 14:15:47.197005
>> ...
>>
>> Can someone comment on the health of the cluster? All events, except for 
>> that on, are being confirmed and purged from the system regularly, so my 
>> assumption is that the cluster is healthy and that the slave is in sync 
>> with the master.
>>
>> Thanks in advance.
>> -- 
>> gurjeet.singh
>> @ EnterpriseDB - The Enterprise Postgres Company
>> http://www.enterprisedb.com
>>
>> singh.gurjeet@{ gmail | yahoo }.com
>> Twitter/Skype: singh_gurjeet
>>
>> Mail sent from my BlackLaptop device
>>
>>
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> Slony1-general mailing list
>> Slony1-general at lists.slony.info
>> http://lists.slony.info/mailman/listinfo/slony1-general
>>     
>
>
>   

-- 
Cyril SCETBON

From JanWieck at Yahoo.com  Wed May 19 19:58:41 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Wed, 19 May 2010 22:58:41 -0400
Subject: [Slony1-bugs] [Slony1-general] An old event not confirmed: A
 possible bug?
In-Reply-To: <4BF53A9B.7020103@orange-ftgroup.com>
References: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com>	<4BEAC184.1000702@Yahoo.com>
	<4BF53A9B.7020103@orange-ftgroup.com>
Message-ID: <4BF4A561.2050909@Yahoo.com>

On 5/20/2010 9:35 AM, Cyril Scetbon wrote:
> 
> Jan Wieck a ?crit :
>> On 5/12/2010 10:31 AM, Gurjeet Singh wrote:
>>   
>>> Hi All,
>>>
>>>     I have two Slony test beds which show the exact same symptoms!
>>>
>>> select * from sl_event order by ev_seqno;
>>>
>>>  ev_origin |  ev_seqno  |        ev_timestamp        |        
>>> ev_snapshot         | ev_type |
>>> -----------+------------+----------------------------+----------------------------+---------+-
>>>          2 | 5000000002 | 2010-04-30 08:32:38.622928 | 
>>> 458:458:                   | SYNC    |
>>>          1 | 5000525721 | 2010-05-12 13:30:22.79626  | 
>>> 72685915:72685915:         | SYNC    |
>>>          1 | 5000525722 | 2010-05-12 13:30:24.800943 | 
>>> 72686139:72686139:         | SYNC    |
>>>          1 | 5000525723 | 2010-05-12 13:30:26.804862 | 
>>> 72686224:72686224:         | SYNC    |
>>> ...
>>>
>>>     
>>
>> Slony always keeps at least the last event per origin around. Otherwise 
>> the view sl_status would not work.
>>   
> Hi Jan, Can you talk more about it ? I've posted a mail today to 
> slony1-bugs cause test_slony_state.pl is warning us about old events 
> (that's exactly the eldest ones). That's a matter for events generated 
> from the local node. I see events from the local node only when I 
> restart it :

I presume that you have set sync_interval_timeout to zero on the 
subscribers, which will prevent the generation of SYNC events on those 
nodes because no actual replication work is ever generated there. Looks 
like test_slony_state.pl depends on that parameter no be non-zero 
(default is -t 10000, meaning every 10 seconds).


Jan

> 
> select * from _OURCLUSTER.sl_event where 
> ev_origin=102;                                  
>  ev_origin | ev_seqno |        ev_timestamp        |     
> ev_snapshot      | ev_type | ev_data1 | ev_data2 | ev_data3 | ev_data4 | 
> ev_data5 | ev_data6 | ev_data7 | ev_data8
> -----------+----------+----------------------------+----------------------+---------+----------+----------+----------+----------+----------+----------+----------+----------
>        102 |       51 | 2010-05-20 12:27:00.099562 | 
> 338318875:338318875: | SYNC    |          |          |          
> |          |          |          |          |
> (1 row)
> 
> select * from _OURCLUSTER.sl_confirm where con_origin=102;
>  con_origin | con_received | con_seqno |       con_timestamp       
> ------------+--------------+-----------+----------------------------
>         102 |          101 |        51 | 2010-05-20 12:27:02.78581
>         102 |          103 |        51 | 2010-05-20 12:27:00.118815
>         102 |          104 |        51 | 2010-05-20 12:27:00.253975
> 
> the SYNC appears in slony logs as "new sl_action_seq 1 - SYNC %d"
> 
>> What should worry you is that there are no newer SYNC events from node 2 
>> available. Slony does create a sporadic SYNC every now and then even if 
>> there is no activity or the node isn't an origin anyway.
>>
>> Is it possible that node 2's clock is way off?
>>
>>
>> Jan
>>
>>   
>>> The reason I think this _might_ be a bug is that on both clusters, slave 
>>> node's sl_event has the exact same record for ev_seqno=5000000002 except 
>>> for the timestamp; same origin, and same snapshot!
>>>
>>> The head of sl_confirm has:
>>>
>>>  select * from sl_confirm order by con_seqno;
>>>
>>>  con_origin | con_received | con_seqno  |       con_timestamp
>>> ------------+--------------+------------+----------------------------
>>>           2 |            1 | 5000000002 | 2010-04-30 08:32:53.974021
>>>           1 |            2 | 5000527075 | 2010-05-12 14:15:41.192279
>>>           1 |            2 | 5000527076 | 2010-05-12 14:15:43.193607
>>>           1 |            2 | 5000527077 | 2010-05-12 14:15:45.196291
>>>           1 |            2 | 5000527078 | 2010-05-12 14:15:47.197005
>>> ...
>>>
>>> Can someone comment on the health of the cluster? All events, except for 
>>> that on, are being confirmed and purged from the system regularly, so my 
>>> assumption is that the cluster is healthy and that the slave is in sync 
>>> with the master.
>>>
>>> Thanks in advance.
>>> -- 
>>> gurjeet.singh
>>> @ EnterpriseDB - The Enterprise Postgres Company
>>> http://www.enterprisedb.com
>>>
>>> singh.gurjeet@{ gmail | yahoo }.com
>>> Twitter/Skype: singh_gurjeet
>>>
>>> Mail sent from my BlackLaptop device
>>>
>>>
>>> ------------------------------------------------------------------------
>>>
>>> _______________________________________________
>>> Slony1-general mailing list
>>> Slony1-general at lists.slony.info
>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>     
>>
>>
>>   
> 


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From cscetbon.ext at orange-ftgroup.com  Thu May 20 07:48:26 2010
From: cscetbon.ext at orange-ftgroup.com (Cyril Scetbon)
Date: Thu, 20 May 2010 16:48:26 +0200
Subject: [Slony1-bugs] [Slony1-general] An old event not confirmed: A
 possible bug?
In-Reply-To: <4BF4A561.2050909@Yahoo.com>
References: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com>	<4BEAC184.1000702@Yahoo.com>	<4BF53A9B.7020103@orange-ftgroup.com>
	<4BF4A561.2050909@Yahoo.com>
Message-ID: <4BF54BBA.2050101@orange-ftgroup.com>



Jan Wieck a ?crit :
> On 5/20/2010 9:35 AM, Cyril Scetbon wrote:
>   
>> Jan Wieck a ?crit :
>>     
>>> On 5/12/2010 10:31 AM, Gurjeet Singh wrote:
>>>   
>>>       
>>>> Hi All,
>>>>
>>>>     I have two Slony test beds which show the exact same symptoms!
>>>>
>>>> select * from sl_event order by ev_seqno;
>>>>
>>>>  ev_origin |  ev_seqno  |        ev_timestamp        |        
>>>> ev_snapshot         | ev_type |
>>>> -----------+------------+----------------------------+----------------------------+---------+-
>>>>          2 | 5000000002 | 2010-04-30 08:32:38.622928 | 
>>>> 458:458:                   | SYNC    |
>>>>          1 | 5000525721 | 2010-05-12 13:30:22.79626  | 
>>>> 72685915:72685915:         | SYNC    |
>>>>          1 | 5000525722 | 2010-05-12 13:30:24.800943 | 
>>>> 72686139:72686139:         | SYNC    |
>>>>          1 | 5000525723 | 2010-05-12 13:30:26.804862 | 
>>>> 72686224:72686224:         | SYNC    |
>>>> ...
>>>>
>>>>     
>>>>         
>>> Slony always keeps at least the last event per origin around. Otherwise 
>>> the view sl_status would not work.
>>>   
>>>       
>> Hi Jan, Can you talk more about it ? I've posted a mail today to 
>> slony1-bugs cause test_slony_state.pl is warning us about old events 
>> (that's exactly the eldest ones). That's a matter for events generated 
>> from the local node. I see events from the local node only when I 
>> restart it :
>>     
>
> I presume that you have set sync_interval_timeout to zero on the 
> subscribers, which will prevent the generation of SYNC events on those 
> nodes because no actual replication work is ever generated there. Looks 
> like test_slony_state.pl depends on that parameter no be non-zero 
> (default is -t 10000, meaning every 10 seconds).
>   
No as you can see :

2010-05-20 15:31:55 CEST CONFIG slon: watchdog ready - pid = 23457
2010-05-20 15:31:55 CEST CONFIG main: Integer option vac_frequency = 3
2010-05-20 15:31:55 CEST CONFIG main: Integer option log_level = 2
2010-05-20 15:31:55 CEST CONFIG main: Integer option sync_interval = 500
2010-05-20 15:31:55 CEST CONFIG main: Integer option 
sync_interval_timeout = 10000
2010-05-20 15:31:55 CEST CONFIG main: Integer option sync_group_maxsize = 20
2010-05-20 15:31:55 CEST CONFIG main: Integer option desired_sync_time = 
60000
2010-05-20 15:31:55 CEST CONFIG main: Integer option syslog = 0
2010-05-20 15:31:55 CEST CONFIG main: Integer option quit_sync_provider = 0
2010-05-20 15:31:55 CEST CONFIG main: Integer option quit_sync_finalsync = 0
2010-05-20 15:31:55 CEST CONFIG main: Integer option sync_max_rowsize = 8192
2010-05-20 15:31:55 CEST CONFIG main: Integer option sync_max_largemem = 
5242880
2010-05-20 15:31:55 CEST CONFIG main: Integer option 
remote_listen_timeout = 300
2010-05-20 15:31:55 CEST CONFIG main: Boolean option log_pid = 0
2010-05-20 15:31:55 CEST CONFIG main: Boolean option log_timestamp = 1
2010-05-20 15:31:55 CEST CONFIG main: Boolean option cleanup_deletelogs = 0
2010-05-20 15:31:55 CEST CONFIG main: Real option real_placeholder = 
0.000000

But this is a receiver and I saw in the code of  function 
generate_sync_event that it does not generate sync interval on a node 
which is not the origin of a set. That's why I presume there is no sync 
created except the one created at startup (mandatory) in syncThread_main :

/*
         * We don't initialize the last known action sequence to the 
actual value.
         * This causes that we create a SYNC event allways on startup, 
just in
         * case.
         */
        last_actseq_buf[0] = '\0';

        /*
         * Build the query that starts a transaction and retrieves the 
last value
         * from the action sequence.
         */
        dstring_init(&query1);
        slon_mkquery(&query1,
                                 "start transaction;"
                                 "set transaction isolation level 
serializable;"
                                 "select last_value from %s.sl_action_seq;",
                                 rtcfg_namespace);

        /*
         * Build the query that calls createEvent() for the SYNC
         */
        dstring_init(&query2);
        slon_mkquery(&query2,
                     "select %s.createEvent('_%s', 'SYNC', NULL)"
                     " from %s.sl_node where no_id = 
%s.getLocalNodeId('_%s') "
                     " and exists (select 1 from %s.sl_set where 
set_origin= no_id);",
                     rtcfg_namespace, rtcfg_cluster_name,
                     rtcfg_namespace, rtcfg_namespace, 
rtcfg_cluster_name, rtcfg_namespace);


>
> Jan
>
>   
>> select * from _OURCLUSTER.sl_event where 
>> ev_origin=102;                                  
>>  ev_origin | ev_seqno |        ev_timestamp        |     
>> ev_snapshot      | ev_type | ev_data1 | ev_data2 | ev_data3 | ev_data4 | 
>> ev_data5 | ev_data6 | ev_data7 | ev_data8
>> -----------+----------+----------------------------+----------------------+---------+----------+----------+----------+----------+----------+----------+----------+----------
>>        102 |       51 | 2010-05-20 12:27:00.099562 | 
>> 338318875:338318875: | SYNC    |          |          |          
>> |          |          |          |          |
>> (1 row)
>>
>> select * from _OURCLUSTER.sl_confirm where con_origin=102;
>>  con_origin | con_received | con_seqno |       con_timestamp       
>> ------------+--------------+-----------+----------------------------
>>         102 |          101 |        51 | 2010-05-20 12:27:02.78581
>>         102 |          103 |        51 | 2010-05-20 12:27:00.118815
>>         102 |          104 |        51 | 2010-05-20 12:27:00.253975
>>
>> the SYNC appears in slony logs as "new sl_action_seq 1 - SYNC %d"
>>
>>     
>>> What should worry you is that there are no newer SYNC events from node 2 
>>> available. Slony does create a sporadic SYNC every now and then even if 
>>> there is no activity or the node isn't an origin anyway.
>>>
>>> Is it possible that node 2's clock is way off?
>>>
>>>
>>> Jan
>>>
>>>   
>>>       
>>>> The reason I think this _might_ be a bug is that on both clusters, slave 
>>>> node's sl_event has the exact same record for ev_seqno=5000000002 except 
>>>> for the timestamp; same origin, and same snapshot!
>>>>
>>>> The head of sl_confirm has:
>>>>
>>>>  select * from sl_confirm order by con_seqno;
>>>>
>>>>  con_origin | con_received | con_seqno  |       con_timestamp
>>>> ------------+--------------+------------+----------------------------
>>>>           2 |            1 | 5000000002 | 2010-04-30 08:32:53.974021
>>>>           1 |            2 | 5000527075 | 2010-05-12 14:15:41.192279
>>>>           1 |            2 | 5000527076 | 2010-05-12 14:15:43.193607
>>>>           1 |            2 | 5000527077 | 2010-05-12 14:15:45.196291
>>>>           1 |            2 | 5000527078 | 2010-05-12 14:15:47.197005
>>>> ...
>>>>
>>>> Can someone comment on the health of the cluster? All events, except for 
>>>> that on, are being confirmed and purged from the system regularly, so my 
>>>> assumption is that the cluster is healthy and that the slave is in sync 
>>>> with the master.
>>>>
>>>> Thanks in advance.
>>>> -- 
>>>> gurjeet.singh
>>>> @ EnterpriseDB - The Enterprise Postgres Company
>>>> http://www.enterprisedb.com
>>>>
>>>> singh.gurjeet@{ gmail | yahoo }.com
>>>> Twitter/Skype: singh_gurjeet
>>>>
>>>> Mail sent from my BlackLaptop device
>>>>
>>>>
>>>> ------------------------------------------------------------------------
>>>>
>>>> _______________________________________________
>>>> Slony1-general mailing list
>>>> Slony1-general at lists.slony.info
>>>> http://lists.slony.info/mailman/listinfo/slony1-general
>>>>     
>>>>         
>>>   
>>>       
>
>
>   

-- 
Cyril SCETBON

From JanWieck at Yahoo.com  Wed May 19 22:00:19 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu, 20 May 2010 01:00:19 -0400
Subject: [Slony1-bugs] [Slony1-general] An old event not confirmed: A
 possible bug?
In-Reply-To: <4BF54BBA.2050101@orange-ftgroup.com>
References: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com>	<4BEAC184.1000702@Yahoo.com>	<4BF53A9B.7020103@orange-ftgroup.com>
	<4BF4A561.2050909@Yahoo.com> <4BF54BBA.2050101@orange-ftgroup.com>
Message-ID: <4BF4C1E3.1050206@Yahoo.com>

On 5/20/2010 10:48 AM, Cyril Scetbon wrote:
> But this is a receiver and I saw in the code of  function 
> generate_sync_event that it does not generate sync interval on a node 
> which is not the origin of a set. That's why I presume there is no sync 
> created except the one created at startup (mandatory) in syncThread_main :

 From the CVS log:

> ----------------------------
> revision 1.19
> date: 2007-03-14 15:59:32 +0000;  author: cbbrowne;  state: Exp;  lines: +20 -6;
> Reduce the quantity of spurious events generated:
> 
> 1.  generate_sync_event() only needs to generate a SYNC on a node
>     that is the origin for a set
> 
> 2.  sync thread generates a SYNC when it starts; in later iterations,
>     it will only generate a SYNC for its node if that node is the origin
>     for a replication set
> 
> Per discussions with Jan Wieck on 2007-02-09; this seemed an experiment
> worth trying.  I tried it, and the tests run fine, so I'm committing this.
> ----------------------------

Seems we finally found a reason why this isn't such a good idea after 
all. Question now is do we want to revert back to the default, where 
slon's of pure receivers create useless SYNC events or not?


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From cscetbon.ext at orange-ftgroup.com  Fri May 21 01:46:52 2010
From: cscetbon.ext at orange-ftgroup.com (Cyril Scetbon)
Date: Fri, 21 May 2010 10:46:52 +0200
Subject: [Slony1-bugs] [Slony1-general] An old event not confirmed: A
 possible bug?
In-Reply-To: <4BF4C1E3.1050206@Yahoo.com>
References: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com>	<4BEAC184.1000702@Yahoo.com>	<4BF53A9B.7020103@orange-ftgroup.com>	<4BF4A561.2050909@Yahoo.com>
	<4BF54BBA.2050101@orange-ftgroup.com> <4BF4C1E3.1050206@Yahoo.com>
Message-ID: <4BF6487C.3040107@orange-ftgroup.com>

the fastest fix is to modify test_slony_state.pl to not take into 
account events or confirms done for the initial SYNC on a receiver node.

Jan Wieck a ?crit :
> On 5/20/2010 10:48 AM, Cyril Scetbon wrote:
>   
>> But this is a receiver and I saw in the code of  function 
>> generate_sync_event that it does not generate sync interval on a node 
>> which is not the origin of a set. That's why I presume there is no sync 
>> created except the one created at startup (mandatory) in syncThread_main :
>>     
>
>  From the CVS log:
>
>   
>> ----------------------------
>> revision 1.19
>> date: 2007-03-14 15:59:32 +0000;  author: cbbrowne;  state: Exp;  lines: +20 -6;
>> Reduce the quantity of spurious events generated:
>>
>> 1.  generate_sync_event() only needs to generate a SYNC on a node
>>     that is the origin for a set
>>
>> 2.  sync thread generates a SYNC when it starts; in later iterations,
>>     it will only generate a SYNC for its node if that node is the origin
>>     for a replication set
>>
>> Per discussions with Jan Wieck on 2007-02-09; this seemed an experiment
>> worth trying.  I tried it, and the tests run fine, so I'm committing this.
>> ----------------------------
>>     
>
> Seems we finally found a reason why this isn't such a good idea after 
> all. Question now is do we want to revert back to the default, where 
> slon's of pure receivers create useless SYNC events or not?
>
>
> Jan
>
>   

-- 
Cyril SCETBON

From JanWieck at Yahoo.com  Thu May 20 11:24:49 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu, 20 May 2010 14:24:49 -0400
Subject: [Slony1-bugs] [Slony1-general] An old event not confirmed: A
 possible bug?
In-Reply-To: <4BF6487C.3040107@orange-ftgroup.com>
References: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com>	<4BEAC184.1000702@Yahoo.com>	<4BF53A9B.7020103@orange-ftgroup.com>	<4BF4A561.2050909@Yahoo.com>	<4BF54BBA.2050101@orange-ftgroup.com>
	<4BF4C1E3.1050206@Yahoo.com> <4BF6487C.3040107@orange-ftgroup.com>
Message-ID: <4BF57E71.50708@Yahoo.com>

On 5/21/2010 4:46 AM, Cyril Scetbon wrote:
> the fastest fix is to modify test_slony_state.pl to not take into 
> account events or confirms done for the initial SYNC on a receiver node.

Wouldn't that mask problems where confirmations don't flow properly back 
to non-origin nodes?

As long as they don't produce any events, that's not much of a problem. 
But it is better to discover such before attempting to use that node as 
a failover target or the like.


Jan


> 
> Jan Wieck a ?crit :
>> On 5/20/2010 10:48 AM, Cyril Scetbon wrote:
>>   
>>> But this is a receiver and I saw in the code of  function 
>>> generate_sync_event that it does not generate sync interval on a node 
>>> which is not the origin of a set. That's why I presume there is no sync 
>>> created except the one created at startup (mandatory) in syncThread_main :
>>>     
>>
>>  From the CVS log:
>>
>>   
>>> ----------------------------
>>> revision 1.19
>>> date: 2007-03-14 15:59:32 +0000;  author: cbbrowne;  state: Exp;  lines: +20 -6;
>>> Reduce the quantity of spurious events generated:
>>>
>>> 1.  generate_sync_event() only needs to generate a SYNC on a node
>>>     that is the origin for a set
>>>
>>> 2.  sync thread generates a SYNC when it starts; in later iterations,
>>>     it will only generate a SYNC for its node if that node is the origin
>>>     for a replication set
>>>
>>> Per discussions with Jan Wieck on 2007-02-09; this seemed an experiment
>>> worth trying.  I tried it, and the tests run fine, so I'm committing this.
>>> ----------------------------
>>>     
>>
>> Seems we finally found a reason why this isn't such a good idea after 
>> all. Question now is do we want to revert back to the default, where 
>> slon's of pure receivers create useless SYNC events or not?
>>
>>
>> Jan
>>
>>   
> 


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From cscetbon.ext at orange-ftgroup.com  Fri May 21 05:22:23 2010
From: cscetbon.ext at orange-ftgroup.com (Cyril Scetbon)
Date: Fri, 21 May 2010 14:22:23 +0200
Subject: [Slony1-bugs] [Slony1-general] An old event not confirmed: A
 possible bug?
In-Reply-To: <4BF57E71.50708@Yahoo.com>
References: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com>	<4BEAC184.1000702@Yahoo.com>	<4BF53A9B.7020103@orange-ftgroup.com>	<4BF4A561.2050909@Yahoo.com>	<4BF54BBA.2050101@orange-ftgroup.com>	<4BF4C1E3.1050206@Yahoo.com>
	<4BF6487C.3040107@orange-ftgroup.com> <4BF57E71.50708@Yahoo.com>
Message-ID: <4BF67AFF.5020006@orange-ftgroup.com>



Jan Wieck a ?crit :
> On 5/21/2010 4:46 AM, Cyril Scetbon wrote:
>   
>> the fastest fix is to modify test_slony_state.pl to not take into 
>> account events or confirms done for the initial SYNC on a receiver node.
>>     
>
> Wouldn't that mask problems where confirmations don't flow properly back 
> to non-origin nodes?
>   
if we don't take into account the event (SYNC) created on a non-origin 
node but confirmed by others what could it mask ? everything worked fine 
and as this the last event it won't be removed (and so live longer than 
the interval defined in test-slony-state)
> As long as they don't produce any events, that's not much of a problem. 
> But it is better to discover such before attempting to use that node as 
> a failover target or the like.
>   
in this case the event must have been confirmed to not be taken into 
account.
>
> Jan
>
>
>   
>> Jan Wieck a ?crit :
>>     
>>> On 5/20/2010 10:48 AM, Cyril Scetbon wrote:
>>>   
>>>       
>>>> But this is a receiver and I saw in the code of  function 
>>>> generate_sync_event that it does not generate sync interval on a node 
>>>> which is not the origin of a set. That's why I presume there is no sync 
>>>> created except the one created at startup (mandatory) in syncThread_main :
>>>>     
>>>>         
>>>  From the CVS log:
>>>
>>>   
>>>       
>>>> ----------------------------
>>>> revision 1.19
>>>> date: 2007-03-14 15:59:32 +0000;  author: cbbrowne;  state: Exp;  lines: +20 -6;
>>>> Reduce the quantity of spurious events generated:
>>>>
>>>> 1.  generate_sync_event() only needs to generate a SYNC on a node
>>>>     that is the origin for a set
>>>>
>>>> 2.  sync thread generates a SYNC when it starts; in later iterations,
>>>>     it will only generate a SYNC for its node if that node is the origin
>>>>     for a replication set
>>>>
>>>> Per discussions with Jan Wieck on 2007-02-09; this seemed an experiment
>>>> worth trying.  I tried it, and the tests run fine, so I'm committing this.
>>>> ----------------------------
>>>>     
>>>>         
>>> Seems we finally found a reason why this isn't such a good idea after 
>>> all. Question now is do we want to revert back to the default, where 
>>> slon's of pure receivers create useless SYNC events or not?
>>>
>>>
>>> Jan
>>>
>>>   
>>>       
>
>
>   

-- 
Cyril SCETBON

From JanWieck at Yahoo.com  Thu May 20 18:41:13 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Thu, 20 May 2010 21:41:13 -0400
Subject: [Slony1-bugs] [Slony1-general] An old event not confirmed: A
 possible bug?
In-Reply-To: <4BF67AFF.5020006@orange-ftgroup.com>
References: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com>	<4BEAC184.1000702@Yahoo.com>	<4BF53A9B.7020103@orange-ftgroup.com>	<4BF4A561.2050909@Yahoo.com>	<4BF54BBA.2050101@orange-ftgroup.com>	<4BF4C1E3.1050206@Yahoo.com>
	<4BF6487C.3040107@orange-ftgroup.com> <4BF57E71.50708@Yahoo.com>
	<4BF67AFF.5020006@orange-ftgroup.com>
Message-ID: <4BF5E4B9.9020308@Yahoo.com>

On 5/21/2010 8:22 AM, Cyril Scetbon wrote:
> 
> Jan Wieck a ?crit :
>> On 5/21/2010 4:46 AM, Cyril Scetbon wrote:
>>   
>>> the fastest fix is to modify test_slony_state.pl to not take into 
>>> account events or confirms done for the initial SYNC on a receiver node.
>>>     
>>
>> Wouldn't that mask problems where confirmations don't flow properly back 
>> to non-origin nodes?
>>   
> if we don't take into account the event (SYNC) created on a non-origin 
> node but confirmed by others what could it mask ? everything worked fine 
> and as this the last event it won't be removed (and so live longer than 
> the interval defined in test-slony-state)
>> As long as they don't produce any events, that's not much of a problem. 
>> But it is better to discover such before attempting to use that node as 
>> a failover target or the like.
>>   
> in this case the event must have been confirmed to not be taken into 
> account.

I don't care much about that one old event. It does no harm other than 
currently confusing test_slony_state. What I worry about is attempting 
to failover in the case of emergency with an only half functioning path 
network.


Jan


>>
>> Jan
>>
>>
>>   
>>> Jan Wieck a ?crit :
>>>     
>>>> On 5/20/2010 10:48 AM, Cyril Scetbon wrote:
>>>>   
>>>>       
>>>>> But this is a receiver and I saw in the code of  function 
>>>>> generate_sync_event that it does not generate sync interval on a node 
>>>>> which is not the origin of a set. That's why I presume there is no sync 
>>>>> created except the one created at startup (mandatory) in syncThread_main :
>>>>>     
>>>>>         
>>>>  From the CVS log:
>>>>
>>>>   
>>>>       
>>>>> ----------------------------
>>>>> revision 1.19
>>>>> date: 2007-03-14 15:59:32 +0000;  author: cbbrowne;  state: Exp;  lines: +20 -6;
>>>>> Reduce the quantity of spurious events generated:
>>>>>
>>>>> 1.  generate_sync_event() only needs to generate a SYNC on a node
>>>>>     that is the origin for a set
>>>>>
>>>>> 2.  sync thread generates a SYNC when it starts; in later iterations,
>>>>>     it will only generate a SYNC for its node if that node is the origin
>>>>>     for a replication set
>>>>>
>>>>> Per discussions with Jan Wieck on 2007-02-09; this seemed an experiment
>>>>> worth trying.  I tried it, and the tests run fine, so I'm committing this.
>>>>> ----------------------------
>>>>>     
>>>>>         
>>>> Seems we finally found a reason why this isn't such a good idea after 
>>>> all. Question now is do we want to revert back to the default, where 
>>>> slon's of pure receivers create useless SYNC events or not?
>>>>
>>>>
>>>> Jan
>>>>
>>>>   
>>>>       
>>
>>
>>   
> 


-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From cscetbon.ext at orange-ftgroup.com  Fri May 21 07:42:29 2010
From: cscetbon.ext at orange-ftgroup.com (Cyril Scetbon)
Date: Fri, 21 May 2010 16:42:29 +0200
Subject: [Slony1-bugs] [Slony1-general] An old event not confirmed: A
 possible bug?
In-Reply-To: <4BF5E4B9.9020308@Yahoo.com>
References: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com>	<4BEAC184.1000702@Yahoo.com>	<4BF53A9B.7020103@orange-ftgroup.com>	<4BF4A561.2050909@Yahoo.com>	<4BF54BBA.2050101@orange-ftgroup.com>	<4BF4C1E3.1050206@Yahoo.com>	<4BF6487C.3040107@orange-ftgroup.com>
	<4BF57E71.50708@Yahoo.com>	<4BF67AFF.5020006@orange-ftgroup.com>
	<4BF5E4B9.9020308@Yahoo.com>
Message-ID: <4BF69BD5.5010204@orange-ftgroup.com>



Jan Wieck a ?crit :
> On 5/21/2010 8:22 AM, Cyril Scetbon wrote:
>   
>> Jan Wieck a ?crit :
>>     
>>> On 5/21/2010 4:46 AM, Cyril Scetbon wrote:
>>>   
>>>       
>>>> the fastest fix is to modify test_slony_state.pl to not take into 
>>>> account events or confirms done for the initial SYNC on a receiver node.
>>>>     
>>>>         
>>> Wouldn't that mask problems where confirmations don't flow properly back 
>>> to non-origin nodes?
>>>   
>>>       
>> if we don't take into account the event (SYNC) created on a non-origin 
>> node but confirmed by others what could it mask ? everything worked fine 
>> and as this the last event it won't be removed (and so live longer than 
>> the interval defined in test-slony-state)
>>     
>>> As long as they don't produce any events, that's not much of a problem. 
>>> But it is better to discover such before attempting to use that node as 
>>> a failover target or the like.
>>>   
>>>       
>> in this case the event must have been confirmed to not be taken into 
>> account.
>>     
>
> I don't care much about that one old event. It does no harm other than 
> currently confusing test_slony_state. What I worry about is attempting 
> to failover in the case of emergency with an only half functioning path 
> network.
>   
I don't really understand the issue you're talking about... Certainly 
I've a weak knowledge of your code :)
You're talking about missing errors in network cause there are no SYNC 
generated on a receiver ? If yes, if it confirms events from others it's 
not enough to say that everything works ?
>
> Jan
>
>
>   
>>> Jan
>>>
>>>
>>>   
>>>       
>>>> Jan Wieck a ?crit :
>>>>     
>>>>         
>>>>> On 5/20/2010 10:48 AM, Cyril Scetbon wrote:
>>>>>   
>>>>>       
>>>>>           
>>>>>> But this is a receiver and I saw in the code of  function 
>>>>>> generate_sync_event that it does not generate sync interval on a node 
>>>>>> which is not the origin of a set. That's why I presume there is no sync 
>>>>>> created except the one created at startup (mandatory) in syncThread_main :
>>>>>>     
>>>>>>         
>>>>>>             
>>>>>  From the CVS log:
>>>>>
>>>>>   
>>>>>       
>>>>>           
>>>>>> ----------------------------
>>>>>> revision 1.19
>>>>>> date: 2007-03-14 15:59:32 +0000;  author: cbbrowne;  state: Exp;  lines: +20 -6;
>>>>>> Reduce the quantity of spurious events generated:
>>>>>>
>>>>>> 1.  generate_sync_event() only needs to generate a SYNC on a node
>>>>>>     that is the origin for a set
>>>>>>
>>>>>> 2.  sync thread generates a SYNC when it starts; in later iterations,
>>>>>>     it will only generate a SYNC for its node if that node is the origin
>>>>>>     for a replication set
>>>>>>
>>>>>> Per discussions with Jan Wieck on 2007-02-09; this seemed an experiment
>>>>>> worth trying.  I tried it, and the tests run fine, so I'm committing this.
>>>>>> ----------------------------
>>>>>>     
>>>>>>         
>>>>>>             
>>>>> Seems we finally found a reason why this isn't such a good idea after 
>>>>> all. Question now is do we want to revert back to the default, where 
>>>>> slon's of pure receivers create useless SYNC events or not?
>>>>>
>>>>>
>>>>> Jan
>>>>>
>>>>>   
>>>>>       
>>>>>           
>>>   
>>>       
>
>
>   

-- 
Cyril SCETBON

From JanWieck at Yahoo.com  Thu May 20 22:24:52 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Fri, 21 May 2010 01:24:52 -0400
Subject: [Slony1-bugs] [Slony1-general] An old event not confirmed: A
 possible bug?
In-Reply-To: <4BF69BD5.5010204@orange-ftgroup.com>
References: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com>	<4BEAC184.1000702@Yahoo.com>	<4BF53A9B.7020103@orange-ftgroup.com>	<4BF4A561.2050909@Yahoo.com>	<4BF54BBA.2050101@orange-ftgroup.com>	<4BF4C1E3.1050206@Yahoo.com>	<4BF6487C.3040107@orange-ftgroup.com>	<4BF57E71.50708@Yahoo.com>	<4BF67AFF.5020006@orange-ftgroup.com>	<4BF5E4B9.9020308@Yahoo.com>
	<4BF69BD5.5010204@orange-ftgroup.com>
Message-ID: <4BF61924.6010806@Yahoo.com>

On 5/21/2010 10:42 AM, Cyril Scetbon wrote:
> 
> Jan Wieck a ?crit :
>> I don't care much about that one old event. It does no harm other than 
>> currently confusing test_slony_state. What I worry about is attempting 
>> to failover in the case of emergency with an only half functioning path 
>> network.
>>   
> I don't really understand the issue you're talking about... Certainly 
> I've a weak knowledge of your code :)
> You're talking about missing errors in network cause there are no SYNC 
> generated on a receiver ? If yes, if it confirms events from others it's 
> not enough to say that everything works ?

Let me try to explain the problem.

In a multi node cluster, not every node necessarily needs to be able to 
talk to every other node. Let us just look at a cascaded 3 node cluster:

     1 - 2 - 3

This setup requires 4 sl_path entries to work:

     server=1, client=2
     server=2, client=1
     server=2, client=3
     server=3, client=2

And it is supposed to generate the following sl_listen rows:

     origin=1, receiver=2, provider=1
     origin=1, receiver=3, provider=2
     origin=2, receiver=1, provider=2
     origin=2, receiver=3, provider=2
     origin=3, receiver=1, provider=2
     origin=3, receiver=2, provider=3

It does not matter which node is currently the origin of any set at all. 
All these paths and connections are important for the health and well 
being of the Slony cluster. If for example the listening for events from 
2, receiver=3 would be broken, then node 3 would still perfectly fine 
replicate data originating from 1. But as soon as you move set to node 
2, it would start falling behind and you effectively lose your second 
level backup.

This is why Slony originally created a SYNC on EVERY node at least every 
10 seconds. Just so there is some harmless event passing going on to 
have something to monitor and keep sl_status looking good.

That is what got removed and that is what I think we should put back.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From bugzilla-daemon at main.slony.info  Fri May 21 11:11:16 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Fri, 21 May 2010 11:11:16 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 127] New: sync events are not generated on nodes
 that are not a set origin
Message-ID: <bug-127-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=127

           Summary: sync events are not generated on nodes that are not a
                    set origin
           Product: Slony-I
           Version: 1.2
          Platform: All
        OS/Version: All
            Status: NEW
          Severity: normal
          Priority: low
         Component: slon
        AssignedTo: slony1-bugs at lists.slony.info
        ReportedBy: ssinger at ca.afilias.info
                CC: slony1-bugs at lists.slony.info
   Estimated Hours: 0.0


See http://lists.slony.info/pipermail/slony1-bugs/2010-May/000495.html

Basically no sync events (other than when slon starts) are being generated on
nodes that are not origins for any set.  This makes the sl_status table seem
out of date.

This change was checked in on Mar 14 2007.

Current thinking is that this is actually a bug.

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.

From ssinger at ca.afilias.info  Fri May 21 11:13:24 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Fri, 21 May 2010 14:13:24 -0400
Subject: [Slony1-bugs] [Slony1-general] An old event not confirmed: A
 possible bug?
In-Reply-To: <4BF61924.6010806@Yahoo.com>
References: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com>	<4BEAC184.1000702@Yahoo.com>	<4BF53A9B.7020103@orange-ftgroup.com>	<4BF4A561.2050909@Yahoo.com>	<4BF54BBA.2050101@orange-ftgroup.com>	<4BF4C1E3.1050206@Yahoo.com>	<4BF6487C.3040107@orange-ftgroup.com>	<4BF57E71.50708@Yahoo.com>	<4BF67AFF.5020006@orange-ftgroup.com>	<4BF5E4B9.9020308@Yahoo.com>	<4BF69BD5.5010204@orange-ftgroup.com>
	<4BF61924.6010806@Yahoo.com>
Message-ID: <4BF6CD44.2070408@ca.afilias.info>

Jan Wieck wrote:
> On 5/21/2010 10:42 AM, Cyril Scetbon wrote:

> This is why Slony originally created a SYNC on EVERY node at least every 
> 10 seconds. Just so there is some harmless event passing going on to 
> have something to monitor and keep sl_status looking good.
> 
> That is what got removed and that is what I think we should put back.

I'm inclined to agree, I have opened a bug on this.
http://bugs.slony.info/bugzilla/show_bug.cgi?id=127

Though I'm not sure if we want to change this in 1.2.x or just look into 
changing this in 2.0


> 
> 
> Jan
> 


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From singh.gurjeet at gmail.com  Fri May 21 19:24:03 2010
From: singh.gurjeet at gmail.com (Gurjeet Singh)
Date: Fri, 21 May 2010 22:24:03 -0400
Subject: [Slony1-bugs] [Slony1-general] An old event not confirmed: A
	possible bug?
In-Reply-To: <4BF61924.6010806@Yahoo.com>
References: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com> 
	<4BF4A561.2050909@Yahoo.com> <4BF54BBA.2050101@orange-ftgroup.com> 
	<4BF4C1E3.1050206@Yahoo.com> <4BF6487C.3040107@orange-ftgroup.com> 
	<4BF57E71.50708@Yahoo.com> <4BF67AFF.5020006@orange-ftgroup.com> 
	<4BF5E4B9.9020308@Yahoo.com> <4BF69BD5.5010204@orange-ftgroup.com> 
	<4BF61924.6010806@Yahoo.com>
Message-ID: <AANLkTilEswv3JiTG4mhPpQoNhmMw8kZctY0csQp3gSoo@mail.gmail.com>

On Fri, May 21, 2010 at 1:24 AM, Jan Wieck <JanWieck at yahoo.com> wrote:

>
> In a multi node cluster, not every node necessarily needs to be able to
> talk to every other node. Let us just look at a cascaded 3 node cluster:
>
>     1 - 2 - 3
>
> This setup requires 4 sl_path entries to work:
>
>     server=1, client=2
>     server=2, client=1
>     server=2, client=3
>     server=3, client=2
>
> And it is supposed to generate the following sl_listen rows:
>
>     origin=1, receiver=2, provider=1
>     origin=1, receiver=3, provider=2
>     origin=2, receiver=1, provider=2
>     origin=2, receiver=3, provider=2
>     origin=3, receiver=1, provider=2
>     origin=3, receiver=2, provider=3
>
> It does not matter which node is currently the origin of any set at all.
> All these paths and connections are important for the health and well
> being of the Slony cluster. If for example the listening for events from
> 2, receiver=3 would be broken, then node 3 would still perfectly fine
> replicate data originating from 1. But as soon as you move set to node
> 2, it would start falling behind and you effectively lose your second
> level backup.
>
> This is why Slony originally created a SYNC on EVERY node at least every
> 10 seconds. Just so there is some harmless event passing going on to
> have something to monitor and keep sl_status looking good.
>
> That is what got removed and that is what I think we should put back.
>
>
This is exactly the kind of Slony black magic I want to understand. Do we
have someplace where we can get these internals of Slony, or design specs;
or would you suggest diving into code?

Regards,
-- 
gurjeet.singh
@ EnterpriseDB - The Enterprise Postgres Company
http://www.enterprisedb.com

singh.gurjeet@{ gmail | yahoo }.com
Twitter/Skype: singh_gurjeet

Mail sent from my BlackLaptop device
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-bugs/attachments/20100521/da6929e5/attachment.htm 

From JanWieck at Yahoo.com  Sun May 23 07:40:07 2010
From: JanWieck at Yahoo.com (Jan Wieck)
Date: Sun, 23 May 2010 10:40:07 -0400
Subject: [Slony1-bugs] [Slony1-general] Slony triggers included in
	pg_dump
In-Reply-To: <20100523135558.GB23842@fetter.org>
References: <AANLkTik3kuhrTf7xiy5ylZi-N4AprDvtBP831Il6oxhS@mail.gmail.com>	<4BF002D6.3030805@ca.afilias.info>	<AANLkTilq9QncnLfnHKwS3YEr3sELyzEHHTMFZrHqdN6b@mail.gmail.com>	<4BF191B9.5000607@ca.afilias.info>
	<20100523135558.GB23842@fetter.org>
Message-ID: <4BF93E47.7010902@Yahoo.com>

On 5/23/2010 9:55 AM, David Fetter wrote:
> On Mon, May 17, 2010 at 02:58:01PM -0400, Steve Singer wrote:
>> Gurjeet Singh wrote:
>> 
>> > The doc-patch does make it more clear, but I don't think DROP
>> > SCHEMA _slony_cluster_name CASCADE is something I'd like to do
>> > with my running Slony replication. A cleaner way would be more
>> > desirable.
>> 
>> Another option you have is to restore the dump that you create of
>> your pgbench schema to another database(a new one).  When you
>> restore the CREATE TRIGGER statements will fail because the slony
>> stored procedures won't exist in your new database.   This should
>> leave you with your database - any slony stuff.  (I agree having to
>> put up with 'errors' during the restore or make a second copy of the
>> database just to do 'DROP SCHEMA' are both less than ideal
>> solutions)
> 
> Would it help if some future version of pg_dump had some option to the
> effect of, "leave out the following objects and everything that
> depends on them?"

That would certainly help and it is a more general use case, not 
something specific to Slony. If you have this

> create schema s1;
> create schema s2;
> create table s1.t1 (id integer primary key, data text);
> create table s2.t2 (id integer primary key, id1 integer references s1.t1 (id));

and then do a pg_dump -n s2, your dump will still contain

> --
> -- Name: t2_id1_fkey; Type: FK CONSTRAINT; Schema: s2; Owner: -
> --
> 
> ALTER TABLE ONLY t2
>     ADD CONSTRAINT t2_id1_fkey FOREIGN KEY (id1) REFERENCES s1.t1(id);

Fortunately, the foreign key in the dump is separated out so that the 
CREATE TABLE still succeeds, but you get an error message here as well.


Jan

-- 
Anyone who trades liberty for security deserves neither
liberty nor security. -- Benjamin Franklin

From singh.gurjeet at gmail.com  Mon May 24 11:04:34 2010
From: singh.gurjeet at gmail.com (Gurjeet Singh)
Date: Mon, 24 May 2010 14:04:34 -0400
Subject: [Slony1-bugs] Slony triggers included in pg_dump
In-Reply-To: <4BF35335.7090502@Yahoo.com>
References: <AANLkTik3kuhrTf7xiy5ylZi-N4AprDvtBP831Il6oxhS@mail.gmail.com> 
	<4BF35335.7090502@Yahoo.com>
Message-ID: <AANLkTinKm6PwIpLppG5ZBW_pk-67Rwxhju-BdSjBBvsc@mail.gmail.com>

On Tue, May 18, 2010 at 10:55 PM, Jan Wieck <JanWieck at yahoo.com> wrote:

> On 5/16/2010 8:22 AM, Gurjeet Singh wrote:
>
>> Hi All,
>>
>>    The docs here http://www.slony.info/documentation/triggers.html says
>> that post Postgres 8.3 version
>>
>> <snip>
>>
>>    * If you take a pg_dump of a Slony-I node, and drop out the Slony-I
>>      namespace, this now cleanly removes /all/ Slony-I components,
>>      leaving the database, /including its schema,/ in a "pristine",
>>      consistent fashion, ready for whatever use may be desired.
>> </snip>
>>
>> But I see that when dumping a schema containing tables monitored by Slony,
>> the dump still shows the log triggers on those tables.
>>
>
> I think you misinterpret the statement.
>
> If you take a complete dump of all schemas, restore that and THEN drop the
> slony schema, you have a pristine stand alone system as the result.
>
> pg_dump does not exclude cross-schema definitions like those triggers. You
> will get the same for example with foreign keys. For a table in the schema
> you are dumping, that references a table in a non-dumped schema, pg_dump
> will still emit the ALTER TABLE attempting to create the constraint.
>
>
Makes sense now. But I guess the statement should have read like this:

<snip>

   * If you restore a backup of a Slony-I node (taken by pg_dump or any
other method), and drop the Slony-I
     namespace, this now cleanly removes /all/ Slony-I components,
     leaving the database, /including its schema,/ in a "pristine",
     consistent fashion, ready for whatever use may be desired.
</snip>


Regards,

-- 
gurjeet.singh
@ EnterpriseDB - The Enterprise Postgres Company
http://www.enterprisedb.com

singh.gurjeet@{ gmail | yahoo }.com
Twitter/Skype: singh_gurjeet

Mail sent from my BlackLaptop device
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-bugs/attachments/20100524/90492445/attachment.htm 

From cscetbon.ext at orange-ftgroup.com  Sat May 22 14:13:36 2010
From: cscetbon.ext at orange-ftgroup.com (Cyril Scetbon)
Date: Sat, 22 May 2010 23:13:36 +0200
Subject: [Slony1-bugs] [Slony1-general] An old event not confirmed: A
 possible bug?
In-Reply-To: <4BF61924.6010806@Yahoo.com>
References: <AANLkTikflK7G1bHIJjQLynJ-VZIwhAvAWazUNqyFQm5i@mail.gmail.com>	<4BEAC184.1000702@Yahoo.com>	<4BF53A9B.7020103@orange-ftgroup.com>	<4BF4A561.2050909@Yahoo.com>	<4BF54BBA.2050101@orange-ftgroup.com>	<4BF4C1E3.1050206@Yahoo.com>	<4BF6487C.3040107@orange-ftgroup.com>	<4BF57E71.50708@Yahoo.com>	<4BF67AFF.5020006@orange-ftgroup.com>	<4BF5E4B9.9020308@Yahoo.com>	<4BF69BD5.5010204@orange-ftgroup.com>
	<4BF61924.6010806@Yahoo.com>
Message-ID: <4BF84900.4070906@orange-ftgroup.com>



Jan Wieck a ?crit :
> On 5/21/2010 10:42 AM, Cyril Scetbon wrote:
>   
>> Jan Wieck a ?crit :
>>     
>>> I don't care much about that one old event. It does no harm other than 
>>> currently confusing test_slony_state. What I worry about is attempting 
>>> to failover in the case of emergency with an only half functioning path 
>>> network.
>>>   
>>>       
>> I don't really understand the issue you're talking about... Certainly 
>> I've a weak knowledge of your code :)
>> You're talking about missing errors in network cause there are no SYNC 
>> generated on a receiver ? If yes, if it confirms events from others it's 
>> not enough to say that everything works ?
>>     
>
> Let me try to explain the problem.
>
> In a multi node cluster, not every node necessarily needs to be able to 
> talk to every other node. Let us just look at a cascaded 3 node cluster:
>
>      1 - 2 - 3
>
> This setup requires 4 sl_path entries to work:
>
>      server=1, client=2
>      server=2, client=1
>      server=2, client=3
>      server=3, client=2
>
> And it is supposed to generate the following sl_listen rows:
>
>      origin=1, receiver=2, provider=1
>      origin=1, receiver=3, provider=2
>      origin=2, receiver=1, provider=2
>      origin=2, receiver=3, provider=2
>      origin=3, receiver=1, provider=2
>      origin=3, receiver=2, provider=3
>
> It does not matter which node is currently the origin of any set at all. 
> All these paths and connections are important for the health and well 
> being of the Slony cluster. If for example the listening for events from 
> 2, receiver=3 would be broken, then node 3 would still perfectly fine 
> replicate data originating from 1. But as soon as you move set to node 
> 2, it would start falling behind and you effectively lose your second 
> level backup.
>
> This is why Slony originally created a SYNC on EVERY node at least every 
> 10 seconds. Just so there is some harmless event passing going on to 
> have something to monitor and keep sl_status looking good.
>
> That is what got removed and that is what I think we should put back.
>   
thanks for the explanation ! I agree too.
>
> Jan
>
>   

-- 
Cyril SCETBON


From ssinger at ca.afilias.info  Tue May 25 06:30:02 2010
From: ssinger at ca.afilias.info (Steve Singer)
Date: Tue, 25 May 2010 09:30:02 -0400
Subject: [Slony1-bugs] [Slony1-general] Slony triggers included
	in	pg_dump
In-Reply-To: <AANLkTinKm6PwIpLppG5ZBW_pk-67Rwxhju-BdSjBBvsc@mail.gmail.com>
References: <AANLkTik3kuhrTf7xiy5ylZi-N4AprDvtBP831Il6oxhS@mail.gmail.com>
	<4BF35335.7090502@Yahoo.com>
	<AANLkTinKm6PwIpLppG5ZBW_pk-67Rwxhju-BdSjBBvsc@mail.gmail.com>
Message-ID: <4BFBD0DA.5040504@ca.afilias.info>

Gurjeet Singh wrote:

> Makes sense now. But I guess the statement should have read like this:
> 
> <snip>
> 
>    * If you restore a backup of a Slony-I node (taken by pg_dump or any 
> other method), and drop the Slony-I
>      namespace, this now cleanly removes /all/ Slony-I components,
>      leaving the database, /including its schema,/ in a "pristine",
>      consistent fashion, ready for whatever use may be desired.
> </snip>
> 
> 

Documentation patch applied.


> Regards,
> 
> -- 
> gurjeet.singh
> @ EnterpriseDB - The Enterprise Postgres Company
> http://www.enterprisedb.com
> 
> singh.gurjeet@{ gmail | yahoo }.com
> Twitter/Skype: singh_gurjeet
> 
> Mail sent from my BlackLaptop device
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Slony1-general mailing list
> Slony1-general at lists.slony.info
> http://lists.slony.info/mailman/listinfo/slony1-general


-- 
Steve Singer
Afilias Canada
Data Services Developer
416-673-1142

From bugzilla-daemon at main.slony.info  Tue May 25 11:15:47 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Tue, 25 May 2010 11:15:47 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 128] New: DROP TABLE replicatedTable leaves the
 cluster in a bad state
Message-ID: <bug-128-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=128

           Summary: DROP TABLE replicatedTable leaves the cluster in a bad
                    state
           Product: Slony-I
           Version: 2.0
          Platform: PC
        OS/Version: Linux
            Status: NEW
          Severity: enhancement
          Priority: low
         Component: stored procedures
        AssignedTo: slony1-bugs at lists.slony.info
        ReportedBy: ssinger at ca.afilias.info
                CC: slony1-bugs at lists.slony.info
   Estimated Hours: 0.0


Add a table to replication (say sometable)

Then do 

DROP TABLE sometable. (even via execute script)

Then try:
1) MOVE SET:  fails with: 
    <stdin>:13: PGRES_FATAL_ERROR select "_disorder_replica".moveSet(3, 3);  -
ERROR:  Slony-I: alterTableConfigureTriggers(): Table with id 8 not found


2) SET DROP TABLE(id=8,origin=1) gives
 <stdin>:12: PGRES_FATAL_ERROR select "_disorder_replica".setDropTable(8);  -
ERROR:  Slony-I: alterTableDropTriggers(): Table with id 8 not found

Forgetting to remove a table from replication before dropping it is a pretty
common mistake.  There needs to be a clean (and obvious) way to recover from
this short of manually changing sl_table.

I propose that the setDropTable be modified so that if the
alterTableDropTriggers() function fails on a table we are about to drop then we
proceed with dropping the table anyway.   Having moveSet() fail when the table
deosn't exist is still resonable.

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.

From singh.gurjeet at gmail.com  Tue May 25 13:18:14 2010
From: singh.gurjeet at gmail.com (Gurjeet Singh)
Date: Tue, 25 May 2010 16:18:14 -0400
Subject: [Slony1-bugs] [Slony1-general] Slony triggers included in
	pg_dump
In-Reply-To: <4BFBD0DA.5040504@ca.afilias.info>
References: <AANLkTik3kuhrTf7xiy5ylZi-N4AprDvtBP831Il6oxhS@mail.gmail.com> 
	<4BF35335.7090502@Yahoo.com>
	<AANLkTinKm6PwIpLppG5ZBW_pk-67Rwxhju-BdSjBBvsc@mail.gmail.com> 
	<4BFBD0DA.5040504@ca.afilias.info>
Message-ID: <AANLkTik9_Iu0yros8UHdsWecnzJORF6mhfkBf290Yr5m@mail.gmail.com>

On Tue, May 25, 2010 at 9:30 AM, Steve Singer <ssinger at ca.afilias.info>wrote:

> Gurjeet Singh wrote:
>
>  Makes sense now. But I guess the statement should have read like this:
>>
>> <snip>
>>
>>   * If you restore a backup of a Slony-I node (taken by pg_dump or any
>> other method), and drop the Slony-I
>>     namespace, this now cleanly removes /all/ Slony-I components,
>>     leaving the database, /including its schema,/ in a "pristine",
>>     consistent fashion, ready for whatever use may be desired.
>> </snip>
>>
>>
>>
> Documentation patch applied.
>

Thanks.

-- 
gurjeet.singh
@ EnterpriseDB - The Enterprise Postgres Company
http://www.enterprisedb.com

singh.gurjeet@{ gmail | yahoo }.com
Twitter/Skype: singh_gurjeet

Mail sent from my BlackLaptop device
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-bugs/attachments/20100525/53487b84/attachment.htm 

From bugzilla-daemon at main.slony.info  Tue May 25 14:29:10 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Tue, 25 May 2010 14:29:10 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 129] New: FAILOVER command does not update
	sl_subscriber
Message-ID: <bug-129-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=129

           Summary: FAILOVER command does not update sl_subscriber
           Product: Slony-I
           Version: 2.0
          Platform: PC
        OS/Version: Linux
            Status: NEW
          Severity: critical
          Priority: low
         Component: slonik
        AssignedTo: slony1-bugs at lists.slony.info
        ReportedBy: ssinger at ca.afilias.info
                CC: slony1-bugs at lists.slony.info
   Estimated Hours: 0.0


This is with 2.0.3

On a cluster where nodes 4 and 5 are cascded from 3.

1===>3=====>4
     \\
      5

The slonik script
--
FAILOVER(id=3,backup node=1);
echo 'the failover command has completed';
---

Slonik exists normally and seems to execute the script with no errors (it
prints - <stdin>:13: the failover command has completed )

but then


 select * FROM _disorder_replica.sl_subscribe ;
 sub_set | sub_provider | sub_receiver | sub_forward | sub_active 
---------+--------------+--------------+-------------+------------
       1 |            1 |            2 | t           | t
       1 |            1 |            3 | t           | t
       1 |            3 |            4 | t           | t
       1 |            3 |            5 | t           | t



shows node 3 still as the provider for nodes 4 and 5.

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.

From bugzilla-daemon at main.slony.info  Wed May 26 14:26:43 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Wed, 26 May 2010 14:26:43 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 130] New: failover does not seem to update sl_set
Message-ID: <bug-130-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=130

           Summary: failover does not seem to update sl_set
           Product: Slony-I
           Version: 2.0
          Platform: PC
        OS/Version: Linux
            Status: NEW
          Severity: enhancement
          Priority: low
         Component: stored procedures
        AssignedTo: slony1-bugs at lists.slony.info
        ReportedBy: ssinger at ca.afilias.info
                CC: slony1-bugs at lists.slony.info
   Estimated Hours: 0.0


Observed with 2.0.3

In a cluster as follows

1
\\
 \\
  3===4
  \\
   \\
     5


 A failover of set 1=>3 seems to work.

However DROP NODE (id=1) fails with 

<stdin>:12: PGRES_FATAL_ERROR select "_disorder_replica".dropNode(1);  - ERROR:
 Slony-I: Node 1 is still origin of one or more sets

When I query sl_set it shows that node 1 is still the origin of the set even
though the failover command seemed to work okay.

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.

From bugzilla-daemon at main.slony.info  Wed May 26 15:13:33 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Wed, 26 May 2010 15:13:33 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 125] New init script
In-Reply-To: <bug-125-4@http.www.slony.info/bugzilla/>
References: <bug-125-4@http.www.slony.info/bugzilla/>
Message-ID: <20100526221333.730D4290467@main.slony.info>

http://www.slony.info/bugzilla/show_bug.cgi?id=125

Jose Arthur Benetasso Villanova <jose.arthur at gmail.com> changed:

           What    |Removed                     |Added
----------------------------------------------------------------------------
  Attachment #41 is|0                           |1
           obsolete|                            |
  Attachment #42 is|0                           |1
           obsolete|                            |

--- Comment #7 from Jose Arthur Benetasso Villanova <jose.arthur at gmail.com> 2010-05-26 15:13:33 PDT ---
Created an attachment (id=43)
 --> (http://www.slony.info/bugzilla/attachment.cgi?id=43)
diff -Nur file using pidfile

Hi Devrim.

I rewrote the init to use pidfile. I've changed some lines from spec, to solve
permission issues.

Jose Arthur.

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are the assignee for the bug.

From bugzilla-daemon at main.slony.info  Mon May 31 13:29:33 2010
From: bugzilla-daemon at main.slony.info (bugzilla-daemon at main.slony.info)
Date: Mon, 31 May 2010 13:29:33 -0700 (PDT)
Subject: [Slony1-bugs] [Bug 131] New: two concurrent subscribe sets can
	deadlock each other
Message-ID: <bug-131-4@http.www.slony.info/bugzilla/>

http://www.slony.info/bugzilla/show_bug.cgi?id=131

           Summary: two concurrent subscribe sets can deadlock each other
           Product: Slony-I
           Version: 2.0
          Platform: All
        OS/Version: Linux
            Status: NEW
          Severity: major
          Priority: low
         Component: slonik
        AssignedTo: slony1-bugs at lists.slony.info
        ReportedBy: ssinger at ca.afilias.info
                CC: slony1-bugs at lists.slony.info
   Estimated Hours: 0.0


I have not verified that something else isn't going on at the same time to
contribute to this deadlock.


If I launch two slonik processes concurrently to subscribe nodes 2 and 3 to set
1 from origin 1.  I seem to sometimes get deadlock notification from Pg.

It is also possible that the slonik doing the subscribe set is deadlocking with
a slon set confiruration information.  Either way I wouldn't expect PG detected
deadlock situtations,  I thought that was the point of the configuration lock.





enThread_1: queue event 3,5000000003 STORE_PATH
2010-05-31 16:12:27,661 [db2 stdout] DEBUG
info.slony.clustertest.testcoordinator.slony.SlonLauncher db2 - 2010-05-31
16:12:27 EDTDEBUG2 remoteWorkerThread_3: Received event #3 from 5000000001
type:STORE_PATH
2010-05-31 16:12:27,665 [db2 stdout] DEBUG
info.slony.clustertest.testcoordinator.slony.SlonLauncher db2 - 2010-05-31
16:12:27 EDTDEBUG2 remoteWorkerThread_1: Received event #1 from 5000000013
type:SET_ADD_TABLE
2010-05-31 16:12:27,711 [slonik subscribe stderr] ERROR
info.slony.clustertest.testcoordinator.slony.SlonikScript slonik subscribe  -
<stdin>:13: PGRES_FATAL_ERROR select "_disorder_replica".subscribeSet(1, 1, 3,
't', 'f');  - ERROR:  deadlock detected
2010-05-31 16:12:27,711 [slonik subscribe stderr] ERROR
info.slony.clustertest.testcoordinator.slony.SlonikScript slonik subscribe  -
DETAIL:  Process 32107 waits for ExclusiveLock on relation 413672 of database
412926; blocked by process 32010.
2010-05-31 16:12:27,711 [slonik subscribe stderr] ERROR
info.slony.clustertest.testcoordinator.slony.SlonikScript slonik subscribe  -
Process 32010 waits for ExclusiveLock on relation 413672 of database 412926;
blocked by process 32107.
2010-05-31 16:12:27,711 [slonik subscribe stderr] ERROR
info.slony.clustertest.testcoordinator.slony.SlonikScript slonik subscribe  -
CONTEXT:  SQL statement "LOCK TABLE _disorder_replica.sl_event IN EXCLUSIVE
MODE; INSERT INTO _disorder_replica.sl_event (ev_origin, ev_seqno,
ev_timestamp, ev_snapshot, ev_type, ev_data1, ev_data2, ev_data3, ev_data4,
ev_data5, ev_data6, ev_data7, ev_data8) VALUES ('1',
nextval('_disorder_replica.sl_event_seq'), now(),
"pg_catalog".txid_current_snapshot(), $1, $2, $3, $4, $5, $6, $7, $8, $9);
SELECT currval('_disorder_replica.sl_event_seq');"
2010-05-31 16:12:27,711 [slonik subscribe stderr] ERROR
info.slony.clustertest.testcoordinator.slony.SlonikScript slonik subscribe  -
PL/pgSQL function "subscribeset" line 59 at assignment
2010-05-31 16:12:27,711 [slonik subscribe  stdout] DEBUG
info.slony.clustertest.testcoordinator.slony.SlonikScript slonik subscribe  -
<stdin>:12: subscribing to set
2010-05-31 16:12:27,713 [main] INFO 
info.slony.clustertest.testcoordinator.TestResult  - slonik exit
status:fail:255.0,0.0



This is with slony 2.0.4 RC1 running against postgresql 8.3.9

-- 
Configure bugmail: http://www.slony.info/bugzilla/userprefs.cgi?tab=email
------- You are receiving this mail because: -------
You are on the CC list for the bug.
You are the assignee for the bug.

From david at fetter.org  Sun May 23 06:55:59 2010
From: david at fetter.org (David Fetter)
Date: Sun, 23 May 2010 13:55:59 -0000
Subject: [Slony1-bugs] [Slony1-general] Slony triggers included in
	pg_dump
In-Reply-To: <4BF191B9.5000607@ca.afilias.info>
References: <AANLkTik3kuhrTf7xiy5ylZi-N4AprDvtBP831Il6oxhS@mail.gmail.com>
	<4BF002D6.3030805@ca.afilias.info>
	<AANLkTilq9QncnLfnHKwS3YEr3sELyzEHHTMFZrHqdN6b@mail.gmail.com>
	<4BF191B9.5000607@ca.afilias.info>
Message-ID: <20100523135558.GB23842@fetter.org>

On Mon, May 17, 2010 at 02:58:01PM -0400, Steve Singer wrote:
> Gurjeet Singh wrote:
> 
> > The doc-patch does make it more clear, but I don't think DROP
> > SCHEMA _slony_cluster_name CASCADE is something I'd like to do
> > with my running Slony replication. A cleaner way would be more
> > desirable.
> 
> Another option you have is to restore the dump that you create of
> your pgbench schema to another database(a new one).  When you
> restore the CREATE TRIGGER statements will fail because the slony
> stored procedures won't exist in your new database.   This should
> leave you with your database - any slony stuff.  (I agree having to
> put up with 'errors' during the restore or make a second copy of the
> database just to do 'DROP SCHEMA' are both less than ideal
> solutions)

Would it help if some future version of pg_dump had some option to the
effect of, "leave out the following objects and everything that
depends on them?"

Cheers,
David.
-- 
David Fetter <david at fetter.org> http://fetter.org/
Phone: +1 415 235 3778  AIM: dfetter666  Yahoo!: dfetter
Skype: davidfetter      XMPP: david.fetter at gmail.com
iCal: webcal://www.tripit.com/feed/ical/people/david74/tripit.ics

Remember to vote!
Consider donating to Postgres: http://www.postgresql.org/about/donate

