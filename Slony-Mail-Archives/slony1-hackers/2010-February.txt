From devrim at gunduz.org  Wed Feb  3 15:57:34 2010
From: devrim at gunduz.org (Devrim =?ISO-8859-1?Q?G=DCND=DCZ?=)
Date: Thu, 04 Feb 2010 01:57:34 +0200
Subject: [Slony1-hackers] Outage Notification: Feb 03, 2010
Message-ID: <1265241454.5156.30.camel@hp-laptop2.gunduz.org>


There will be an outage starting at 2010-02-03 at9pm PST. which will
last about 30 minutes.

To convert PST to your local time, run

date -d '2010-02-03 21:00 PST'

Affected services:

* Web
* Bugzilla
* Lists
* CVS

Unaffected services:

* None

Reason for Outage: 

* Command Prompt staff will replace one of the disks, to prevent
possible data loss.

We will attempt to minimize any impact. Thanks for your patience.

Contact Information:

Please join #slony in irc.freenode.net or send us an e-mail.

Regards,
-- 
Devrim G?ND?Z, RHCE
Command Prompt - http://www.CommandPrompt.com 
devrim~gunduz.org, devrim~PostgreSQL.org, devrim.gunduz~linux.org.tr
http://www.gunduz.org  Twitter: http://twitter.com/devrimgunduz
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 198 bytes
Desc: This is a digitally signed message part
Url : http://lists.slony.info/pipermail/slony1-hackers/attachments/20100204/3fb99ab7/attachment.pgp 

From lacey.powers at commandprompt.com  Thu Feb  4 00:54:24 2010
From: lacey.powers at commandprompt.com (Lacey Powers)
Date: Thu, 04 Feb 2010 00:54:24 -0800
Subject: [Slony1-hackers] Outage Notification: Feb 03, 2010
In-Reply-To: <1265241454.5156.30.camel@hp-laptop2.gunduz.org>
References: <1265241454.5156.30.camel@hp-laptop2.gunduz.org>
Message-ID: <4B6A8B40.6030804@commandprompt.com>

Devrim G?ND?Z wrote:
> There will be an outage starting at 2010-02-03 at9pm PST. which will
> last about 30 minutes.
>
> To convert PST to your local time, run
>
> date -d '2010-02-03 21:00 PST'
>
> Affected services:
>
> * Web
> * Bugzilla
> * Lists
> * CVS
>
> Unaffected services:
>
> * None
>
> Reason for Outage: 
>
> * Command Prompt staff will replace one of the disks, to prevent
> possible data loss.
>
> We will attempt to minimize any impact. Thanks for your patience.
>
> Contact Information:
>
> Please join #slony in irc.freenode.net or send us an e-mail.
>
> Regards,
>   

Hello Everyone,

The disk in the slony server was replaced successfully, though the 
rebuild took longer than expected.

Again, thank you for your patience during this urgent maintenance.

Lacey

-- 
Lacey Powers

The PostgreSQL Company - Command Prompt, Inc. 1.503.667.4564 ext 104
PostgreSQL Replication, Consulting, Custom Development, 24x7 support


From vivek at khera.org  Thu Feb  4 07:26:46 2010
From: vivek at khera.org (Vick Khera)
Date: Thu, 4 Feb 2010 10:26:46 -0500
Subject: [Slony1-hackers] [Slony1-general] Outage Notification: Feb 03,
	2010
In-Reply-To: <1265241454.5156.30.camel@hp-laptop2.gunduz.org>
References: <1265241454.5156.30.camel@hp-laptop2.gunduz.org>
Message-ID: <2968dfd61002040726k52802f4en410c77b89a4b9bd5@mail.gmail.com>

2010/2/3 Devrim G?ND?Z <devrim at gunduz.org>:
> To convert PST to your local time, run
>
> date -d '2010-02-03 21:00 PST'
>

All the world's not linux.  Your command works not on FreeBSD nor MacOS 10.6

Please either post in UTC or link to a world time calculator like the
apache folk tend to do.

Thanks.

From analogue at glop.org  Tue Feb 23 00:50:34 2010
From: analogue at glop.org (Laurent Raufaste)
Date: Tue, 23 Feb 2010 09:50:34 +0100
Subject: [Slony1-hackers] Cleaning the sl_confirm table
Message-ID: <669dc9711002230050u4a19c0e8hf7a9ab2eb9396333@mail.gmail.com>

Hi,

I have a cool Slony cluster with 12 PGs running for years.
But I noticed some strange data in the origin's sl_confirm table:

SELECT *
FROM _ob2replication.sl_confirm
WHERE con_seqno NOT IN (SELECT ev_seqno FROM _ob2replication.sl_event);

 con_origin | con_received | con_seqno |       con_timestamp
------------+--------------+-----------+----------------------------
        21 |           26 |   3277845 | 2009-09-26 15:01:12.61598
        21 |           27 |   3277845 | 2009-10-22 15:13:26.370957
        21 |           25 |   3277845 | 2009-09-26 14:54:16.162632
        21 |            5 |   3277845 | 2009-10-22 15:13:26.669225

con_timestamp should be some date near today (february 2010)

Or from another server:

 con_origin | con_received | con_seqno |       con_timestamp
------------+--------------+-----------+----------------------------
        21 |           26 |   3277845 | 2009-09-26 15:01:12.61598
        21 |           27 |   3277845 | 2009-09-26 15:01:12.549303
        21 |           22 |   3277845 | 2009-09-26 15:01:12.627592
        21 |            5 |   3277845 | 2009-09-26 15:01:12.831765
        21 |           25 |   3277845 | 2009-09-26 14:54:16.162632

And from another:

 con_origin | con_received | con_seqno |       con_timestamp
------------+--------------+-----------+----------------------------
        21 |           26 |   3277845 | 2009-09-26 15:01:12.61598
        21 |            5 |   3277845 | 2009-09-26 15:01:12.831765
        21 |           27 |   3277845 | 2009-09-26 15:01:12.549303
        21 |           25 |   3277845 | 2009-09-26 14:54:16.162632
        21 |           22 |   3277845 | 2009-09-26 15:01:12.627592
        17 |           25 |         0 | 2010-01-26 11:18:22.45564
        17 |           26 |         0 | 2010-01-26 11:18:22.455672
        17 |           27 |         0 | 2010-01-26 11:18:22.455704
        17 |           22 |         0 | 2010-01-26 11:18:22.455729
        17 |            5 |         0 | 2010-01-26 11:18:22.455753
        17 |           21 |         0 | 2010-01-26 11:18:22.455794
        17 |           12 |         0 | 2010-01-26 11:18:22.455835
        17 |           16 |         0 | 2010-01-26 11:18:22.455861
        17 |            2 |         0 | 2010-01-26 11:18:22.455898
        17 |           24 |         0 | 2010-01-26 11:18:22.455926

On every server I have entries in the sl_confirm tables with events
references which do not exist anymore.

The cluster works, but I don't like thoses entries staying there. What
can I do ? Delete them ?

Those entries should be cleaned up but the cleanupEvent() plpgsql
function, but they are not.

I launched a debugged version of the cleanupEvent() function and here
what I got:
SELECT _ob2replication.mycleanup();

NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND con_received=2
AND con_seqno < 1060869
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND con_received=5
AND con_seqno < 1060869
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=12 AND con_seqno < 1060869
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=16 AND con_seqno < 1060869
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=17 AND con_seqno < 1060869
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=22 AND con_seqno < 3277845
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=23 AND con_seqno < 1060928
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=24 AND con_seqno < 1060869
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=25 AND con_seqno < 3277845
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=26 AND con_seqno < 3277845
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=27 AND con_seqno < 3277845


From bogus@does.not.exist.com  Thu Feb  4 01:26:20 2010
From: bogus@does.not.exist.com ()
Date: Thu, 04 Feb 2010 09:26:20 -0000
Subject: No subject
Message-ID: <mailman.1.1266915038.4644.slony1-hackers@lists.slony.info>

in sl_confirm with a seqno ~3277845 are preventing cleanupEvent() to
clean those.

And if I take a look at the seqno range on the master I get:

SELECT con_origin,
 MIN(con_seqno) AS first_con_seqno,
 MAX(con_seqno) AS last_con_seqno,
 MAX(con_seqno) - MIN(con_seqno) AS delta
FROM _ob2replication.sl_confirm
GROUP BY con_origin
ORDER BY con_origin ASC;

 con_origin | first_con_seqno | last_con_seqno |  delta
------------+-----------------+----------------+---------
         2 |         1850804 |        1850903 |      99
         5 |         1437305 |        1437403 |      98
        12 |          992789 |         992888 |      99
        16 |          957046 |         957145 |      99
        17 |          230040 |         230139 |      99
        21 |         1060965 |        3277845 | 2216880
        22 |         4790482 |        4790906 |     424
        23 |         1050820 |        1050919 |      99
        24 |           99661 |          99694 |      33
        25 |         1858636 |        1858734 |      98
        26 |         2629674 |        2629773 |      99
        27 |         1788901 |        1789000 |      99
(12 rows)

As you can see, the node 21 is keeping a huge range of confirmations
while the usual is to keep less than a thousand.

My supposition: Last year in september, there was a replication
maintenance, it is possible we might have to reinstall slony on the
node 21. If this was the case, the seqno was reset. But not every
event on the cluster was cleaned, and some confirm lines in sl_confirm
stayed there.

As every one of those confirm lines in sl_confirm in the cluster refer
to unexistent sl_event, can I delete those sl_confirm lines without fearing
any replication problem ?

Thanks again for the help =)

-- 
Laurent Raufaste
<http://www.glop.org/>

From analogue at glop.org  Tue Feb 23 00:53:41 2010
From: analogue at glop.org (Laurent Raufaste)
Date: Tue, 23 Feb 2010 09:53:41 +0100
Subject: [Slony1-hackers] Cleaning the sl_confirm table
In-Reply-To: <669dc9711002230050u4a19c0e8hf7a9ab2eb9396333@mail.gmail.com>
References: <669dc9711002230050u4a19c0e8hf7a9ab2eb9396333@mail.gmail.com>
Message-ID: <669dc9711002230053re4ab037rb9bfbad08243288@mail.gmail.com>

My mail got cut =(

2010/2/23 Laurent Raufaste <analogue at glop.org>:
> Hi,
>
> I have a cool Slony cluster with 12 PGs running for years.
> But I noticed some strange data in the origin's sl_confirm table:

Here's the end:


From bogus@does.not.exist.com  Thu Feb  4 01:26:20 2010
From: bogus@does.not.exist.com ()
Date: Thu, 04 Feb 2010 09:26:20 -0000
Subject: No subject
Message-ID: <mailman.2.1266915224.4644.slony1-hackers@lists.slony.info>

in sl_confirm with a seqno ~3277845 are preventing cleanupEvent() to
clean those.

And if I take a look at the seqno range on the master I get:

SELECT con_origin,
 MIN(con_seqno) AS first_con_seqno,
 MAX(con_seqno) AS last_con_seqno,
 MAX(con_seqno) - MIN(con_seqno) AS delta
FROM _ob2replication.sl_confirm
GROUP BY con_origin
ORDER BY con_origin ASC;

 con_origin | first_con_seqno | last_con_seqno |  delta
------------+-----------------+----------------+---------
         2 |         1850804 |        1850903 |      99
         5 |         1437305 |        1437403 |      98
        12 |          992789 |         992888 |      99
        16 |          957046 |         957145 |      99
        17 |          230040 |         230139 |      99
        21 |         1060965 |        3277845 | 2216880
        22 |         4790482 |        4790906 |     424
        23 |         1050820 |        1050919 |      99
        24 |           99661 |          99694 |      33
        25 |         1858636 |        1858734 |      98
        26 |         2629674 |        2629773 |      99
        27 |         1788901 |        1789000 |      99
(12 rows)

As you can see, the node 21 is keeping a huge range of confirmations
while the usual is to keep less than a thousand.

My supposition: Last year in september, there was a replication
maintenance, it is possible we might have to reinstall slony on the
node 21. If this was the case, the seqno was reset. But not every
event on the cluster was cleaned, and some confirm lines in sl_confirm
stayed there.

As every one of those confirm lines in sl_confirm in the cluster refer
to unexistent sl_event, can I delete those sl_confirm lines without fearing
any replication problem ?

Thanks again for the help =)

-- 
Laurent Raufaste
<http://www.glop.org/>

From analogue at glop.org  Tue Feb 23 00:55:51 2010
From: analogue at glop.org (Laurent Raufaste)
Date: Tue, 23 Feb 2010 09:55:51 +0100
Subject: [Slony1-hackers] Cleaning the sl_confirm table
In-Reply-To: <669dc9711002230053re4ab037rb9bfbad08243288@mail.gmail.com>
References: <669dc9711002230050u4a19c0e8hf7a9ab2eb9396333@mail.gmail.com>
	<669dc9711002230053re4ab037rb9bfbad08243288@mail.gmail.com>
Message-ID: <669dc9711002230055n7fdbf2d5v48fe04d6aa49d8a6@mail.gmail.com>

I'm sorry for the spam, but it seems you ML manager does'nt like lines
starting with "From ", it cuts the mail body just before it.

Here(s my full mail this time:

Hi,

I have a cool Slony cluster with 12 PGs running for years.
But I noticed some strange data in the origin's sl_confirm table:

SELECT *
FROM _ob2replication.sl_confirm
WHERE con_seqno NOT IN (SELECT ev_seqno FROM _ob2replication.sl_event);

 con_origin | con_received | con_seqno |       con_timestamp
------------+--------------+-----------+----------------------------
        21 |           26 |   3277845 | 2009-09-26 15:01:12.61598
        21 |           27 |   3277845 | 2009-10-22 15:13:26.370957
        21 |           25 |   3277845 | 2009-09-26 14:54:16.162632
        21 |            5 |   3277845 | 2009-10-22 15:13:26.669225

con_timestamp should be some date near today (february 2010)

Or from another server:

 con_origin | con_received | con_seqno |       con_timestamp
------------+--------------+-----------+----------------------------
        21 |           26 |   3277845 | 2009-09-26 15:01:12.61598
        21 |           27 |   3277845 | 2009-09-26 15:01:12.549303
        21 |           22 |   3277845 | 2009-09-26 15:01:12.627592
        21 |            5 |   3277845 | 2009-09-26 15:01:12.831765
        21 |           25 |   3277845 | 2009-09-26 14:54:16.162632

And from another:

 con_origin | con_received | con_seqno |       con_timestamp
------------+--------------+-----------+----------------------------
        21 |           26 |   3277845 | 2009-09-26 15:01:12.61598
        21 |            5 |   3277845 | 2009-09-26 15:01:12.831765
        21 |           27 |   3277845 | 2009-09-26 15:01:12.549303
        21 |           25 |   3277845 | 2009-09-26 14:54:16.162632
        21 |           22 |   3277845 | 2009-09-26 15:01:12.627592
        17 |           25 |         0 | 2010-01-26 11:18:22.45564
        17 |           26 |         0 | 2010-01-26 11:18:22.455672
        17 |           27 |         0 | 2010-01-26 11:18:22.455704
        17 |           22 |         0 | 2010-01-26 11:18:22.455729
        17 |            5 |         0 | 2010-01-26 11:18:22.455753
        17 |           21 |         0 | 2010-01-26 11:18:22.455794
        17 |           12 |         0 | 2010-01-26 11:18:22.455835
        17 |           16 |         0 | 2010-01-26 11:18:22.455861
        17 |            2 |         0 | 2010-01-26 11:18:22.455898
        17 |           24 |         0 | 2010-01-26 11:18:22.455926

On every server I have entries in the sl_confirm tables with events
references which do not exist anymore.

The cluster works, but I don't like thoses entries staying there. What
can I do ? Delete them ?

Those entries should be cleaned up but the cleanupEvent() plpgsql
function, but they are not.

I launched a debugged version of the cleanupEvent() function and here
what I got:
SELECT _ob2replication.mycleanup();

NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND con_received=2
AND con_seqno < 1060869
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND con_received=5
AND con_seqno < 1060869
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=12 AND con_seqno < 1060869
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=16 AND con_seqno < 1060869
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=17 AND con_seqno < 1060869
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=22 AND con_seqno < 3277845
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=23 AND con_seqno < 1060928
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=24 AND con_seqno < 1060869
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=25 AND con_seqno < 3277845
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=26 AND con_seqno < 3277845
NOTICE:  DELETE FROM sl_confirm WHERE con_origin=21 AND
con_received=27 AND con_seqno < 3277845

F*r*o*m origin 21, the legitimate seqno is ~1060869. But the bad entries
in sl_confirm with a seqno ~3277845 are preventing cleanupEvent() to
clean those.

And if I take a look at the seqno range on the master I get:

SELECT con_origin,
 MIN(con_seqno) AS first_con_seqno,
 MAX(con_seqno) AS last_con_seqno,
 MAX(con_seqno) - MIN(con_seqno) AS delta
FROM _ob2replication.sl_confirm
GROUP BY con_origin
ORDER BY con_origin ASC;

 con_origin | first_con_seqno | last_con_seqno |  delta
------------+-----------------+----------------+---------
         2 |         1850804 |        1850903 |      99
         5 |         1437305 |        1437403 |      98
        12 |          992789 |         992888 |      99
        16 |          957046 |         957145 |      99
        17 |          230040 |         230139 |      99
        21 |         1060965 |        3277845 | 2216880
        22 |         4790482 |        4790906 |     424
        23 |         1050820 |        1050919 |      99
        24 |           99661 |          99694 |      33
        25 |         1858636 |        1858734 |      98
        26 |         2629674 |        2629773 |      99
        27 |         1788901 |        1789000 |      99
(12 rows)

As you can see, the node 21 is keeping a huge range of confirmations
while the usual is to keep less than a thousand.

My supposition: Last year in september, there was a replication
maintenance, it is possible we might have to reinstall slony on the
node 21. If this was the case, the seqno was reset. But not every
event on the cluster was cleaned, and some confirm lines in sl_confirm
stayed there.

As every one of those confirm lines in sl_confirm in the cluster refer
to unexistent sl_event, can I delete those sl_confirm lines without fearing
any replication problem ?

Thanks again for the help =)
-- 
Laurent Raufaste
<http://www.glop.org/>

From analogue at glop.org  Fri Feb 26 01:53:27 2010
From: analogue at glop.org (Laurent Raufaste)
Date: Fri, 26 Feb 2010 10:53:27 +0100
Subject: [Slony1-hackers] Cleaning the sl_confirm table
In-Reply-To: <669dc9711002230055n7fdbf2d5v48fe04d6aa49d8a6@mail.gmail.com>
References: <669dc9711002230050u4a19c0e8hf7a9ab2eb9396333@mail.gmail.com>
	<669dc9711002230053re4ab037rb9bfbad08243288@mail.gmail.com>
	<669dc9711002230055n7fdbf2d5v48fe04d6aa49d8a6@mail.gmail.com>
Message-ID: <669dc9711002260153kf6460c8kde90cd9850dacdc1@mail.gmail.com>

2010/2/23 Laurent Raufaste <analogue at glop.org>:
>
> As every one of those confirm lines in sl_confirm in the cluster refer
> to unexistent sl_event, can I delete those sl_confirm lines without fearing
> any replication problem ?
>

I deleted the faulty lines in sl_confirm and nothing wrong happened.
My Slony cluster is now as shiny as it was when it was created ! =)

-- 
Laurent Raufaste
<http://www.glop.org/>

From nagaraj.shindagi at gmail.com  Sun Feb 28 03:41:43 2010
From: nagaraj.shindagi at gmail.com (nagaraj.shindagi at gmail.com)
Date: Sun, 28 Feb 2010 03:41:43 -0800 (PST)
Subject: [Slony1-hackers] Spice up your mobile
Message-ID: <527442560.1267357304765.JavaMail.SYSTEM@smtp.gmail.com>

An HTML attachment was scrubbed...
URL: http://lists.slony.info/pipermail/slony1-hackers/attachments/20100228/45bdee41/attachment.htm 

